\title{}
\author{}
\jkeywords{}

\etitle{A Comparative Study of Automatic Extraction\\
of Collocations from Corpora:\\
Mutual Information vs. Cost Criteria}

\eauthor{Kenji Kita\affiref{TokushimaUniv}   \and
        Yasuhiko Kato\affiref{Kokken}        \and
        Takashi Omoto\affiref{TokushimaUniv} \and
        Yoneo Yano\affiref{TokushimaUniv}}

\headauthor{Kita,~K.~et~al.}
\headtitle{A Comparative Study of Automatic Extraction of Collocations}

\affilabel{TokushimaUniv}
          {Tokushima University, Faculty of Engineering}
          {Tokushima University, Faculty of Engineering}
\affilabel{Kokken}
          {The National Language Research Institute, Section for Dictionary Research}
          {The National Language Research Institute, Section for Dictionary Research}


\eabstract{
While corpus-based studies are now becoming a new methodology
in natural language processing,
second language learning offers one interesting potential application.
In this paper, we are primarily concerned with the acquisition
of collocational knowledge from corpora
for use in language learning.
First we discuss the importance of collocational knowledge
in second language learning,
and then take up two measures, mutual information and cost criteria,
for automatically identifying or extracting collocations from corpora.
Comparative experiments are made between the two measures
using both Japanese and English corpora.
In our experiments,
the cost criteria measure proved more effective in extracting interesting collocations
such as fundamental idiomatic expressions and phrases.
}

\ekeywords{Corpus, Collocations, Mutual Information,
Cost Criteria, Second Language Learning}


\begin{document}

\maketitle

\section{Introduction}

Recent rapid advances in computer technology,
especially the advent of large storage devices and parallel computers,
and numerous data collection efforts
have caused a shift in natural language applications
from a knowledge-based to a corpus-based or data-intensive approach.
The knowledge-based approach focused on abstraction of language,
describing linguistic phenomena through minimal core knowledge
such as parts-of-speech, syntactic and semantic rules.
Linguistic phenomena, however, vary so vastly that
they cannot be described through core knowledge.
In addition, hand-coding knowledge takes a lot of time and hard work.
The knowledge-based approach, therefore, has been found wanting
in developing large-scale practical NLP systems.

On the other hand,
the corpus-based approach makes no claim about the compactness
of the knowledge.
Rather, the corpus-based approach derives more power
from massive quantities of textual data than from hand-coded knowledge,
being able to compensate for the weakness of the knowledge-based approach
through authentic examples and various statistics of language use.
With the availability of large corpora in recent years,
many successful results have been derived from corpus-based studies.
These include part-of-speech tagging @xcite,
parsing @xcite,
example-based machine translation@xcite,
statistical machine translation @xcite,
language modeling @xcite
and many other related areas.

One interesting potential use of corpora
is for second language learning.
Kita et al. @xcite discussed various way of using corpora
in language learning,
which includes translation skill acquisition through bilingual corpora,
oral/aural skill acquisition through speech corpora,
multimedia language learning through structured corpora, and so forth.
The greatest advantage of using corpora in language learning is that
the corpora provide a body of evidence for the function and usage of words
and expressions.
At the same time,
deriving lexical knowledge
from large-scale corpora via automated procedures,
as well as its use in language learning CAI systems,
is one of the most important issues.

In this paper, we are primarily concerned with the acquisition
of collocational knowledge from corpora.
In Section~2, we describe why collocational knowledge is
important in second language learning.
In Section~3, we discuss the automatic extraction of collocations,
taking up two measures, mutual information and cost criteria,
for identifying or extracting collocations from corpora.
In Section~4, we describe comparative experiments in extracting collocations
and discuss the two measures.


\section{Importance of Collocational Knowledge in Language Learning}

There has been much theoretical and applied research
on collocations,
both from a linguistic and an engineering point of view.
Consequently, the definition of collocation differs according to
the researcher's interest and standpoint.
This paper adopts the most comprehensive definition:
a collocation is a cohesive word cluster,
including idioms, frozen expressions and compound words.

The importance of collocations has been stressed
in an extensive literature.
From a language learning viewpoint, it can be summarized as follows:

\begin{itemize}
\item
In language learning, learners must pay attention to
how words are used rather than to individual words by themselves.
Collocational knowledge indicates
which words co-occur frequently with other words and
how they combine within a sentence.
Therefore, collocational knowledge is especially effective
in sentence generation
@xcite.

\item
Collocational knowledge is very difficult
to acquire for second language learners.
A typical example is the pair of words ``strong'' and ``powerful''
@xcite.
These two words have similar meanings,
but their usage is quite different.
For example, native English speakers prefer saying
``strong tea'' to ``powerful tea'',
and prefer saying ``powerful car'' to ``strong car''.
For non-natives, however,
it is difficult to catch the subtle distinctions between these two words.
These lexical preferences were sometimes ignored
in the traditional knowledge-based approach;
nevertheless they are the most important source
for word choice and word ordering.

\item
It is pointed out that human translation process
is based on analogical thinking @xcite.
First, a human translator properly decomposes a sentence
into certain fragmental phrases, then s/he translates
each fragmental phrase by analogy with other examples,
and finally composes fragmental translations into one sentence.
Recently, following this idea,
example-based machine translations (EBMT) have been widely explored.
In some EBMT systems, several kinds of translation knowledge
are utilized, such as the string-, pattern-, and grammar-level
knowledge types.
Collocations are particularly suitable for translation units
in either the string- or pattern-level knowledge.


\item
From a cognitive point of view,
it is said that human language acquisition is governed by
the law of maximal efficiency @xcite.
In other words, data compression, often called chunking, is performed to
minimize storage demands in the brain.
A chunk is considered to be a pattern which repeatedly appears
in a variety of contexts.
Collocations are good candidates for chunk units.
\end{itemize}


\section{Extracting Collocations from Corpora}

In the past, several approaches have been proposed to
extract collocations from corpora.
Church et al. @xcite introduced the association ratio,
which indicates how strongly two words are related,
based on the information-theoretic concept of mutual information.
Smadja et al. @xcite take into account word distance
as well as word strength for a measure of word association.
Also, Basili et al. @xcite proposed a syntax-based approach.
Particularly, mutual information plays a central role
in recent lexical statistical research.
To take a few examples,
Hindle and Rooth @xcite applied mutual information to disambiguate
prepositional phrase attachments,
and Brown et al. @xcite used it in determining word classes.

In this section, after surveying how mutual information
can be used to extract collocational information,
we introduce another measure, called {\em cost criteria} @xcite,
to automatically extract interesting collocations
from corpora.
Comparative experiments and discussions will be described
in the next section.

\subsection{Mutual Information}

The mutual information between two words @xmath0 and @xmath1 is defined
as follows @xcite:
\begin{equation}
  I(x,y) = \log \frac{P(x,y)}{P(x)P(y)}
\end{equation}
Here, @xmath2 and @xmath3 are word occurrence probabilities,
and can be estimated from the number of occurrences of the words, @xmath4 and @xmath5,
and the number of words in the corpus, @xmath6.
\begin{equation}
  P(x) = \frac{f(x)}{N} \,\,\,\,\, \mbox{and} \,\,\,\,\, P(y) = \frac{f(y)}{N}
\end{equation}
@xmath7, the joint probability of @xmath8 and @xmath9, is estimated in a similar way.
\begin{equation}
  P(x,y) = \frac{f(x,y)}{N}
\end{equation}
where @xmath10 is the number of occurrences of
@xmath11 followed by @xmath12.

The mutual information @xmath13 compares the probability
of observing @xmath14 and @xmath15 together
with the probabilities of observing @xmath16 and @xmath17 simply by chance.
Thus, a large value indicates that the two words @xmath18 and @xmath19
have a strong relationship.
By extracting word pairs with large mutual information values,
we can obtain common collocations.

Because mutual information values are defined for two words,
this simple method can only extract collocations of length two.
However, a generalization is suggested in @xcite as follows:
\begin{enumerate}
\item   Start out from the basic vocabulary @xmath20. Set @xmath21.
\item   Augment the vocabulary @xmath22 by all word sequences ``@xmath23 @xmath24''
        for which @xmath25, where @xmath26 is a predetermined threshold.
\item   From Step~2, a new vocabulary @xmath27 is established.
\item   Adjust the vocabulary size @xmath28 to reflect the new vocabulary @xmath29.
\item   Resume from Step~1 with @xmath30 as its basis.
\end{enumerate}
With this iterative procedure, the final vocabulary includes
collocations of arbitrary length.

\subsection{Cost Criteria}

The cost criteria measure is based on the assumptions that
(1) collocations are recurrent word sequences,
and (2) the recurrent property is captured by
the absolute frequency of a word sequence.
However, a simple absolute frequency approach does not work,
because the frequency of a sub-sequence is always higher
than that of the original word sequence.
Therefore, as a measure for identifying or extracting collocations,
absolute frequency is not appropriate.
Instead, we consider a processing cost for a word sequence,
and introduce cost criteria 
which can quantitatively estimate
the extent to which processing is reduced
by considering a word sequence as one unit.

Before the presentation of a formal definition,
we introduce the following notation:
\begin{eqnarray}
        \alpha    & \cdots & \mbox{a word sequence.}\\
        |\alpha|  & \cdots & \mbox{the length of @xmath31.}\\
                  &        & \mbox{(the number of words in @xmath32)} \nonumber \\
        f(\alpha) & \cdots & \mbox{number of occurrences of @xmath33 in a corpus.}
\end{eqnarray}

We define a {\em reduced cost} @xmath34 for a sequence @xmath35 as:
\begin{equation}
        K(\alpha) = (|\alpha| - 1) \times f(\alpha)
\end{equation}
@xmath36 is interpreted as follows.
Assume here that, in the corpus, there exists a word sequence @xmath37,
which is composed of @xmath38 words and occurs @xmath39 times.
Also assume that the cost of processing one word is 1.
Similarly, when processing @xmath40 as a single unit,
its processing cost is 1.
Here, we assume that:
if a word sequence is processed one word at a time,
its processing cost is proportional to its length.
This assumption is made because of the following reason.
We are now concerned with the problem of identifying the sequence @xmath41
as a collocation.
But since this problem is reduced to a string pattern matching problem,
it is natural to assume that
the cost is proportional to the length of the sequence @xmath42.
That is, the processing cost for @xmath43 is @xmath44.
By considering @xmath45 as one unit, the processing cost is reduced
to @xmath46.
Since @xmath47 appears @xmath48 times,
we can conclude that the total reduced cost becomes
@xmath49, which is the definition of @xmath50.

In reality, however, the problem is not so simple,
because word sequences are not mutually disjoint.
Consider the case where a word sequence @xmath51 is a sub-sequence of @xmath52
(for example, @xmath53, @xmath54).
Then, we have:
\begin{equation}
        f(\alpha) \geq f(\beta)
\end{equation}
Further, the word sequence @xmath55,
@xmath56 times out of @xmath57 times, will be identified as @xmath58.
Thus, the actual reduced cost for @xmath59 is defined as:
\begin{equation}
        K(\alpha) = (|\alpha| - 1) \times (f(\alpha) - f(\beta))

\end{equation}

Finally, we can extract collocations from a corpus by the following steps:
\begin{enumerate}
\item   Calculate @xmath60 for each word sequence @xmath61 in a corpus.
\item   Rank a word sequence @xmath62 by using the value @xmath63.
\item   Extract higher rank word sequences as collocation candidates.
\item   Re-calculate @xmath64 for each @xmath65 in the collocation candidates.
To be more precise,
by checking the sequence/sub-sequence relation
between every two word sequences in the collocation cancidates,
modify the @xmath66 values according to Equation~[Eq:Kmod].
\end{enumerate}


\section{Experiments and Discussions}

\subsection{The ADD Corpus}

In our experiments, the ADD (ATR Dialogue Database) Corpus @xcite
created by ATR Interpreting Telephony Research Laboratories in Japan
was used.
The ADD Corpus is a large structured database of dialogues collected
from simulated telephone or keyboard conversations
which are spontaneously spoken or typed in Japanese or English.
This corpus consists of parallel texts of Japanese and English,
aligned by utterance.
Also, sentences in ADD are morphologically analyzed
and annotated with various kinds of syntactic, semantic, and phonological information.

Currently, the ADD Corpus contains textual data from two tasks (text categories);
one consists of simulated dialogues between a secretary and participants
at international conferences (Conference Task),
and the other of simulated dialogues between travel agents and customers (Travel Task).

In our experiments,
we used the keyboard dialogues from the Travel Task,
which include approximately 120,000 Japanese words
and 100,000 English words.
The telephone dialogue include linguistic phenomena,
such as filled pauses (``ah'', ``uh'', etc.),
restarts (repeating a word or phrase) and interjections,
so we did not use them for our experiments.
The aim in our research is to extract linguistically
neat expressions,
thus we excluded telephone dialogues from the experiments.


\subsection{Results and Discussions}

Table~[Tab:CollocationJapanese]
shows some interesting Japanese collocations extracted
using respectively mutual information and cost criteria.
Table~[Tab:CollocationEnglish] shows some English ones.
In these tables, collocations are listed in descending order
with respect to their values
(i.e. mutual information or cost reduction values).







In the experiments,
we examined word sequences up to a 10 word length.
When using mutual information,
the threshold was set to 0.0.
This means that we calculated mutual information values
for all the word sequences which appeared in the corpus.
When using cost criteria,
we generated approximately 10,000 collocation candidates.
The collocations listed in the tables were selected manually
from the higher ranked ones.

Before discussing the results,
we first overview the characteristics of Japanese phrases.
In general, the order of major constituents in a Japanese sentence is rather free.
However, predicate phrase positioning is dominated by
the so-called predicate-phrase ending constraint:
a predicate phrase appears at the end of its clause.
Furthermore, a predicate phrase often has a complex form,
consisting of a main predicate such as a verbal noun, verb or adverb,
combinations of auxiliary predicates, and a sentence-final particle.
These auxiliary predicates and sentence-final particles
add various complementary meanings to a sentence,
such as honorific, causative,
and prohibitive meanings, etc.

As can be seen from the experimental results
(Table~[Tab:CollocationJapanese]),
the method based on mutual information tends to
extract task-dependent compound noun phrases,
while cost criteria tends to extract complex predicate phrase patterns.
Almost all the collocations extracted are in this category.
For example,
the collocations ``desho u ka'' and ``desu ka'',
which had a high cost reduction,
are used very often to make interrogative sentences in Japanese.
The collocation ``tai no desu ga'' is usually used to
express a speaker's request,
whose meaning is ``(I) would like to''.

The comments above are also true of the English data.
Mutual information tends to extract compound noun phrases,
while cost criteria tends to extract frozen phrase patterns
such as ``thank you very much'' and ``I would like to''.

Why does mutual information fail to extract these patterns?
Here, let us take ``I will'' as an illustrative example,
which has been picked out by cost criteria
(``I will'' is omitted from Table~[Tab:CollocationEnglish])
but not by mutual information.
In our corpus,
``I'' occurs 2,907 times, ``will'' occurs 920 times,
and ``I  will'' occurs 264 times.
Therefore, we have
\begin{eqnarray}
  I(\mbox{I},\mbox{will}) & = & \log \frac{\frac{264}{100,000}}{\frac{2907}{100,000} \frac{920}{100,000}} \nonumber \\
        & = & 3.3
\end{eqnarray}
This value is not so large,
so the two words ``I'' and ``will'' cannot be considered to
have a significant relationship.

According to the same reasoning,
patterns such as ``I would like to'' and ``thank you very much''
are excluded as collocation candidates.
However, in the ADD Corpus,
more than fifty per cent of the sentences that involve the word ``would''
are subsumed under the pattern ``(I) would like to @xmath67''.
Therefore, this pattern should be included in the collocation list.

Another drawback using mutual information is the sparseness of data.
A corpus cannot provide sufficient data about every word-word relationship.
Some word pairs may have high mutual information values
in spite of their low frequency in the corpus.
For example,
the first ranked collocation was ``yacht harbor'',
which occurs only twice in the ADD Corpus.
On the contrary, since the cost criteria measure is based on absolute frequency,
such phenomena never happens.

Now, let us consider why mutual information
tends to extract task-dependent compound nouns.
As noted above, mutual information values become unstable
for low-frequency words.
Consequently, even if the incidence of a word pair occuring is low,
it is quite possible that the mutual information value becomes greater.
These low-frequency words, however, often depend on the topic of the text.
This is one reason
why mutual information tends to extract task-dependent compound nouns.


\section{Conclusion}

With the growing availability of large textual resources,
corpus-based studies are gaining more and more attention among computational linguists
and computer scientists.
In particular, automatic acquisition of lexical knowledge from corpora
is one of the most important and interesting issues.
In this paper, we have taken up the problem of how to acquire collocational knowledge
and discussed its importance for language learning.
We have also described comparative experiments
using mutual information and cost criteria.
Our experiments demonstrated that
mutual information tends to
extract task-dependent compound noun phrases,
while cost criteria tends to extract predicate phrase patterns.

Unfortunately,
the current implementation can only extract collocations
of uninterrupted word sequences.
Our next plan is to refine the method to extract collocations
of interrupted sequences,
and to utilize lexical information such as parts-of-speech
in order to prevent an improper word sequence
from being recognized as a collocation.
For example, when applying cost criteria to English corpora,
unwelcome word pairs, such as ``on the'', ``for the'' or ``of the'',
are extracted
because such pairs occur frequently in the language.
However, these word pairs can be excluded by using
parts-of-speech information.
Also, we hope to incorporate extracted collocations
into a language learning CAI system.

\vspace*{2mm}

\end{document}