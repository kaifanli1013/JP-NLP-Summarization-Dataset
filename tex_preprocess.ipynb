{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "class TexProcesser():\n",
    "    def __init__(self, latex_dir_path, output_path):\n",
    "        self.latex_folder = Path(latex_dir_path)\n",
    "        self.output_path = output_path\n",
    "        \n",
    "    def pipeline(self,):\n",
    "        self.processed_tex_dir = self.output_path + '/' + 'processed_tex'\n",
    "        self._preprocess(input_dir=self.latex_folder, output_dir=self.processed_tex_dir)\n",
    "        \n",
    "        output_csv_path = self.output_path + '/latex_info.csv'\n",
    "        self._extract_field_to_csv(output_csv_path=output_csv_path)\n",
    "        \n",
    "        # TODO:把文本批量转换为md文件\n",
    "        self._postprocess()\n",
    "        \n",
    "    def _preprocess(self, input_dir, output_dir):\n",
    "        '''\n",
    "        删除一些额外的表格和公式等等\n",
    "        '''\n",
    "\n",
    "        for file_path in sorted(input_dir.glob('**/*.tex')):\n",
    "        # latex = './NLP_LATEX_CORPUS/V04/V04N01-07.tex'\n",
    "            file_name = os.path.basename(file_path)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    latex_content = file.read()\n",
    "                print(f'processing {file_name} ...')\n",
    "                # 执行其他操作\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"UnicodeDecodeError: Failed to decode {file_path}. Skipping this file.\")\n",
    "                continue  # 跳过当前文件的处理\n",
    "            \n",
    "            # 删除\\begin{document}之前的所有内容\n",
    "            latex_content = re.sub(r'[\\s\\S]*?(?=\\\\begin\\{document\\})', '', latex_content, count=1, flags=re.DOTALL)\n",
    "                    \n",
    "            # 删除bibliographystyle之后的所有文本\n",
    "            # latex_content = re.sub(r'\\\\bibliographystyle\\{.*?\\}[\\s\\S]*', '', latex_content, flags=re.DOTALL)\n",
    "            \n",
    "            # 删除所有的图片\n",
    "            latex_content = re.sub(r'\\\\begin{figure}.*?\\\\end{figure}', '', latex_content, flags=re.DOTALL)\n",
    "            \n",
    "            # 删除所有的表格\n",
    "            latex_content = re.sub(r'\\\\begin{table}.*?\\\\end{table}', '', latex_content, flags=re.DOTALL)\n",
    "            latex_content = re.sub(r'\\\\begin{tabular}.*?\\\\end{tabular}', '', latex_content, flags=re.DOTALL)\n",
    "            \n",
    "            # 替换引用\\ref{...}文本为[...]\n",
    "            latex_content = re.sub(r'\\\\ref\\{([^{}]*)\\}', r'[\\1]', latex_content, flags=re.DOTALL)\n",
    "            \n",
    "            # 删除格式调整符号\n",
    "            # latex_content = re.sub(r'\\\\hspace\\*\\{[^{}]*\\}', '', latex_content, flags=re.DOTALL)\n",
    "            # latex_content = re.sub(r'\\\\vspace\\*\\{[^{}]*\\}', '', latex_content, flags=re.DOTALL)        \n",
    "        \n",
    "            # 删除所有的标签\\label{...}\n",
    "            latex_content = re.sub(r'\\\\label\\{.*?\\}', '', latex_content, flags=re.DOTALL)\n",
    "            \n",
    "            # 替换掉文本中有\\underline{...}的内容            \n",
    "            # latex_content = re.sub(r'\\\\underline\\{([^{}]*)\\}', r'\\1', latex_content, flags=re.DOTALL)            \n",
    "                                    \n",
    "            # 替换掉所有文献引用为@xcite\n",
    "            latex_content = re.sub(r'\\\\cite\\{.*?\\}', '@xcite', latex_content, flags=re.DOTALL)\n",
    "            \n",
    "            # 将所有的公式都替换为@xmath0, @xmath1, @xmath2, ...\n",
    "            formula_count = 0\n",
    "            def replace_formula(match):\n",
    "                nonlocal formula_count\n",
    "                new_formula = f'@xmath{formula_count}'\n",
    "                formula_count += 1\n",
    "                return new_formula\n",
    "            \n",
    "            latex_content = re.sub(r'\\$.*?\\$', replace_formula, latex_content, flags=re.DOTALL)\n",
    "\n",
    "            # 删除所有脚注\n",
    "            # NOTE: 必须在替换公式之后在进行删除，否则会引发冲突\n",
    "            # latex_content = re.sub(r'\\\\footnote\\{.*?\\}', '', latex_content, flags=re.DOTALL)\n",
    "            # latex_content = re.sub(r'(?s)\\\\footnote\\{.*?\\}', '', latex_content)\n",
    "            # latex_content = re.sub  (r'\\\\footnotemark', '', latex_content, flags=re.DOTALL)\n",
    "            # latex_content = re.sub(r'\\\\footnotetext\\{.*?\\}', '', latex_content, flags=re.DOTALL)       \n",
    "            \n",
    "            # # 使用正则表达式找到最后一个 \\section{} 的位置并删除其后的内容\n",
    "            # match_sections = [m.end() for m in re.finditer(r'\\\\section\\{.*?\\}', latex_content)]  # 查找所有 \\section{} 的位置\n",
    "            # if match_sections:\n",
    "            #     last_section_index = match_sections[-1]  \n",
    "            #     content_after_section = latex_content[last_section_index:]  # 获取最后一个 \\section{} 之后的内容\n",
    "\n",
    "            #     match_next_unknown = re.search(r'\\n\\\\[a-zA-Z]+\\{.*?\\}', content_after_section)  # 寻找下一个未知标记\n",
    "            #     if match_next_unknown:\n",
    "            #         next_unknown_index = last_section_index + match_next_unknown.start()  # 获取下一个未知标记的位置\n",
    "            #         latex_content = latex_content[:next_unknown_index]  # 保留标记之前的内容\n",
    "            \n",
    "            # # 在文章的末尾添加\\end{document}\n",
    "            # latex_content += '\\n\\\\end{document}'\n",
    "            \n",
    "            # 删除\\acknowledgment\n",
    "            # latex_content = re.sub(r'\\\\acknowledgment.*?(?=\\\\end\\{document\\})', '', latex_content, flags=re.DOTALL)\n",
    "            \n",
    "            # 将处理后的内容保存到新文件中\n",
    "            output_file = output_dir + '/' + file_name\n",
    "            with open(output_file, 'w', encoding='utf-8') as file:\n",
    "                file.write(latex_content)    \n",
    "                 \n",
    "    def _extract_field_to_csv(self, output_csv_path=None):\n",
    "        '''\n",
    "        extract title, etitle, jabstract, eabstract ... from latex files\n",
    "        '''\n",
    "        \n",
    "        # 定义csv文件的表头\n",
    "        output_csv_path = Path(output_csv_path)\n",
    "        csv_header = ['file_name', 'title', 'etitle', 'jabstract', 'eabstract', 'section_names',\n",
    "                      'sec_intro', 'sec_method', 'sec_result', 'sec_conclusion',\n",
    "                      'abs_intro', 'abs_method', 'abs_result', 'abs_conclusion']\n",
    "        \n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=csv_header)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for file_path in sorted(self.latex_folder.glob('**/*.tex')):\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        latex_content = file.read()\n",
    "                    print(f'processing {file_path} ...')\n",
    "                except UnicodeDecodeError:\n",
    "                    print(f\"UnicodeDecodeError: Failed to decode {file_path}. Skipping this file.\")\n",
    "                    continue\n",
    "                    \n",
    "                matches = self._extract_info(latex_content=latex_content)\n",
    "                matches['file_name'] = file_path.name\n",
    "                \n",
    "                extra_columns = [\n",
    "                    'sec_intro', 'sec_method', 'sec_result', 'sec_conclusion',\n",
    "                    'abs_intro', 'abs_method', 'abs_result', 'abs_conclusion'\n",
    "                ]\n",
    "                for column in extra_columns:\n",
    "                    matches[column] = ''\n",
    "                \n",
    "                writer.writerow(matches)\n",
    "        \n",
    "    def _extract_info(self, latex_content=None):\n",
    "        \n",
    "        patterns = {\n",
    "            # TODO: title 和 jtitle 有时候会同时出现，需要处理\n",
    "            'title': re.compile(r'\\\\title\\{(.*?)\\}', re.DOTALL),\n",
    "            'etitle': re.compile(r'\\\\etitle\\{(.*?)\\}', re.DOTALL),\n",
    "            'jabstract': re.compile(r'\\\\jabstract\\{(.*?)\\}', re.DOTALL),\n",
    "            'eabstract': re.compile(r'\\\\eabstract\\{(.*?)\\}', re.DOTALL),     \n",
    "            'section_names': re.compile(r'\\\\section\\{(.*?)\\}', re.DOTALL),                 \n",
    "        }\n",
    "        \n",
    "        matches = {key: '' if key != 'section_names' else [] for key in patterns.keys()}\n",
    "        \n",
    "        for key, pattern in patterns.items():\n",
    "            matches[key] = pattern.findall(latex_content)\n",
    "            \n",
    "            if key == 'section_names':\n",
    "                matches[key] = [value.strip() for value in matches[key]]  # 移除首尾空白并存储在列表中\n",
    "                matches[key] = list(filter(None, matches[key]))  # 移除空字符串\n",
    "            \n",
    "            if key != 'section_names' and matches[key]:\n",
    "                matches[key] = re.sub(r'\\s*\\\\\\\\\\n\\s*', ' ', matches[key][0]).strip()\n",
    "            elif key != 'section_names':\n",
    "                matches[key] = ''\n",
    "\n",
    "        return matches\n",
    "    \n",
    "    def _postprocess(self):\n",
    "        \"\"\"\n",
    "        convert all processed tex files to markdown files\n",
    "        \"\"\"\n",
    "        markdown_output_dir = Path(self.output_path) / 'markdown'\n",
    "        markdown_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for file_path in sorted(Path(self.processed_tex_dir).glob('*.tex')):\n",
    "            md_output_path = markdown_output_dir / file_path.name.replace('.tex', '.txt')\n",
    "\n",
    "            try:\n",
    "                subprocess.run([\"pandoc\", str(file_path), \"-o\", str(md_output_path)])\n",
    "                print(f\"Converted {file_path} to Markdown: {md_output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Conversion failed for {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_processer = TexProcesser(latex_dir_path='./data/NLP_LATEX_CORPUS/', output_path='./data')\n",
    "tex_processer.pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/latex_info.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('NLP_JP_CORPUS.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./NLP_JP_CORPUS_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>etitle</th>\n",
       "      <th>jabstract</th>\n",
       "      <th>eabstract</th>\n",
       "      <th>section_names</th>\n",
       "      <th>sec_intro</th>\n",
       "      <th>sec_method</th>\n",
       "      <th>sec_result</th>\n",
       "      <th>sec_conclusion</th>\n",
       "      <th>abs_bg</th>\n",
       "      <th>abs_method</th>\n",
       "      <th>abs_result</th>\n",
       "      <th>abs_conclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V01N01-01.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>表層表現中の情報に基づく文章構造の自動抽出</td>\n",
       "      <td>Automatic Detection of Discourse Structure by ...</td>\n",
       "      <td>テキストや談話を理解するためには，まずその文章構造を理解する必要があ\\nる．文章構造に関する...</td>\n",
       "      <td>To understand a text or dialogue, one must tra...</td>\n",
       "      <td>['はじめに', '文章構造のモデルと結束関係', '文章構造の自動抽出', '実験と考察'...</td>\n",
       "      <td>テキストや談話を理解するためには，文章構造の理解，すなわち各文が\\n他のどの文とどのような関...</td>\n",
       "      <td>従来，文章構造のモデルとしてはその基本単位の結束関係\\n(2項関係)を再帰的に組み合わせる\\...</td>\n",
       "      <td>実験には科学雑誌サイエンスのテキスト，\\n「科学技術のためのコンピューター」(Vol.17,...</td>\n",
       "      <td>本論文では，手がかり表現，語連鎖，文間の類似性，という\\n表層表現中の3つ情報に基づいて文章...</td>\n",
       "      <td>テキストや談話を理解するためには，まずその文章構造を理解する必要があ\\nる．文章構造に関する...</td>\n",
       "      <td>本論文では，知識に基づく文理解という処理を行なわずに，表層表現中の種々の情報を用いることによ...</td>\n",
       "      <td>実験の結果これらの情報を組み合わせて利用することにより\\n科学技術文の文章構造のかなりの部分...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V01N01-02.tex</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>A Comparative Study of Automatic Extraction of...</td>\n",
       "      <td></td>\n",
       "      <td>While corpus-based studies are now becoming a ...</td>\n",
       "      <td>['Introduction', 'Importance of Collocational ...</td>\n",
       "      <td>Recent rapid advances in computer technology, ...</td>\n",
       "      <td>In the past, several approaches have been prop...</td>\n",
       "      <td>In our experiments, the ADD (ATR Dialogue Data...</td>\n",
       "      <td>With the growing availability of large textual...</td>\n",
       "      <td>While corpus-based studies are now becoming a ...</td>\n",
       "      <td>In this paper, we are primarily concerned with...</td>\n",
       "      <td>Comparative experiments are made between the t...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V01N01-03.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>並列構造の検出に基づく長い日本語文の構文解析</td>\n",
       "      <td>A Syntactic Analysis Method of Long Japanese S...</td>\n",
       "      <td>従来の構文解析法は十分な精度の解析結果を得ることができず，とくに長い文の解析が困難であった．...</td>\n",
       "      <td>Conventional parsing methods can not analyze l...</td>\n",
       "      <td>['はじめに', '並列構造の検出と文の簡単化', '係り受け解析', '文解析の結果とその...</td>\n",
       "      <td># はじめに\\n\\n従来の構文解析法は基本的に句構造文法あるいは格文法をその拠り所としてきた...</td>\n",
       "      <td># 並列構造の検出と文の簡単化\\n\\n## 並列構造の検出の概要\\n\\n1文中の並列する部分...</td>\n",
       "      <td># 文解析の結果とその評価\\n\\n本手法による文解析の実験をテストサンプルの150文に対して...</td>\n",
       "      <td># おわりに\\n\\n従来方式の構文解析では長い文の中に多く存在する並列構造を正しく認識する\\...</td>\n",
       "      <td>従来の構文解析法は十分な精度の解析結果を得ることができず，とくに長い文の解析が困難であった．...</td>\n",
       "      <td>本論文では，そのようにして検出した並列構造の情報を利用して\\n構文解析を行なう手法を示す．\\...</td>\n",
       "      <td>各部分の係り受け解析としては，基本的に，\\n係り受け関係の非交差条件を満たした上で各文節が係...</td>\n",
       "      <td>我々は，このような考え方に基づき，長い文の中に多く存在する並列構造が\\n文節列同士の類似性を...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V01N01-04.tex</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>A System for Finding Translation Patterns by C...</td>\n",
       "      <td></td>\n",
       "      <td>When the example-based approach is used for ma...</td>\n",
       "      <td>['Introduction', 'System Overview', 'Example-B...</td>\n",
       "      <td>The example-based approach (EBA), an emerging ...</td>\n",
       "      <td>The system flow of  is shown in . An input Jap...</td>\n",
       "      <td></td>\n",
       "      <td>Strong recent interest in corpus-based process...</td>\n",
       "      <td>When the example-based approach is used for ma...</td>\n",
       "      <td>This\\npaper describes a system for finding par...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V02N01-01.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>日英機械翻訳における利用者登録語の意味属性の自動推定</td>\n",
       "      <td>Automatic Determination of Semantic Attributes...</td>\n",
       "      <td>機械翻訳システムを使用して現実の文書を翻訳する場合, 通常, \\n翻訳対象文書に合った利用者...</td>\n",
       "      <td>User dictionaries are important for practical ...</td>\n",
       "      <td>['はじめに', 'システム辞書と利用者辞書', '意味属性推定の方法', '意味属性推定精...</td>\n",
       "      <td># はじめに\\n\\n機械翻訳システムを使用する時, 利用者はシステム辞書に登録されていない\\...</td>\n",
       "      <td># 意味属性推定の方法\\n\\n利用者登録語の日本語表記と英語訳語が与えられたとき, 機械翻訳...</td>\n",
       "      <td># 意味属性推定精度の評価\\n\\n## 実験の条件\\n\\n表\\[tab:3\\]に示すような新...</td>\n",
       "      <td>## 考察\\n\\n### 訳文品質向上効果について\\n\\n最適意味属性を決定する繰り返し実験...</td>\n",
       "      <td>機械翻訳システムを使用して現実の文書を翻訳する場合, 通常, \\n翻訳対象文書に合った利用者...</td>\n",
       "      <td>そこで本論文では, 利用者が登録したい日本語名詞 (複合名詞を含む) と\\n英語訳語を与える...</td>\n",
       "      <td>本方式を, 新聞記事102文とソフトウエア設計書105文\\nの翻訳に必要な利用者辞書作成に適...</td>\n",
       "      <td>以上の結果, 利用者辞書\\n作成への単語の登録において, 最も熟練度の要求される単語意味属性...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>V26N03-03.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>事前学習された文の分散表現を用いた機械翻訳の自動評価</td>\n",
       "      <td>Metric for Automatic Machine Translation Evalu...</td>\n",
       "      <td>本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．現在のデファクトスタン...</td>\n",
       "      <td>This study describes a segment-level metric fo...</td>\n",
       "      <td>['はじめに', '関連研究', '事前学習された文の分散表現を用いた機械翻訳の自動評価',...</td>\n",
       "      <td># はじめに\\n\\n本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．\\...</td>\n",
       "      <td># 事前学習された文の分散表現を用いた機械翻訳の自動評価\\n\\n従来手法に多く見られる文字や...</td>\n",
       "      <td># 評価実験\\n\\n本節では，WMT Metrics Shared\\nTaskにおける人手の...</td>\n",
       "      <td># 分析\\n\\n## 訓練データの文対数と性能の関係\\n\\n本節では，WMT-2017のlv...</td>\n",
       "      <td>本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．現在のデファクトスタン...</td>\n",
       "      <td>そこで本研究では，文全体の大域的な情報を考慮するために，事前学習された文の分散表現を用いる機...</td>\n",
       "      <td>WMT-2017 Metrics Shared Taskにおける翻訳品質のラベル付きデータセ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>V26N03-04.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>単語埋め込みに基づくサプライザル</td>\n",
       "      <td>Surprisal through Word Embeddings</td>\n",
       "      <td>ヒトの文処理のモデル化として Hale によりサプライザルが提案されている．サプライザルは文...</td>\n",
       "      <td>The concept of surprisal was proposed by Hale ...</td>\n",
       "      <td>['はじめに', '前提', '分析手法', '結果と考察', 'おわりに', '分析結果（...</td>\n",
       "      <td># はじめに\\n\\n本研究では眼球運動に基づき文の読み時間を推定し，ヒトの文処理機構の解明を...</td>\n",
       "      <td># 分析手法\\n\\n分析においては，いくつかの要因に基づく線形式に基づいて，読み時間をベイジ...</td>\n",
       "      <td># 結果と考察\\n\\n表 \\[tbl:result\\] に各モデルの分析結果を示す．詳細な結...</td>\n",
       "      <td># おわりに\\n\\n本研究では，日本語の読み時間の推定のために単語埋め込みを用いることを提案...</td>\n",
       "      <td>ヒトの文処理のモデル化として Hale によりサプライザルが提案されている．サプライザルは文...</td>\n",
       "      <td>本論文では，この問題を解決するために単語埋め込みを用いる．skip-gram の単語埋め込み...</td>\n",
       "      <td></td>\n",
       "      <td>さらに，skip-gram の単語埋め込みに基づいて構成した文節のベクトルのノルムが，日本語...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>V26N04-01.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>複単語表現を考慮した依存構造コーパスの構築と解析</td>\n",
       "      <td>Construction and Analysis of Multiword Express...</td>\n",
       "      <td>複単語表現 (MWE) は統語的または意味的な非構成性を有する複数の単語からなるまとまりであ...</td>\n",
       "      <td>Multiword expressions (MWEs) consist of multip...</td>\n",
       "      <td>['はじめに', 'MWEを考慮した依存構造コーパスの構築', '連続MWEを考慮した依存構...</td>\n",
       "      <td># はじめに\\n\\n複単語表現 (MWE)\\nは，統語的もしくは意味的な単位として扱う必要が...</td>\n",
       "      <td># MWEを考慮した依存構造コーパスの構築\\n\\n本章では，英語の複合機能語および形容詞MW...</td>\n",
       "      <td># 連続MWEを考慮した依存構造解析，およびVMWE認識を行うモデルの評価実験\\n\\n本章で...</td>\n",
       "      <td># 結論\\n\\n本研究では，複合機能語と形容詞MWEの双方を考慮した依存構造コーパスをOnt...</td>\n",
       "      <td>複単語表現 (MWE) は統語的または意味的な非構成性を有する複数の単語からなるまとまりであ...</td>\n",
       "      <td>広範囲の連続MWEを依存構造で考慮するために，本稿では Ontonotes コーパスに対して...</td>\n",
       "      <td>実験の結果，連続MWE認識ではパイプラインモデルとマルチタスクモデルがほぼ同等のF値を示し，...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>V26N04-02.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>多義語分散表現の文脈化</td>\n",
       "      <td>Contextualized Multi-Sense Word Embedding</td>\n",
       "      <td>近年，多くの自然言語処理タスクにおいて単語分散表現が利用されている．しかし，各単語に 1 つ...</td>\n",
       "      <td>Currently, distributed word representations ar...</td>\n",
       "      <td>['はじめに', '関連研究', '提案手法', '実験設定', '文脈中での単語間の意味的...</td>\n",
       "      <td># はじめに\\n\\n単語を密ベクトルで表現する単語分散表現 \\@xciteが，機械翻訳\\n\\...</td>\n",
       "      <td># 提案手法\\n\\n本研究では，所与の文脈を手がかりとして，各単語に品詞 \\@xciteやト...</td>\n",
       "      <td># 実験設定\\n\\n提案手法の有効性を検証するために，文脈中での単語間の意味的類似度推定タス...</td>\n",
       "      <td># DMSEの分析\\n\\n## 語彙的換言タスクにおける意味的類似度推定手法の比較\\n\\n表...</td>\n",
       "      <td>近年，多くの自然言語処理タスクにおいて単語分散表現が利用されている．しかし，各単語に 1 つ...</td>\n",
       "      <td>そこで，本研究では各単語に対してより粒度の細かい複数の分散表現を生成するための 2 つの手法...</td>\n",
       "      <td>単語間の意味的類似度推定タスクおよび語彙的換言タスクにおける評価実験の結果，より細かい粒度で...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>V26N04-03.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>ニューラル機械翻訳における単語報酬モデルに基づく対訳辞書の利用</td>\n",
       "      <td>Word Rewarding Model Using a Bilingual Diction...</td>\n",
       "      <td>ニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に流暢な翻訳を出力でき...</td>\n",
       "      <td>Even though outputs of neural machine translat...</td>\n",
       "      <td>['はじめに', '注意機構付きニューラル機械翻訳', '単語報酬モデル', '実験設定',...</td>\n",
       "      <td># はじめに\\n\\nニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に...</td>\n",
       "      <td># 単語報酬モデル\\n\\n本章では，提案する単語報酬モデルの各要素について説明する．\\n単語...</td>\n",
       "      <td># 実験設定\\n\\n本稿では単語報酬モデルの性能を評価するため，日本語から英語，英語から日本...</td>\n",
       "      <td># おわりに\\n\\n本稿ではニューラル機械翻訳における単語報酬モデルによる対訳辞書の活用手法...</td>\n",
       "      <td>ニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に流暢な翻訳を出力でき...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>提案手法は，テスト時のデコーディングの際に対訳辞書に存在する単語に報酬を与えることでそれらの...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>532 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_name language                            title  \\\n",
       "0    V01N01-01.tex       jp            表層表現中の情報に基づく文章構造の自動抽出   \n",
       "1    V01N01-02.tex       en                                    \n",
       "2    V01N01-03.tex       jp           並列構造の検出に基づく長い日本語文の構文解析   \n",
       "3    V01N01-04.tex       en                                    \n",
       "4    V02N01-01.tex       jp       日英機械翻訳における利用者登録語の意味属性の自動推定   \n",
       "..             ...      ...                              ...   \n",
       "527  V26N03-03.tex       jp       事前学習された文の分散表現を用いた機械翻訳の自動評価   \n",
       "528  V26N03-04.tex       jp                 単語埋め込みに基づくサプライザル   \n",
       "529  V26N04-01.tex       jp         複単語表現を考慮した依存構造コーパスの構築と解析   \n",
       "530  V26N04-02.tex       jp                      多義語分散表現の文脈化   \n",
       "531  V26N04-03.tex       jp  ニューラル機械翻訳における単語報酬モデルに基づく対訳辞書の利用   \n",
       "\n",
       "                                                etitle  \\\n",
       "0    Automatic Detection of Discourse Structure by ...   \n",
       "1    A Comparative Study of Automatic Extraction of...   \n",
       "2    A Syntactic Analysis Method of Long Japanese S...   \n",
       "3    A System for Finding Translation Patterns by C...   \n",
       "4    Automatic Determination of Semantic Attributes...   \n",
       "..                                                 ...   \n",
       "527  Metric for Automatic Machine Translation Evalu...   \n",
       "528                  Surprisal through Word Embeddings   \n",
       "529  Construction and Analysis of Multiword Express...   \n",
       "530          Contextualized Multi-Sense Word Embedding   \n",
       "531  Word Rewarding Model Using a Bilingual Diction...   \n",
       "\n",
       "                                             jabstract  \\\n",
       "0    テキストや談話を理解するためには，まずその文章構造を理解する必要があ\\nる．文章構造に関する...   \n",
       "1                                                        \n",
       "2    従来の構文解析法は十分な精度の解析結果を得ることができず，とくに長い文の解析が困難であった．...   \n",
       "3                                                        \n",
       "4    機械翻訳システムを使用して現実の文書を翻訳する場合, 通常, \\n翻訳対象文書に合った利用者...   \n",
       "..                                                 ...   \n",
       "527  本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．現在のデファクトスタン...   \n",
       "528  ヒトの文処理のモデル化として Hale によりサプライザルが提案されている．サプライザルは文...   \n",
       "529  複単語表現 (MWE) は統語的または意味的な非構成性を有する複数の単語からなるまとまりであ...   \n",
       "530  近年，多くの自然言語処理タスクにおいて単語分散表現が利用されている．しかし，各単語に 1 つ...   \n",
       "531  ニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に流暢な翻訳を出力でき...   \n",
       "\n",
       "                                             eabstract  \\\n",
       "0    To understand a text or dialogue, one must tra...   \n",
       "1    While corpus-based studies are now becoming a ...   \n",
       "2    Conventional parsing methods can not analyze l...   \n",
       "3    When the example-based approach is used for ma...   \n",
       "4    User dictionaries are important for practical ...   \n",
       "..                                                 ...   \n",
       "527  This study describes a segment-level metric fo...   \n",
       "528  The concept of surprisal was proposed by Hale ...   \n",
       "529  Multiword expressions (MWEs) consist of multip...   \n",
       "530  Currently, distributed word representations ar...   \n",
       "531  Even though outputs of neural machine translat...   \n",
       "\n",
       "                                         section_names  \\\n",
       "0    ['はじめに', '文章構造のモデルと結束関係', '文章構造の自動抽出', '実験と考察'...   \n",
       "1    ['Introduction', 'Importance of Collocational ...   \n",
       "2    ['はじめに', '並列構造の検出と文の簡単化', '係り受け解析', '文解析の結果とその...   \n",
       "3    ['Introduction', 'System Overview', 'Example-B...   \n",
       "4    ['はじめに', 'システム辞書と利用者辞書', '意味属性推定の方法', '意味属性推定精...   \n",
       "..                                                 ...   \n",
       "527  ['はじめに', '関連研究', '事前学習された文の分散表現を用いた機械翻訳の自動評価',...   \n",
       "528  ['はじめに', '前提', '分析手法', '結果と考察', 'おわりに', '分析結果（...   \n",
       "529  ['はじめに', 'MWEを考慮した依存構造コーパスの構築', '連続MWEを考慮した依存構...   \n",
       "530  ['はじめに', '関連研究', '提案手法', '実験設定', '文脈中での単語間の意味的...   \n",
       "531  ['はじめに', '注意機構付きニューラル機械翻訳', '単語報酬モデル', '実験設定',...   \n",
       "\n",
       "                                             sec_intro  \\\n",
       "0    テキストや談話を理解するためには，文章構造の理解，すなわち各文が\\n他のどの文とどのような関...   \n",
       "1    Recent rapid advances in computer technology, ...   \n",
       "2    # はじめに\\n\\n従来の構文解析法は基本的に句構造文法あるいは格文法をその拠り所としてきた...   \n",
       "3    The example-based approach (EBA), an emerging ...   \n",
       "4    # はじめに\\n\\n機械翻訳システムを使用する時, 利用者はシステム辞書に登録されていない\\...   \n",
       "..                                                 ...   \n",
       "527  # はじめに\\n\\n本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．\\...   \n",
       "528  # はじめに\\n\\n本研究では眼球運動に基づき文の読み時間を推定し，ヒトの文処理機構の解明を...   \n",
       "529  # はじめに\\n\\n複単語表現 (MWE)\\nは，統語的もしくは意味的な単位として扱う必要が...   \n",
       "530  # はじめに\\n\\n単語を密ベクトルで表現する単語分散表現 \\@xciteが，機械翻訳\\n\\...   \n",
       "531  # はじめに\\n\\nニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に...   \n",
       "\n",
       "                                            sec_method  \\\n",
       "0    従来，文章構造のモデルとしてはその基本単位の結束関係\\n(2項関係)を再帰的に組み合わせる\\...   \n",
       "1    In the past, several approaches have been prop...   \n",
       "2    # 並列構造の検出と文の簡単化\\n\\n## 並列構造の検出の概要\\n\\n1文中の並列する部分...   \n",
       "3    The system flow of  is shown in . An input Jap...   \n",
       "4    # 意味属性推定の方法\\n\\n利用者登録語の日本語表記と英語訳語が与えられたとき, 機械翻訳...   \n",
       "..                                                 ...   \n",
       "527  # 事前学習された文の分散表現を用いた機械翻訳の自動評価\\n\\n従来手法に多く見られる文字や...   \n",
       "528  # 分析手法\\n\\n分析においては，いくつかの要因に基づく線形式に基づいて，読み時間をベイジ...   \n",
       "529  # MWEを考慮した依存構造コーパスの構築\\n\\n本章では，英語の複合機能語および形容詞MW...   \n",
       "530  # 提案手法\\n\\n本研究では，所与の文脈を手がかりとして，各単語に品詞 \\@xciteやト...   \n",
       "531  # 単語報酬モデル\\n\\n本章では，提案する単語報酬モデルの各要素について説明する．\\n単語...   \n",
       "\n",
       "                                            sec_result  \\\n",
       "0    実験には科学雑誌サイエンスのテキスト，\\n「科学技術のためのコンピューター」(Vol.17,...   \n",
       "1    In our experiments, the ADD (ATR Dialogue Data...   \n",
       "2    # 文解析の結果とその評価\\n\\n本手法による文解析の実験をテストサンプルの150文に対して...   \n",
       "3                                                        \n",
       "4    # 意味属性推定精度の評価\\n\\n## 実験の条件\\n\\n表\\[tab:3\\]に示すような新...   \n",
       "..                                                 ...   \n",
       "527  # 評価実験\\n\\n本節では，WMT Metrics Shared\\nTaskにおける人手の...   \n",
       "528  # 結果と考察\\n\\n表 \\[tbl:result\\] に各モデルの分析結果を示す．詳細な結...   \n",
       "529  # 連続MWEを考慮した依存構造解析，およびVMWE認識を行うモデルの評価実験\\n\\n本章で...   \n",
       "530  # 実験設定\\n\\n提案手法の有効性を検証するために，文脈中での単語間の意味的類似度推定タス...   \n",
       "531  # 実験設定\\n\\n本稿では単語報酬モデルの性能を評価するため，日本語から英語，英語から日本...   \n",
       "\n",
       "                                        sec_conclusion  \\\n",
       "0    本論文では，手がかり表現，語連鎖，文間の類似性，という\\n表層表現中の3つ情報に基づいて文章...   \n",
       "1    With the growing availability of large textual...   \n",
       "2    # おわりに\\n\\n従来方式の構文解析では長い文の中に多く存在する並列構造を正しく認識する\\...   \n",
       "3    Strong recent interest in corpus-based process...   \n",
       "4    ## 考察\\n\\n### 訳文品質向上効果について\\n\\n最適意味属性を決定する繰り返し実験...   \n",
       "..                                                 ...   \n",
       "527  # 分析\\n\\n## 訓練データの文対数と性能の関係\\n\\n本節では，WMT-2017のlv...   \n",
       "528  # おわりに\\n\\n本研究では，日本語の読み時間の推定のために単語埋め込みを用いることを提案...   \n",
       "529  # 結論\\n\\n本研究では，複合機能語と形容詞MWEの双方を考慮した依存構造コーパスをOnt...   \n",
       "530  # DMSEの分析\\n\\n## 語彙的換言タスクにおける意味的類似度推定手法の比較\\n\\n表...   \n",
       "531  # おわりに\\n\\n本稿ではニューラル機械翻訳における単語報酬モデルによる対訳辞書の活用手法...   \n",
       "\n",
       "                                                abs_bg  \\\n",
       "0    テキストや談話を理解するためには，まずその文章構造を理解する必要があ\\nる．文章構造に関する...   \n",
       "1    While corpus-based studies are now becoming a ...   \n",
       "2    従来の構文解析法は十分な精度の解析結果を得ることができず，とくに長い文の解析が困難であった．...   \n",
       "3    When the example-based approach is used for ma...   \n",
       "4    機械翻訳システムを使用して現実の文書を翻訳する場合, 通常, \\n翻訳対象文書に合った利用者...   \n",
       "..                                                 ...   \n",
       "527  本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．現在のデファクトスタン...   \n",
       "528  ヒトの文処理のモデル化として Hale によりサプライザルが提案されている．サプライザルは文...   \n",
       "529  複単語表現 (MWE) は統語的または意味的な非構成性を有する複数の単語からなるまとまりであ...   \n",
       "530  近年，多くの自然言語処理タスクにおいて単語分散表現が利用されている．しかし，各単語に 1 つ...   \n",
       "531  ニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に流暢な翻訳を出力でき...   \n",
       "\n",
       "                                            abs_method  \\\n",
       "0    本論文では，知識に基づく文理解という処理を行なわずに，表層表現中の種々の情報を用いることによ...   \n",
       "1    In this paper, we are primarily concerned with...   \n",
       "2    本論文では，そのようにして検出した並列構造の情報を利用して\\n構文解析を行なう手法を示す．\\...   \n",
       "3    This\\npaper describes a system for finding par...   \n",
       "4    そこで本論文では, 利用者が登録したい日本語名詞 (複合名詞を含む) と\\n英語訳語を与える...   \n",
       "..                                                 ...   \n",
       "527  そこで本研究では，文全体の大域的な情報を考慮するために，事前学習された文の分散表現を用いる機...   \n",
       "528  本論文では，この問題を解決するために単語埋め込みを用いる．skip-gram の単語埋め込み...   \n",
       "529  広範囲の連続MWEを依存構造で考慮するために，本稿では Ontonotes コーパスに対して...   \n",
       "530  そこで，本研究では各単語に対してより粒度の細かい複数の分散表現を生成するための 2 つの手法...   \n",
       "531                                                      \n",
       "\n",
       "                                            abs_result  \\\n",
       "0    実験の結果これらの情報を組み合わせて利用することにより\\n科学技術文の文章構造のかなりの部分...   \n",
       "1    Comparative experiments are made between the t...   \n",
       "2    各部分の係り受け解析としては，基本的に，\\n係り受け関係の非交差条件を満たした上で各文節が係...   \n",
       "3                                                        \n",
       "4    本方式を, 新聞記事102文とソフトウエア設計書105文\\nの翻訳に必要な利用者辞書作成に適...   \n",
       "..                                                 ...   \n",
       "527  WMT-2017 Metrics Shared Taskにおける翻訳品質のラベル付きデータセ...   \n",
       "528                                                      \n",
       "529  実験の結果，連続MWE認識ではパイプラインモデルとマルチタスクモデルがほぼ同等のF値を示し，...   \n",
       "530  単語間の意味的類似度推定タスクおよび語彙的換言タスクにおける評価実験の結果，より細かい粒度で...   \n",
       "531                                                      \n",
       "\n",
       "                                        abs_conclusion  \n",
       "0                                                       \n",
       "1                                                       \n",
       "2    我々は，このような考え方に基づき，長い文の中に多く存在する並列構造が\\n文節列同士の類似性を...  \n",
       "3                                                       \n",
       "4    以上の結果, 利用者辞書\\n作成への単語の登録において, 最も熟練度の要求される単語意味属性...  \n",
       "..                                                 ...  \n",
       "527                                                     \n",
       "528  さらに，skip-gram の単語埋め込みに基づいて構成した文節のベクトルのノルムが，日本語...  \n",
       "529                                                     \n",
       "530                                                     \n",
       "531  提案手法は，テスト時のデコーディングの際に対訳辞書に存在する単語に報酬を与えることでそれらの...  \n",
       "\n",
       "[532 rows x 15 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna('', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>etitle</th>\n",
       "      <th>jabstract</th>\n",
       "      <th>eabstract</th>\n",
       "      <th>section_names</th>\n",
       "      <th>sec_intro</th>\n",
       "      <th>sec_method</th>\n",
       "      <th>sec_result</th>\n",
       "      <th>sec_conclusion</th>\n",
       "      <th>abs_bg</th>\n",
       "      <th>abs_method</th>\n",
       "      <th>abs_result</th>\n",
       "      <th>abs_conclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V01N01-01.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>表層表現中の情報に基づく文章構造の自動抽出</td>\n",
       "      <td>Automatic Detection of Discourse Structure by ...</td>\n",
       "      <td>テキストや談話を理解するためには，まずその文章構造を理解する必要があ\\nる．文章構造に関する...</td>\n",
       "      <td>To understand a text or dialogue, one must tra...</td>\n",
       "      <td>['はじめに', '文章構造のモデルと結束関係', '文章構造の自動抽出', '実験と考察'...</td>\n",
       "      <td>テキストや談話を理解するためには，文章構造の理解，すなわち各文が\\n他のどの文とどのような関...</td>\n",
       "      <td>従来，文章構造のモデルとしてはその基本単位の結束関係\\n(2項関係)を再帰的に組み合わせる\\...</td>\n",
       "      <td>実験には科学雑誌サイエンスのテキスト，\\n「科学技術のためのコンピューター」(Vol.17,...</td>\n",
       "      <td>本論文では，手がかり表現，語連鎖，文間の類似性，という\\n表層表現中の3つ情報に基づいて文章...</td>\n",
       "      <td>テキストや談話を理解するためには，まずその文章構造を理解する必要があ\\nる．文章構造に関する...</td>\n",
       "      <td>本論文では，知識に基づく文理解という処理を行なわずに，表層表現中の種々の情報を用いることによ...</td>\n",
       "      <td>実験の結果これらの情報を組み合わせて利用することにより\\n科学技術文の文章構造のかなりの部分...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V01N01-03.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>並列構造の検出に基づく長い日本語文の構文解析</td>\n",
       "      <td>A Syntactic Analysis Method of Long Japanese S...</td>\n",
       "      <td>従来の構文解析法は十分な精度の解析結果を得ることができず，とくに長い文の解析が困難であった．...</td>\n",
       "      <td>Conventional parsing methods can not analyze l...</td>\n",
       "      <td>['はじめに', '並列構造の検出と文の簡単化', '係り受け解析', '文解析の結果とその...</td>\n",
       "      <td># はじめに\\n\\n従来の構文解析法は基本的に句構造文法あるいは格文法をその拠り所としてきた...</td>\n",
       "      <td># 並列構造の検出と文の簡単化\\n\\n## 並列構造の検出の概要\\n\\n1文中の並列する部分...</td>\n",
       "      <td># 文解析の結果とその評価\\n\\n本手法による文解析の実験をテストサンプルの150文に対して...</td>\n",
       "      <td># おわりに\\n\\n従来方式の構文解析では長い文の中に多く存在する並列構造を正しく認識する\\...</td>\n",
       "      <td>従来の構文解析法は十分な精度の解析結果を得ることができず，とくに長い文の解析が困難であった．...</td>\n",
       "      <td>本論文では，そのようにして検出した並列構造の情報を利用して\\n構文解析を行なう手法を示す．\\...</td>\n",
       "      <td>各部分の係り受け解析としては，基本的に，\\n係り受け関係の非交差条件を満たした上で各文節が係...</td>\n",
       "      <td>我々は，このような考え方に基づき，長い文の中に多く存在する並列構造が\\n文節列同士の類似性を...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V02N01-01.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>日英機械翻訳における利用者登録語の意味属性の自動推定</td>\n",
       "      <td>Automatic Determination of Semantic Attributes...</td>\n",
       "      <td>機械翻訳システムを使用して現実の文書を翻訳する場合, 通常, \\n翻訳対象文書に合った利用者...</td>\n",
       "      <td>User dictionaries are important for practical ...</td>\n",
       "      <td>['はじめに', 'システム辞書と利用者辞書', '意味属性推定の方法', '意味属性推定精...</td>\n",
       "      <td># はじめに\\n\\n機械翻訳システムを使用する時, 利用者はシステム辞書に登録されていない\\...</td>\n",
       "      <td># 意味属性推定の方法\\n\\n利用者登録語の日本語表記と英語訳語が与えられたとき, 機械翻訳...</td>\n",
       "      <td># 意味属性推定精度の評価\\n\\n## 実験の条件\\n\\n表\\[tab:3\\]に示すような新...</td>\n",
       "      <td>## 考察\\n\\n### 訳文品質向上効果について\\n\\n最適意味属性を決定する繰り返し実験...</td>\n",
       "      <td>機械翻訳システムを使用して現実の文書を翻訳する場合, 通常, \\n翻訳対象文書に合った利用者...</td>\n",
       "      <td>そこで本論文では, 利用者が登録したい日本語名詞 (複合名詞を含む) と\\n英語訳語を与える...</td>\n",
       "      <td>本方式を, 新聞記事102文とソフトウエア設計書105文\\nの翻訳に必要な利用者辞書作成に適...</td>\n",
       "      <td>以上の結果, 利用者辞書\\n作成への単語の登録において, 最も熟練度の要求される単語意味属性...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>V02N01-02.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>主観的動機に関する意味および語用論的制約を利用した日本語複文の理解システム 〜「ので」「のに...</td>\n",
       "      <td>A zero anaphora resolution systemfor Japanese ...</td>\n",
       "      <td>我々は，接続助詞「ので」による順接の複文と接続助詞「のに」による逆接の複\\n文を対象とする理...</td>\n",
       "      <td>Our aim is to construct a system which is able...</td>\n",
       "      <td>['はじめに', '日本語複文に関する制約および素性構造による表現', '制約変換による日本...</td>\n",
       "      <td># はじめに\\n\\n我々が目標とするのは，日本語の複文の理解システムである．このようなシステ...</td>\n",
       "      <td># 日本語複文に関する制約および素性構造による表現\\n\\n本論文で述べるシステムにより意味解...</td>\n",
       "      <td># 制約変換による日本語複文の意味解析システム\\n\\n以上のように，意味および語用論的役割の...</td>\n",
       "      <td># おわりに\\n\\n本論文では順接の「ので」による複文の解析を例にとり，日本語の複文の意味解...</td>\n",
       "      <td>我々は，接続助詞「ので」による順接の複文と接続助詞「のに」による逆接の複\\n文を対象とする理...</td>\n",
       "      <td>この際には，ゼロ代名詞の照応の解析が重要な問題となるが，文献にあるように，本論文で扱う形式の...</td>\n",
       "      <td>そこで，日本語の複文に対する形態素解析や構文解析の結果を素性構造で記述し，こ\\nの結果に対し...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>V02N01-03.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>文章内構造を複合的に利用した論説文要約システムGREEN</td>\n",
       "      <td>GREEN: An Experimental System Generating Summa...</td>\n",
       "      <td>日本語文章要約システムについて報告する. 一般に, 質の良い文章要約を\\n行うためには, あ...</td>\n",
       "      <td>This paper describes an experimental system fo...</td>\n",
       "      <td>['はじめに', 'システム構成', '要約文選択', '文要約解析', '段落分け解析',...</td>\n",
       "      <td># はじめに\\n\\n本論文では日本語の論説文を対象にした要約文章作成実験システム[^1] (...</td>\n",
       "      <td># システム構成\\n\\n要約システム は̏ Sun SPARC Station I 上で, ...</td>\n",
       "      <td># 文要約解析\\n\\n文中の修飾句を削減することにより, 一文内での要約を行う. 文の中心内...</td>\n",
       "      <td># 議論\\n\\nここでは, 機械処理した大量の要約結果の考察から得られた知見や明らかになっ\\...</td>\n",
       "      <td>日本語文章要約システムについて報告する. 一般に, 質の良い文章要約を\\n行うためには, あ...</td>\n",
       "      <td>本研究ではこの観点から, 日本語での様々な表層的特徴をできるだけ多く利用\\nして, 日本語文...</td>\n",
       "      <td>本稿では実際に計算機上で試作した論説文\\n要約システムに関して, これで用いられている論説文...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>V26N03-03.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>事前学習された文の分散表現を用いた機械翻訳の自動評価</td>\n",
       "      <td>Metric for Automatic Machine Translation Evalu...</td>\n",
       "      <td>本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．現在のデファクトスタン...</td>\n",
       "      <td>This study describes a segment-level metric fo...</td>\n",
       "      <td>['はじめに', '関連研究', '事前学習された文の分散表現を用いた機械翻訳の自動評価',...</td>\n",
       "      <td># はじめに\\n\\n本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．\\...</td>\n",
       "      <td># 事前学習された文の分散表現を用いた機械翻訳の自動評価\\n\\n従来手法に多く見られる文字や...</td>\n",
       "      <td># 評価実験\\n\\n本節では，WMT Metrics Shared\\nTaskにおける人手の...</td>\n",
       "      <td># 分析\\n\\n## 訓練データの文対数と性能の関係\\n\\n本節では，WMT-2017のlv...</td>\n",
       "      <td>本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．現在のデファクトスタン...</td>\n",
       "      <td>そこで本研究では，文全体の大域的な情報を考慮するために，事前学習された文の分散表現を用いる機...</td>\n",
       "      <td>WMT-2017 Metrics Shared Taskにおける翻訳品質のラベル付きデータセ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>V26N03-04.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>単語埋め込みに基づくサプライザル</td>\n",
       "      <td>Surprisal through Word Embeddings</td>\n",
       "      <td>ヒトの文処理のモデル化として Hale によりサプライザルが提案されている．サプライザルは文...</td>\n",
       "      <td>The concept of surprisal was proposed by Hale ...</td>\n",
       "      <td>['はじめに', '前提', '分析手法', '結果と考察', 'おわりに', '分析結果（...</td>\n",
       "      <td># はじめに\\n\\n本研究では眼球運動に基づき文の読み時間を推定し，ヒトの文処理機構の解明を...</td>\n",
       "      <td># 分析手法\\n\\n分析においては，いくつかの要因に基づく線形式に基づいて，読み時間をベイジ...</td>\n",
       "      <td># 結果と考察\\n\\n表 \\[tbl:result\\] に各モデルの分析結果を示す．詳細な結...</td>\n",
       "      <td># おわりに\\n\\n本研究では，日本語の読み時間の推定のために単語埋め込みを用いることを提案...</td>\n",
       "      <td>ヒトの文処理のモデル化として Hale によりサプライザルが提案されている．サプライザルは文...</td>\n",
       "      <td>本論文では，この問題を解決するために単語埋め込みを用いる．skip-gram の単語埋め込み...</td>\n",
       "      <td></td>\n",
       "      <td>さらに，skip-gram の単語埋め込みに基づいて構成した文節のベクトルのノルムが，日本語...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>V26N04-01.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>複単語表現を考慮した依存構造コーパスの構築と解析</td>\n",
       "      <td>Construction and Analysis of Multiword Express...</td>\n",
       "      <td>複単語表現 (MWE) は統語的または意味的な非構成性を有する複数の単語からなるまとまりであ...</td>\n",
       "      <td>Multiword expressions (MWEs) consist of multip...</td>\n",
       "      <td>['はじめに', 'MWEを考慮した依存構造コーパスの構築', '連続MWEを考慮した依存構...</td>\n",
       "      <td># はじめに\\n\\n複単語表現 (MWE)\\nは，統語的もしくは意味的な単位として扱う必要が...</td>\n",
       "      <td># MWEを考慮した依存構造コーパスの構築\\n\\n本章では，英語の複合機能語および形容詞MW...</td>\n",
       "      <td># 連続MWEを考慮した依存構造解析，およびVMWE認識を行うモデルの評価実験\\n\\n本章で...</td>\n",
       "      <td># 結論\\n\\n本研究では，複合機能語と形容詞MWEの双方を考慮した依存構造コーパスをOnt...</td>\n",
       "      <td>複単語表現 (MWE) は統語的または意味的な非構成性を有する複数の単語からなるまとまりであ...</td>\n",
       "      <td>広範囲の連続MWEを依存構造で考慮するために，本稿では Ontonotes コーパスに対して...</td>\n",
       "      <td>実験の結果，連続MWE認識ではパイプラインモデルとマルチタスクモデルがほぼ同等のF値を示し，...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>V26N04-02.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>多義語分散表現の文脈化</td>\n",
       "      <td>Contextualized Multi-Sense Word Embedding</td>\n",
       "      <td>近年，多くの自然言語処理タスクにおいて単語分散表現が利用されている．しかし，各単語に 1 つ...</td>\n",
       "      <td>Currently, distributed word representations ar...</td>\n",
       "      <td>['はじめに', '関連研究', '提案手法', '実験設定', '文脈中での単語間の意味的...</td>\n",
       "      <td># はじめに\\n\\n単語を密ベクトルで表現する単語分散表現 \\@xciteが，機械翻訳\\n\\...</td>\n",
       "      <td># 提案手法\\n\\n本研究では，所与の文脈を手がかりとして，各単語に品詞 \\@xciteやト...</td>\n",
       "      <td># 実験設定\\n\\n提案手法の有効性を検証するために，文脈中での単語間の意味的類似度推定タス...</td>\n",
       "      <td># DMSEの分析\\n\\n## 語彙的換言タスクにおける意味的類似度推定手法の比較\\n\\n表...</td>\n",
       "      <td>近年，多くの自然言語処理タスクにおいて単語分散表現が利用されている．しかし，各単語に 1 つ...</td>\n",
       "      <td>そこで，本研究では各単語に対してより粒度の細かい複数の分散表現を生成するための 2 つの手法...</td>\n",
       "      <td>単語間の意味的類似度推定タスクおよび語彙的換言タスクにおける評価実験の結果，より細かい粒度で...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>V26N04-03.tex</td>\n",
       "      <td>jp</td>\n",
       "      <td>ニューラル機械翻訳における単語報酬モデルに基づく対訳辞書の利用</td>\n",
       "      <td>Word Rewarding Model Using a Bilingual Diction...</td>\n",
       "      <td>ニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に流暢な翻訳を出力でき...</td>\n",
       "      <td>Even though outputs of neural machine translat...</td>\n",
       "      <td>['はじめに', '注意機構付きニューラル機械翻訳', '単語報酬モデル', '実験設定',...</td>\n",
       "      <td># はじめに\\n\\nニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に...</td>\n",
       "      <td># 単語報酬モデル\\n\\n本章では，提案する単語報酬モデルの各要素について説明する．\\n単語...</td>\n",
       "      <td># 実験設定\\n\\n本稿では単語報酬モデルの性能を評価するため，日本語から英語，英語から日本...</td>\n",
       "      <td># おわりに\\n\\n本稿ではニューラル機械翻訳における単語報酬モデルによる対訳辞書の活用手法...</td>\n",
       "      <td>ニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に流暢な翻訳を出力でき...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>提案手法は，テスト時のデコーディングの際に対訳辞書に存在する単語に報酬を与えることでそれらの...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_name language  \\\n",
       "0    V01N01-01.tex       jp   \n",
       "2    V01N01-03.tex       jp   \n",
       "4    V02N01-01.tex       jp   \n",
       "5    V02N01-02.tex       jp   \n",
       "6    V02N01-03.tex       jp   \n",
       "..             ...      ...   \n",
       "527  V26N03-03.tex       jp   \n",
       "528  V26N03-04.tex       jp   \n",
       "529  V26N04-01.tex       jp   \n",
       "530  V26N04-02.tex       jp   \n",
       "531  V26N04-03.tex       jp   \n",
       "\n",
       "                                                 title  \\\n",
       "0                                表層表現中の情報に基づく文章構造の自動抽出   \n",
       "2                               並列構造の検出に基づく長い日本語文の構文解析   \n",
       "4                           日英機械翻訳における利用者登録語の意味属性の自動推定   \n",
       "5    主観的動機に関する意味および語用論的制約を利用した日本語複文の理解システム 〜「ので」「のに...   \n",
       "6                         文章内構造を複合的に利用した論説文要約システムGREEN   \n",
       "..                                                 ...   \n",
       "527                         事前学習された文の分散表現を用いた機械翻訳の自動評価   \n",
       "528                                   単語埋め込みに基づくサプライザル   \n",
       "529                           複単語表現を考慮した依存構造コーパスの構築と解析   \n",
       "530                                        多義語分散表現の文脈化   \n",
       "531                    ニューラル機械翻訳における単語報酬モデルに基づく対訳辞書の利用   \n",
       "\n",
       "                                                etitle  \\\n",
       "0    Automatic Detection of Discourse Structure by ...   \n",
       "2    A Syntactic Analysis Method of Long Japanese S...   \n",
       "4    Automatic Determination of Semantic Attributes...   \n",
       "5    A zero anaphora resolution systemfor Japanese ...   \n",
       "6    GREEN: An Experimental System Generating Summa...   \n",
       "..                                                 ...   \n",
       "527  Metric for Automatic Machine Translation Evalu...   \n",
       "528                  Surprisal through Word Embeddings   \n",
       "529  Construction and Analysis of Multiword Express...   \n",
       "530          Contextualized Multi-Sense Word Embedding   \n",
       "531  Word Rewarding Model Using a Bilingual Diction...   \n",
       "\n",
       "                                             jabstract  \\\n",
       "0    テキストや談話を理解するためには，まずその文章構造を理解する必要があ\\nる．文章構造に関する...   \n",
       "2    従来の構文解析法は十分な精度の解析結果を得ることができず，とくに長い文の解析が困難であった．...   \n",
       "4    機械翻訳システムを使用して現実の文書を翻訳する場合, 通常, \\n翻訳対象文書に合った利用者...   \n",
       "5    我々は，接続助詞「ので」による順接の複文と接続助詞「のに」による逆接の複\\n文を対象とする理...   \n",
       "6    日本語文章要約システムについて報告する. 一般に, 質の良い文章要約を\\n行うためには, あ...   \n",
       "..                                                 ...   \n",
       "527  本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．現在のデファクトスタン...   \n",
       "528  ヒトの文処理のモデル化として Hale によりサプライザルが提案されている．サプライザルは文...   \n",
       "529  複単語表現 (MWE) は統語的または意味的な非構成性を有する複数の単語からなるまとまりであ...   \n",
       "530  近年，多くの自然言語処理タスクにおいて単語分散表現が利用されている．しかし，各単語に 1 つ...   \n",
       "531  ニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に流暢な翻訳を出力でき...   \n",
       "\n",
       "                                             eabstract  \\\n",
       "0    To understand a text or dialogue, one must tra...   \n",
       "2    Conventional parsing methods can not analyze l...   \n",
       "4    User dictionaries are important for practical ...   \n",
       "5    Our aim is to construct a system which is able...   \n",
       "6    This paper describes an experimental system fo...   \n",
       "..                                                 ...   \n",
       "527  This study describes a segment-level metric fo...   \n",
       "528  The concept of surprisal was proposed by Hale ...   \n",
       "529  Multiword expressions (MWEs) consist of multip...   \n",
       "530  Currently, distributed word representations ar...   \n",
       "531  Even though outputs of neural machine translat...   \n",
       "\n",
       "                                         section_names  \\\n",
       "0    ['はじめに', '文章構造のモデルと結束関係', '文章構造の自動抽出', '実験と考察'...   \n",
       "2    ['はじめに', '並列構造の検出と文の簡単化', '係り受け解析', '文解析の結果とその...   \n",
       "4    ['はじめに', 'システム辞書と利用者辞書', '意味属性推定の方法', '意味属性推定精...   \n",
       "5    ['はじめに', '日本語複文に関する制約および素性構造による表現', '制約変換による日本...   \n",
       "6    ['はじめに', 'システム構成', '要約文選択', '文要約解析', '段落分け解析',...   \n",
       "..                                                 ...   \n",
       "527  ['はじめに', '関連研究', '事前学習された文の分散表現を用いた機械翻訳の自動評価',...   \n",
       "528  ['はじめに', '前提', '分析手法', '結果と考察', 'おわりに', '分析結果（...   \n",
       "529  ['はじめに', 'MWEを考慮した依存構造コーパスの構築', '連続MWEを考慮した依存構...   \n",
       "530  ['はじめに', '関連研究', '提案手法', '実験設定', '文脈中での単語間の意味的...   \n",
       "531  ['はじめに', '注意機構付きニューラル機械翻訳', '単語報酬モデル', '実験設定',...   \n",
       "\n",
       "                                             sec_intro  \\\n",
       "0    テキストや談話を理解するためには，文章構造の理解，すなわち各文が\\n他のどの文とどのような関...   \n",
       "2    # はじめに\\n\\n従来の構文解析法は基本的に句構造文法あるいは格文法をその拠り所としてきた...   \n",
       "4    # はじめに\\n\\n機械翻訳システムを使用する時, 利用者はシステム辞書に登録されていない\\...   \n",
       "5    # はじめに\\n\\n我々が目標とするのは，日本語の複文の理解システムである．このようなシステ...   \n",
       "6    # はじめに\\n\\n本論文では日本語の論説文を対象にした要約文章作成実験システム[^1] (...   \n",
       "..                                                 ...   \n",
       "527  # はじめに\\n\\n本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．\\...   \n",
       "528  # はじめに\\n\\n本研究では眼球運動に基づき文の読み時間を推定し，ヒトの文処理機構の解明を...   \n",
       "529  # はじめに\\n\\n複単語表現 (MWE)\\nは，統語的もしくは意味的な単位として扱う必要が...   \n",
       "530  # はじめに\\n\\n単語を密ベクトルで表現する単語分散表現 \\@xciteが，機械翻訳\\n\\...   \n",
       "531  # はじめに\\n\\nニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に...   \n",
       "\n",
       "                                            sec_method  \\\n",
       "0    従来，文章構造のモデルとしてはその基本単位の結束関係\\n(2項関係)を再帰的に組み合わせる\\...   \n",
       "2    # 並列構造の検出と文の簡単化\\n\\n## 並列構造の検出の概要\\n\\n1文中の並列する部分...   \n",
       "4    # 意味属性推定の方法\\n\\n利用者登録語の日本語表記と英語訳語が与えられたとき, 機械翻訳...   \n",
       "5    # 日本語複文に関する制約および素性構造による表現\\n\\n本論文で述べるシステムにより意味解...   \n",
       "6    # システム構成\\n\\n要約システム は̏ Sun SPARC Station I 上で, ...   \n",
       "..                                                 ...   \n",
       "527  # 事前学習された文の分散表現を用いた機械翻訳の自動評価\\n\\n従来手法に多く見られる文字や...   \n",
       "528  # 分析手法\\n\\n分析においては，いくつかの要因に基づく線形式に基づいて，読み時間をベイジ...   \n",
       "529  # MWEを考慮した依存構造コーパスの構築\\n\\n本章では，英語の複合機能語および形容詞MW...   \n",
       "530  # 提案手法\\n\\n本研究では，所与の文脈を手がかりとして，各単語に品詞 \\@xciteやト...   \n",
       "531  # 単語報酬モデル\\n\\n本章では，提案する単語報酬モデルの各要素について説明する．\\n単語...   \n",
       "\n",
       "                                            sec_result  \\\n",
       "0    実験には科学雑誌サイエンスのテキスト，\\n「科学技術のためのコンピューター」(Vol.17,...   \n",
       "2    # 文解析の結果とその評価\\n\\n本手法による文解析の実験をテストサンプルの150文に対して...   \n",
       "4    # 意味属性推定精度の評価\\n\\n## 実験の条件\\n\\n表\\[tab:3\\]に示すような新...   \n",
       "5    # 制約変換による日本語複文の意味解析システム\\n\\n以上のように，意味および語用論的役割の...   \n",
       "6    # 文要約解析\\n\\n文中の修飾句を削減することにより, 一文内での要約を行う. 文の中心内...   \n",
       "..                                                 ...   \n",
       "527  # 評価実験\\n\\n本節では，WMT Metrics Shared\\nTaskにおける人手の...   \n",
       "528  # 結果と考察\\n\\n表 \\[tbl:result\\] に各モデルの分析結果を示す．詳細な結...   \n",
       "529  # 連続MWEを考慮した依存構造解析，およびVMWE認識を行うモデルの評価実験\\n\\n本章で...   \n",
       "530  # 実験設定\\n\\n提案手法の有効性を検証するために，文脈中での単語間の意味的類似度推定タス...   \n",
       "531  # 実験設定\\n\\n本稿では単語報酬モデルの性能を評価するため，日本語から英語，英語から日本...   \n",
       "\n",
       "                                        sec_conclusion  \\\n",
       "0    本論文では，手がかり表現，語連鎖，文間の類似性，という\\n表層表現中の3つ情報に基づいて文章...   \n",
       "2    # おわりに\\n\\n従来方式の構文解析では長い文の中に多く存在する並列構造を正しく認識する\\...   \n",
       "4    ## 考察\\n\\n### 訳文品質向上効果について\\n\\n最適意味属性を決定する繰り返し実験...   \n",
       "5    # おわりに\\n\\n本論文では順接の「ので」による複文の解析を例にとり，日本語の複文の意味解...   \n",
       "6    # 議論\\n\\nここでは, 機械処理した大量の要約結果の考察から得られた知見や明らかになっ\\...   \n",
       "..                                                 ...   \n",
       "527  # 分析\\n\\n## 訓練データの文対数と性能の関係\\n\\n本節では，WMT-2017のlv...   \n",
       "528  # おわりに\\n\\n本研究では，日本語の読み時間の推定のために単語埋め込みを用いることを提案...   \n",
       "529  # 結論\\n\\n本研究では，複合機能語と形容詞MWEの双方を考慮した依存構造コーパスをOnt...   \n",
       "530  # DMSEの分析\\n\\n## 語彙的換言タスクにおける意味的類似度推定手法の比較\\n\\n表...   \n",
       "531  # おわりに\\n\\n本稿ではニューラル機械翻訳における単語報酬モデルによる対訳辞書の活用手法...   \n",
       "\n",
       "                                                abs_bg  \\\n",
       "0    テキストや談話を理解するためには，まずその文章構造を理解する必要があ\\nる．文章構造に関する...   \n",
       "2    従来の構文解析法は十分な精度の解析結果を得ることができず，とくに長い文の解析が困難であった．...   \n",
       "4    機械翻訳システムを使用して現実の文書を翻訳する場合, 通常, \\n翻訳対象文書に合った利用者...   \n",
       "5    我々は，接続助詞「ので」による順接の複文と接続助詞「のに」による逆接の複\\n文を対象とする理...   \n",
       "6    日本語文章要約システムについて報告する. 一般に, 質の良い文章要約を\\n行うためには, あ...   \n",
       "..                                                 ...   \n",
       "527  本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．現在のデファクトスタン...   \n",
       "528  ヒトの文処理のモデル化として Hale によりサプライザルが提案されている．サプライザルは文...   \n",
       "529  複単語表現 (MWE) は統語的または意味的な非構成性を有する複数の単語からなるまとまりであ...   \n",
       "530  近年，多くの自然言語処理タスクにおいて単語分散表現が利用されている．しかし，各単語に 1 つ...   \n",
       "531  ニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に流暢な翻訳を出力でき...   \n",
       "\n",
       "                                            abs_method  \\\n",
       "0    本論文では，知識に基づく文理解という処理を行なわずに，表層表現中の種々の情報を用いることによ...   \n",
       "2    本論文では，そのようにして検出した並列構造の情報を利用して\\n構文解析を行なう手法を示す．\\...   \n",
       "4    そこで本論文では, 利用者が登録したい日本語名詞 (複合名詞を含む) と\\n英語訳語を与える...   \n",
       "5    この際には，ゼロ代名詞の照応の解析が重要な問題となるが，文献にあるように，本論文で扱う形式の...   \n",
       "6    本研究ではこの観点から, 日本語での様々な表層的特徴をできるだけ多く利用\\nして, 日本語文...   \n",
       "..                                                 ...   \n",
       "527  そこで本研究では，文全体の大域的な情報を考慮するために，事前学習された文の分散表現を用いる機...   \n",
       "528  本論文では，この問題を解決するために単語埋め込みを用いる．skip-gram の単語埋め込み...   \n",
       "529  広範囲の連続MWEを依存構造で考慮するために，本稿では Ontonotes コーパスに対して...   \n",
       "530  そこで，本研究では各単語に対してより粒度の細かい複数の分散表現を生成するための 2 つの手法...   \n",
       "531                                                      \n",
       "\n",
       "                                            abs_result  \\\n",
       "0    実験の結果これらの情報を組み合わせて利用することにより\\n科学技術文の文章構造のかなりの部分...   \n",
       "2    各部分の係り受け解析としては，基本的に，\\n係り受け関係の非交差条件を満たした上で各文節が係...   \n",
       "4    本方式を, 新聞記事102文とソフトウエア設計書105文\\nの翻訳に必要な利用者辞書作成に適...   \n",
       "5    そこで，日本語の複文に対する形態素解析や構文解析の結果を素性構造で記述し，こ\\nの結果に対し...   \n",
       "6    本稿では実際に計算機上で試作した論説文\\n要約システムに関して, これで用いられている論説文...   \n",
       "..                                                 ...   \n",
       "527  WMT-2017 Metrics Shared Taskにおける翻訳品質のラベル付きデータセ...   \n",
       "528                                                      \n",
       "529  実験の結果，連続MWE認識ではパイプラインモデルとマルチタスクモデルがほぼ同等のF値を示し，...   \n",
       "530  単語間の意味的類似度推定タスクおよび語彙的換言タスクにおける評価実験の結果，より細かい粒度で...   \n",
       "531                                                      \n",
       "\n",
       "                                        abs_conclusion  \n",
       "0                                                       \n",
       "2    我々は，このような考え方に基づき，長い文の中に多く存在する並列構造が\\n文節列同士の類似性を...  \n",
       "4    以上の結果, 利用者辞書\\n作成への単語の登録において, 最も熟練度の要求される単語意味属性...  \n",
       "5                                                       \n",
       "6                                                       \n",
       "..                                                 ...  \n",
       "527                                                     \n",
       "528  さらに，skip-gram の単語埋め込みに基づいて構成した文節のベクトルのノルムが，日本語...  \n",
       "529                                                     \n",
       "530                                                     \n",
       "531  提案手法は，テスト時のデコーディングの際に対訳辞書に存在する単語に報酬を与えることでそれらの...  \n",
       "\n",
       "[414 rows x 15 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jp_df = df[df['language']=='jp']\n",
    "jp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'abs_bg': 'abs_intro'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2013502/981833588.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  jp_df.rename(columns={'abs_bg': 'abs_intro'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "jp_df.rename(columns={'abs_bg': 'abs_intro'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_df_copy = jp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对文本列进行post processing\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def process_text(text):\n",
    "    # 删除换行符 \\n\n",
    "    text = text.replace('\\n', '')\n",
    "    \n",
    "    # 删除 {.underline}\n",
    "    text = text.replace(\"{.underline}\", \"\")\n",
    "\n",
    "    # 删除 \\underline\n",
    "    text = text.replace(\"\\\\underline\", \"\")\n",
    "    \n",
    "    # 删除 \\begin{enumerate} 和 \\end{enumerate} 以及 \\item\n",
    "    text = re.sub(r'\\\\begin\\{enumerate\\}|\\\\end\\{enumerate\\}|\\\\item', '', text)\n",
    "    \n",
    "    # 更换网址为 @url\n",
    "    text = re.sub(r'(http|https)://[^\\s]+', '@url', text)    \n",
    "    \n",
    "    # 删除::: 开头的文本（带有空格）\n",
    "    text = re.sub(r':::\\s(sent|center|table\\*|flashleft|flashright|exe|algorithm\\(ic\\)|indent|xlist|lingexample|'\n",
    "                  r'figure\\*|ex|exB|description|itembox|DEPEX|breakbox|df|enumerate|savenotes|dependency|'\n",
    "                  r'algorithm(ic)?|簡体中文)\\s?', '', text)\n",
    "    # 删除:::本身\n",
    "    text = text.replace(':::', '')\n",
    "    \n",
    "    # 匹配$$之间的数学公式\n",
    "    pattern = r'\\$\\$(.*?)\\$\\$'\n",
    "    \n",
    "    # 使用正则表达式替换数学公式为 @xlmath\n",
    "    text = re.sub(pattern, r'@xlmath', text)\n",
    "    \n",
    "    # 删除\\\n",
    "    text = text.replace('\\\\', '')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_df_copy['sec_method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_string_indices = jp_df_copy[~jp_df_copy['sec_method'].apply(lambda x: isinstance(x, str))].index\n",
    "jp_df_copy.drop(non_string_indices, inplace=True)\n",
    "print(jp_df_copy[['file_name', 'sec_method']])\n",
    "non_string_indices = jp_df_copy[~jp_df_copy['sec_result'].apply(lambda x: isinstance(x, str))].index\n",
    "jp_df_copy.drop(non_string_indices, inplace=True)\n",
    "print(jp_df_copy[['file_name', 'sec_method']])\n",
    "non_string_indices = jp_df_copy[~jp_df_copy['abs_method'].apply(lambda x: isinstance(x, str))].index\n",
    "# jp_df_copy.drop(non_string_indices, inplace=True)\n",
    "print(jp_df_copy[['file_name', 'sec_method']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_df_copy['sec_intro'] = jp_df_copy['sec_intro'].apply(process_text)\n",
    "jp_df_copy['abs_intro'] = jp_df_copy['abs_intro'].apply(process_text)\n",
    "\n",
    "jp_df_copy['sec_method'] = jp_df_copy['sec_method'].apply(process_text)\n",
    "jp_df_copy['abs_method'] = jp_df_copy['abs_method'].apply(process_text)\n",
    "\n",
    "jp_df_copy['sec_result'] = jp_df_copy['sec_result'].apply(process_text)\n",
    "jp_df_copy['abs_result'] = jp_df_copy['abs_result'].apply(process_text)\n",
    "\n",
    "jp_df_copy['sec_conclusion'] = jp_df_copy['sec_conclusion'].apply(process_text)\n",
    "jp_df_copy['abs_conclusion'] = jp_df_copy['abs_conclusion'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'その際，最良解釈を求めるために必要な処理だけを行ない，それ以外の処理の\\n実行は必要が生じるまで保留することによって無駄な処理を避ける．\\n保留した処理を必要に応じて再開することによって，最良解釈以外の解釈も選\\nび出せる．'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jp_df['abs_conclusion'][39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'その際，最良解釈を求めるために必要な処理だけを行ない，それ以外の処理の実行は必要が生じるまで保留することによって無駄な処理を避ける．保留した処理を必要に応じて再開することによって，最良解釈以外の解釈も選び出せる．'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jp_df_copy['abs_conclusion'][39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_df_copy.to_csv('NLP_JP_CORPUS_jp_only_processed.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_incremental = pd.read_csv('./NLP_JP_CORPUS_jp_only_processed.csv')\n",
    "df_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_incremental['sections'] = pd.Series([np.nan] * len(df_incremental), dtype=object)\n",
    "df_incremental['abs_conclusion'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incremental = df_incremental.fillna('')\n",
    "df_incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incremental['sections'] = [[] for _ in range(len(df_incremental))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incremental['abs_incremental'] = [[] for _ in range(len(df_incremental))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file_name', 'language', 'title', 'etitle', 'jabstract', 'eabstract',\n",
       "       'section_names', 'sec_intro', 'sec_method', 'sec_result',\n",
       "       'sec_conclusion', 'abs_intro', 'abs_method', 'abs_result',\n",
       "       'abs_conclusion', 'sections', 'abs_incremental'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_incremental.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incremental['sections'] = df_incremental[['sec_intro', 'sec_method', 'sec_result', 'sec_conclusion']].values.tolist()\n",
    "df_incremental['abs_incremental'] = df_incremental[['abs_intro', 'abs_method', 'abs_result', 'abs_conclusion']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_incremental['abs_incremental'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['テキストや談話を理解するためには，まずその文章構造を理解する必要がある．文章構造に関する従来の多くの研究では，解析に用いられる知識の問題に重点がおかれていた．しかし，量的/質的に十分な計算機用の知識が作成されることはしばらくの間期待できない．',\n",
       "  '本論文では，知識に基づく文理解という処理を行なわずに，表層表現中の種々の情報を用いることにより科学技術文の文章構造を自動的に推定する方法を示す．表層表現中の情報としては，種々の手がかり表現，同一/同義の語/句の出現，2文間の類似性，の3つのものに着目した．',\n",
       "  '実験の結果これらの情報を組み合わせて利用することにより科学技術文の文章構造のかなりの部分が自動的に推定可能であることがわかった．150文に対して実験を行なったところ，96%の文節について正しい係り先を求めることができた．'],\n",
       " ['従来の構文解析法は十分な精度の解析結果を得ることができず，とくに長い文の解析が困難であった．このことは従来の方式が局所的な解析を基本としていたことに原因があり，これを解決するためには文内のできるだけ広い範囲を同時的に調べることが必要である．',\n",
       "  '本論文では，そのようにして検出した並列構造の情報を利用して構文解析を行なう手法を示す．長い日本語文の場合は1文内に複数の並列構造が存在することも多い． そこでまず，文内の並列構造相互間の位置関係を調べ，それらの入れ子構造などを整理する．多くの場合，並列構造の情報を整理した形で利用できれば，文を簡単化した形でとらえることができる．そこで，簡単化した各部分に対して単純な係り受け解析を行ない，その結果を組み合わせることによって文全体の依存構造を求めることが可能となる．',\n",
       "  '各部分の係り受け解析としては，基本的に，係り受け関係の非交差条件を満たした上で各文節が係り得る最も近い文節に係るという優先規則によって決定論的に動作する処理を考えた．我々は，このような考え方に基づき，長い文の中に多く存在する並列構造が文節列同士の類似性を発見するという手法でうまく検出できることを示した．'],\n",
       " ['機械翻訳システムを使用して現実の文書を翻訳する場合, 通常, 翻訳対象文書に合った利用者辞書が必要となる. 特に, 高品質翻訳を狙った機械翻訳システムでは, 各単語に対して, 約2,000種以上の分解精度を持つ単語意味属性の付与が必要であると言われており, 一般の利用者が, このような精密な情報を付与するのは困難であった.',\n",
       "  'そこで本論文では, 利用者が登録したい日本語名詞 (複合名詞を含む) と英語訳語を与えるだけで, システムがシステム辞書の知識を応用して, 名詞種別を自動的に判定し, それに応じた単語の意味属性を付与する方法を提案する.',\n",
       "  '本方式を, 新聞記事102文とソフトウエア設計書105文の翻訳に必要な利用者辞書作成に適用した結果, 自動推定方式では, 専門家の付与した意味属性よりも多くの属性が付与されるが, 40〜80%の再現率が得られることが分かった. また, 人手で作成した利用者辞書を使用する場合と同等の訳文品質が得られることが分かった.以上の結果, 利用者辞書作成への単語の登録において, 最も熟練度の要求される単語意味属性付与作業を自動化できる見通しとなった.'],\n",
       " ['我々は，接続助詞「ので」による順接の複文と接続助詞「のに」による逆接の複文を対象とする理解システムを計算機上に構築することを目的とする．',\n",
       "  'この際には，ゼロ代名詞の照応の解析が重要な問題となるが，文献にあるように，本論文で扱う形式の複文では動機保持者という語用論的役割を新たに定義し用いることにより，従属節と主節それぞれで設定される意味役割や語用論的役割の間の関係を制約として記述することができる．',\n",
       "  'そこで，日本語の複文に対する形態素解析や構文解析の結果を素性構造で記述し，この結果に対して制約論理プログラミングの手法を用いることにより意味および語用論的役割間の制約を解消し，ゼロ代名詞照応などを分析する理解システムを計算機上に構築した．'],\n",
       " ['日本語文章要約システムについて報告する. 一般に, 質の良い文章要約を行うためには, ある一つの言語現象だけをとらえた談話解析だけでは不十分である. なぜなら, 談話に関わる言語現象は相互に関連しているからである. ',\n",
       "  '本研究ではこの観点から, 日本語での様々な表層的特徴をできるだけ多く利用して, 日本語文章の要約を試みる. ',\n",
       "  '本稿では実際に計算機上で試作した論説文要約システムに関して, これで用いられている論説文要約の手法の紹介と, これによって出力された文章の評価を行う.'],\n",
       " ['文章(テキスト)の執筆者の推定問題などに対して，文章の内容や成立に関する歴史的事実の考証とは別に，文章から著者の文体の計量的な特徴を抽出し，その統計分析によって問題解決を試みる研究が多くの人々の注目をあつめつつある．文章に関するどのような要素に著者の特徴が現れるかについて，欧米文に関してはいくつかの研究の例があるが，それは言語によって異なるとも考えられるため，欧米文に関する研究成果が日本文の場合にもあてはまるかについて実証的な研究が必要である．また，各言語はその言語における著者の特徴を表す独特な要素があることも考えられる．',\n",
       "  '本論文では，今まで明らかにされていない，日本文における動詞の長さの分布に著者の特徴が現れることと，その結果が動詞中の漢語・和語，合成語・非合成語の使用率の影響ではないことを著者3人の計21の文章を用いて明らかにした．計量分析の手法としては，同一著者の文章における動詞の長さの分布間の距離の平均値と，異なる著者の文章における動詞の長さの分布の距離の平均値との差，および距離マトリックスを用いて主成分分析を行うという方法を用いて数量・視覚的文章の分類を試みた．',\n",
       "  ''],\n",
       " ['比喩の一種である「駄洒落」は, 言語記号(音声)とその記号が表す概念の意味との両方に, 比喩を成立させる「根拠(ground)」(比喩における被喩辞(tenor)と喩辞(vehicle)とを結びつける関係)があるという点で, 高度な修辞表現に位置づけられる. ',\n",
       "  '筆者らは, 「併置型」と呼ぶ駄洒落の一種(例「トイレに行っといれ」)を, 外国語専攻の大学生54名に筆記によって創作させ, 203個を収集した. そしてこのデータに対して, 駄洒落理解システムの構築に必要な知見を得るという観点から, 「先行喩辞」(例では「トイレ」)と「後続喩辞」(例では「....といれ」)の関係, 及び「出現喩辞」(例では「....といれ」)と「復元喩辞」(例では「....ておいで」)の関係に着目し, 以下の３つの分析を行った. (1)先行−後続出現喩辞間の音素列は, どれ位の長さの一致が見られるか. (2)先行−後続出現喩辞間の音素の相違にはどのような特徴があるか. (3)出現−復元喩辞間の音素の相違にはどのような特徴があるか. ',\n",
       "  'その結果,出現喩辞の音節数は先行と後続とで一致する場合が多いこと, 先行−後続出現喩辞間及び出現−復元喩辞間の音素の相違は比較的少なく,相違がある場合もかなり高い規則性があること, などがわかった. 以上の知見から, 計算機による駄洒落理解手法, 即ち出現喩辞と復元喩辞を同定するアルゴリズムを構築できる見通しが得られた.'],\n",
       " ['三浦文法は、時枝誠記により提唱され三浦つとむにより発展的に継承された言語過程説に基づく日本語文法である。言語過程説によれば、言語は対象−認識−表現の過程的構造をもち、対象のあり方が話者の認識を通して表現されている。',\n",
       "  '本論文では、三浦文法に基づいて体系化した日本語品詞体系および形態素処理用の文法記述形式を提案し、日本語の形態素処理や構文解析におけるその有効性を論じた。日本語の単語を、対象の種類とその捉え方に着目し、約４００通りの階層化された品詞に分類して、きめ細かい品詞体系を作成した。',\n",
       "  '本論文で提案した品詞体系と形態素処理用文法記述形式に基づき、実際に形態素処理用の日本語文法を構築した結果によれば、本文法記述形式により例外的な規則も含めて文法を簡潔に記述できるだけでなく、拡張性の点でも優れていることが分かった。本品詞体系により、三浦の入れ子構造に基づく意味と整合性の良い日本語構文解析が実現できるものと期待される。'],\n",
       " ['機械翻訳システムでは動詞の訳語を選択するために格フレームがよく利用される．格フレームは従来主として人手で記述されていたが，一貫性を保って記述するのが難しいこと，格フレームを部分的に変更した場合に起こる影響が把握しにくいことなどの重大な問題があった．',\n",
       "  'そこでこれらの問題を解決するため，本論文では格フレームを決定木の形で表し(これを格フレーム木と呼ぶ)，これを英日の対訳コーパスから統計的な帰納学習プログラムを利用して学習することを提案する．本論文ではまず，この提案によって上記の問題が軽減される根拠を述べた後，本論文で作成した英日対訳コーパスについて述べる．続いて7つの英語動詞について格フレーム木の獲得実験を2つ報告する．',\n",
       "  '最初の実験は，格要素の制約として英語の単語を使う格フレーム木を学習したものである．これにより得られた格フレーム木を観察したところ，人間の直観に近く，かつ直観を越えた非常に精密な訳し分けの情報が得られたことが明らかになった．次に，この格フレームの一般性を高めるために，英語の単語の代わりに意味分類コードを制約として利用する手法を提案し，これに基づいて格フレーム木を学習する実験を行った．得られた格フレーム木で未学習のデータの動詞の訳語を決定する評価を行ったところ，2.4%ないし32.2%の誤訳率が達成された．この誤訳率と，先の英語単語を利用した格フレーム木での誤訳率との差は13.6%ないし55.3%となり，意味分類コードが有効に機能したことが示された．'],\n",
       " ['本論文では「目を盗む」や「かたずを飲む」などの述語型定型表現をコーパスから自動抽出することを目的に，従来の相互情報量の条件を緩める方向で，名詞動詞間の共起性を測る新たな基準を提案する．',\n",
       "  '概略，名詞，動詞のどちらかを固定して，その単語と共起する集合内の各単語に，どの程度特異な頻度になっているかの数値を与える．この数値は集合内のその単語の頻度の割合と，集合内の単語の種類数から計算される．この数値の上位のものを取り出すことで定型表現の抽出を行う．',\n",
       "  '本手法の特徴は，名詞を固定した場合に抽出できる表現と，動詞を固定した場合に抽出できる表現はほとんど共通のものがなく，しかもどちらの場合も相互情報量による抽出程度の正解率を得られることである．このため，目的の抽出数の半数づつを名詞固定と動詞固定の各々の場合から取り出せば，相互情報量を用いて抽出する場合よりも高い正解率が得られる．']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_incremental['sections'] = df_incremental['sections'].apply(lambda x: x[:2] + [''.join(x[2:])])\n",
    "df_incremental['abs_incremental'] = df_incremental['abs_incremental'].apply(lambda x: x[:2] + [''.join(x[2:])])\n",
    "df_incremental['abs_incremental'][:10].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_join(lst):\n",
    "    result = []\n",
    "    incremental_str = ''\n",
    "    for item in lst:\n",
    "        incremental_str = ''.join([incremental_str, item]).strip()\n",
    "        result.append(incremental_str)\n",
    "    return result\n",
    "\n",
    "df_incremental['abs_incremental'] = df_incremental['abs_incremental'].apply(incremental_join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_incremental['abs_incremental'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_incremental['abs_incremental'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = df_incremental.to_json(orient='records', force_ascii=False)\n",
    "\n",
    "\n",
    "# 将 JSON 数据保存到文件\n",
    "with open('./NLP_JP_CORPUS_INCREMENTAL.json', 'w', encoding='utf-8') as file:\n",
    "    file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "\n",
    "df_incremental = pd.read_json('./NLP_JP_CORPUS_INCREMENTAL.json', orient='records', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['テキストや談話を理解するためには，まずその文章構造を理解する必要がある．文章構造に関する従来の多くの研究では，解析に用いられる知識の問題に重点がおかれていた．しかし，量的/質的に十分な計算機用の知識が作成されることはしばらくの間期待できない．',\n",
       " 'テキストや談話を理解するためには，まずその文章構造を理解する必要がある．文章構造に関する従来の多くの研究では，解析に用いられる知識の問題に重点がおかれていた．しかし，量的/質的に十分な計算機用の知識が作成されることはしばらくの間期待できない．本論文では，知識に基づく文理解という処理を行なわずに，表層表現中の種々の情報を用いることにより科学技術文の文章構造を自動的に推定する方法を示す．表層表現中の情報としては，種々の手がかり表現，同一/同義の語/句の出現，2文間の類似性，の3つのものに着目した．',\n",
       " 'テキストや談話を理解するためには，まずその文章構造を理解する必要がある．文章構造に関する従来の多くの研究では，解析に用いられる知識の問題に重点がおかれていた．しかし，量的/質的に十分な計算機用の知識が作成されることはしばらくの間期待できない．本論文では，知識に基づく文理解という処理を行なわずに，表層表現中の種々の情報を用いることにより科学技術文の文章構造を自動的に推定する方法を示す．表層表現中の情報としては，種々の手がかり表現，同一/同義の語/句の出現，2文間の類似性，の3つのものに着目した．実験の結果これらの情報を組み合わせて利用することにより科学技術文の文章構造のかなりの部分が自動的に推定可能であることがわかった．150文に対して実験を行なったところ，96%の文節について正しい係り先を求めることができた．']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_incremental['abs_incremental'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['テキストや談話を理解するためには，文章構造の理解，すなわち各文が他のどの文とどのような関係(結束関係)でつながっているかを知る必要がある．文章構造に関する従来の多くの研究では，文章構造の認識に必要となる知識，またそれらの知識に基づく推論の問題に重点がおかれていた．しかしそのような知識からのアプローチには次のような問題があると考えられる．辞書やコーパスからの知識の自動獲得，あるいは人手による知識ベース構築の現状をみれば，量的/質的に十分な計算機用の知識が作成されることはしばらくの間期待できない． 一方，オンラインテキストの急増にともない，文章処理の技術は非常に重要になってきている@xcite．そのため，現在利用可能な知識の範囲でどのような処理が可能であるかをまず明らかにする必要がある．現在の自然言語処理のターゲットの中心である科学技術文では，文章構造理解の手がかりとなる情報が表層表現中に明示的に示されていることが多い．科学技術の専門的内容を伝えるためにはそのように明示的表現を用いることが必然的に必要であるといえる．このような観点から，本論文では，表層表現中の種々の情報を用いることにより科学技術文の文章構造を自動的に推定する方法を示す．文章構造抽出のための重要な情報の一つは，多くの研究者が指摘しているように「なぜなら」，「たとえば」などの手がかり語である. しかし，それらだけで文章全体の構造を推定することは不可能であることから，我々はさらに2つの情報を取り出すことを考えた．そのひとつは同一/同義の語/句の出現であり，これによって主題連鎖/焦点-主題連鎖の関係@xciteを推定することができる．もうひとつは2文間の類似性で，類似性の高い2文を見つけることによってそれらの間の並列/対比の関係を推定することができる．これらの3つの情報を組み合わせて利用することにより科学技術文の文章構造のかなりの部分が自動推定可能であることを示す．',\n",
       " '従来，文章構造のモデルとしてはその基本単位の結束関係(2項関係)を再帰的に組み合わせることによる木構造(文章構造木)が一般的に用いられてきた．しかし，何をその基本単位とするか，また基本単位間にどのような結束関係を考えるかについては研究者ごとに独自の定義が与えられてきた．本論文では，文章構造の自動抽出の可能性を示すことを第一義的に考え，句点で区切られた文を基本単位とするもっとも単純な文章構造モデルを採用する一方，結束関係としてどれだけのものを考えればよいかという問題は，対象とするテキストの種類に大きく依存する@xcite．たとえば，物語文などでは過去の事象間の時系列の関係が中心となるが，科学技術文や論説文などではそのような関係はほとんどみられない．本論文では，従来の研究で扱われてきた種々の結束関係のうち，対象とする科学技術文の構造を説明するために必要なものとして表[tab:CRelations]に示すものを考える．さらに，文章構造モデルに対して以下の仮定を行なうことにする．新たな文(入力文)は，それまでの文章構造木の右端の節点に対応する文のいずれかに接続される．これは，「新しい主題が導入された後は，古い主題に関する詳しい説明は参照されない」ということを意味する(図[fig:Assump])．計算量の点で有効であり，また科学技術文の場合直観的に妥当であると考えられたことからこの仮定を採用したなお，本論文で扱う実験テキスト(約200文)の各章(9章)についてはこの仮定のもとでそれぞれに適当な文章構造木を考えることが可能であった．以下ではある入力文に対してそれが接続される文を接続文，また，接続文になり得る文章構造木の右端の節点に対応する文を接続候補文とよぶことにする．前節で示したモデルに基づくと文章構造の解析は，入力文章を前から1文づつ順に処理し，各文(入力文)に対して適切な接続文と適切な結束関係を決定するという問題になる．この処理を行うために，表層表現中の次の3つの情報に着目する．種々の結束関係を示す手がかり表現主題連鎖または焦点-主題連鎖関係における同一/同義の語/句の出現並列/対比関係にある2文の間の類似性以降に示す方法によって，入力文と接続候補文に対してこれらの情報を自動的に抽出し，対応する結束関係への確信度に変換することができる．この処理によって入力文と各接続候補文との間のすべての結束関係に対する確信度を計算し，最終的に最大の確信度をもつ接続候補文と結束関係を選択する(図[fig:NS_CS]). 文章構造には初期状態として初期節点(文には対応しない)を与え，この初期節点と入力文の間の初期化関係に一定の確信度を与えておく(入力文と初期節点の組については上で述べた情報の抽出処理は行なわない)．いずれの接続候補文に対してもこの値よりも大きな確信度を持つ結束関係が存在しない場合には，その入力文は初期節点に接続されるとする．これはその入力文が新しい段落の始まりの文であるような場合に対応する．以下，各情報に対してそれらを抽出し結束関係への確信度に変換する方法を説明する．種々の結束関係を示す手がかり表現を取り出しその関係への確信度を得るために，ヒューリスティック・ルールを用意した．ルールは以下のものからなる．ルールの適用条件 :ルールの適用範囲 (どれだけ離れた接続候補文までルールを適用するか)接続候補文とその接続文との結束関係ある入力文に対する処理を行なう時点では，それより前の部分の文章構造はすでに決定されている．そこで，接続候補文がどのような結束関係でそれ以前の文(接続候補文の接続文)と接続されているかということをルールの適応条件とすることができる．接続候補文の依存構造のパターン入力文の依存構造のパターン 対応する結束関係と確信度接続候補文と入力文のパターンはそれぞれの文の依存構造解析結果に対して適用される@xcite．この処理は依存構造木に対する柔軟なパターン照合機能を用いて実現した．そこでは，依存構造木とその構成要素である文節(単語の並び)に対するパターンが，正規表現，論理和，論理積，否定などによって指定できる@xcite．ルールは各接続候補文と入力文の組に対して適用され，条件部が満たされれば対応する結束関係に指定された確信度の得点が与えられる(複数のルールがマッチした場合確信度の得点は加算されていく)．ルールの一例を表[tab:HR]に示す(すべてのルールを付録[sec:rule]に示す)．たとえば，ルールa(表[tab:HR])は入力文が「なぜなら」で始まる場合，その入力文と直前の接続候補文の間の理由関係に得点を与える(ルールの適用範囲が1であるので，直前の接続候補文との間の関係に対してのみ得点が与えられる)．ルールb(表[tab:HR])の場合は，条件部で同一の語の出現を指定しており，直前の接続候補文だけでなく他の接続候補文に対しても適用される．ルールc(表[tab:HR])では，条件として「接続候補文とその接続文との結束関係」を指定している．このルールは，「例提示関係によって具体例が導入されれば，次にその例の説明が続く場合がある」ことを表現している．ルールd(表[tab:HR])は時制の変化を手がかりとして文章構造の区切れを検出するためのルールで，連続する2文の時制が現在から過去に移行する場合，その間の全ての結束関係の確信度を指定された値だけ減少させる．このペナルティの値によって入力文が直前の文以外の接続候補文へ接続されることが優先される．一般に文は主題を示す部分(主題部分)とそれ以外の部分(非主題部分)に分けることができる．2つの文が同じ主題について述べられている場合，それら2文は主題連鎖関係にあるとする．この関係は2つの文の主題部分に同一/同義の語/句(以下これを語連鎖とよぶ)が現れていることで発見できる．一方，ある文の主題以外の要素がその後の文の主題となるような結束関係を焦点-主題連鎖関係とよぶことにする(この時，後の文で主題となる要素は前の文において焦点要素であると考えられるため)．この関係は2文間の主題部分から非主題部分への語連鎖を調べることで発見できる．しかし，多くの場合一つの入力文に対して各接続候補文との種々の関係を支持する複数の情報が存在する．そのため，単に語連鎖を発見するだけでなく，その連鎖の強さに応じて主題連鎖関係あるいは焦点-主題連鎖関係に確信度を与えることが必要となる．そこで，主題部分，非主題部分の各語に対して，文中での重要度に応じた得点を与え，また同一/同義の語/句の照合に対してもその一致度に応じた得点を定義した．その上で，連鎖する2語/句のそれぞれの文での重要度の得点とその一致度の得点の総和を語連鎖の得点とし，これを主題連鎖関係あるいは焦点-主題連鎖関係に確信度として与えるということを行った(図[fig:TF])．これらの処理は，依存構造に対するパターンと得点からなるルールを適用するという形で実現した(ルールの一例を表[tab:TDCRule]に示す)．ルールa,b(表[tab:TDCRule])は主題を示す助詞「は」を伴う語とその修飾語/句に，ルールc,d(表[tab:TDCRule])は条件節内の語に，主題部分としての得点を与える．ここでは，主題部分の中でもっとも重要であると考えられる，助詞「は」を直接ともなう語に最大の得点を与えている．ルールe(表[tab:TDCRule])は「〜Aがある」という文のAに対して非主題部分の要素として高い得点を与える．このような文体ではAが重要な新情報であり，この語が以降の文で主題として取り上げられる(焦点-主題連鎖関係が存在する) 場合が多い．ルールf,g,h(表[tab:TDCRule])は語/句の一致に対して得点を与えるルールである．ここでは「XのY」のような句の一致(ルールg)に対しては単なる語の一致(ルールf)よりも高い得点を与える．語連鎖の検出における最大の問題は，著者が，単に同一の語/句を繰り返すのではなく，微妙に異なった表現によってそのような連鎖を示す傾向にあるという問題である．シソーラスの利用，ルールh(表[tab:TDCRule])などによってそのような表現の一部は検出可能であるが，検出できない微妙な表現は多数存在する．それらの取扱いについては本論文の範囲外とした．並列/対比の結束関係にある2文の間にはある種の類似性が認められる．しかしそれらは文全体についての類似性であるため，これまでに示したような文中の比較的狭い部分を調べるルールを適用することでは検出できない．そこで，1文内の並列構造の範囲を調べるために開発したダイナミックプログラミングによる類似性検出の方法@xciteを拡張するということを行った．この方法では任意の長さの文節列間の類似度を計算することが可能である．そこでは，まず文節間の類似度を，品詞の一致，語の一致，シソーラス辞書中での語の近さなどによって計算し，その上でそれらの文節間の類似度を組み合わせることによって文節列間の類似度を計算する．この手法は並列構造の構成要素を決定するために1文内の句/節間の類似度を計算するものとして開発した．これを2文間(入力文と接続候補文)の類似度を計算するように拡張することは，その2文を連結しそれらが仮想的に並列構造を構成すると見なすことによって簡単に実現することができる．このことにより，その仮想的並列構造の構成要素である2文間の類似度をまったく同じ枠組みで計算することができるからである．最終的には，この方法によってえられた類似度を2文の長さの和によって正規化した値を，それらの間の並列関係と対比関係の確信度に加算する．',\n",
       " '実験には科学雑誌サイエンスのテキスト，「科学技術のためのコンピューター」(Vol.17,No.12)を用いた(付録[sec:text]にその一部を示す)．実験文はあらかじめ章ごとに分割し(全9章，1章は平均24文)，文章構造の推定は章単位で行なった．実験は以下の手順で行なった．まずはじめの3章に対して．主題部分/非主題部分の重要度のルール，語/句の照合ルール，手がかり表現のルールを作成し，できるだけ正しい文章構造が求まるようにそれらの得点を人手で調整した2文間の類似度の計算については，単に並列構造検出のシステムを用いただけで，得点付けの調整/変更は行なわなかった．次に，残りの6章に対して手がかり表現のルールだけを追加し，その新しいルールセットによって残り6章に対する解析実験を行なった．これらの実験結果を表[tab:Experiment]に示す．ここでは，各入力文に対して正しい接続文と正しい結束関係，また結束関係が主題連鎖または焦点-主題連鎖の場合はその正しい語連鎖が求まった場合を正解とした．結束関係ごとの解析結果の集計では，解析失敗のものについてはその正しい結束関係の欄に分類した．なお，残りの6章に対するルールを追加した新しいルールセットでもとの3章を解析したところ，解析結果は全く同じであった．これらの結果から，科学技術文の場合，表層表現中の情報をうまく取り出すことができればその文章構造のかなりの部分が自動的に推定可能であることがわかる．手がかり表現についての汎用性のあるルールセットを用意するためには，かなりの規模のテキストを対象としたルール作成作業が必要であると思われる．しかし，手がかり表現に関するルールの多くは排他的なものとして記述することができるので，本実験においてそうであったように，新しく追加したルールがもとのルールと競合して副作用を起こすということは非常に少ないと予想される．まず，文章構造推定の経緯の具体例を示す．付録のS1-11からS1-20までの文章は以下に示す情報によって図[fig:DSExam]-aに示す構造に変換された．また付録のS4-1からS4-7までの文章は以下の手順で図[fig:DSExam]-bに示す構造に変換された．接続詞「しかし」は，S1-12，S1-20のように変化関係を示す手がかり語であるだけでなく，S3-4のように対比関係を示す場合もある．この区別は，この手がかり語と他の情報を組み合わせて調べることによって可能となる．すなわち，S1-12，S1-20の場合は過去から現在への時制の変化を見ることによって変化関係を推定することができる．一方，S3-3とS3-4の間には高い類似度(得点23)があるため，これと手がかり語「しかし」によって対比関係が推定できる(これに対して，S1-11とS1-12の類似度は0，S1-19とS1-20の類似度は3である). 連続する2文間の結束関係だけでなく，離れた2文間の関係も，種々の情報によって正しく推定することが可能である．例えば，S1-16，S1-12間の例提示関係は表層表現中の手がかり語によって，S1-19，S1-13間の主題連鎖関係は語連鎖によって，またS4-6とS4-3の並列関係は2文間の類似度によってそれぞれ正しく推定されている．S4-7については正しい結束関係が推定できなかった，ここではS4-6の「温度が上昇する」ことによって生じる「熱」がS4-7で主題となっている．すなわち，推論を介した焦点-主題連鎖関係が存在している．このように結束関係の推定に推論を必要とするような問題は本論文では対象としなかった．このような問題を含めて，実験テキストの解析誤りの原因は次のように分類できる．なお，学習サンプル，テストサンプルともに解析誤りの原因の種類は同じものであった．語連鎖の抽出について 3.3節でも述べたように，主題連鎖/焦点-主題連鎖関係を示す語連鎖にはシソーラスやある種のパターン(表[tab:TDCRule]のルールhなど)を用いるだけでは検出不可能なものがある．上記のS4-7はその一例である．また逆に，主題連鎖/焦点-主題連鎖関係を示すものではない同一語句の出現に対して得点が与えられ，それが他の正しい関係の推定の妨げとなる場合がある．2文間の類似度の計算についてたとえば，「銀河の中には、互いに近接していて、橋がかかっているように見えるものがある。しかし大多数の銀河は、ほぼ対称的な渦巻き形、あるいは単純な円形または楕円形をしている。」という対比関係の2文では，同一/同義の単語も少なく構造(品詞の並び)的にも似ていないため我々の方法では高い類似度が与えられない．このような場合並列/対比関係が正しく推定されないということになる．また逆に，並列/対比関係にない2文でも同一の語を多数含んでいるような場合高い類似度が与えられ，誤ってそれら2文間の並列/対比関係が推定されてしまう場合がある．詳細化関係ではたとえば次の例のように手がかり表現といえるものが存在しない場合がある．「こうした終局的な型は、初期条件に左右される。遭遇時の速度や傾きの角度を少し変えるだけで、複雑な３体の動きは大幅に変わる。」このような2文間の詳細化関係は本手法では正しく推定できない．表[tab:Experiment]に示すとおり解析誤りの大部分は並列/対比関係，主題連鎖/焦点-主題連鎖関係についての誤りで，これらの多くは上の1,2の原因が組み合わさって生じたものである．たとえば，微妙な表現の語連鎖が抽出できない場合に，別の不適当な文との間の高い類似度によって並列/対比関係が推定されてしまったというような場合である．なお，本論文では同一著者のテキストを学習サンプル，テストサンプルとして実験を行なった．種々のテキストに対して本手法の有効性を検証することは今後の課題である．しかし，科学技術文の本質的目的がその内容を正確に伝えることであるということを考えれば，著者・テキストによって文体に差があるとしても，それは本手法の精度に大きな影響を与えるほどのものではないと考えられる．本論文では，手がかり表現，語連鎖，文間の類似性，という表層表現中の3つ情報に基づいて文章構造を自動的に推定する手法を示した．科学技術文の場合，これら3つの表層的情報を組み合わせて利用することにより，知識に基づく文理解という処理を行なわなくても，文章構造のかなりの部分が推定できることがわかった．微妙な表現による語連鎖の扱い，手がかり表現についての汎用性のあるルールセットの作成などの問題を克服すれば，大規模なテキストに対して文単位でなく文章単位の処理を実現することが可能となるだろう．']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_incremental['sections'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_json('./NLP_JP_CORPUS_INCREMENTAL.json', orient='records', encoding='utf-8')\n",
    "\n",
    "# 定义划分比例\n",
    "train_ratio = 0.75\n",
    "val_ratio = 0.125\n",
    "test_ratio = 0.125\n",
    "\n",
    "# 划分数据集\n",
    "train_val, test = train_test_split(df, test_size=test_ratio, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=val_ratio/(train_ratio+val_ratio), random_state=42)\n",
    "\n",
    "# 保存分割后的数据集为 json 文件\n",
    "train.to_csv('train.json', index=False)\n",
    "val.to_csv('val.json', index=False)\n",
    "test.to_csv('test.json', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file_name', 'language', 'title', 'etitle', 'jabstract', 'eabstract', 'section_names', 'sec_intro', 'sec_method', 'sec_result', 'sec_conclusion', 'abs_intro', 'abs_method', 'abs_result', 'abs_conclusion', 'sections', 'abs_incremental'],\n",
       "    num_rows: 414\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_json('./NLP_JP_CORPUS_INCREMENTAL_JUMAN.json', orient='records', encoding='utf-8')\n",
    "df.head()\n",
    "print('test')\n",
    "\n",
    "from datasets import Dataset\n",
    "issues_dataset = Dataset.from_pandas(df)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset = issues_dataset.train_test_split(test_size=0.125, seed=42)\n",
    "test = issues_dataset['train'].train_test_split(test_size=0.125/(0.75+0.125), seed=42)\n",
    "issues_dataset['train'], issues_dataset['validation'] = test['train'], test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file_name', 'language', 'title', 'etitle', 'jabstract', 'eabstract', 'section_names', 'sec_intro', 'sec_method', 'sec_result', 'sec_conclusion', 'abs_intro', 'abs_method', 'abs_result', 'abs_conclusion', 'sections', 'abs_incremental'],\n",
       "        num_rows: 310\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file_name', 'language', 'title', 'etitle', 'jabstract', 'eabstract', 'section_names', 'sec_intro', 'sec_method', 'sec_result', 'sec_conclusion', 'abs_intro', 'abs_method', 'abs_result', 'abs_conclusion', 'sections', 'abs_incremental'],\n",
       "        num_rows: 52\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['file_name', 'language', 'title', 'etitle', 'jabstract', 'eabstract', 'section_names', 'sec_intro', 'sec_method', 'sec_result', 'sec_conclusion', 'abs_intro', 'abs_method', 'abs_result', 'abs_conclusion', 'sections', 'abs_incremental'],\n",
       "        num_rows: 52\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = issues_dataset['train'].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_dataset['train'] = test['train']\n",
    "issues_dataset['eval'] = test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classification of Words Expressing the Emotional Affects Based on the Subjective Similarity Measures'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset['train'][0]['etitle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file_name', 'language', 'title', 'etitle', 'jabstract', 'eabstract', 'section_names', 'sec_intro', 'sec_method', 'sec_result', 'sec_conclusion', 'abs_intro', 'abs_method', 'abs_result', 'abs_conclusion', 'sections', 'abs_incremental'],\n",
       "        num_rows: 264\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file_name', 'language', 'title', 'etitle', 'jabstract', 'eabstract', 'section_names', 'sec_intro', 'sec_method', 'sec_result', 'sec_conclusion', 'abs_intro', 'abs_method', 'abs_result', 'abs_conclusion', 'sections', 'abs_incremental'],\n",
       "        num_rows: 67\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1163.47it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/builder.py:1917\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1916\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1917\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, table \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m writer\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m>\u001b[39m max_shard_size:\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/packaged_modules/json/json.py:82\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoding_errors) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 82\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# We keep only the field we are interested in\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03ma JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03mkwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/NLP_JP_CORPUS_INCREMENTAL_JUMAN/test.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/load.py:2152\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2149\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2152\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2162\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2163\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2164\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/builder.py:948\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 948\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/builder.py:1043\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1039\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1046\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1047\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1050\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/builder.py:1805\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1803\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1805\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1806\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1807\u001b[0m     ):\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1809\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/builder.py:1950\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, SchemaInferenceError) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1949\u001b[0m         e \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1952\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('json', data_files='./data/NLP_JP_CORPUS_INCREMENTAL_JUMAN/test.json', field='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find '/home/is/kaifan-l/private_room/proj-repos/JP-NLP-Summarization-Dataset/data/NLP_JP_CORPUS_INCREMENTAL_JUMAN/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 5\u001b[0m df_json \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/NLP_JP_CORPUS_INCREMENTAL_JUMAN/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/load.py:2128\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2123\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2124\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2125\u001b[0m )\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2128\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/load.py:1814\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1813\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1814\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1823\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/load.py:1429\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \n\u001b[1;32m   1421\u001b[0m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/load.py:957\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    955\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mas_posix()\n\u001b[1;32m    956\u001b[0m patterns \u001b[38;5;241m=\u001b[39m sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns(base_path)\n\u001b[0;32m--> 957\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m supports_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m _MODULE_SUPPORTS_METADATA\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m supports_metadata \u001b[38;5;129;01mand\u001b[39;00m patterns \u001b[38;5;241m!=\u001b[39m DEFAULT_PATTERNS_ALL:\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/data_files.py:686\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    683\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    685\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 686\u001b[0m         \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m patterns_for_key\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/data_files.py:591\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 591\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m         )\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/datasets/data_files.py:380\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    379\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/home/is/kaifan-l/private_room/proj-repos/JP-NLP-Summarization-Dataset/data/NLP_JP_CORPUS_INCREMENTAL_JUMAN/'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "df_json = load_dataset('json', data_files='data/NLP_JP_CORPUS_INCREMENTAL_JUMAN/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['従来の構文解析法は十分な精度の解析結果を得ることができず，とくに長い文の解析が困難であった．このことは従来の方式が局所的な解析を基本としていたことに原因があり，これを解決するためには文内のできるだけ広い範囲を同時的に調べることが必要である．',\n",
       " '従来の構文解析法は十分な精度の解析結果を得ることができず，とくに長い文の解析が困難であった．このことは従来の方式が局所的な解析を基本としていたことに原因があり，これを解決するためには文内のできるだけ広い範囲を同時的に調べることが必要である．本論文では，そのようにして検出した並列構造の情報を利用して構文解析を行なう手法を示す．長い日本語文の場合は1文内に複数の並列構造が存在することも多い． そこでまず，文内の並列構造相互間の位置関係を調べ，それらの入れ子構造などを整理する．多くの場合，並列構造の情報を整理した形で利用できれば，文を簡単化した形でとらえることができる．そこで，簡単化した各部分に対して単純な係り受け解析を行ない，その結果を組み合わせることによって文全体の依存構造を求めることが可能となる．',\n",
       " '従来の構文解析法は十分な精度の解析結果を得ることができず，とくに長い文の解析が困難であった．このことは従来の方式が局所的な解析を基本としていたことに原因があり，これを解決するためには文内のできるだけ広い範囲を同時的に調べることが必要である．本論文では，そのようにして検出した並列構造の情報を利用して構文解析を行なう手法を示す．長い日本語文の場合は1文内に複数の並列構造が存在することも多い． そこでまず，文内の並列構造相互間の位置関係を調べ，それらの入れ子構造などを整理する．多くの場合，並列構造の情報を整理した形で利用できれば，文を簡単化した形でとらえることができる．そこで，簡単化した各部分に対して単純な係り受け解析を行ない，その結果を組み合わせることによって文全体の依存構造を求めることが可能となる．各部分の係り受け解析としては，基本的に，係り受け関係の非交差条件を満たした上で各文節が係り得る最も近い文節に係るという優先規則によって決定論的に動作する処理を考えた．我々は，このような考え方に基づき，長い文の中に多く存在する並列構造が文節列同士の類似性を発見するという手法でうまく検出できることを示した．']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json['abs_incremental'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
