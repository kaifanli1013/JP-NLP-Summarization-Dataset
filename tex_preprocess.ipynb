{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class TexProcesser():\n",
    "    '''\n",
    "    Latex Preprocessing \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, latex_dir_path, output_path):\n",
    "        self.latex_folder = Path(latex_dir_path)\n",
    "        self.output_path = output_path\n",
    "        \n",
    "    def process_tex_files(self,):\n",
    "        '''\n",
    "        Latex Preprocessing\n",
    "        '''\n",
    "        \n",
    "        for file_path in self.latex_folder.glob('**/*.tex'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                latex_content = file.read()\n",
    "        \n",
    "            matches = self._extract_info(latex_content=latex_content)\n",
    "            print(matches)\n",
    "            json_str = json.dumps(matches, ensure_ascii=False)\n",
    "            \n",
    "            with open(self.output_path, 'a', encoding='utf-8') as file:\n",
    "                file.write(json_str + '\\n')\n",
    "        \n",
    "    def _extract_info(self, latex_content=None):\n",
    "        \n",
    "        patterns = {\n",
    "            'title': re.compile(r'\\\\title\\{(.*?)\\}', re.DOTALL),\n",
    "            'etitle': re.compile(r'\\\\etitle\\{(.*?)\\}', re.DOTALL),\n",
    "            'jabstract': re.compile(r'\\\\jabstract\\{(.*?)\\}', re.DOTALL),\n",
    "            'eabstract': re.compile(r'\\\\eabstract\\{(.*?)\\}', re.DOTALL),                      \n",
    "        }\n",
    "        \n",
    "        matches = {key: [] for key in patterns.keys()}\n",
    "        \n",
    "        for key, pattern in patterns.items():\n",
    "            matches[key] = pattern.findall(latex_content)\n",
    "            \n",
    "        for key, values in matches.items():\n",
    "            for idx, value in enumerate(values):\n",
    "                if key == 'title' or key == 'etitle':\n",
    "                    matches[key][idx] = re.sub(r'\\s*\\\\\\\\\\n\\s*', ' ', value).strip()\n",
    "                elif key == 'jabstract' or key == 'eabstract':\n",
    "                    matches[key][idx] = re.sub(r'\\s*\\n\\s*', ' ', value).strip()\n",
    "        \n",
    "        return matches\n",
    "        \n",
    "        \n",
    "    # def _strip_equation(self,):\n",
    "    #     pass\n",
    "    \n",
    "    # def _extract_title(self, latex_content=None):\n",
    "    #     '''\n",
    "    #     Extract title from latex file\n",
    "    #     '''\n",
    "    #     title = self._match_title(latex_content=latex_content, title_type='title')\n",
    "    #     title = re.sub(r'\\s*\\\\\\\\\\n\\s*', ' ', title)\n",
    "        \n",
    "    #     etitle = self._match_title(latex_content=latex_content, title_type='etitle')\n",
    "    #     etitle = re.sub(r'\\s*\\\\\\\\\\n\\s*', ' ', etitle)\n",
    "        \n",
    "    #     return title, etitle\n",
    "        \n",
    "    # def _match_title(self, latex_content=None, title_type=None,):\n",
    "    #     '''\n",
    "    #     re matching for title/etitle\n",
    "    #     '''\n",
    "    #     if title_type == 'title':\n",
    "    #         pattern = re.compile(r'\\\\title\\{(.*?)\\}', re.DOTALL)\n",
    "    #     elif title_type == 'etitle':\n",
    "    #         pattern = re.compile(r'\\\\etitle\\{(.*?)\\}', re.DOTALL)\n",
    "    #     match = pattern.search(latex_content)\n",
    "        \n",
    "    #     if match:\n",
    "    #         title = match.group(1).strip()\n",
    "    #     else:\n",
    "    #         title = None\n",
    "            \n",
    "    #     return title\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': ['表層表現中の情報に基づく文章構造の自動抽出'], 'etitle': ['Automatic Detection of Discourse Structure by Checking Surface Information in Sentences'], 'jabstract': ['テキストや談話を理解するためには，まずその文章構造を理解する必要があ る．文章構造に関する従来の多くの研究では，解析に用いられる知識の問題 に重点がおかれていた．しかし，量的/質的に十分な計算機用の知識が作成さ れることはしばらくの間期待できない．本論文では，知識に基づく文理解と いう処理を行なわずに，表層表現中の種々の情報を用いることにより科学技 術文の文章構造を自動的に推定する方法を示す．表層表現中の情報としては， 種々の手がかり表現，同一/同義の語/句の出現，2文間の類似性，の3つのも のに着目した．実験の結果これらの情報を組み合わせて利用することにより 科学技術文の文章構造のかなりの部分が自動的に推定可能であることがわかっ た．'], 'eabstract': ['To understand a text or dialogue, one must track the discourse structure. While work on discourse structure has mainly focused on knowledge employed in the analysis, detailed knowledge with broad coverage availability to computers is unlikely to be constructed for the present. In this paper, we propose an automatic method for detecting discourse structure by a variety of keys existing in the surface information of sentences. We have considered three types of clue information: clue expressions, occurrence of identical/synonymous words/phrases, and similarity between two sentences. Experimental results have shown that, in the case of scientific and technical texts, considerable part of the discourse structure can be estimated by incorporating the three types of clue information, without performing sentence understanding processes by giving knowledge to computers.']}\n",
      "{'title': [''], 'etitle': ['A Comparative Study of Automatic Extraction of Collocations from Corpora: Mutual Information vs. Cost Criteria'], 'jabstract': [], 'eabstract': ['While corpus-based studies are now becoming a new methodology in natural language processing, second language learning offers one interesting potential application. In this paper, we are primarily concerned with the acquisition of collocational knowledge from corpora for use in language learning. First we discuss the importance of collocational knowledge in second language learning, and then take up two measures, mutual information and cost criteria, for automatically identifying or extracting collocations from corpora. Comparative experiments are made between the two measures using both Japanese and English corpora. In our experiments, the cost criteria measure proved more effective in extracting interesting collocations such as fundamental idiomatic expressions and phrases.']}\n",
      "{'title': ['並列構造の検出に基づく長い日本語文の構文解析'], 'etitle': [\"A Syntactic Analysis Method of Long Japanese Sentences based on Coordinate Structures' Detection\"], 'jabstract': ['従来の構文解析法は十分な精度の解析結果を得ることができず，とくに 長い文の解析が困難であった． このことは従来の方式が局所的な解析を基本としていたことに原因があり， これを解決するためには文内のできるだけ広い範囲を同時的に調べることが 必要である． 我々は，すでに，このような考え方に基づき，長い文の中に多く存在する並列構造が 文節列同士の類似性を発見するという手法でうまく検出できることを示した． 本論文では，そのようにして検出した並列構造の情報を利用して 構文解析を行なう手法を示す． 長い日本語文の場合は1文内に複数の並列構造が存在する ことも多い． そこでまず，文内の並列構造相互間の位置関係を調べ， それらの入れ子構造などを整理する． 多くの場合，並列構造の情報を整理した形で利用できれば， 文を簡単化した形でとらえることができる． そこで，簡単化した各部分に対して単純な係り受け解析を行ない， その結果を組み合わせることによって文全体の依存構造を求める ことが可能となる． 各部分の係り受け解析としては，基本的に， 係り受け関係の非交差条件を満たした上で各文節が係り得る最も近い 文節に係るという優先規則によって決定論的に動作する処理を考えた． 150文に対して実験を行なったところ， 96\\\\%の文節について正しい係り先を求めることができた．'], 'eabstract': ['Conventional parsing methods can not analyze long sentences precisely, since these methods consider no information in a wide-range series of words in a sentence. We have succeeded in developing a method of detecting coordinate structures by a similarity measure of two arbitrary series of words. This paper describes a method of parsing a sentence by using the information of coordinate structures. A long sentence can be reduced into a shorter form by recognizing coordinate structures in it. Consequently the total dependency structure of a sentence can be obtained by relatively simple modifier/modifiee rules. We report the results of analyzing 150 Japanese sentences to illustrate the effectiveness of this method.']}\n",
      "{'title': [''], 'etitle': ['A System for Finding Translation Patterns by Comparing an MT Result and Its Correction'], 'jabstract': [], 'eabstract': ['When the example-based approach is used for machine translation, it is important to collect a large volume of translation patterns, because most systems use as a translation example a pair of parsed structures in the source and target languages. Such parsed translation pairs are hard to collect. This paper describes a system for finding parsed translation pairs (or translation patterns) that are valid for the translation pattern base by comparing wrong translations and corresponding correct translations.']}\n"
     ]
    }
   ],
   "source": [
    "tex_processer = TexProcesser(latex_dir_path='./NLP_LATEX_CORPUS/V01/', output_path='./data/processed.json')\n",
    "tex_processer.process_tex_files()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
