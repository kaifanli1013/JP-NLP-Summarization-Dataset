file_name,title,etitle,jabstract,eabstract,section_names,sec_intro,sec_method,sec_result,sec_conclusion,abs_intro,abs_method,abs_result,abs_conclusion
V01N01-01.tex,表層表現中の情報に基づく文章構造の自動抽出,Automatic Detection of Discourse Structure by Checking Surface Information in Sentences,"テキストや談話を理解するためには，まずその文章構造を理解する必要があ
る．文章構造に関する従来の多くの研究では，解析に用いられる知識の問題
に重点がおかれていた．しかし，量的/質的に十分な計算機用の知識が作成さ
れることはしばらくの間期待できない．本論文では，知識に基づく文理解と
いう処理を行なわずに，表層表現中の種々の情報を用いることにより科学技
術文の文章構造を自動的に推定する方法を示す．表層表現中の情報としては，
種々の手がかり表現，同一/同義の語/句の出現，2文間の類似性，の3つのも
のに着目した．実験の結果これらの情報を組み合わせて利用することにより
科学技術文の文章構造のかなりの部分が自動的に推定可能であることがわかっ
た．","To understand a text or dialogue, one must track the discourse
structure. While work on discourse structure has mainly focused on
knowledge employed in the analysis, detailed knowledge with broad
coverage availability to computers is unlikely to be constructed for
the present. In this paper, we propose an automatic method for
detecting discourse structure by a variety of keys existing in the
surface information of sentences. We have considered three types of
clue information: clue expressions, occurrence of identical/synonymous
words/phrases, and similarity between two sentences. Experimental
results have shown that, in the case of scientific and technical
texts, considerable part of the discourse structure can be estimated
by incorporating the three types of clue information, without
performing sentence understanding processes by giving knowledge to
computers.","['はじめに', '文章構造のモデルと結束関係', '文章構造の自動抽出', '実験と考察', '結論', '実験テキスト', '手がかり表現のルール']",,,,,,,,
V01N01-02.tex,,A Comparative Study of Automatic Extraction of Collocations from Corpora: Mutual Information vs. Cost Criteria,,"While corpus-based studies are now becoming a new methodology
in natural language processing,
second language learning offers one interesting potential application.
In this paper, we are primarily concerned with the acquisition
of collocational knowledge from corpora
for use in language learning.
First we discuss the importance of collocational knowledge
in second language learning,
and then take up two measures, mutual information and cost criteria,
for automatically identifying or extracting collocations from corpora.
Comparative experiments are made between the two measures
using both Japanese and English corpora.
In our experiments,
the cost criteria measure proved more effective in extracting interesting collocations
such as fundamental idiomatic expressions and phrases.","['Introduction', 'Importance of Collocational Knowledge in Language Learning', 'Extracting Collocations from Corpora', 'Experiments and Discussions', 'Conclusion']",,,,,,,,
V01N01-03.tex,並列構造の検出に基づく長い日本語文の構文解析,A Syntactic Analysis Method of Long Japanese Sentences based on Coordinate Structures' Detection,"従来の構文解析法は十分な精度の解析結果を得ることができず，とくに
長い文の解析が困難であった．
このことは従来の方式が局所的な解析を基本としていたことに原因があり，
これを解決するためには文内のできるだけ広い範囲を同時的に調べることが
必要である．
我々は，すでに，このような考え方に基づき，長い文の中に多く存在する並列構造が
文節列同士の類似性を発見するという手法でうまく検出できることを示した．
本論文では，そのようにして検出した並列構造の情報を利用して
構文解析を行なう手法を示す．
長い日本語文の場合は1文内に複数の並列構造が存在する
ことも多い． そこでまず，文内の並列構造相互間の位置関係を調べ，
それらの入れ子構造などを整理する．
多くの場合，並列構造の情報を整理した形で利用できれば，
文を簡単化した形でとらえることができる．
そこで，簡単化した各部分に対して単純な係り受け解析を行ない，
その結果を組み合わせることによって文全体の依存構造を求める
ことが可能となる． 各部分の係り受け解析としては，基本的に，
係り受け関係の非交差条件を満たした上で各文節が係り得る最も近い
文節に係るという優先規則によって決定論的に動作する処理を考えた．
150文に対して実験を行なったところ，
96\%の文節について正しい係り先を求めることができた．","Conventional parsing methods can not analyze long sentences precisely, 
since these methods consider no information in a wide-range series of words  
in a sentence.
We have succeeded in developing a method of detecting coordinate structures 
by a similarity measure of two arbitrary series of words.
This paper describes a method of parsing a sentence
by using the information of coordinate structures.
A long sentence can be reduced into a shorter form by recognizing 
coordinate structures in it.
Consequently the total dependency structure of a sentence can be 
obtained by relatively simple modifier/modifiee rules.
We report the results of analyzing 150 Japanese sentences 
to illustrate the effectiveness of this method.","['はじめに', '並列構造の検出と文の簡単化', '係り受け解析', '文解析の結果とその評価', 'おわりに']",,,,,,,,
V01N01-04.tex,,A System for Finding Translation Patterns by Comparing an MT Result and Its Correction,,"When the example-based approach is used for machine
  translation, it is important to collect a large volume of
  translation patterns, because most systems use as a translation
  example a pair of parsed structures in the source and target
  languages. Such parsed translation pairs are hard to collect. This
  paper describes a system for finding parsed translation pairs (or
  translation patterns) that are valid for the translation pattern
  base by comparing wrong translations and corresponding correct
  translations.","['Introduction', 'System Overview', 'Example-Based Transfer System', 'Mapper', 'Pattern Extractor', 'Example', 'Discussion', 'Conclusion']",,,,,,,,
V02N01-01.tex,日英機械翻訳における利用者登録語の意味属性の自動推定,Automatic Determination of Semantic Attributes for User Defined Words in Japanese to English Machine Translation,"機械翻訳システムを使用して現実の文書を翻訳する場合, 通常, 
翻訳対象文書に合った利用者辞書が必要となる. 特に, 
高品質翻訳を狙った機械翻訳システムでは, 各単語に対して, 約2,000種
以上の分解精度を持つ単語意味属性の付与が必要であると言われており, 一般
の利用者が, このような精密な情報を付与するのは困難であった.  
そこで本論文では, 利用者が登録したい日本語名詞 (複合名詞を含む) と
英語訳語を与えるだけで, システムがシステム辞書の知識を応用して, 名詞
種別を自動的に判定し, それに応じた単語の意味属性を付与する
方法を提案する. 本方式を, 新聞記事102文とソフトウエア設計書105文
の翻訳に必要な利用者辞書作成に適用した結果, 自動推定方式では, 
専門家の付与した意味属性よりも多くの属性が付与されるが, 40〜80\%の
再現率が得られることが分かった. また, 人手で作成した利用者辞書を使用す
る場合と同等の訳文品質が得られることが分かった. 以上の結果, 利用者辞書
作成への単語の登録において, 最も熟練度の要求される単語意味属性付与作業
を自動化できる見通しとなった.","User dictionaries are important for practical machine
  translation.  However it is difficult for users to enter the
  detailed semantic attributes that a system may require. This paper
  proposes a method of automatically determining semantic attributes
  for noun pairs entered by users. The method compares the Japanese
  and English words with words already in the system dictionary. When
  this method was applied to words in two user dictionaries (for
  newspaper articles and software manuals) it generated more
  attributes than trained humans did, with a recall rate of 40--80\%.
  Evaluation showed that translations using the machine-made
  dictionary were similar in quality to translations using the
  human-made dictionary.  Thus automatic determination of semantic
  attributes removes the need for highly trained lexographers to make
  user dictionaries.","['はじめに', 'システム辞書と利用者辞書', '意味属性推定の方法', '意味属性推定精度の評価', '訳文品質の向上効果', 'あとがき']",,,,,,,,
V02N01-02.tex,"主観的動機に関する意味および語用論的制約を利用した \\ 日本語複文
の理解システム \\ 〜「ので」「のに」による接続を中心として〜","A zero anaphora resolution system \\ for Japanese complex
sentences \\ based on semantic and pragmatic constraints","我々は，接続助詞「ので」による順接の複文と接続助詞「のに」による逆接の複
文を対象とする理解システムを計算機上に構築することを目的とする．この際に
は，ゼロ代名詞の照応の解析が重要な問題となるが，文献\cite{中川:複文の意
味論,COLING94","Our aim is to construct a system which is able to deal with semantics of
Japanese complex sentences.  One of the most important problem to be
solved is the zero anaphora resolution.  According to (Nakagawa, 1994;
Nakagawa \& Nishizawa, 1994), we can use a new pragmatic role called
{\it motivated\/","['はじめに', '日本語複文に関する制約および素性構造による表現', '制約変換による日本語複文の意味解析システム', 'おわりに']",,,,,,,,
V02N01-03.tex,文章内構造を複合的に利用した論説文要約システムGREEN,GREEN: An Experimental System Generating Summary of Japanese Editorials by Combining Multiple Discourse Characteristics,"日本語文章要約システム\G について報告する. 一般に, 質の良い文章要約を
行うためには, ある一つの言語現象だけをとらえた談話解析だけでは不十分
である. なぜなら, 談話に関わる言語現象は相互に関連しているからである. 
本研究ではこの観点から, 日本語での様々な表層的特徴をできるだけ多く利用
して, 日本語文章の要約を試みる. 本稿では実際に計算機上で試作した論説文
要約システム\G に関して, これで用いられている論説文要約の手法の紹介と, 
これによって出力された文章の評価を行う.","This paper describes an experimental system \G \ for summarizing
Japanese texts. Analyzing several aspects of discourse phenomena is in
general indispensable to make a summary of good quality because each
linguistic phenomenon mutually affects in a complex manner. From this
point of view, we have developed a system \G, which is capable of
summarizing Japanese editorial newspaper articles. This system uses
several surface linguistic characteristics appeared in Japanese texts.
We evaluate the performance of the system using a method of
questionnaire where human subjects evaluate the quality of the
summaries generated by the system.","['はじめに', 'システム構成', '要約文選択', '文要約解析', '段落分け解析', '評価', '議論', 'おわりに']",,,,,,,,
V02N01-04.tex,動詞の長さの分布に基づいた文章の分類と和語\\及び合成語の比率,"Classification of Sentences \\by Word-length Distribution of
Verbs\\ with Proportions of Japanese Words\\ and those of Compound Words","文章(テキスト)の執筆者の推定問題などに対して，文章の内容や成立に関す
る歴史的事実の考証とは別に，文章から著者の文体の計量的な特徴を抽出し，
その統計分析によって問題解決を試みる研究が多くの人々の注目をあつめつ
つある．文章に関するどのような要素に著者の特徴が現れるかについて，欧米
文に関してはいくつかの研究の例があるが，それは言語によって異なるとも
考えられるため，欧米文に関する研究成果が日本文の場合にもあてはまるか
について実証的な研究が必要である．また，各言語はその言語における著者の
特徴を表す独特な要素があることも考えられる．本論文では，今まで明らかに
されていない，日本文における動詞の長さの分布に著者の特徴が現れること
と，その結果が動詞中の漢語・和語，合成語・非合成語の使用率の影響では
ないことを著者3人の計21の文章を用いて明らかにした．計量分析の手法とし
ては，同一著者の文章における動詞の長さの分布間の距離の平均値と，異な
る著者の文章における動詞の長さの分布の距離の平均値との差，および距離
マトリックスを用いて主成分分析を行うという方法を用いて数量・視覚的文
章の分類を試みた．","For the identification of a unknown author of a certain literature,
it is significant to find the main elements of writing styles which 
characterize authors or distinguish them.  It was previously  known that the 
authors' writing styles are characterized in  distributions fo lengths of words  are categorized by parts of speech. This paper presents a quantitative analysis of the classification of sentences by word-length distribution of verbs, and the   
relationships between writing styles in word-length distribution  of verbs and the proportions of Japanese words and Chinese words  or those of compound words and non-compound words in verbs. This analysis shows that word-length distribution of verbs characterizes writing styles even where there are no differences in the proportions of Japanese words and Chinese words or those of compound words and non-compound words.","['まえがき', '単語の長さの分布に基づいた文章の分類', '動詞における和語・漢語, 合成語・非合成語の比率', 'むすび']",,,,,,,,
V02N02-01.tex,記述された「併置型駄洒落」の音素上の性質,Several Phonemic Features of Written Puns,"比喩の一種である「駄洒落」は, 言語記号(音声)とその記号が表
す概念の意味との両方に, 比喩を成立させる「根拠(ground)」(比喩における
被喩辞(tenor)と喩辞(vehicle)とを結びつける関係)があるという点で, 高度
な修辞表現に位置づけられる. 筆者らは, 「併置型」と呼ぶ駄洒落の一種(例
「トイレに行っといれ」)を, 外国語専攻の大学生54名に筆記によって創作さ
せ, 203個を収集した. そしてこのデータに対して, 駄洒落理解システムの構
築に必要な知見を得るという観点から, 「先行喩辞」(例では「トイレ」)と
「後続喩辞」(例では「....といれ」)の関係, 及び「出現喩辞」(例では「....
といれ」)と「復元喩辞」(例では「....ておいで」)の関係に着目し, 以下の
３つの分析を行った. (1)先行−後続出現喩辞間の音素列は, どれ位の長さの一致
が見られるか. (2)先行−後続出現喩辞間の音素の相違にはどのような特徴がある
か. (3)出現−復元喩辞間の音素の相違にはどのような特徴があるか. その結果,
出現喩辞の音節数は先行と後続とで一致する場合が多いこと, 先行−後
続出現喩辞間及び出現−復元喩辞間の音素の相違は比較的少なく,相違がある
場合もかなり高い規則性があること, などがわかった. 以上の知見から, 計算
機による駄洒落理解手法, 即ち出現喩辞と復元喩辞を同定するアルゴリズムを
構築できる見通しが得られた.","The author aims to develop a pun recognition system as
pilot research on machine understanding techniques for rhetorical
expressions in natural language. This report describes several
phonemic features of a type of written puns called the ""separately
located type"". A pun of this type essentially consists of two
separately located words called ""actual vehicles"" which are
phonemically distorted from ""restored vehicles"". For example, a pun ""I
am angary with the Hangaryan."" consists of the former actual vehicle
""angary"" which is distorted from the restored vehicle ""angry"", and the
latter actual vehicle ""Hangaryan"" which is distorted from the restored
vehicle ""Hungar- ian"". In this report, the following comparisons are
performed for each of 203 puns : (1) The lengths of phoneme sequences
of the former actual vehicles are compared with those of the latter
ones. (2) The phonemes of the former actual vehicles are compared with
those of the latter ones. (3) The phonemes of the actual vehicles are
compared with those of the restored vehicles. Results of these
comparisons suggest that there are relativery few phonemic distortions
but they are regular. These results are useful in developing a pun
recognition system.","['まえがき', '分析のための準備', '併置型駄洒落の音素上の分析', '考察と課題', 'むすび']",,,,,,,,
V02N02-02.tex,,Design of Efficient Unification for Natural Language,,"Graph unification has become a central processing mechanism of many
natural language \hspace{0.2mm","['Motivation', 'Our Observations and Criteria', 'The Quasi-Destructive Graph Unification Algorithm', 'Empirical Results', 'Discussion: Comparison to Other Approaches', 'Conclusion']",,,,,,,,
V02N02-03.tex,,Integration of Morphological and Syntactic Analysis based on LR Parsing Algorithm,,"Morphological analysis of Japanese is very different from that of
English, because no spaces are placed between words. This is also the
case in many Asian languages such as Korean, Chinese, Thai and so
forth. In the Indo-European family, some languages such as German have
the same phenomena in forming complex noun phrases. Processing such
languages requires the identification of the word boundaries in the
first place. This process is often called {\em segmentation\/","['Introduction', 'Morphological analysis of Japanese', 'Generating LR table', 'Algorithm', 'A worked example', 'Conclusion']",,,,,,,,
V02N02-04.tex,,A New Direction for Sublanguage N.L.P.,,"There have been a number of theoretical studies devoted to the notion of
sublanguage.
Furthermore, there are some successful natural language
processing systems which have explicitly or implicitly utilized
sublanguage restrictions. 
However, two big problems are still unsolved to utilize the sublanguage
notion:
1) automatic definition and dynamic identification of a text to sublanguage, and
2) automatic linguistic knowledge acquisition for sublanguage.
There are now new opportunities to address these problems
owing to the appearance of large machine-readable corpora.
Although there have been several experiments to try to solve
the second problem listed above,
the first problem has not received so much attention.
In the previous sublanguage N.L.P. systems, 
the domain the system is dealing with was defined by a human.
This is actually one method to define the sublanguage of a text,
and, in a sense, it seems to work well.
However, it is not always possible and sometimes it may be wrong.
In order to maximize the benefit of the sublanguage notion,
we need automatic definition and dynamic sublanguage identification.
We will report preliminary experiments on sublanguage definition and
identification based on lexical appearance.
The results of the experiments show that
the methods proposed can be useful in processing a new text.
In particular, the fact that the first two sentences can reliably identify
a text's sublanguage encourages us in further investigation of this line of research.
In conclusion, it appears that the inductive definition of sublanguage
and sublanguage identification would be beneficial for natural language processing.","['Introduction', 'Two Definitions of Sublanguage', 'Benefit from Large Corpora', 'Sublanguage Definition', 'Sublanguage Identification for a New Text', 'Experiment with First Two Sentences', 'Discussion', 'Conclusion', 'Acknowledgments']",,,,,,,,
V02N03-01.tex,言語過程説に基づく日本語品詞の体系化とその効用,A Japanese Syntactic Category System Based on the Constructive Process Theory and its Use,"三浦文法は、時枝誠記により提唱され三浦つとむにより発展的に継承された
言語過程説に基づく日本語文法である。言語過程説によれば、言語は対象−認
識−表現の過程的構造をもち、対象のあり方が話者の認識を通して表現されて
いる。本論文では、三浦文法に基づいて体系化した日本語品詞体系および形態
素処理用の文法記述形式を提案し、日本語の形態素処理や構文解析におけるそ
の有効性を論じた。日本語の単語を、対象の種類とその捉え方に着目し、約４
００通りの階層化された品詞に分類して、きめ細かい品詞体系を作成した。本
論文で提案した品詞体系と形態素処理用文法記述形式に基づき、実際に形態素
処理用の日本語文法を構築した結果によれば、本文法記述形式により例外的な
規則も含めて文法を簡潔に記述できるだけでなく、拡張性の点でも優れている
ことが分かった。本品詞体系により、三浦の入れ子構造に基づく意味と整合性
の良い日本語構文解析が実現できるものと期待される。","Miura grammar is a Japanese grammar based on the
Constructive Process Theory\\proposed \hspace*{0.7mm","['まえがき', '三浦文法の言語観', '日本語の品詞体系', '形態素処理用の文法記述形式', '本品詞体系の効用', 'むすび']",,,,,,,,
V02N03-02.tex,,Example-Based Machine Translation using Associative Processors,,"This paper proposes an Example-Based Approach (EBA) 
using Associative Processors (APs) for machine translation, 
especially speech-to-speech translation, that  requires (1) high accuracy and 
(2) a quick response.
EBAs translate by mimicking the  best-match 
translation examples (hereafter, ``examples''), which are derived from corpora.
These approaches are known to perform structural disambiguation, target word
selection, and whole translation accurately.
Therefore, EBAs fulfill the first requirement.
The second requirement is also fulfilled by an EBA using APs as follows.
The central mechanism of EBAs, Example-Retrieval (ER), retrieves 
the examples most similar to the input expression from an example database. ER
becomes the dominant component
as the size of the example database increases.
We have parallelized
ER by using APs  consisting
of an Associative Memory and a 
Transputer.
Experimental results show that  ER can be
drastically accelerated by our method.
Moreover, a study of communication among APs and 
an extrapolation from  the sustained performance of 10 APs
demonstrate the scalability of our method against the  size of 
the example database.
Consequently, the EBA using APs meets the critical requirements of 
machine translation.","['Introduction', 'Example-Based Approach', 'Analysis of Computational Cost', 'Acceleration Using Associative Processors', 'Scalability toward Large-Scale System using \nAssociative Processors', 'Related Research', 'Concluding Remarks']",,,,,,,,
V02N03-03.tex,動詞訳語選択のための「格フレーム木」の統計的な学習,Statistical Learning of ``Case Frame Tree'' for Translating English Verbs,"機械翻訳システムでは動詞の訳語を選択するために格フレームがよく利用さ
  れる．格フレームは従来主として人手で記述されていたが，一貫性を保って
  記述するのが難しいこと，格フレームを部分的に変更した場合に起こる影響
  が把握しにくいことなどの重大な問題があった．そこでこれらの問題を解決
  するため，本論文では格フレームを決定木の形で表し(これを格フレーム木
  と呼ぶ)，これを英日の対訳コーパスから統計的な帰納学習プログラムを利
  用して学習することを提案する．
  本論文ではまず，この提案によって上記の問題が軽減される根拠を述べた後，
  本論文で作成した英日対訳コーパスについて述べる．続いて7つの英語動詞
  について格フレーム木の獲得実験を2つ報告する．
  最初の実験は，格要素の制約として英語の単語を使う格フレーム木を学習し
  たものである．これにより得られた格フレーム木を観察したところ，人間の
  直観に近く，かつ直観を越えた非常に精密な訳し分けの情報が得られたこと
  が明らかになった．
  次に，この格フレームの一般性を高めるために，英語の単語の代わりに意味
  分類コードを制約として利用する手法を提案し，これに基づいて格フレーム
  木を学習する実験を行った．得られた格フレーム木で未学習のデータの動詞
  の訳語を決定する評価を行ったところ，2.4\%ないし32.2\%の誤訳率が達成
  された．この誤訳率と，先の英語単語を利用した格フレーム木での誤訳率と
  の差は13.6\%ないし55.3\%となり，意味分類コードが有効に機能したことが
  示された．","Our English to Japanese machine translation system uses
  surface verbal case frames (case frames) to select a Japanese
  translation for an English verb.  The need to acquire and accumulate
  case frames leads directly to two problems:
\begin{itemize","['はじめに', '人手による格フレーム獲得の問題点', '対訳コーパスからの格フレーム木の学習', '英日対訳コーパス', '語形を利用した実験', '意味コードを用いた実験', 'むすび', '決定木作成アルゴリズム', 'コーパスに付与したラベルの詳細', '実験で学習された格要素']",,,,,,,,
V02N03-04.tex,片方向の共起性による述語型定型表現の自動抽出,Automatic Acquisition of Predicative Frozen Patterns on One Directional Association,"本論文では「目を盗む」や「かたずを飲む」などの述語型定型表現を
コーパスから自動抽出することを目的に，
従来の相互情報量の条件を緩める方向で，
名詞動詞間の共起性を測る新たな基準を提案する．
概略，名詞，動詞のどちらかを固定して，
その単語と共起する集合内の各単語に，どの程度特異な頻度になっているかの
数値を与える．この数値は集合内のその単語の頻度の割合と，
集合内の単語の種類数から計算される．
この数値の上位のものを取り出すことで定型表現の抽出を行う．
本手法の特徴は，名詞を固定した場合に抽出できる表現と，動詞を固定した場合に
抽出できる表現はほとんど共通のものがなく，しかもどちらの場合も
相互情報量による抽出程度の正解率を得られることである．
このため，目的の抽出数の半数づつを
名詞固定と動詞固定の各々の場合から取り出せば，
相互情報量を用いて抽出する場合よりも高い正解率が得られる．","This paper presents an alternative method to measure word
association strength on predicative patterns in order to
automatically extract predicative frozen patterns and idioms from a
corpus.  For this aim, mutual information is traditionally used.  We
improve the method on mutual information from a view of linguistics.
The proposed method are realized by following steps.  First, a verb
(or noun) is fixed.  Next, the set of nouns (or verbs) which
associates the verb (or noun) is built up.  Last, nouns (or verbs)
which have peculiar frequency are chosen from this set.  The
peculiarity is confirmed from two characteristics, which are ratio
of the word frequency for total frequency of the set, and the number
of kind of word in the set.  Predicative frozen patterns are
constructed from chosen words and the fixed word.  The advantage of
this method is that patterns extracted by fixing a verb and patterns
extracted by fixing a noun have few common patterns, and each
extraction has equivalent ratio of correctness to extraction by
mutual information.  Therefore, extracting same number patterns,
this method can get more correct patterns than mutual information.","['はじめに', '相互情報量からの共起性測定法の問題点', '片方向の強さのみによる共起性の測定', '実験', '考察', 'おわりに']",,,,,,,,
V02N04-01.tex,日本語マニュアル文における条件表現\protect 「と」「れば」「たら」「なら」から導かれる制約,Pragmatic Constraints of Japanese Conditionals \protect\\ for {\sf TO,"本稿では、日本語マニュアル文の理解を行なう際に必要となるゼロ代名詞の照
応問題を解決する一つの手がかりとして，マニュアル文の操作手順においてしば
しば現れる条件表現の語用論的性質を利用することを提案する．条件表現の前件
と後件を動作主の種類と述語の性質により分類するという方法により，実際の例
文を調べた結果，後件に関して，1)「と」と「れば」，「たら」と「なら」がそ
れぞれ同じ分布を示すこと，2)「と」「れば」と「たら」「なら」は相補的な分
布になっていること，が分かった．この性質より，動作主に関するゼロ代名詞の
照応に利用できる制約ならびにデフォールト規則が得られた．","This paper proposes a method of the zero anaphora
resolution, which is one of the essential processes in Japanese manual
sentences understanding system which uses pragmatic properties of
Japanese conditionals.  We examined a number of sentences in Japanese
manuals according to the classification based on the types of agents and
the types of verb phrase.  As a result, we obtained the following
pattern of usage in main clauses: 1) The connective particles {\sf TO","['背景と目的', 'マニュアル文における基本的制約', '条件表現の{\\dg 主', 'おわりに']",,,,,,,,
V02N04-02.tex,日本語マニュアル文におけるテイル，テアル，テオク，\\テミル，テシマウの語用論,Pragmatic Analysis of Aspect Morphemes \\in Manual Sentences in Japanese,"本論文では，テイル，テアル，テシマウ，テオク，テミルといったアスペクト辞の
マニュアル文における意味を検討する．これらのアスペクト辞は，時間的なアス
ペクトを表す他に，書き手の態度などのいわゆるモダリティをも表現することが
あるので，モダリティについての解釈から，アスペクト辞の隣接する動詞句の主語
に関する制約を明らかにする．さらに，実際にマニュアル文から例文を集め，提
案する制約の正当性を検証する．このような制約は，省略された主語などの推定
に役立ち，マニュアル文からの知識抽出や機械翻訳に応用できる．","In this paper, we analyze the pragmatic aspects of Japanese verbal
suffixes {\it teiru","['はじめに', 'マニュアル文に登場する人物と事物', 'アスペクト辞の意味と，動詞との隣接について', 'マニュアル文における{\\dg 主', 'テシマウの被害性に関する考察', 'おわりに']",,,,,,,,
V02N04-03.tex,日本語形態素解析システムのための形態素文法,A Morpheme Grammar\\for Japanese Morphological Analyzers,"本稿では，動詞の語尾変化について体系的な扱いが可能な派生文法に基づいて，
日本語形態素解析システムのための形態素文法を記述した。但し，派生文法に
おける音韻単位での扱いを日本語の文字単位の扱いに変更する方法を示し，よ
り形態素解析処理に適した形で記述した．さらに，これを実働する形態素解
析システムに適用し，EDRコーパスと比較することによって精度を測定した．","This paper shows a morpheme grammar for Japanese morphological
analyzers. The grammar bases on Derivational Grammar in which the
inflection of Japanese verbs can be managed systematically.  Though
Derivational Grammar uses phonological description, this paper shows a
method to process Japanese character strings di- rectly. A Japanese
morphological analyzer with our grammar is evaluated with EDR corpus.","['まえがき', '形態素の体系', '派生文法による動詞の語尾変化', '形態素解析システムへの適用', '問題点の検討', '性能評価', 'まとめ']",,,,,,,,
V02N04-04.tex,話者の対象認識過程に基づく日本語助詞「が」 と「は」の意味分類とパーザへの実装,Semantic Classification of Japanese Particle ``ga'' and ``ha'' based on the Viewpoint of Speaker's Recognition and its Implementation on a Paser,"本論文では,話者の対象認識過程に基づく日本語助詞「が」と「は」の
意味分類を行ない, これを一般化 LR 法に基づいて構文解析する SGLR パーザの
上に実装し, その有用性を確認した結果について述べる. 
話者の対象認識過程とは, 対象を認識し, それを言語として表現する対象を概念化し, 
対象に対する話者の見方や捉え方, 判断等を識別する過程のことをいう. 
筆者らは, 特に, 三浦文法に基づいて考案された日本語の助詞「が」と「は」, 
及び「を」と「に」についての意味規則を考案し, これを用いてその規則の
動作機構を DCG の補強項で実現し, SGLR パーザで実行できるようにしている. 
実験の結果, 意味解析と構文解析の融合に成功し, 構文的曖昧性を意味分類により, 
著しく削減できることがわかった.","This paper proposes a semantic classification of Japanese
particle ``ga'' and ``ha'' based on the viewpoint of speaker's
recognition and its implementation on SGLR \hspace*{0.4mm","['まえがき', '言語の過程的構造', '助詞「が」と「は」の意味分析', '助詞「が」「は」のコア概念', '助詞の意味分類', 'パーザの基本的枠組', '文法と辞書の試作', 'パーザへの実装', '結論']",,,,,,,,
V03N01-01.tex,英語名詞の多義性解消における文脈としての場面情報の評価,Evaluation of Scene Information as Context for English Noun Disambiguation,"本論文では，意味解析を主とする自然言語処理システムにおける語
の意味の曖昧性解消の処理の効率化を場面に基づく文脈情報を利用
して実現する方法を提案する．現在における文脈依存の意味処理で
は文脈の定義方法と知識の獲得方法が問題とされる．また実装時に
は意味選択の組合せ爆発による計算量の大きさが問題となる．文脈
情報により一部の語に対してでも語義の優先順位づけが有意に行な
われれば，共起関係などを用いて他の語の多義性や構文解析の解空
間を早く絞り込むことができる．本論文では，談話内の言語外知識
である場面情報を空間的連想による文脈情報と位置付け，画像理解
による知識獲得の近似として視覚辞書を利用し，実際の物語文を対
象にし評価した結果を示す．場面に関連する各英語名詞に対し独立
に平均 1.5 倍以上の処理速度向上が得られている．また多義性解
消率は全く情報がない場合の 51 \% に比べ本手法では 83 \% まで
上昇する．あわせて場面情報の知識表現の違いによる効果の違いに
ついての考察，手法の限界点，およびシステムに加えて必要とされ
る場面解析機構についての考察を述べる．","We propose an efficient method to disambiguate English nouns
using scene information as context. Recent research directions
into context dependent analysis point out the difficulty of 
defining context and acquiring knowledge. Another problem is 
the efficiency of resolving semantic constraints. To resolve 
these problems, we use knowledge from a pictorial dictionary
available on the market. It provides spatial-scene information
as extra-lingual knowledge required for discourse analysis.
One of its targets is to disambiguate words. We compared our 
method with random search approach on a narrative story. The 
experimental analysis supports that the word-disambiguating 
speed with our method is over 1.5 times faster than that with
the random search. Also it indicates that the disambiguating 
rate with our method is 83 \%, higher than that with the random 
search (51 \%). We discuss the importance of the representational
type for the scene information, evaluate our method's limits, 
and argue future technologies required to analyze the spatial-scene.","['まえがき', '文脈としての場面情報の構築', '文脈としての場面情報を利用した多義語の優先順位づけ', '実験と評価', '考察', '今後の課題', 'むすび']",,,,,,,,
V03N01-02.tex,名詞間の意味的共起情報を用いた複合名詞の解析,Analysis of Japanese Compound Noun \\Using Collocational Information,"複合名詞は名詞を結合することによって数限りなく生成できるので，
全てを辞書に登録することは不可能である．したがって，辞書に登録されてい
る名詞の組み合わせとして複合名詞を解析する手法が必要である．そのために
は，複合名詞をそれを構成している名詞に分割し，名詞間の係り受け構造を
同定しなくてはならない．
これらの処理は統語的な手係りが少ないために難しく，
何らかの意味的な情報が必要である．
しかし，大規模な意味的情報を人手で構築し保守することはコストが
大きいため，計算機によって自動的に知識を獲得することが望ましい。
本論文では，コーパスから自動的に抽出した名詞間の意味的共起情報を用いて
複合名詞の構造を解析する方法を提案する．この方法では，共起情報を統計的
に処理して名詞間の意味的関係の強さを評価し，係り受け関係の曖昧性解消に
利用する．
まず，4文字漢字語16万語から意味クラスの共起データを抽出した．
抽出した共起データから統計的に名詞間の意味的関係の強さを
計算する．そのための尺度として相互情報量を基にした評価尺度を提案する．
この尺度と複合名詞の構造に関するヒューリスティクス，機械可読辞書から得
られる言語知識を用いて複合名詞を解析する．評価のために新聞や用語集から
抽出した漢字複合名詞を解析し，平均語長5.5文字の漢字複合名詞を約78\%の
精度で解析できた．","Analyzing compound nouns is one of the crucial issues for
  natural language processing systems. Registering all compound nouns
  in a dictionary is an impractical approach, since we can create a
  new compound noun by combining nouns.  Therefore, a mechanism to
  analyze the structure of a compound noun from the individual nouns
  is necessary. However, the analysis are difficult only when using
  syntactic knowledge. Therefore, we have to use semantic knowledge.
  It is hard to construct and maintain a large semantic knowledge\_base,
  so we need a method to acquire semantic knowledge and use such the
  knowledge for the analysis.  In this paper, we propose a method to
  analyze structures of Japanese compound nouns by using word
  collocational information and a thesaurus. The collocational
  information is acquired from a corpus of four {\it kanzi","['はじめに', '名詞の意味的クラスの共起情報の獲得', '共起情報を用いた複合名詞の解析', '実験', 'ヒューリスティクスの導入による解析方法の改良', '実験2', 'おわりに']",,,,,,,,
V03N01-03.tex,,Towards a Linguistic Treatment of Compounds in a Machine Translation Environment,,"This paper concentrates on research into the translation of
compounds in a transfer based MT system. We determine how
contemporary linguistic theory can contribute to the
characterisation, representation and translation of compounds. We
determine productive, compositional and translatable compounds.
We discuss evolving strategies for effecting translation of these
compound types. The results obtained are readily adaptable to
models which have a stratificational linguistic framework are
able to emulate feature value percolation. The research
undertaken was reductionist in nature, leading to a set of
compositionally translatable productive compound types being
isolated.","['Introduction', 'Definition of compounds', 'Characteristics of compounds', 'State of the Art - Linguistics', 'State of the Art - NLP', 'Proposed abstract representation for transfer', 'Calculation of IS representation', 'Problems']",,,,,,,,
V03N01-04.tex,名詞の指示性を利用した日本語文章における名詞の指示対象の推定,An Estimate of Referent of Nouns in Japanese Sentences with Referential Property of Nouns,"日本語文章における名詞の指す対象が何であるかを把握することは，
対話システムや高品質の機械翻訳システムを実現するために必要である．
そこで，本研究では
名詞の指示性と修飾語と所有者の情報を用いて名詞の指示対象を推定する．
日本語には冠詞がないことから，
二つの名詞が照応関係にあるかどうかを判定することが困難である．
これに対して，我々は冠詞にほぼ相当する名詞の指示性を
表層表現から推定する研究を行なっており\cite{match","It is important to clarify 
referents of nouns in machine translation and \mbox{conversational","['はじめに', '名詞の指示性', '名詞の指示対象の推定方法', '代名詞等の指示対象の推定方法', '名詞等の指示対象を推定する枠組', '実験と考察', 'おわりに']",,,,,,,,
V03N02-01.tex,日本語の終助詞の機能 \\ --- 「よ」「ね」「な」を中心として ---,"Function of Japanese Sentence Final Particles \\ 
         ---about `YO' `NE' and `NA'---","終助詞「よ」「ね」「な」は，書き言葉の文には殆んど用いられないが，日常
会話において頻繁に使われており，文全体の解釈に及ぼす影響が大きい．  そ
のため，機械による会話理解には，終助詞の機能の研究は不可欠である．
本論文では，代表的な終助詞「よ」「ね」「な」について，階層的記憶モデ
ルによる終助詞の機能を提案する．まず，終助詞「よ」の機能は，文の表す命題が発
話以前に記憶中のある階層に存在することを表すことである．次に，終助詞「ね」「な」
の機能は，文の表す命題を記憶中に保存する処理をモニターすることである．
本稿で提案する機能は，従来の終助詞の機能が説明してきた終助詞
「よ」「ね」「な」の用法を全て説明できるだけでなく，従来のものでは
説明できなかった終助詞の用法を説明できる．",Japanese \hspace{-0.2mm,"['はじめに', '認知主体の記憶のモデル', '終助詞「よ／ね／な」の機能', 'おわりに']",,,,,,,,
V03N02-02.tex,ネットニュースグループfj.wantedの \\ ダイジェスト自動生成,~\vspace*{10mm,"本稿では，fj.wantedのダイジェストの自動生成を実現する方法について述べる．
その中心技術は，ニュース記事からのサマリ抽出法である．
この方法は，言わば「斜め読みを模擬した処理」であり，
まず，表層的な表現を手がかりとして，42の特徴を抽出し，
それらの特徴を用いて，記事のサマリ（カテゴリとサマリ文）を抽出する．
ブラインドデータに対する実験において，本方法は，カテゴリ判定正解率81\%，
サマリ文抽出正解率76\%という値を示した．
抽出されたサマリはカテゴリ毎に整理され，
HTML形式のダイジェストとして出力される．
このとき，元の記事へのポインタは，ハイパーテキストのリ
ンクとして埋め込まれる．
作成されたダイジェストは，WWWのクライアントプログラムによって
読むことができる．","This paper proposes an automatic digesting system for a Network 
Newsgroup ``fj.wanted''.
The key component of the system is the {\em summary extraction","['はじめに', 'ニュースグループfj.wanted', 'サマリ抽出', '実験', 'ダイジェストシステム', '議論', '関連研究', 'おわりに']",,,,,,,,
V03N02-03.tex,国文学研究とコンピュータ -- 電子本「漱石と倫敦」考を作る,"Japanese Literary Research by Computer -- Creating an Electronic Book on the Study of ""Soseki in London""","コンピュータを用いて，文学研究を進めるための検討を行った．
これには，研究過程の中核である研究ファイルの組織化が必要で，そのモデル
を定義した．モデルは研究過程で利用され，生成される様々な情報資源の構造
認識による組織化である．モデルの検証のために，具体的な文学テーマを設定
し，その実装を行い，評価した．その試みとして電子本「漱石と倫敦」考の研
究開発を進めている．
  研究者による評価実験では，概して評判がよい．例えば，漱石の作品倫敦塔
を読む場合に，各種参照情報を随時に利用できること，メモなどを自由に書き
込むことができ，自分の研究環境の整備がコンピュータ上で可能であることが
評価されている．さらに，モデルは実際の文学研究に有効であること，とくに
教育用ツールとして効果的であるとの評価が得られた．","This paper describes a study on the Japanese classical literary
researches by the computer. A model has been developed with a
definition and description for the structure of the
research process, which is named the FR.(Files for Research) system.
The model is considered as the simulation of the research process and
organization of materials, information, data, etc., which are widely
used in the research processing.
 Then, a prototype system also has set up to the personal
computer, Macintosh, under the FR. model for the feasibility tests. The
system designs as an electronic book named the
""Soseki in London"" based on the substantial model for the
Japanese literary work. In the experiments, the book has
been evaluated highly, that is, it is available and effective
for the researching in the literary work and particularly
is useful for educational applications.","['まえがき', '国文学研究におけるコンピュータ', '研究ファイルのモデル', '実験的FR「漱石と倫敦」考', 'あとがき']",,,,,,,,
V03N02-04.tex,順接複文における主語の共参照関係の分析,"Analysis of Coreference between Subjects of \\Main and
Subordinate Clauses for Japanese \\{\it NODE","日本語においては主語が頻繁に省略されるため，省略された主語す
なわちゼロ主語の指示対象同定が重要である．複文は従属節と主節からなるので，
主節主語と従属節主語がある．したがって，複文の理解に不可欠なゼロ主語の指
示対象同定の問題は，2段階に分けて考えるべきである．第一の段階では，主節
主語と従属節主語が同じ指示対象を持つかどうか，すなわち共参照関係にあるか
どうかの分析である．第二の段階では，第一段階で得られた共参照関係を利用し
て，実際のゼロ主語の指示対象同定を行なう．このうち，第一の共参照関係の有
無は，複文のゼロ主語の扱いにおいて固有の問題であり，本論文ではこの第一の
問題について主として小説に現れるノデ，カラで接続される順接複文について分
析した．分析は，主節および従属節の述語の意味をIPAの動詞形容詞辞書の分類
に従って分類し，各々の述語がどのような分類の場合において共参照するかしな
いかを調べた．この結果，共参照関係の同定に有力であるいくつかのデフォール
ト規則を見い出した．","Since complex sentences consist of more than two clauses, we have
more than two subjects in one complex sentence. Therefore it is
important to recognize coreference relations among these subjects for
identifying the referents of these subjects. We focus here on subjects
coreference relations of Japanese complex sentences conjoined by {\it
node","['はじめに', '順接複文の性質', 'IPALの述語の素性', '例文の分析', 'おわりに']",,,,,,,,
V03N02-05.tex,構文付きコーパスの作成と類似用例検索システムへの応用,"Building a Large Corpus with Skeltal Syntactic \\ 
 	Structure and its Application to Similar Sentence Retrieval System","本論文では，まず，表層的情報のみを用いて安定的・高精度に構文
解析を行う骨格構造解析の方法について述べる．次に，これを用い
て行った 8万用例文に対する構文付きコーパスの作成について触れ，
骨格構造解析の有効性について述べる．さらに，この構文付きコー
パスを対象として構築した類似用例検索システムについて述べる．
本システムは，
 (1) 構文的制約(係り受け構造)を指定して検索できるので，単語レベルの
検索では検索されてしまうような多くの不適切な用例を絞り込むことができる
 (2) 分類語彙表を利用した意味コード化により，類似用例の検索も
可能である
 (3) インデックスに構文情報を含めることにより，高速な構造検索を実現して
いる
 (4) ユーザはウインドウ上で，三角表を用いた構文構造のブラウジング
，検索パターンの指定などができ，使いやすいインタフェースを実現している，
などの特徴がある．","In this paper we discuss firstly a method for skeltal syntactic
analysis which uses only surface information.
By this method we can successfully abstract the skeltal syntactic structure
of a long Japanese sentence. 
Secondly we discuss its use on building a large corpus with skeltal
syntactic structure consisting of about 80 thousand example sentences
from two Japanese-English dictionaries.
Thirdly we discuss our development of a similar sentence retrieval
system over this corpus. 
The advantage of this system is that 
(1) Syntactic structure as input search pattern enables the system to
dismiss unnecessary example sentences. Thus it raises the precision
of the sentence retrieval.
(2) Similar sentence retrieval realized on the basis of word encoding
according to  the thesaurus hierarchy.
(3) Index table including syntactic information enables the system to
make high speed retrieval.
(4) The system has an excellent graphical user interface using the triangle
representation of syntactic structure.","['はじめに', '構文付きコーパス', '類似用例検索システム', '検索実験', 'おわりに']",,,,,,,,
V03N03-01.tex,日本語の述部階層構造に基づく形態論的な文法規則の記述法,A Method for describing Japanese Grammar Rules \hspace*{8mm,"自然言語処理システムに求められている分析性能が向上するにつれ
て，そのシステムで用いる文法規則や辞書データといった言語知識
ベースも複雑化，巨大化してきた．一方，自然言語処理システムを
用いる応用分野がますます多様化することが予想され，応用分野ご
とに新たな分析性能が要求される．言語知識ベースにおいても追加
と修正の作業が発生する．しかし，現状では，その開発には多数の
人員と多くの時間を必要とするため，言語知識ベースの再構築は困
難な作業である．応用分野に適合するシステムを，より効率的に開
発する手段が必要である．そのためには，融通性を持ち容易に修正
できる文法規則や辞書データの作成技法と，作成された言語知識ベー
スの保守性の向上を図る必要がある．この課題は，応用分野の多様
化に伴う需要と規模が増大する中でますます重要となっている．
この稿では，この技術課題に対して，言語知識ベースのうち，文法
規則の系統的な記述の方法を提案し，その方法に従って作成した機
械処理を指向した文法規則について述べる．まず，形態素と表層形
態の概念区分をした上で，日本語の持つ階層構造に注目した．形態
素の述部階層位置との関係から，表層での形態の現れ方を構文構造
に結び付ける形態構文論的な文法作成のアプローチを採用し，文法
規則の開発手続きを確立した．融通性を持ち容易に修正できること
を例証するため，試作した文法規則を新聞の論説文の分析に適用し，
分析の出来なかった言語現象を検討した．そして，その言語現象を
取り上げて，これを新たな分析性能を満たす要求仕様と見なし，同
じ手続きを用いて文法規則を拡張した．この結果，拡張した文法規
則の分析性能が漸増していることを確認した．
系統的な記述の手続きに従うことによって，文法規則記述の一貫性
を維持しながら，その分析性能を向上させることが可能となった．
このため工学上，文法規則の開発作業手順に一般性が生じ，開発時
間を短縮することができる．試作した文法規則は実際に計算機上に
実装している．本稿は機械処理を指向した文法規則記述のノウハウ
を体系化する試みとして位置づけられる．","As the analysis performance of the Natural Language
Processing (NLP) systems has improved, grammar rules and
lexicons to be used with NLP systems become complicated and
large than before.  On the other hand, it must be expected
that new application filed using a NLP system is enlarging.
The underlying task is to develop and modify the NLP systems
with reconstruction of the grammar rules and lexi- cons that
has the desired performance.
Since the work needs many staffs and much time for the
development, reconstruction of grammar rules and lexicons is
hard work and also expensive.  Methods to develop more
efficiently the desired system which can be applied to new
application filed are required.
In this paper, a new method of a systematic description of
grammar rules is proposed for the purpose of creating
grammar rules and modify them. The way of abstracting
grammar rules from linguistic phenomena has been studied
with special reference to the formation of morph-syntactic
structure of Japanese. Based on the generalization of the
way, the new method was made for further improvement for
grammar writing.
The paper also describes the grammar rules created by using
the proposed method to illustrate the effectiveness of the
method. The grammar rules was applied to analysis of a set
of newspaper editorial sentences.
First, the linguistic phenomenon that the analysis was not
completed were specified and extracted. We considered that
the phenomenon were a demand specification of meeting a new
analysis performance. According to the specification, the
grammar rules were extended using the same procedure of the
method. Next, we checked that the analysis performance of
the extended grammar rules was increasing.
The result shows that it becomes possible to raise the
analysis performance, maintain consistency of grammar rule
description by following the method through the experiment. 
Generality raises for development work procedure of a
grammar rule on engineering. Development time can also be
shortened.
The grammar rules described as an experiment runs on
computer and are published from ICOT(Institute for New
Generation Computer Technology). The proposed method in the
paper is considered to be a trial which systematizes
know-how of description of grammar rules as a knowledge
base.","['はじめに', '文法規則の体系だった記述法', '文法規則の記述の手順', '記述手続きの評価', 'おわりに']",,,,,,,,
V03N03-02.tex,被喩詞の意味と比喩表現の意味との違いを示す指標,"The Measurement of the meaning difference \\ 
between tenor and metaphor/simile","本稿では，比喩の理解過程における再解釈の段階について考察し
た．名詞の意味を確率を利用して表現した．比喩表現の意味は，喩詞の意味に
影響された被喩詞の意味である．被喩詞の意味と比喩表現の意味との違いを示
す指標として，明瞭性と新奇性とを 情報量を用いて 定義した．明瞭性が大き
い値となるときは，比喩表現において属性に関する不確定さが減少したときで
ある．新奇性が大きい値となるときは，比喩表現が稀な事象を表わすときであ
る．SD法により比喩表現の意味を測定し，明瞭性と新奇性とを求めた．明瞭性
が属性の顕著性のパターンに対応する数値であることと，明瞭性と新奇性とが
比喩の理解容易性の指標として適当であることとを示した．","This paper describes the re-interpretation stage of
understanding metaphor/simile. We define the meaning of a noun by
using probability. The meaning of a metaphor/simile is the meaning of
the tenor which is affected by the vehicle. We define {\it
``clarity''\/","['はじめに', '情報量を用いた 比喩の定式化', 'SD法による比喩表現の意味の測定', 'おわりに']",,,,,,,,
V03N03-03.tex,人手作成ルールと事例に基づく英語動詞選択ルールの学習,~~~~~~~~~~~~~A Revision Learner\\ to Acquire English Verb Selection Rules,"機械学習により日英翻訳のための英語動詞選択ルールを獲得する
手法を提案する．英語動詞選択ルールの学習手法としては，既に，翻訳事例の
みから獲得する手法が知られている．この従来の翻訳事例のみから獲得する手
法では，ルールの正解率を向上させるために，多数の事例を必要とする．しか
し，現実には，動詞出現頻度の偏りにより，全動詞に十分な翻訳事例を収集す
る事は極めて困難である．そこで，本論文では，人手作成のルールと収集され
た少数の翻訳事例から，英語動詞選択ルールを獲得する修正型の学習手法を提
案する．具体的には，本手法は，(1)人手作成のルールから仮の事例(仮事例)
を生成し，(2)その仮事例と現実の事例を訓練事例として，既存の学習アルゴ
リズム(内部学習アルゴリズム)に入力する，の２ステップから構成される．内
部学習アルゴリズムの出力が，最終的に獲得されたルールである．評価を目的
として，NTT が開発中の日英機械翻訳システム ALT-J/Eの英語動詞選択ルール
を本手法により実験的に学習した．その結果，学習されたルールは，実事例の
みから学習されたルールや人手作成のルールより高い正解率を示し，本提案
手法の有効性を確認できた．","This paper proposes a learning method that automatically
acquires English verb selection rules for machine translation using a
machine learning technique. If the rules are learned from only real
translation examples, many examples are necessary for good translation
quality. It is, however, difficult to gather a sufficiently large
number of real translation examples. The main causes are verbs of low
frequency and the frequent usage of the same sentences. To resolve
this problem, the proposed method learns English verb selection rules
from hand-made translation rules and a small number of real
translation examples. The proposed method has two steps: generating
artificial examples from the hand-made rules, and then putting the
resultant artificial examples and real examples into an internal
learner as the training set. The internal learner outputs the final
rules with improved verb selection accuracy. The most notable feature
of the proposed learner is that any attribute-type learning algorithm
can be adopted as the internal learner. To evaluate the validity of
the proposed learner, English verb selection rules of NTT's
Japanese-English Machine Translation System ALT-J/E are experimentally
learned from hand-made rules and real examples. The resultant rules
have better accuracy than either those constructed from the real
examples or those that are hand-made.","['はじめに', '英語動詞選択ルールとその獲得', '従来手法とその問題点\\label{TheFormerWork', '修正型学習手法', '実験結果', '関連学習手法との比較', 'むすび']",,,,,,,,
V03N03-04.tex,$\A^*$法に従うアジェンダ制御による構文解析,Parsing based on $\A^*$ Algorithm Agenda Control,"本稿では，構文解析を探索問題と捉えた上で，$\A^*$法の探索戦
略に従ってチャート法のアジェンダを制御し，最も適切な構文構造から順に必
要なだけ生成する構文解析手法を提案する．
文脈自由文法形式の費用付き構文規則が与えられたとき，規則に従って生成さ
れうる各部分構造について，その構造に相当する現在状態からその構造を構成
要素として持つ全体構造に相当する目標状態までの費用を，構文解析に先立っ
て，$\A^*$法の最適性条件を満たすように推定しておく．
従って，構文解析では，競合する構造のうちその生成費用と推定費用の和が最
も小さいものから優先的に処理していくと，生成費用の最も小さい全体構造が
必ず得られる．
また，優先すべき構造は，個々の規則に付与された費用に基づいて定まるので，
優先すべき構造をきめ細かく指定でき，優先したい構造の変更も規則の費用を
変更するだけで容易に行なえる．
費用付き構文規則は，記述力の点で，確率文脈自由文法規則の拡張とみなすこ
とができる．","In this study we propose a method of syntactic analysis
which approaches parsing as a search problem, controls the agenda of
the chart parser following the strategy of $\A^*$ \hspace{0.1mm","['はじめに', '$\\A^*$法の探索戦略に従うアジェンダ制御', '弧の生成費用の計算', '目標弧までの費用の推定', '解析例', '関連研究との比較', 'おわりに']",,,,,,,,
V03N03-05.tex,開発者の視点からの機械翻訳システムの技術的評価 ---テストセットを用いた品質評価法---,Technical Evaluation of MT Systems from the Developer's Point of View: Exploiting Test-Sets for Quality Evaluation,"機械翻訳システムの開発者がシステムの技術的評価を翻訳品質に注目して客観的
に行う手法を開発した。
評価過程の客観性と評価結果の解釈の客観性を維持するために、本手法では単な
る評価用例文集ではなく、システムの出力を評価するための設
問と、その設問がどのような言語現象を対象としているかについての解説とを各例文に付
与したテストセットを用いている。各例文は基本的な言語現象と現在の機械翻訳
システムにおいて処理が困難である言語現象のそれぞれを出来る限り網羅するよ
うに収集された。
今回、英日機械翻訳システム、日英機械翻訳システムのそれぞれについての評価
用テストセットを作成した。これらを用いて商用の
機械翻訳システムでの評価実験を繰り返すことにより、機械翻訳システムの能力の差異を提
示できることが示された。","This paper describes a method of evaluating quality for developers of
machine translation systems to easily check imperfections in their own
systems.  
Our method employs test-sets in which example sentences, their model
translations, yes/no questions for evaluating the system output, similar
examples (if any), and grammatical explanations have been systematically
aligned.  The example sentences have been collected focusing on wide
coverage of both basic linguistic phenomena and linguistic phenomena
problematic to MT systems. 
The questions in the test-sets are designed to clarify the evaluation
viewpoints.  Given the system outputs for each example sentence in
question, the system developer needs only to answer the question
assigned to the example sentence.  This judgment does not vary among
evaluators, thus enabling an objective evaluation.","['はじめに', 'テストセットを用いた機械翻訳システムの品質評価法', '英日機械翻訳システム品質評価用テストセット', '日英機械翻訳システム品質評価用テストセット', 'おわりに']",,,,,,,,
V03N04-01.tex,国文学作品のテキストデータ記述ルールについて,Text Data Description Rules for Japanese \hspace*{30mm,"国文学作品の電子化テキストのためのデータ記述ルールについて検
討した．様々な国文学作品の特徴を整理し，電子化テキストの目的
と研究対象をまとめ，必要なデータ記述のための機能や情報構造に
ついて考察している．データ記述の基本原則を作品の構造とテキス
トの構造に分けて考察し，またデータ表記の様相を分析，評価し，
ルール化についてまとめている．
３つの基本ルールから成るKOKIN ルールと呼ぶデータ記述文法を定
義し，作成した．これは国文学作品を対象とする電子化テキスト記
述用のマークアップ文法である．ルールに基づく電子化データを実
際に作成し，ルールの構文の正当性やルールの有用性などについて
評価している．例えば，電子化データのテキストデータベースや
CD-ROMへの登録，SGMLへの変換などを行っている．これらの結果，
ルールの有効性が確認され，実用性についても高く評価された．","This paper describes a study on the text-data description
rules for the Japanese classical literature.  We have
investigated the various functions for the text-data
description\hspace{-.1em","['まえがき', '電子化本文作成のための条件整理', 'データ記述のための基本原則', 'データ記述文法 --- KOKIN ルール', '評価', 'あとがき', '国文学の用語', '噺本大系の作品構造(TTDの例)', '噺本大系第5巻「軽口大わらひ」のデータ記述例']",,,,,,,,
V03N04-02.tex,表層表現を手がかりとした日本語名詞句の指示性と数の推定,"An Estimate of Referential Property and Number\\ 
of Japanese Noun Phrases from  Surface Expressions","日本語を英語に翻訳する時には，
日本語にはないが英語では必要な冠詞や数の問題に直面する．
この難しい問題を解決するために，
われわれは文章における名詞句の指示性と数をそれぞれ三種類に分類した．
指示性には総称名詞句，定名詞句，不定名詞句を設け，
数には単数，複数，不可算を設けた．
この論文では，名詞句の指示性と数が
その名詞句の現れる文中の言葉によりかなりの程度推定できることを示した．
その推定のための規則は
確信度付きのエキスパートシステムの書き換え規則に類する形で，
文法書などから得られる知識をもとに経験的に作成した．
この方法は，確信度を用いて推定するので，
指示性や数のような曖昧な問題には適した方法である．
規則を作るのに利用したテキストでの正解率は，
指示性で85.5\%，数で89.0\%であった．
規則を作るのに利用していないいくつかのテキストでの正解率は
平均して指示性で68.9\%，数で85.6\%という結果となった．
この指示性と数は冠詞の決定に利用されるのみならず，
照応処理，談話解析にも利用されていくと考えられる．","When we are translating Japanese nouns into English, we face the problem of
articles and numbers which the Japanese language does not have, 
but which are necessary for the English composition.  
To solve this difficult problem we classified 
the referential property and the number of noun phrases 
into three types respectively: 
generic, definite and indefinite for 
the referential property of noun phrases, 
singular, plural, and uncountable for 
the number of noun phrases. 
This paper shows that the referential property and the number of noun phrases
can be estimated fairly reliably by the words in the sentence.
Many rules for the estimation were written
in forms similar to rewriting rules in expert systems with scores.
Since this method uses scores, 
it is good to deal with 
vague problems like referential properties and numbers.
We obtained the correct recognition scores of 85.5\% and 89.0\% 
in the estimation of referential property and number 
respectively for the sentences
which were used for the construction of our rules.
We tested these rules for some other texts,
and obtained the scores of 68.9\% and 85.6\% respectively.
These referential properties and numbers of noun phrases will be used 
not only for determination of articles and numbers 
but also for anaphora resolution and discourse analysis.","['はじめに', '名詞句の指示性と数の分類', '名詞句の指示性と数の推定方法', '推定に用いる規則', '実験と考察', 'おわりに']",,,,,,,,
V03N04-03.tex,語用論的・意味論的制約を用いた\\ 日本語ゼロ代名詞の文内照応解析,Intrasentential Resolution of Japanese Zero Pronouns using Pragmatic and Semantic Constraints,\vspace*{-2.18mm,\vspace*{-2.18mm,"['はじめに', '日英機械翻訳システム評価用例文でのゼロ代名詞の出現傾向', 'ゼロ代名詞の同一文内照応解析', '評価', 'まとめ']",,,,,,,,
V03N04-04.tex,確率モデルによるゼロ主語の補完,Zero-subject Resolution by Probabilistic Model,主語のない日本語文に対し，確率モデルを用いて自動的にゼロ主語を補完する手法について述べる．これは，日英機械翻訳の前処理としての自動短文分割の後で適用されるものである．確率モデルを用いる方法として，従来 (1)多次元正規分布に基づくモデルを利用するものがあった．本稿では，新たに３種類のゼロ主語補完のためのモデルを提案する．それらは，連続分布に対して，(2)正規分布に基づくGram-Charlier展開を多次元に拡張した分布（疑似正規分布）に基づくモデル，離散分布に対しては，(3)１次対数線形分布，(4)２次対数線形分布に基づくモデルである．これら４種の確率モデルについて，補完精度を比較する実験を行った．その結果，(1)〜(4)の精度は，順に，７３％，７８％，７８％，８１％であり，２次対数線形分布を用いる方法が最も精度が高かった．また，補完を誤った事例について考察を加えた結果，主語と述語の意味的整合性をより正確に計算する必要があることなどがわかった．,"Probabilistic resolution method for Japanese zero-subjects is described.
 It is designed to be used for the back-end processor of an automatic shortening system of long Japanese sentences in a Japanese to English machine translation system.
 Ordinary probabilistic resolution method uses (1) normal distribution model in the continuous probability space. 
 In this article, we propose 3 new models.
 They are (2) quasi-normal distribution model in the continuous space, (3) 1st order log-linear distribution model in the discrete space and (4) 2nd order log-linear distribution model in the discrete space.
 For these four models, we make an experiment to measure the resolution accuracy.
 The test sample is from television broadcasting news.
 The measured accuracy by the cross validation test are 73\%, 78\%, 78\% and 81\% for (1), (2), (3) and (4) models, respectively.
 The unresolved examples show that semantic agreement between subject and predicate should be observed more accurately.","['はじめに', '主語補完の方法', '確率モデル', '補完実験', 'おわりに', '疑似正規分布の導出と母数の推定', '比例反復法による２次対数線形分布の母数の推定']",,,,,,,,
V03N04-05.tex,縮退型共起関係を用いた学習機能付き係り受け解析システム,Kakari-Uke Dependency Analysis\\ With Learning Function Based On Reduced Type Cooccurrence Relation,"実用的な自然言語処理を開発するには大規模な言語資源が必要になる．語彙解析では
辞書が共通の言語資源である．一方，構文解析では文法規則が主流になってくる．規
則ベースは抽象的で解析時の挙動を理解することは困難であり，規則の規模が大きく
なると保守改良が困難になるという課題がある．本論文では実際の文章から縮退型共
起関係という２文節間の係り受け関係を品詞と付属語列で表現するデータを抽出し，
係り受け解析の唯一の言語資源として利用したシステムを示す．本方式を用いて４０
００文から抽出した８０００の縮退型共起関係データを用いたプロトタイプシステム
を構築し，５０文の新聞社説で評価実験を行ったところ，８０％の係り受けが成功し
た．本システムの特徴として，規則駆動では困難な構文規則を学習したり拡張するこ
とが共起関係によるデータ駆動の良さから実現可能になっていることが挙げられる．","Large scale language resources are key materials for practical natural
language processing. The most common language resource is a dictionary
which plays an important role in lexical processing. On the other hand many
syntactic processing systems are based on context free grammars of phrase
structure. CFG rules take complementary position of lexical data resource.
In general the rules are absolute and difficult to get exact image in the
analysis system. These properties make the syntactic analysis difficult to
understand the total behavior when the size of rules grows. In this paper 
reduced type cooccurrence relations are collected from real text as a
unique language resource of the Japanese kakari-uke dependency analysis.
The data is a simple binary relation format of phrase dependency. It is
extracted automatically using a syntactic analysis. The prototype system
with eight thousand of the reduced cooccurrence relations showed eighty
percent accuracy in kakari-uke dependency analysis of editorial articles.
The system provides learning and incremental facility for the cooccurrency
relation database.","['まえがき', 'データ構造', '新解析系', '解析実験と評価', 'むすび']",,,,,,,,
V03N04-06.tex,,Mixture Probabilistic Context-Free Grammar: An Improvement of a Probabilistic Context-Free Grammar Using Cluster-Based Language Modeling,,,"['Introduction', 'Probabilistic CFG: An Overview', 'Mixture Probabilistic CFG', 'Evaluation Experiments', 'Sentence-Level Evaluation Experiments', 'Conclusion']",,,,,,,,
V03N04-07.tex,日本語の文法および未知の認知単位の自動獲得のための一方法,"A method for automatic acquisition of Japanese\\ grammar
and unknown cognitive units","筆者らは,コーパスに基づいて形態素を基本とした日本語文法を自動獲得する方法を既に提案している.
本論文は，この方法における処理単位として，形態素の代わりにより
長い単位 --- 認知単位 --- を用いた新しい方法を提案するものである．
認知単位は，人間を被験者とした知覚実験の結果から
得られた人間の文解析の単位である．こうした，形態素より長い単位を解析に
用いることにより，構文解析における経路数を抑えることができる．
しかし，単純に認知単位を辞書に登録して用いるだけでは，
未知認知単位の出現確率が高まり，結果として文解析の正解率が低下する．
この現象を抑えるため，既知認知単位から未知認知単位を推定する新しい方法を
更に取り入れた．
この方法で天気概況文コーパスを処理し，得られた文法に基づき構文解析を行った
結果，形態素を処理単位とした解析に比べ高い処理効率を得ることができた．","A method for automatic acquisition of a grammar of Japanese based on morphemes
from a corpus has already been proposed by the authors.
This paper proposes a new method based on cognitive units, which have been
experimentally found to be the units of the
human process of sentence analysis, and have been known to be larger than
morphemes. While the use of cognitive units can 
reduce the number of search paths, it may increase the number of
unknown units and may degrade system performance. In order to cope
with this problem, a method has been further introduced for identifying
an unknown cognitive unit from known cognitive units. The proposed method was
applied to the analysis of the `weather-forecast' corpus,
and the acquired grammar was used for parsing. The results indicated a higher
processing efficiency for systems using cognitive units than for those using
morphemes.","['まえがき', '文法の自動獲得法', '認知単位の知覚実験', '認知単位を用いた文法の獲得法', '未知の認知単位の自動獲得法', '未知の認知単位の自動獲得実験', '獲得した知識に基づく構文解析', 'むすび']",,,,,,,,
V03N04-08.tex,認知単位の{\bf bigram,A method for sentence analysis of Japanese based on bigrams of cognitive units,"現在，自然言語処理システムの多くは，処理単位として形態素を
用いているが，人間はもっと大きな単位で文を処理していることが
既に分かっている．この単位を認知単位と呼ぶ．
この知見から，人間の文解析処理は，認知単位の検出処理と，
検出した認知単位の取捨選択の2段階に分離できるものと考えられている．
本論文では，この考えに基づき，第一段階として状態遷移図を用いて認知単位を検出し，
第二段階として bigram を用いて認知単位を選択する，計二段階からなる
文解析法を提案するものである．この方法を用いて誤りを含んだテキストに対し
誤り訂正を行う実験を行った結果，形態素を単位とした bigram を
用いるよりも良い結果を得ることができた．","While most natural language processing systems adopt morphemes as units of
processing, humans are known to use larger processing units, which we call
cognitive units. The human process of sentence analysis can be considered
as consisting of two stages: detection and selection of cognitive units.
Based on this idea, this paper proposes a method for sentence analysis which
first detects possible cognitive units using a state transition diagram, and then
selects correct cognitive units on the basis of their bigrams. The proposed method was applied to text error correction, and the experimental results
confirmed that it can achieve a higher performance than that can be
attained using morpheme bigrams.","['まえがき', '認知単位の知覚実験', '認知単位の検出方法', 'bigram による認知単位の取捨選択', '評価', 'むすび']",,,,,,,,
V04N01-01.tex,Text-Wide Grammarに基づくテキスト解析,Text Analysis based on Text-Wide Grammar,\vspace*{-1mm,\vspace*{-1.5mm,"['はじめに', 'Text-Wide Grammar', '意味・照応解析機構', '実験', 'おわりに']",,,,,,,,
V04N01-02.tex,統一モデルに基づく話し言葉の解析,A Uniform Approach to\\ Spoken Language Analysis,"近年の音声認識技術の進歩によって，話し言葉の解析は自然言語処
理の中心的なテーマの1つになりつつある．話し言葉の特徴は，言
い淀み，言い直し，省略などのさまざまな不適格性である．書き言
葉には見られないこれらの現象のために，従来の適格文の解析手法
はそのままでは話し言葉の解析には適用できない．本稿では，テキ
スト(漢字仮名混じり文)に書き起こされた日本語の話し言葉の文
からその文の格構造を取り出す構文・意味解析処理の中で，言い淀
み，言い直しなどの不適格性を適切に扱う手法について述べる．本
手法は，適格文と不適格文を統一的に扱う統一モデルに基づいてお
り，具体的には，係り受け解析の拡張によって実現される．まず，
音声対話コーパスからの実例をあげながら統一モデルの必要性を述
べ，次に，本手法の詳細を説明した後，その有効性を解析の実例を
あげるとともに実験システムの性能を評価することで示す．その結
果，さまざまな不適格性を含む複雑な話し言葉の文が，係り受け解
析を基本とする本手法によってうまく扱えることを示し，さらに，
定量的にも，試験文の約半数に完全に正しい依存構造が与えられる
ことを示す．","Recent advances in speech processing technologies have made the analysis
of spoken language one of the central issues in natural
language processing. One big feature of spoken language, that distinguishes
it from written language, is that it is in various
ways ill-formed, containing hesitations, repairs, ellipses, and so on. This
makes it difficult to apply traditional linguistic-based methods to spoken
language analysis. This paper proposes a method for properly dealing
with ill-formedness, such as hesitations and repairs, in the course
of syntactic/semantic analysis of spoken Japanese where sentences transcribed
in Kanji-Kana characters are parsed and interpreted to obtain their
frame representations. The method is based on a uniform model,
which handles well- and ill-formed sentences in a uniform way,
and realized by extending traditional \hspace*{0.3mm","['はじめに', '日本語の話し言葉における不適格性', '統一モデルに基づく話し言葉の解析', '評価', '従来の手法との比較', 'おわりに']",,,,,,,,
V04N01-03.tex,話し言葉解析のためのコーパスに基づく優先度計算法,A Corpus-based Preference Decision Method for Spoken Language Analysis,\vspace*{-1mm,\vspace*{-1mm,"['はじめに', '統一モデルに基づく話し言葉の解析', 'コーパスに基づく優先度計算法', '評価', 'おわりに']",,,,,,,,
V04N01-04.tex,概念情報に基づく前置詞句係り先の曖昧性の解消,\hspace*{2mm,"英語前置詞句の係り先の曖昧性は文の構造的曖昧性の典型例をなすものである.
本論文は, 選好規則と電子辞書から得られる様々な情報に基づき, 前置詞句の
係り先を決定する手法を提案する. 最初に, 係り先を決める上での概念情報の
役割と, それを電子辞書から抽出する方法を述べる. 次に, 概念情報をはじめ
統語情報, 語彙情報に基づく前置詞句の係り先を決める選好規則について述べ, 
選好的曖昧性解消モデルを提案する. このモデルでは選好規則によって一意的
に係り先が決まらなかった場合, 補助的に確率を使い, コーパスから得られる
データから確率計算をすることにより係り先の決定を行っている. 使用頻度の
高い12の前置詞句を含む2877文について行った実験では, 86.9\%の正解率を得
た. これは既存の手法に比べ, 2％から5％よい結果となっている.",,"['はじめに', '概念情報に基づく曖昧性解消']",,,,,,,,
V04N01-05.tex,語の連接関係を利用した未知語の形態素辞書情報の獲得手法,Acquisition Method of Unknown Word's Morpheme Dictionary Information Using Word's Juxtapositional Relationships,"本稿では, 大量の未知語の形態素情報の自動的な蓄積手法の研究
について述べる. その内容は, 形態素の品詞・活用種類・活用形
（これをここでの形態素属性とする）の推定及び統計的手段によ
る推定の精度向上と, 日本語における形態素の推定である. 文章
内の語間の連接関係に注目することによって, 未知語の形態素属
性を推定する. そして, 形態素の字種と連接関係の頻度統計を適
用することによって, 未知語の形態素属性の推定精度を向上させ
る. また, ``分ち書き''されていない日本語においては, 形態素
の推定が必要になる. 特定の品詞(助詞と助動詞)を完全な情報と
みなし, 形態素を構成する文字種の並び規則から分割の基点をも
とめ, すでに登録されている単語にもとづき, 形態素推定を行な
う. これを形態素属性の推定を行なうプロセスに送ることで, 推
定結果から形態素であるものが選択される. 以上の手法を日本語
に対して適用するシステムを構築し,朝日新聞社説6ヶ月分のコー
パス中の約240,000形態素を用いて実験を行なった. 
その結果,活用品詞に対しては90.5\%,その他の品詞に対しては
95.2\%,全体の平均としては94.6\%の形態素の推定成功率を得
て228,450形態素の形態素属性を推定し, 
新たにユニークな形態素15,523個を蓄積することができた.","This paper describes an inference method for acquiring
morpheme information of unknown \hspace{0.3mm","['はじめに', 'システム概要と統計知識', '形態素への分割', '形態素属性の推定', '辞書再構成処理', '実験結果及び考察']",,,,,,,,
V04N01-06.tex,用例や表層表現を用いた日本語文章中の指示詞 ・代名詞・ゼロ代名詞の指示対象の推定,An Estimate of Referents of Pronouns in Japanese Sentences using Examples and Surface Expressions,"日本語文章における代名詞などの代用表現の指す対象が何であるかを把握することは，
対話システムや高品質の機械翻訳システムを実現するために必要である．
そこで，本研究では用例，表層表現，主題・焦点などの情報を用いて
指示詞・代名詞・ゼロ代名詞などの指示対象を推定する．
従来の研究では，代名詞などの指示対象の推定の際には
意味的制約として意味素性が用いられてきたが，本研究では
対照実験を通じて用例を意味素性と同様に用いることができることを示す．
また，連体詞形態指示詞の推定に意味的制約として「AのB」の用例を用いる
などの新しい手法を提案する．
指示対象を推定する枠組は，以下のとおりである．
指示対象の推定に必要な情報をすべて規則にする．
この規則により指示対象の候補をあげながら，その候補に得点を与える．
得点の合計点が最も高い候補を指示対象とする．
この枠組では規則を柔軟に書くことができるという利点がある．
この枠組で実際に実験を行なった結果，
指示詞・代名詞・ゼロ代名詞の指示対象を
学習サンプルにおいて87\%の正解率で，
テストサンプルにおいて78\%の正解率で，
推定することができた．","It is necessary to clarify referents of pronouns 
in machine translation and conversational processing. 
We present a method of estimating 
referents of demonstrative pronouns, personal pronouns, 
and zero pronouns in Japanese sentences
using examples, surface expressions, topics and focuses.
In conventional work, 
semantic markers have been used 
for semantic constraints. 
On the other hand, we use examples for semantic constraints 
and show that 
examples are as useful as semantic markers 
through control experiments. 
We also propose many new methods for estimating referents of pronouns. 
For example, we use examples of the form ``A of B'' for 
estimating referents of demonstrative adjectives. 
The framework of estimating referents is as follows. 
We make the rules from the informations 
that are necessary for estimating referents of pronouns. 
By these rules, 
we list possible referents of a pronoun and give them points. 
We estimate that 
the possible referent with the highest score is the referent. 
This framework has the advantage of writing rules flexibly. 
When we experimented in this framework, 
we obtained a precision rate of 87\% 
in the estimation 
of referent of demonstrative pronouns, personal pronouns, 
and zero pronouns 
on training sentences, 
and obtained a precision rate of 78\% 
on held-out test sentences.","['はじめに', '指示対象を推定する枠組', '指示詞の指示対象を推定するための規則', '代名詞の指示対象を推定するための規則', 'ゼロ代名詞の指示対象を推定するための規則', '実験と考察', 'おわりに']",,,,,,,,
V04N01-07.tex,支配従属構造照合による文と名詞句の前方照応解析,\hspace*{16mm,"本稿では，文とその後方に位置する名詞句との照応を，複雑な知
識や処理機構を用いず，表層的な情報を用いた簡単な処理によって解析する
方法を提案する．
文と名詞句の構文構造を支配従属構造で表現し，それらの構造照合を行ない，
照合がとれた場合，照応が成立するとみなす．
構造照合に用いる規則は，文が名詞句に縮約されるときに観察される現象のう
ち，主に，用連助詞から体連助詞への変化，情報伝達に必須でない語の削除に
着目して定義する．
このような簡単な処理によって前方照応がどの程度正しく捉えられるかを検証
するための実験を，サ変動詞が主要部である文と，そのサ変動詞の語幹が主要
部である名詞句の組を対象として行なった．
実験では，新聞記事から抽出した178組のうち133組(74.7\%)について，本手法
による判定と人間による判定が一致した．
また，構造照合で類似性が最も高いと判断された支配従属構造の組を優先解釈
として出力することによって，入力の時点で一組当たり平均3.4通り存在した
曖昧性が1.8通りへ絞り込まれた．","We propose a simple method of analysing correferentiality
between sentences and later occurring noun phrases. 
Our method uses surface information and requires no complex data or
processing mechanism.
We represent a sentence and a later occurring noun phrase as dependency
structures, and examine whether the two structures are matched.
Where a matching between them can be established, we assume that the
two are correferential.
The rules for establishing structural matching are part of the
paradigm of theme packing, namely the predictable changes of adverbal
particles into adnominal particles and the disappearance of some
non-essential information.
In order to ascertain to what degree anaphora can be correctly traced
by such simple processing, we have carried out an experiment centred
upon sentences governed by a verb of the SAHEN category and later
occurring noun phrases in which the head noun is formally identical
with the invariable part of the SAHEN verb.
Of the 178 pairs of such sentences and noun phrases selected
from newspaper articles, 133 pairs (74.7\%) were correctly identified
as correferential or otherwise, in accordance with human judgement.
Furthermore, as a side effect, the number of dependency
structures to be considered can be 
reduced by selecting only the pairs of dependency structures with the 
best affinity through structural matching.
By this method the average 3.4-fold structural ambiguity was reduced to
average 1.8-fold.","['はじめに', 'サ変動詞文とサ変名詞句の構造照合規則', '構造照合による意味合成の近似', '実験と考察', 'おわりに']",,,,,,,,
V04N01-08.tex,括弧付きコーパスからの日本語確率文脈自由文法の自動抽出,Automatic Extraction of Japanese Probabilistic Context Free Grammar From a Bracketed Corpus,"\quad 
本論文では，括弧付きコーパスから確率文脈自由文法を
効率良く自動的に抽出する方法を提案する．
文法規則の抽出は，
日本語の主辞が句の一番最後の要素であるという特徴を利用して，
括弧付けによる構文構造の内部ノードに適切な非終端記号を
与えることによって行う．
また，文法規則の確率は規則のコーパスにおける出現頻度から推定する．
さらに，文法サイズの縮小と解析木数の抑制という2つの観点から，
抽出した文法を改良するいくつかの方法を提案する．
文法サイズの縮小は，文法に含まれる冗長な規則を自動的に
削除することによって行う．
解析木数の抑制は，
(1)同一品詞列に対して右下がりの二分木のみを生成し，
(2)``記号''と``助詞''の2つの品詞を細分化し，
(3)法や様態を表わす助動詞に対する構造を統一する
ことにより行う．
最後に，本手法の評価実験を行った．
約180,000の日本語文から確率文脈自由文法の抽出およびその改良を
行ったところ，2,219の文法規則を抽出することができた．
抽出された文法を用いて20,000文のテスト例文を統語解析したところ，
受理率が約92\%となり，適用範囲の広い文法が得られたことを確認した．
また，生成確率の上位30位の解析木の評価を行ったところ，
括弧付けの再現率が約62\%，括弧付けの適合率が約74\%，
文の正解率が約29\%という結果が得られた．","\quad 
In this paper, we describe a method to extract
a probabilistic context free grammar of Japanese
from a bracketed corpus.
To extract grammar rules,
we assign appropriate non-terminal symbols
to the intermediate nodes of the bracketed trees
by taking account of the heads of phrases.
We estimate the probabilities of the rules
based on their frequency of occurrence.
We also propose several improvements to the extracted grammar.
The size of the grammar is reduced
by removing any redundant rules.
The number of the parse tree is reduced
(1) by allowing only a right linear binary branching tree
for a constituent that consists of items of the same POS,
(2) by subcategorizing the POSs ``symbol''(``KIGOU'')
and ``postposition''(``JOSI''),
and (3) by assigning a consistent structure
to constructs representing clausal modality.
Finally, we conducted an experiment that evaluated the proposed methods.
2,219 grammar rules were extracted from about 180,000 sentences.
When we analyzed 20,000 test sentences with the extracted grammar,
a 92 \% acceptance rate was calculated, 
showing that the grammar has a broad coverage.
For the most probable 30 parse trees,
we obtained a 62 \% brackets recall, 74 \% brackets precision
and 29\% sentence accuracy.","['序論', '括弧付きコーパスからの文法抽出', '文法の改良', '評価実験', '結論']",,,,,,,,
V04N02-01.tex,文節間係り受け距離の統計的性質を用いた\\日本語文の係り受け解析,"Dependency  Analysis of Japanese Sentences \\Using 
the Statistical  Property of \\ Dependency Distance between Phrases","日本語における2文節間の係り受け頻度は,\ その距離に依存することが
知られている．\ すなわち,\ 文中の文節はその直後の文節に係るこ
とが最も多く,\ 文末の文節に係る場合を除いては,\ 距離が離れるにし
たがってその頻度が減少する．\ この統計的性質は,\ 日本語文の係り受け解析
においてしばしば用いられるヒューリスティクス：「文中の文節は係り得る文節の中で最も近
いものに係る」の根拠となっている．\ しかし,\  このヒューリスティクスは,
\ 日本語に見られるこのような統計的性質の一部しか利用
していない．\ したがって,\ 係り受け距離の頻度分布をもっと有効に利用
することにより,\ 解析性能が向上する可能性がある．\ 本研究では,\ ATR 
503文コーパスから抽出した係り受け距離の頻度分布に基づいて2文節間の
係り受けペナルティ関数を定義し,\ 「総ペナルティ最小化法」を用いて係り
受け解析実験を行なった．\ その結果を,\ 上のヒューリスティクスに基づく
決定論的解析法による解析結果と比較したところ,\ かなりの解析性能向上が認めら
れた．\ また,\ 係り文節を分類し,\ その種類別に抽出した係り受け頻度の情報を
用いることにより,\ さらに解析性能を改善できることが明らかになった．","It is well known  that the   frequency  of  phrase-pairs  in  
modifier-modified 
relation   de- \\ pends \  on  the  distance  between \  the 
phrases \  constituting \  the  pair \ in the Japanese\\ 
language.  That is,  a  phrase  in  a  sentence  modifies
 its  immediate  successor most frequently, and the frequency of modification
decreases as the distance between the modifier   phrase and the   modified
phrase   increases   unless the modified phrase is the last one in the
sentence.  This paper discusses a method of exploiting this statistical knowledge
for dependency analysis of Japanese sentences. The $minimum\ total
\ penalty\ method$ was used for  dependency   analysis in this work.
 The  method requires  a penalty function, which specifies the
association strength between phrases.\ Several penalty functions were 
defined based on the frequency distribution of dependency
distance extracted from ATR 503-sentence corpus, and the analysis performances
were compared.\ Another experiment was conducted by using a deterministic dependency
analysis method for comparison.\ It is concluded that the knowledge of the 
dependency distance distribution is effective,\ and that detailed knowledge of the 
dependency distance distribution extracted for each modifier phrase group is
 still more effective for improving the analysis performance.","['まえがき', '係り受け距離の統計的知識', '係り受け解析法', '実験と結果', 'むすび']",,,,,,,,
V04N02-02.tex,コーパスに基づく動詞の多義解消,"Word-Sense Disambiguation Using the Extracted \\ Polysemous
Information from Corpora","本稿では, コーパスから抽出した動詞の語義情報を利用し, 文中に含まれる多
義語の曖昧性を解消する手法を提案する.  先ずコーパスから動詞の多義解消
に必要な情報を抽出する手法について述べる.  本手法では, 多義を判定しな
がら意味的なクラスタリングを行なうことで多義解消に必要な情報を抽出する. 
そこで, 表層上は一つの要素である多義語動詞を, 多義が持つ各意味がまとまっ
た複数要素であると捉え, これを一つ一つの意味に対応させた要素 (仮想動詞
べクトルと呼ぶ) に分解した上でクラスタを作成するという手法を用いた. 本
手法の有効性を検証するため, 丹羽らの提案した単語ベクトルを用いた多義語
の解消手法と比較実験を行なった結果, 14種類の多義語動詞を含む1,226文に
対し, 丹羽らの手法が平均62.7\%の正解率に対し, 本手法では71.1\%の正解率
を得た.","In this paper, we focus on a definition of polysemy in terms of
distributional behaviour of words in monolingual texts and propose a
method for disambiguating word-senses in sentences containing
occurrences of polysemous verbs.  We first discuss existing work on
some corpus-related approaches on word-sense disambiguation and show
the significance of our approach by comparing it with other related
work.  Then we give a definition of polysemy from the viewpoint of
clustering and propose a clustering method which {\it automatically","['まえがき', '関連した研究', '多義解消に必要な情報の抽出', '多義語の解消', '実験', '考察', 'むすび']",,,,,,,,
V04N02-03.tex,意味的制約を用いた日本語名詞における間接照応解析,Indirect Anaphora Resolution in Japanese Nouns using Semantic Constraint,"照応現象の一つに，文章中に現れていないがすでに言及されたこと
に関係する事物を間接的に指示する間接照応という用法がある．
間接照応の研究はこれまで自然言語処理においてあまり行な
われていなかったが，文章の結束性の把握や意味理解において重要
な問題である．間接照応の解析を行なうには，
二つの名詞間の関係に関する知識として
名詞格フレーム辞書が必要となるが，
名詞格フレーム辞書はまだ存在していないので，
「名詞Aの名詞B」の用例と用言格フレーム辞書を代わりに利用することにした．
この方法で，テストサンプルにおいて
再現率63\%，適合率68\%の精度で解析できた．
このことは，名詞格フレーム辞書が存在しない現在においても
ある程度の精度で間接照応の解析ができることを意味している．
また，完全な名詞格フレーム辞書が利用できることを仮定した実験も
行なったが，この精度はテストサンプルにおいて
再現率71\%，適合率82\%であった．
また，名詞格フレーム辞書の作成に
「名詞Aの名詞B」を利用する方法を示した．","A definite noun phrase can indirectly refer to an entity that has 
already been mentioned before. For example, ``There is a house. The roof is
white.'' indicates that ``the roof'' is associated with ``a house'',
which was mentioned in a previous sentense. This kind of references 
(indirect anaphora) has not been 
studied well in natural language processing, but is important for 
coherence resolution, language understanding, and machine translation. 
When we analyze indirect anaphora, 
we need a case frame dictionary for nouns 
containing a knowledge about relations between two nouns. 
But no noun case frame dictionary exists at present. 
Therefore, we are forced to use examples of ``A of B'' 
and a verb case frame dictionary, instead. 
We experimented the estimation of indirect anaphoras 
by using this information, 
and obtained a recall rate of 63\% and 
a precision rate of 68\% on held-out test sentences. 
This indicates that 
the information of `A of B` 
is useful to a certain extent 
when we can not make use of a noun case frame dictionary. 
We made an estimation in the case 
that we can use a good noun case frame dictionary, 
and obtained the result with the recall and the precision rates of 
71\% and 82\%, respectively. 
Finally we proposed how to construct a noun case frame dictionary 
from examples of ``A of B''.","['はじめに', '間接照応の解析方法', '照応処理システム', '実験', '名詞格フレーム辞書の作成に関する考察', 'おわりに']",,,,,,,,
V04N02-04.tex,,On Semantic Interpretation\\ of Japanese Compound Nouns,,Toward \hspace{0.5mm,"['Introduction', 'Semantic Categories', 'Paraphrasing System for Elliptical Words', 'Experimental Results and Discussion', 'Conclusions and Future Perspectives']",,,,,,,,
V04N02-05.tex,,Clustering Words with the MDL Principle,,"We address the problem of automatically constructing a
  thesaurus (hierarchically clustering words) based on corpus data.
  We view the problem of clustering words as that of estimating a
  joint distribution over the Cartesian product of a partition of a
  set of nouns and a partition of a set of verbs, and propose an
  estimation algorithm using simulated annealing with an energy
  function based on the Minimum Description Length (MDL) Principle.
  We empirically compared the performance of our method based on 
  the MDL Principle against that of one based on the Maximum Likelihood
  Estimator, and found that the former outperforms the latter. We also
  evaluated the method by conducting pp-attachment disambiguation
  experiments using an automatically constructed thesaurus. Our
  experimental results indicate that we can improve accuracy in
  disambiguation by using such a thesaurus.","['Introduction', 'The Problem Setting', 'Clustering with MDL', 'Advantages of Our Method', 'Experimental Results', 'Concluding Remarks']",,,,,,,,
V04N02-06.tex,文脈依存の度合を考慮した重要パラグラフの抽出,An Automatic Extraction of Key Paragraphs \\ Based on the Degree of Context Dependency,"本稿では, 文脈依存の度合いに注目し, 重要パラグラフを抽出する手法を提案
する.  本手法では, Luhn らにより提唱されたキーワード密度方式と同様, 
「主題と関係の深い語はパラグラフを跨り一貫して出現する」という前提に基
づく.  我々は, 文脈依存の度合, すなわち, 記事中の任意の語が, 設定され
た文脈にどのくらい深く関わっているかという度合いの強さを用いることで, 
主題と関係の深い語を抽出し, その語に対し重み付けを行なった.  本手法の
精度を検証するため人手により抽出したパラグラフと比較した結果, 抽出率
を30\%とした場合, 50記事の抽出総パラグラフ数84に対し75パラグラフが正解で
あり, 正解率は89.2\%に達した.","In this paper, we propose a method for extracting key paragraph in
articles based on the degree of context dependency.  Like Luhn's
technique, our method assumes that the words which are relative to
theme in an article appear throughout paragraphs.  Our technique for
extraction of keywords is based on the degree of context dependency
that how every word is strongly related to a given context.  The
results of experiments demonstrate the applicability of our proposed
method.","['まえがき', '文脈依存の度合い', '語の重み付け', '重要パラグラフの抽出', '実験', '考察', 'むすび']",,,,,,,,
V04N02-07.tex,,Case Contribution in Example-Based Verb Sense \\ Disambiguation,,"Word sense disambiguation has recently been utilized in corpus-based
  approaches, reflecting the growth in the number of machine readable
  texts. One category of approaches disambiguates an input verb sense
  based on the similarity between its governing case fillers and those
  in given examples. In this paper, we introduce the degree of case
  contribution to verb sense disambiguation into this existing method. 
  In this, greater diversity of semantic range of case filler examples
  will lead to that case contributing to verb sense disambiguation
  more.  We also report the result of a comparative experiment, in
  which the performance of disambiguation is improved by considering
  this notion of semantic contribution.","['Introduction', 'Motivation', 'Algorithm', 'Computation of CCD and RSSR', 'Evaluation', 'Conclusion']",,,,,,,,
V04N03-01.tex,,"Generation in Machine Translation:
  the right place to choose between translation equivalents",,"This paper looks at the problem of choosing between alternative lexical
translation equivalents in machine translation.
It argues that the knowledge base used for making choices between
alternative translations is part of the target language grammar and that
it should be applied and the translation chosen as part of generation of
the target language text.
This contrasts with previous work on the problem, which has assumed that
these choices should be made in analysis or transfer.
Various types of knowledge base could be used to make the choice between
alternatives. The method outlined here uses information about
stereotypical contexts of use of words, stored as part of an ontological
network.","['Introduction', 'An example', 'Current techniques', 'Lexical choice in generation', 'A framework for machine translation', 'A method for choosing between alternative translations', 'Knowledge base --- content', 'Knowledge base --- representation', 'Discourse representation', 'Conclusion']",,,,,,,,
V04N03-02.tex,シソーラス上に動的に構成される標本空間における 動詞の多義性解消,Verb Sense Disambiguation in Dynamically Constructed Sample Spaces on a Thesaurus,"本稿では，語義の尤度パラメータの標本空間を，シソーラスに沿っ
て動的に拡張することにより，動詞の多義性を解消する手法を提案する．提案
手法では，尤度1位の語義と2位の語義とを比較し，尤度差が統計的に有意なら
ば，1位の語義を選ぶ．有意でなければ，シソーラスに沿って標本空間を一段
拡張し，多義性解消を試みる．もし，最大の標本空間でも尤度差が有意でなけ
れば，語義は判定しない．本稿の実験では，EDR日本語コーパスから頻度500以
上の動詞74語を抽出し，延べで約89,000の動詞について多義性を解消した．こ
のとき，最頻の語義を常に選ぶ場合の適合率は0.65，判定率は1.00であった．
ただし，判定率とは，多義性の解消を試みたなかで，実際に語義が判定された
割合である．クラスベースの手法と提案手法とを比較すると，分類語彙表を利
用した場合には，適合率は共に0.71であったが，判定率は，クラスベースの手
法が0.68，提案手法が0.73であった．EDR概念体系を利用した場合には，適合
率は共に0.70であったが，判定率は，クラスベースの手法が0.76，提案手法が
0.87であった．両者の判定率を比べると，提案手法の方が統計的に有意に高く，
その有効性が示された．","This paper proposes a method which disambiguates verb
senses using co-occurrence-based likelihood parameters whose sample
spaces are extended according to a thesaurus. The method selects the
most plausible sense if its likelihood is significantly greater than
that of the second most plausible one. If not, the sample space is
extended and the significance test is tried again. If it cannot be
extended anymore, the system gives up disambiguation.  The method was
applied to 74 polysemous verbs (about 89,000 instances) extracted from
the EDR Japanese Corpus.  When the most frequent sense was selected,
the precision was 0.65 and the applicability, i.e. the ratio of the
disambiguated verbs to the treated verbs, was 1.00. The proposed
method was compared with a class-based method.  With {\it
Bunruigoihyou\/","['はじめに', '動詞の多義性の解消法', '実験', '考察', 'おわりに']",,,,,,,,
V04N03-03.tex,意味的類似性と多義解消を用いた文書検索手法,Document Retrieval Method Using Semantic Similarity and Word Sense Disambiguation,"単語間の意味的類似性に基づく検索(以下，類似検索と呼ぶ)は
 文書検索技術において，重要な課題の一つである．
 類似性に関する従来研究では，階層構造が平衡しているシソーラスを
 使った単語間の類似度が提案され，言語翻訳，文書検索などの応用における
 有効性が示されている．
 本論文では，階層構造が平衡していないシソーラスにも適用できる，
 より一般的な単語間の意味的類似度を提案する．
 本提案では
 各単語が担う概念間の最下位共通上位概念が有する下位概念の総数が少ないほど，
   単語間の類似度が大きくなる．
 筆者らは，この意味的類似度と大規模シソーラスの一つであるEDR
 シソーラスを使って，類似検索システムを実装した．
 さらに，精度を向上させるために，単語の多義解消手法を
 この検索システムに導入した．
 本類似検索システムは，単語間の物理的近さと単語の重要度を用いた
 拡張論理型の従来システムに基づいている．
 この従来システムとの比較実験を行ない，
 意味的類似性と多義解消を用いた提案の類似検索手法によって
 再現率・適合率が向上したことを確認した．","Retrieval based on semantic similarity between words (hereafter, 
similarity-based retrieval) is one of the important problems in
document retrieval technologies.
 In previous research on semantic similarity, measures of word-similarity
using the thesaurus whose hierarhical structure is balanced, were
used and thier effectiveness were shown in applications 
such as language translation and document retrieval.
 This paper proposes a general measure of similarity which is
applicable for both balanced and unbalanced thesauri.
In this proposed measure, the lesser the number of concepts under the
most specific common abstraction between concepts of words,
the larger the similarity between words.
 The authors have implemented a similarity-based retrieval system 
using this semantic similarity and one of  large-scale thesauri, EDR
thesaurus.
 Moreover, in order to improve its accuracy, they have incorporated 
word sense disambiguation method into the retrieval system.
 This retrieval system is based on a conventional system, an extended 
boolean retrieval system using the phisical nearness between words and
the weight of words.
 Through contrastive experiments with the extended boolean system, the
authors have shown the improvement in both recall and precision by the 
proposed similarity-based method.","['はじめに', '類似検索', '実験', 'おわりに']",,,,,,,,
V04N03-04.tex,確率的言語モデルに基づく多言語コーパスからの 言語系統樹の再構築,Reconstructing the Language Family Tree from Multilingual Corpus Based on Probabilistic Language Modeling,"本論文では，言語のクラスタリングに関する新しい手法を提案する．
提案する手法では，まず各言語の言語データから確率的言語モデルを
構築し，次に確率的言語モデルの間に導入した距離に基づき，
元の言語に対するクラスタリングを実行する．
本論文では，以上の手法を $N$-gram モデルの場合について
詳しく述べている．
また，提案した手法を用いて，ECI 多言語コーパス
(European Corpus Initiative Multilingual Corpus)中の 19 ヶ国語の
テキスト・データから，言語の系統樹を再構築する実験を行った．
本実験で得られた結果は，言語学で確立された言語系統樹と非常に
似ており，提案した手法の有効性を示すことができた．","This paper proposes a new method for automatically clustering languages.
The basic idea of this method involves
developing a probabilistic model for each language from the given linguistic data,
and then computing the distances between languages
according to the distance measure defined on the language models.
Clustering is performed based on this distance measure.
The paper embodies this idea
when the $N$-gram language model is concerned.
The effectiveness of the proposed method
has been confirmed by evaluation experiments
using multilingual texts of nineteen different languages
from the ECI Corpus (European Corpus Initiative Multilingual Corpus).
The results were very encouraging.
They were very close to the family tree of languages
established in linguistics.","['はじめに', '従来の研究', '確率モデルに基づく言語のクラスタリング', '評価実験', 'おわりに']",,,,,,,,
V04N03-05.tex,文字連鎖を用いた複合語同音異義語誤りの検出手法とその評価,A Method for Detecting Japanese Homophone Errors in Compound Nouns based on Character Cooccurrence and Its Evaluation,"本論文では，文字連鎖を用いた複合語同音異義語誤りの検出手法とその評価について述べる．ワードプロセッサによって作成された日本語文書には，変換誤りに起因する同音異義語誤りが生じやすい．同音異義語誤りは，同じ読みの単語を誤った単語へと変換してしまう誤りである．このため，推敲支援システムにおいて同音異義語誤りを検出する機能を実現することは重要な課題の1つとなっている．我々は，意味的制約に基づく複合語同音異義語誤りの検出／訂正支援手法を提案した．しかし，この手法においてもいくつかの短所が存在する．本論文では，これらの短所を補うための手法として，文字連鎖を誤り検出知識として用いた複合語同音異義語誤りの検出手法について述べる．文字連鎖は，既存の文書を解析することなしに容易に収集することができる．また，本手法は文字連鎖のみを用いているので，複合語同音異義語誤りに限らず，文字削除誤りなどの別のタイプの誤りに適用することも可能である．
さらに本論文では，本手法の有効性を検証するために行った評価実験の結果についても述べ，意味的制約を用いた複合語同音異義語誤り検出／訂正支援手法との比較についても述べる．","Most Japanese texts are produced with Japanese word processors. As Japanese texts consist of phonograms, KANA, and ideograms, KANJI, Japanese word processors always use KANA-KANJI conversion in which KANA sequences input through the keyboard are converted into KANA-KANJI sequences. Therefore, Japanese texts suffer from homophone errors caused by erroneous KANA-KANJI conversion. A homophone error occurs when a KANA sequence is converted into the wrong word which has the same reading. Detecting homophone errors is an important topic in Japanese text revision support systems. We have already proposed a high performance method for handling Japanese homophone errors in compound nouns used in REVISE. The method, however, has some drawbacks. To compensate for these drawbacks, this paper describes a method for detecting Japanese homophone errors in compound nouns that uses character cooccurrence. Character cooccurrence can be easily collected from existing texts without any analysis. Therefore, this method can be used, in a Japanese revision support system, as a complementary method for handling Japanese homophone errors in compound nouns. Moreover, as this method depends only on character cooccurrence, it can be applied not only to homophone errors but also other types of errors such as character deletion.","['まえがき', '用語の定義', '意味的制約に基づく複合語同音異義語誤り検出の概要', '文字連鎖を用いた複合語同音異義語誤り検出手法の提案', '評価実験', 'むすび']",,,,,,,,
V04N04-01.tex,動詞と主体の属性を用いた複文の連接関係の解析,Analysis of Coherence Relation for Complex Sentences Using Attributes of Verbs and Subjects,"本論文は，動詞と主体の属性を用いて，複文中の連接関係を解析
するモデルを作成し，評価した結果を述べる．複文中の連接関係の
関係的意味は，接続詞，助詞等の接続の表現だけでは決まらず，曖
昧性がある．例えば，助詞「て」による連接関係には，「時間的継
起」のほかに「方法」，「付帯状態」，「理由」，「目的」，
「並列」などがある．これらの関係的意味は，従属節や主節の述語
の表している事象の意味タイプ，およびその組み合わせによって決
まってくる．従って，動詞と名詞の意味的関係を表すために，動詞
と名詞の意味分類を用いた格パターンがあると同様に，従属節と主
節の連接関係にも，各々の節を構成する動詞と主体の属性を用いた
連接関係パターンが存在すると考えることができる．本論文のモデ
ルでは，従属節と主節の，動詞と主体の属性を用いて，連接関係の
関係的意味を推定する．動詞の属性として，意志性，意味分類，慣
用的表現，ムード・アスペクト・ヴォイス，主体の属性として，主
節と従属節の主体が同一かどうか，無生物主体かどうかを用いた．
このモデルを，技術文書に適用した結果，95\%の文が正しく解析で
きた．","This paper presents a model to analyze the coherence 
relation within complex sentences by using the attributes 
of verbs and subjects. The relations between subordinate 
and main clauses can not be understood only by the 
connectives that link them. The coherence relations by 
connectives are often ambiguous.  For example, a Japanese 
conjunctive particle ``te"" expresses coherence relations, 
such as sequential, method, manner, reason, purpose and 
parallel. The coherence relations by connectives depend on 
the semantic types of the predicates in subordinate and 
main clauses, and the combination of them. It is presumed 
that there are the patterns of coherence relations in the 
relations between subordinate and main clauses, just like 
there are the patterns of thematic roles to express 
semantic relations between verbs and nouns. The pattern of 
coherence relation uses the attributes of verbs and 
subjects in a subordinate and a main clause, just like the 
pattern of thematic roles uses a verb and the semantic 
types of nouns. The model infers coherence relations by 
using the attributes of verbs and subjects in subordinate 
and main clauses. It uses volition, semantic types, 
idiomatic expressions, mood/aspect/voice, as the attributes 
of verbs, and uses whether the subject in a subordinate 
clause is identical with the one in the main clause or not, 
whether the subject is unanimate or not as the attributes 
of subjects. The model is evaluated and the result shows 
that 95\% of the text taken from science and technical 
documents can be analyzed successfully.\vspace*{5mm","['はじめに', '連接関係の曖昧性', '動詞と主体の属性と連接関係の関係的意味', '複文の連接関係解析モデル', '連接関係解析モデルの評価結果', 'むすび']",,,,,,,,
V04N04-02.tex,,A Method of Ordering English Adverbs --- as exemplified in Japanese-to-English Machine Translation ---,,"This paper proposes a new method for ordering English adverbs.  First, we
propose a classification of adverbs for English adverb generation.
Adverbs are classified into 41 classes by grammatical function
(adjuncts, subjuncts, disjuncts and conjuncts), meaning (process,
space, time etc.) and their default positions in sentences (initial,
medial, end, pre, and post).  
Then a method to order English adverbs correctly is described, 
using the proposed adverb classification and principles of
word ordering for adverbs (principles of ordering between adverbs and
other sentence constituents and principles of ordering between
adverbs).  In particular we give detailed rules for
deciding precedence when two adverbs have the same default position.
Exceptions to the default adverb generating process are also
described.  Finally, the proposed method is examined in three
experiments from the point of view of Japanese-to-English machine
translation.  The first experiment focuses on aspects of various
types of adverbs and a comparison of the proposed method and the
previous method.  The second experiment focuses on aspects of
quantitative coverage, and the third looks at aspects of practical use.  
The results show an accuracy of 97\% or more in all experiments
which highlights the efficiency of the proposed method.  The third
experiment, in particular, with an accuracy of 99\%, confirms that the
proposed method is effective in practical applications.","['Introduction', 'Classification of English Adverbs', 'Determining Word Order for English Adverbs', 'Experimental Results', 'Conclusion']",,,,,,,,
V04N04-03.tex,表層表現による日本語動詞句のアスペクトの推定,An Estimate of Aspect of Japanese Verb Phrases from Surface Expressions,"日本語のアスペクトの研究は，継続相，完成相というような分類と
  それぞれの意味を記述していく段階から，副詞的成分などの関わりを含め，ア
  スペクト的な意味の決まり方のプロセスを整理する方向へと発展してきている．
  本稿では，アスペクト形式や副詞句の意味を時間軸におけるズーミングや焦点
  化といった認知的プロセスを表示するものとしてとらえ，動詞句の意味に対す
  る動的な操作であると考える．その上で，動詞句の意味をコーパスに現れた表
  層表現から推定し，素性構造として表現する．実験の結果得られた動詞句の分
  類を評価するために，最も基本的なアスペクトの形態である「シテイル」形の
  意味を自動的に決定する処理を行なった．２００文における正解率は71\%で
  あった．これらの情報は，動詞句のアスペクト的な意味のあり方の類型を与え
  るだけではなく，文間の関係をとらえる際の手がかりを提供するものであると
  考える．","The study on aspect in Japanese has evolved from the
  description of the meaning for each type such as ``progressive'' or
  ``perfective'' into the process of the determination of the aspectual
  meaning including adverbial phrases etc.  In this paper, we consider
  the meaning of the aspectual markers or adverbs as the indicator of
  the cognitive process like ``zooming'' or ``focusing'' on the
  time-line and define them as the dynamic operations on the meaning of
  the verb phrases.  We estimate the aspectual meaning of verbs from
  surface expressions in a corpus and represent them as a bundle of
  features.  To evaluate the result of the experiment, we examined the
  meaning of {\em si-teiru","['はじめに', 'ズーミングと焦点化', 'アスペクト決定の過程', '動詞の分類実験', 'おわりに']",,,,,,,,
V04N04-04.tex,日本語の自由会話における談話構造の推定 \\ 〜因果関係を表す接続詞の場合〜,"A Method of Discourse Structure Understanding \\ in Japanese Task-Free
Conversation \\ for Causal Conjunction","日本語の談話理解を考える際には，文脈，すなわち「会話の流れ」を把握する必
要がある．
一般的に日本語では，「会話の流れ」を明示する語として，順接・逆接・話題の
転換・因果性，などを表す接続(助)詞が用いられることが多い．
これらの語は，スケジュール設定など何らかの話題・目的が存在する会話だけで
はなく，雑談などの場合でも，聞き手が「会話の流れ」を把握するために利用し
ているものと考えられる．
本技術資料では，「だって」や「から」などの接続表現によって，因果関係の前
件および後件の関係が談話中で明示されている場合を対象とし，そのような因果
関係が談話中で示す特徴について検討する．
この検討から，いくつかの観察結果が得られるが，それについて自由会話コーパ
スを用いた検討を行ない，実際にそのような特徴が成立することを確認した．
この結果は，接続表現に依存する前件と後件の順序関係，および前件と後件の隣
接性という2項目にまとめることができ，機械的処理による談話理解への足掛か
りと考えられる．","We discuss here how a discourse structure for a causal relation between
two or more utterances is linguistically expressed in Japanese task-free 
conversations.
When we have a conversation, we often use conjunctions as a sign of
causal relation, change of topic, etc.
So, these words are important to understand a discourse structure
because we often use these words to represent ``an intention of
utterance'' or ``a stream of discourse.''
Here, we discuss a case where causal conjunctions are used in Japanese
task-free conversations by examining our corpus of causal conversation,
and show how to identify relations, represented by these conjunctions,
among a few adjacent sentences systematically.","['はじめに', '因果関係を表わす発話の性質', 'コーパスによる検証', 'おわりに']",,,,,,,,
V04N04-05.tex,発話タイプ付きコーパスを用いた確率的対話モデルの\\自動生成,"Automatically Deducing Probabilistic Dialogue \\Models
from an IFT-Annotated Dialogue Corpus","コーパスに基づく確率的言語モデルとして，従来は主に語彙統語論的な
モデルが扱われてきた．
我々は，より高次の言語情報である対話に対する確率的モデルを，
コーパスから自動的に生成するための研究を行った．
本研究で用いたコーパスは，ATR 対話データベース中の「国際会議参加登録」
に関する対話データであり，各発話文には，発話者のラベルおよび
陳述・命令・約束などの発話行為タイプが付与されている．
本技術資料では，これらのコーパスから，２種類の方法を用いて，
確率的な対話モデルを生成する．
まず初めに，エルゴード HMM (Hidden Markov Model)を用いて，
コーパス中の話者ラベルおよび発話行為タイプの系列をモデル化した．
次に，ALERGIA アルゴリズムと呼ばれる，状態マージング手法に基づいた
学習アルゴリズムを用いて，話者ラベルおよび発話行為タイプの系列を
モデル化した．
エルゴード HMM の場合には，確率モデルの学習に先立ち，
モデルの状態数をあらかじめ決めておく必要があるが，
ALERGIA アルゴリズムでは，状態の統合化を繰り返すことにより，
最適な状態数を持つモデルを自動的に構成することが可能である．
エルゴード HMM あるいは ALERGIA アルゴリズムを用いることにより，
話者の交替や質問・応答・確認と
いった会話の基本的な構造を確率・統計的にモデル化することができた．
また，得られた確率的対話モデルを，情報理論的な観点から評価した．","One of the most interesting issues in corpus-based studies
is deriving linguistic knowledge via automated procedures.
Most works, however, have focused on deriving lexico-syntactic knowledge.
In the work described here,
we automatically deduce dialogue models from a corpus
with probabilistic methods.
The corpus is a subset of the ATR Dialogue Database, and consists of
simulated dialogues between a secretary and a questioner at international conferences.
Each utterance is annotated with a speaker label and an utterance type,
called IFT (Illocutionary Force Type), which is an abstraction of
the speaker's intention in terms of the type of action the speaker
intends by the utterance.
We use two kinds of probabilistic methods
to model the speaker-IFT sequences of the corpus:
(1) an Ergodic HMM (Hidden Markov Model) and (2) the ALERGIA algorithm,
an algorithm for learning probabilistic automata by means of
state merging.
By analyzing the derived dialogue models,
we see that both methods capture the basic characteristics
of the local discourse structure, such as turn-taking and speech act sequencing.
We also describe the quality measurement of the dialogue models
from the information-theoretic viewpoint.","['はじめに', 'IFT 付きコーパス', 'エルゴード HMM による対話構造のモデル化', '状態マージング手法による対話構造のモデル化', 'おわりに']",,,,,,,,
V05N01-01.tex,,Reinventing Part--Of--Speech Tagging,,"Part--of--speech tagging methodology has succeeded, 
but on problems that may lack real--world application. Redirection of
the field is indicated, toward potentially more useful, but harder and
more sophisticated tagging tasks: (1) using much more detailed tagsets
(semantically {\em and","['Introduction: Building On The Successes To Date In Part--Of--Speech Tagging', 'Real--World Part--Of--Speech Tagging', 'The Non--Dictionary', 'Appendix']",,,,,,,,
V05N01-02.tex,待遇表現の丁寧さの計算モデル\\ − 語尾の付加による待遇値変化 −,A computational model for  politeness of expressions - changes of politeness with word endings addition -,"待遇表現の丁寧さの計算モデルとして，待遇表現に語尾を付加した際の待遇値（待遇表現の丁寧さの度合い）の変化に
関する定量的なモデルを提案した．このモデルでは
（１）それぞれの待遇表現に対し，その表現が用いられるべき状況が待遇値に関する正規分布として表される，
（２）それぞれの語尾に対し，その語尾が付加される待遇表現が用いられるべき状況が待遇値に関する正規分布
として表される，というふたつの仮定を立て，待遇表現に語尾を付加した際に得られる情報量を定義した．そして更に，
語尾の付加による待遇値の変化量は，付加の際得られる情報量に関する一次式で表すことができる，という仮定を立て，
語尾の付加による待遇値の変化量を，語尾が付加される前の待遇表現に対する待遇値の関数として定義した．この
モデルの妥当性を検証するため，ふたつの異なった発話状況において用いられる待遇表現のグループそれぞれに対し，
語尾の付加による待遇値変化を求める心理実験を行った．その結果，いずれのグループにおいても語尾の付加による待遇値変化は，提案されたモデルによって予測された傾向に従い，モデルの妥当性が支持された．","A computational model for  the degree of politeness changes with the addition of word endings  in  
polite expressions is proposed.  In this model, two stochastic features are assumed as follows: 
(1) for each polite expression, the situation where the polite expression is likely to use can be 
expressed  as a probability distribution of politeness value, and 
(2) for each ending word, the situation corresponding to the most suitable  polite expression to which  
each ending word adds  can be expressed  as a  probability distribution of politeness value.
The degree of politeness changes with the addition of word endings is defined by 
the  amount of information obtained from the addition of word endings.
 The result of the psychological experiments   supports  the validity of the proposed model.","['はじめに', '語尾の付加による待遇値変化の計算モデル', '計算モデルから予測される，語尾の付加による待遇値変化', 'モデルの妥当性の検証実験', '実験結果', '考察', 'まとめ']",,,,,,,,
V05N01-03.tex,,PP Attachment Ambiguity Resolution\\ through Supervised Learning,,"This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity. We propose a new supervised learning method for PP-attachment based on a semantically tagged corpus. Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags. We present the results of our approach, which not only surpasses any existing method but also draws near human performance.","['Introduction', 'Word Sense Disambiguation', 'PP-Attachment', 'Training and Testing Data', 'Evaluation and Experimental Results', 'Conclusion and Further Work']",,,,,,,,
V05N01-04.tex,セグメントの分割と統合による文章の構造解析,Text Structuring by Composition\\ and Decomposition of Segments,"本研究では，論説文の文章構造についてモデル化し，それに基づいた文章解析手
法について論じる．
\indent
近年のインターネットや，電子媒体の発達などにより大量の電子化された文書が
個人の周囲にあふれてきているが，大量の文書を高速に処理するためには，記述
されている領域に依存した知識を前提とせず，なるべく深い意味解析に立ち入ら
ない「表層的」な処理により行なうことが求められる．
\indent
ここで提案する手法での構造化は，文末の表層的な情報によるモダリティの解析
に依る．これを基に文章の論説モデルを定義する．
\indent
文章解析のトップダウン的アプローチとしては，文章のセグメンテーションの手
法を応用し，評価関数の値の大きい箇所から分割していく．文章解析のボトムアッ
プ的アプローチとしては，修辞関係に着目したセグメント統合により隣接してい
て関係が強いところから統合していく．ここで提案する手法は，構造木の葉に近
い部分をボトムアップ的解析で，根に近い部分をトップダウン的解析で処理する
ことにより，一方の欠点を他方の利点で補う効果的なものである．
\indent
本研究のような対象においては，解析結果を正解と不正解の２値に分けてしまう
のでは評価としては不十分であり，正解に近いものはそれなりに評価してやる必
要がある．これについて，構造木の根に近い部分は形式段落の位置に基づく客観
的評価，葉に近い部分は人間が解析したものとの比較，全体的な構造に対しては
個々の解析結果を人間が検討することにより本手法の評価を行う．","In this paper, we present a structure model for editorial texts and
discuss a text analysis method based on the model.
\indent
A large amount of digitalized documents flow through the media of the
INTERNET, CD-ROMs and so on even for personal surroundings.  In order to
proceed such documents at high speed, the process should be as
``superficial'' as possible and any specialized knowledge should be
required as little as possible.
\indent
The structuring in our method relies on the analysis of modalities which
appear superficially at the tail of Japanese sentences.  We define the
text structure model of editorials.
\indent
As a top-down approach for text analysis, we apply a text segmentation
method, in which a text is incrementally divided according to the the
evaluation function.  As a bottom-up approach, based on the rhetorical
relation between two neighboring segments, the segments are composed to
one according to the strength of the relation.  Our approach emploies
only the merits of the two, that is, the leaves of a structure tree are
analyzed in a bottom-up manner whereas nodes around the root are
decomposed in a top-down manner.
\indent
For the evaluation, we discuss our method from three points of view:
(1)objectively agreements checking between formal paragraphs and the
upper part around the root of structure trees, (2) agreements checking
of the lower part around leaves of trees between human and our method,
and (3) human checking of structures generated by our method.","['はじめに', '文章の論説モデル', '文章解析のトップダウン的アプローチ', '文章解析のボトムアップ的アプローチ', 'トップダウン的アプローチとボトムアップ的アプローチの融合', '解析システムと実験', 'まとめ']",,,,,,,,
V05N01-05.tex,単語の位置情報に基づくコーパスからの コロケーションの自動抽出,Automatically Extracting Collocations Based on Words Position Information in Corpora,"コロケーションの知識は，単語間の共起情報を与える言語学的に
重要な知識源であり，機械翻訳をはじめとする自然言語処理において，
重要な意味をもっている．
本論文では，コーパスからコロケーションを自動的に抽出する
新しい手法を提案する．提案する手法では，コーパス中の各単語の
位置情報を用いて，任意の文中のコロケーションを連続型・不連続型
の別に抽出する．
また，提案した自動抽出法を用いて，ATR対話コーパスから
コロケーションを抽出する実験を行った．
本実験で得られた結果は，連続型・不連続型コロケーションともに
重要な表現が抽出されており，提案した抽出法の有効性を示すこと
ができた．","Collocations, which are cohesive and recurrent word clusters,
play an \mbox{important","['はじめに', 'コロケーションの自動抽出の考え方と特徴', 'ALERGIAアルゴリズムによる決定性確率有限オートマトンの構成', '単語位置情報に基づく自動抽出アルゴリズム', 'コロケーションの抽出実験', 'おわりに']",,,,,,,,
V05N01-06.tex,確率的クラスタリングを用いた文書連想検索,Associative Document Search using a \\Probabilistic Document Clustering,"本論文では，指定した文書と類似する文書を検索する文書連想検索のための確
率的クラスタリング HBC (Hierarchical Bayesian Clustering) を提案する．
文書連想検索を実現する際の問題点は，類似文書の検索に時間がかかることで
ある．単純な網羅検索では，比較対象の大きさ $N$ に比例した $O(N)$ の検
索時間を要する．本論文では，クラスタ検索と呼ばれる検索手法を用いること
でこの問題を解決する．クラスタ検索では，通常，クラスタリングによりクラ
スタの二分木をあらかじめ構築しておき，その上でトップダウンに二分木検索
を行うため，検索時間を $O(\log_2 N)$ に抑えることができる．ところが，
従来のクラスタ検索では，検索時に使う距離尺度とクラスタリング時に使う距
離尺度が直接関係ないため，単純な二分木検索では十分な検索精度が得られな
かった．それに対し HBC は，クラスタリングの対象文書を自己検索した際の
精度を最大化するため，検索により適したクラスタリングである．実験では，
「現代用語の基礎知識」を用いて，HBC を用いたクラスタ検索が Ward 法を用
いた従来のクラスタ検索よりも優れていることを実証する．また，「Wall
Street Journal」を用いて，HBC を用いたクラスタ検索が網羅検索に比べノイ
ズ頑健性に優れていることを実証する．","This paper presents a hierarchical clustering algorithm called HBC
(Hierarchical Bayesian Clustering) for associative document search which
is retrieving similar documents to a given query document. A major issue
in realizing an associative document search is its efficiency in
searching similar documents. A straightforward exhaustive search takes
$O(N)$ search time. In this paper we discuss the use of cluster-based
search in which a document collection is automatically organized \mbox{into","['はじめに', 'クラスタ検索', '確率的クラスタリング(HBC)', '実験', 'おわりに', '$P(C|d)$ の推定法', '階層的ベイズクラスタリング(Hierarchical Bayesian Clustering)']",,,,,,,,
V05N01-07.tex,日本語文章における表層表現と用例を用いた\\動詞の省略の補完,Resolution of Verb Phrase Ellipsis\\ in Japanese Sentences using Surface Expressions and Examples,"自然言語では，
動詞を省略するということがある．
この省略された動詞を復元することは，
対話システムや高品質の機械翻訳システムの実現には不可欠なことである．
そこで本研究では，この省略された動詞を
表層の表現(手がかり語)と用例から補完することを行なう．
解析のための規則を作成する際，
動詞の省略現象を
補完する動詞がテキスト内にあるか
いなかなどで分類した．
小説を対象にして実験を行なったところ，
テストサンプルで再現率84\%，適合率82\%の精度で解析できた．
このことは本手法が有効であることを示している．
テキスト内に補完すべき動詞がある場合は非常に精度が良かった．
それに比べ，
テキスト内に補完すべき動詞がない場合はあまり良くなかった．
しかし，テキスト内に補完すべき動詞がない場合の問題の難しさから
考えると，少しでも解析できるだけでも
価値がある．また，コーパスが多くなり，
計算機の性能もあがり大規模なコーパスが利用できるようになった際には，
本稿で提案した用例を利用する手法は
重要になるだろう．","Verb phrases are sometimes omitted in natural language (ellipsis). 
It is necessary 
to resolve the verb phrase ellipses in language understanding, 
machine translation, and dialogue processing. 
This paper describes a practical way to resolve verb phrase ellipses 
by using surface expressions and examples. 
To make heuristic rules for ellipsis resolution 
we classified verb phrase ellipses 
by checking whether the referent of a verb phrase ellipsis appears 
in the surrounding sentences or not. 
We experimented with the resolution of verb phrase elipses 
on a novel and obtained a recall rate of 84\% and 
a precision rate of 82\% on test sentences. 
This indicates that our method is effective. 
In the case 
when the referent of a verb phrase ellipsis appeared 
in the surrounding sentences, 
the accuracy rate was very high. 
But, in the case 
when the referent of a verb phrase ellipsis did not appear 
in the surrounding sentences, 
the accuracy rate was not so high. 
Since the analysis of this phenomena is very difficult, 
it is valuable to propose 
a way of solving the problem to a certain extent. 
When the size of corpus becomes larger and 
the machine performance becomes greater, 
the method of using corpus will become effective.","['はじめに', '動詞の省略現象の分類', '照応処理システム', '実験と考察', 'おわりに']",,,,,,,,
V05N02-01.tex,日韓機械翻訳における様相テーブルに基づいた\\韓国語述部の生成処理,Generation of Korean Predicates Based on Modality-Feature Ordering and Lexicalizing Table in Japanese-Korean Machine Translation,"日韓機械翻訳を研究している多くの研究者らは両国語の文節単位の
語順一致のような類似性を最大に生かすため，直接翻訳方式を採択
している．しかし，日本語と韓国語の述部間には，対応する品詞の
不一致，局部的な語順の不一致，活用ルールの不一致，時制表現の
不一致などが解決しにくい問題として残っている．本稿では述部表
現の不一致を解決するため“様相テーブルに基づいた韓国語の生成
方法”を提案し，それに対して体系的な評価を行なう．この方法は
述部だけを対象にする抽象的で意味記号的な様相資質をテーブル化
し，両国語の述部表現のＰＩＶＯＴとして用いることにより，述部
の様相表現の効果的な翻訳を可能とする．朝日新聞と日本語の文法
本から抽出した２，３３８個の例文を対象に述部の翻訳処理を試み
た結果，約９７．５％が自然に翻訳され，述部翻訳の際，本方法が
有効であることが確認できた．","Both Korean and Japanese share many grammatical 
characteristics including the same word order, so that almost all 
Japanese-to-Korean machine translation systems would have 
adopted the direct translation strategy to take advantage of the 
similarities. Even in the direct translation for the very similar 
language pair, however there are still a lot of problems that have 
to be solved for high-quality translation. Out of them we only 
focus on the predicate translation, whose difficulty is caused not 
only by complex conjugation but also by the inconsistent syntactic 
category and the different relative order of modal expressions 
between two languages. To solve the difficulty, we propose a 
table-driven predicate generation in which a modality-feature 
ordering and lexicalizing table (called MFOLT) plays an 
important role to map Japanese predicates into their Korean 
equivalents via abstract pivot of symbolic modality features. 
Experimental evaluation was done with 2,338 sentences 
extracted from Asahi newspaper and some Japanese grammar 
books, which turned out that the proposed method would make a 
good effect on predicate translation, showing the success rate of 
97.5\%.","['はじめに', '様相テーブルの導入', 'ＭＦＯＬＴを用いた韓国語述部の生成', '実験および評価', '結論']",,,,,,,,
V05N02-02.tex,,A Method for Syntactic Behavior Analysis,,"We show how a treebank can be used to cluster words on the basis of their
    syntactic behavior. By extracting statistics on the structures in which
    words appear it is possible to discover similarities and differences in usage 
    between words with the same part-of-speech.
    This clustering is compared to the conventional clustering based on
    co-occurrences. While conventional clustering can discover semantical 
    similarities or the tendency to appear together, the method we present
    ignores these factors and places the focus on syntactical usage,
    in other words the sort of structures it appears in.
    We present a case study on prepositions, showing how they can be 
    automatically subdivided by their syntactic behavior and we discuss the 
    appropriateness of such a subdivision.
    We have also carried out experiments to compare the quality of clusters
    quantitatively. For this goal we used clusters based on syntactic behavior
    for improving the estimation of the distribution of the dependency relation
    between words. Since such a distribution is necessarily estimated with
    sparse data, an entropy test can show how informative the classes are
    about syntactic usage.
    Finally, we discuss a number of ways in which a classification of words 
    can contribute to applications of natural language processing.","['Introduction', 'Headwords and Dependencies', 'Collecting Statistics for Individual Words', 'Comparison with Co-Occurrence Based Clustering', 'Similarity Measure and Algorithm', 'A Case Study of Prepositions', 'Quantitative Evaluation: Dependency Relations', 'Experimental Results', 'Applications', 'Conclusion']",,,,,,,,
V05N02-03.tex,,"General Word Sense Disambiguation Method\\ 
Based on a Full Sentential Context",,"This paper presents a new general supervised word sense disambiguation method based on a relatively small syntactically parsed and semantically tagged training corpus. The method exploits a full sentential context and all the explicit semantic relations in a sentence to identify the senses of all of that sentence's content words. It solves the sparse data problem of a small training corpus by substituting the words by their semantic classes. In spite of a very small training corpus, we report an overall accuracy of 80.3\% (85.7, 63.9, 83.6 and 86.5\%, for nouns, verbs, adjectives and adverbs, respectively), which exceeds the accuracy of a statistical sense-frequency based semantic tagging, the only really applicable general disambiguating technique. Because the method uses the sentential syntactic structure it is particularly suitable for integration with a probabilistic syntactic analyser.","['Introduction', 'The Task Specification', 'General Word Sense Disambiguation', 'Learning', 'Disambiguation Algorithm', 'DISCOURSE CONTEXT', 'EVALUATION', 'Related Work', 'Conclusion']",,,,,,,,
V05N02-04.tex,形態素クラスタリングによる形態素解析精度の向上,An Improvement of a Morphological Analysis \\ by a Morpheme Clustering,"本論文では，形態素クラスタリングと未知語モデルの改良による確率的形態素解析器の精
  度向上を提案する．形態素クラスタリングとしては，形態素$n$-gramモデルをクロスエン
  トロピーを基準としてクラス$n$-gramモデルに改良する方法を提案する．未知語モデルの
  改良としては，確率モデルの枠組の中で学習コーパス以外の辞書などで与えられる形態素
  を追加する方法を提案する．bi-gramモデルを実装しEDRコーパスを用いて実験を行なった
  結果，形態素解析の精度の向上が観測された．両方の改良を行なったモデルによる形態素
  解析実験の結果の精度は，先行研究として報告されている品詞tri-gramモデルの精度を上
  回った．これは，我々のモデルが形態素解析の精度という点で優れていることを示す結果
  である．これらの実験に加えて，品詞体系と品詞間の接続表を文法の専門家が作成した形
  態素解析器との精度比較の実験を行なった．この結果，確率的形態素解析器の誤りは文法
  の専門家による形態素解析器の誤りに対して有意に少なかった．形態素解析における確率
  的な手法は，このような人間の言語直感に基づく形態素解析器と比較して，現時点で精度
  がより高いという長所に加えて，今後のさらなる改良にも組織的取り組みが可能であると
  いう点で有利である．","This paper proposes improving a stochastic Japanese morphological analyzer
  through a morpheme clustering and an amelioration of the unknown word model.  As
  a morpheme clustering, we propose a method which ameliorates a morpheme-based
  $n$-gram model into a class-based $n$-gram model with cross entropy criterion.
  As an amelioration of the unknown word model, we propose a method to incorporate
  a given morpheme set, such as dictionary, into it.  As the result of experiments
  on the EDR corpus, we observed improvements of the accuracy. The analyzer
  adopting both methods marked a higher accuracy than an anteriorly reported
  part-of-speech-based tri-gram model. This result tells us that our morphological
  analyzer is better than the previous one in terms of accuracy.  In addition to
  these experiments, we compared our analyzer with a grammarian's intuition-based
  analyser. The experimental results have shown the error rate of the stochastic
  analyzer was meaningfully smaller than that of the heuristic analyzer. The
  stochastic approach to Japanese morphological analysis is of great advantage to
  the ad-hoc method in higher accuracy, as well as in facility of further
  organized improvements.","['まえがき', '確率的形態素解析', '未知語モデルの改良', '形態素クラスタリング', '実験結果とその評価', 'むすび']",,,,,,,,
V05N03-01.tex,,An Affective-Similarity-Based Method \\ for Comprehending Attributional Metaphors,,"This paper proposes a new computational method for 
comprehending attributional metaphors.
The proposed method generates deeper interpretations of metaphors 
than other methods through the process of figurative mapping that transfers 
affectively similar features of the source concept onto the target concept.
Any features are placed on a common two-dimensional space 
revealed in the domain of psychology, and 
similarity of two features is calculated as a distance between them in
the space.
A computational model of metaphor comprehension based on the method 
has been implemented in a computer program called {\sfi PROMIME","['Introduction', 'Constructing Metaphorical Correspondences of Features', 'Comprehending Attributional Metaphors by Computer', 'Testing the System', 'Discussion', 'Concluding Remarks']",,,,,,,,
V05N03-02.tex,"Probabilistic GLR Parsing: A New Formalization\\ and Its Impact on
  Parsing Performance",,,,"['Introduction', 'A PGLR Language Model', ""Comparison with Briscoe and Carroll's Model"", 'Expected Impact on Parsing Performance', 'Conclusion']",,,,,,,,
V05N03-05.tex,統計的構文解析における構文的統計情報と 語彙的統計情報の統合について,A Framework of Integrating Syntactic and\\Lexical Statistics in Statistical Parsing,"\quad 
本論文では，構文解析の曖昧性解消を行うために，
構文的な統計情報と語彙的な統計情報を統合する手法を提案する．
我々が提案する統合的確率言語モデルは，
構文的優先度などの構文的な統計情報を反映する構文モデルと，
単語の出現頻度や単語の共起関係などの語彙的な統計情報を反映する
語彙モデルの2つの下位モデルから成る．
この統合的確率言語モデルは，
構文的な統計情報と語彙的な統計情報を同時に学習する
過去の多くのモデルと異なり，
両者を個別に学習する点に特徴がある．
構文的な統計情報と語彙的な統計情報を独立に取り扱うことにより，
それぞれの統計情報を異なる言語資源から独立に学習することが
できるだけでなく，
それぞれの統計情報が曖昧性解消においてどのような効果を
果たすのかを容易に分析することができる．
この統合的確率言語モデルを評価するために，
日本語文の文節の係り受け解析を行った．
構文モデルを用いたときの文節の正解率は73.38\%となり，
ベースラインに比べて11.70\%向上した．
また，構文モデルと語彙モデルを組み合わせることにより，
文節の正解率はさらに10.96\%向上し84.34\%となった．
この結果，本研究で提案する枠組において，
語彙的な統計情報は構文的な統計情報と
同程度に曖昧性解消に貢献することを確認した．","\quad
In this paper,
we propose a new framework of statistical language modeling
integrating syntactic statistics and lexical statistics.
Our model consists of two submodels,
the syntactic model and lexical model.
The syntactic model reflects syntactic statistics,
such as structural preferences,
whereas the lexical model reflects lexical statistics,
such as the occurrence of each word and word collocations.
One of the characteristics of our model
is that it learns both types of statistics separately,
although many previous models learn them simultaneously.
Learning each submodel separately enables us to
use a different language source for different submodels,
and to make understanding of each submodel's behavior
much easier.
We conducted a preliminary experiment,
where our model was applied to the disambiguation
of dependency structures of Japanese sentences.
The syntactic model achieved 73.38\% in {\it Bunsetu\/","['はじめに', '統合的確率言語モデル', '評価実験', 'おわりに']",,,,,,,,
V05N04-01.tex,,"A Hybrid Approach for Resolving Ambiguities \\ 
\hspace*{26mm",,"This paper describes a  method  in determining syntactic structure 
for coordinate constructions.   It is based on  the information taken from semantic 
similarities, selectional restrictions, and  some other linguistic cues.
We discuss the role the information plays in resolving ambiguities that
appear in coordinate constructions, describe the means of acquiring the
necessary information automatically
from two on-line corpora and a lexical database, and devise two algorithms for
disambiguating coordinate constructions.  An experiment that follows 
shows effectiveness  of our  method and its applicability to  resolving
ambiguities in  some other syntactic structures.","['Introduction', 'Identifying Modification Relation in Coordination', 'Disambiguation for Structure of Coordinate Constructions', 'Discussion']",,,,,,,,
V05N04-02.tex,,The Application of Classification Trees to Bunsetsu Segmentation of Japanese Sentences,,"In conventional bunsetsu segmentation methods  for  Japanese sentences，
segmentation rules have been given manually. This causes
difficulties in maintaining the consistency of the rules,
and in deciding an efficient order of rule application.
This paper proposes a method of automatic bunsetsu segmentation
using a classification tree,
by which knowledge about bunsetsu boundaries is automatically
acquired from a corpus, and an  efficient order of rule application 
 is realized automatically.
 It can adapt quickly to a new system of parts of speech, and
also to a new task domain without
the need for changing the algorithm.
Generation of classification
trees for bunsetsu segmentation and  evaluation experiments  were
carried out on an ATR corpus and an EDR corpus.
The  segmentation  accuracy of 98.9\%   was achieved   for the ATR
corpus, and 96.2\% for the EDR corpus. The method was compared  with
a simple rule-based method and  the Bayes decision rule on the ATR  corpus. 
The proposed method outperformed  the 
rule-based method when the training data size was larger than about 20
sentences, and outperformed the Bayes decision rule over the whole range of
training data sizes.
The superiority of the proposed method   was more evident
over the former when the training data size was larger,  and  over the latter 
 when the training data size was  smaller.","['Introduction', 'Classification Tree', 'Application of Classification Trees to Bunsetsu Segmentation', 'Experiments on  ATR Corpus', 'Experiments on  EDR Corpus', 'Conclusion']",,,,,,,,
V05N04-03.tex,文書走査を用いた複合名詞解析,Analysis of Japanese Compound Nouns by Direct Text Scanning,"複合名詞は文書の内容を凝縮できる程の情報を担うことができるため重要語となりやすく，しばしば文書内容を理解する上での鍵となる．このため，複合名詞解析（＝その構成要素間の掛かり受け解析）は，機械翻訳にとどまらず，情報抽出や情報検索の高度化にも貢献すると期待されている．しかし，複合名詞は単なる名詞の連鎖に過ぎないため構文上の手掛かりが無く，人手で構成したルールや，シソーラスに記述された概念の共起尤度等を用いて解析する方法が提案されてきた．しかし，新聞記事などの未登録語が頻出する開いた大規模テキストを扱う場合は想定されてこなかったため，そのような場合には頑健性の点で問題が生じる．
本論文は，大量の電子化文書が高速に処理可能な昨今の状況を念頭に置き，シソーラス等の予め固定されたデータを用いるのではなく，文書中から直接文字列レベルの共起情報を抽出するだけで，高い精度で複合名詞解析が可能なことを示す．まず，与えられた複合名詞を暫定的に形態素解析し，得られた構成単語の共起情報を複数のテンプレートを用いて抽出する．共起情報を抽出する段階で，語の出現状況から，複合名詞内の短い複合名詞や，誤って過分割された略称等の未登録語を検出すると同時に，これらの共起情報を抽出することにより，未登録語に対する頑健性が達成される．これに加えて，共起情報が不足する場合のヒューリスティクスに関して検討を加え，文書から直接得られる共起情報と若干のルールを併用することにより，高精度な複合名詞解析が達成できた．
新聞記事から抽出した長さ5, 6, 7, 8の複合名詞各100個を対象に実験を行った結果，新聞1年分を\break
用いて，それぞれ90，86，84，84個の正解が得られた．","Compound nouns tend to be important words because a compound noun conveys a lot of information which can even summarize a document. Therefore the analysis of compound nouns can contribute to machine translation, information extraction, or information retrieval. 
	Since compound nouns lack syntactic clues, existing methods have utilized manually written rules and thesauri in order to analyze word dependency structure in compound nouns. Consequently the methods lack robustness in treating open corpora such as newspaper articles which contain a number of unregistered words. 
	This paper presents a thesaurus-free corpus-based approach which scans a corpus with a set of templates and extracts co-occurrence data of the nouns which construct the compound noun. Unregistered words such as abbreviations and short compound nouns are detected in the process of template-matching and the co-occurrence data of the newly found words are additionally extracted, which leads to the robustness and high accuracy of the analysis. 
	The accuracy of the method was evaluated using 400 compound nouns of length 5, 6, 7, and 8. The numbers of the correct analysis were 90, 86, 84, and 84 in 100 compound nouns of length 5, 6, 7, and 8 respectively.","['はじめに', '複合名詞解析の構成', '従来手法と問題点の分析', '文書走査による複合名詞解析', '実験結果', '考察', 'おわりに']",,,,,,,,
V05N04-04.tex,,Symmetric Pattern Matching Analysis \\ for English Coordinate Structures,,"The authors propose a model for analyzing English sentences including
coordinate conjunctions such as ``and'',``or'',``but'' and 
equivalent words.
The syntactic analysis of English coordinate sentences is one of the
most difficult problems in machine translation (MT) systems.  The
problem is selecting, from all possible candidates, the correct
syntactic structure formed by an individual coordinate conjunction,
i.e. determining which constituents are coordinated by the conjunction.
Typically, so many possible structures are produced that MT systems
cannot select the correct one, even if the grammars allow us to write the
rules in simple notations.
This paper presents an English coordinate structure analysis model,
which provides top-down scope information on the correct syntactic
structure by taking advantage of the symmetric patterns of
parallelism.
The model is based on a balance-matching operation for two lists of
feature sets. It has four effects, namely: a reduction in
analysis costs, a decrease in word disambiguation, the
interpretation of ellipses, and robust analysis.
This model was practically implemented and incorporated into the
English-Japanese MT system, and it had about 70\% accuracy for 3215 Wall
Street Journal sentences.","['Introduction', 'Problems with conjunctions', 'Parallelism of conjunctions', 'Balance matching analysis model', 'Experiments and Discussion', 'Concluding Remarks', 'Acknowledgments']",,,,,,,,
V05N04-05.tex,コンパラブルコーパスと対訳辞書による\\日英クロス言語検索,"Japanese-English Cross Language\\ 
Information Retrieval based on Comparable Corpora and Bilingual Dictionary","クロス言語検索手法 GDMAX は, 日本語入力から英語ドキュメントの検索を可
能にする. GDMAXは, 対訳辞書によって入力キュエリから翻訳キュエリ候補を
生成し, キュエリからそれぞれの言語のコーパスにおけるキュエリタームの共起
頻度を成分とする共起頻度ベクトルを生成する. 
入力共起頻度ベクトルと翻訳共起頻度ベクトルとの距離によって, 翻訳キュ
エリ候補をランキングし, 上位の英語キュエリ集合を検索キュエリとする. こ
の手法によって, 一つの対訳だけでなく適切な複数の訳語集合を英語キュエリ
として得ることができる. ウォールストリートジャーナルやAP通信な
ど2ギガの英語ドキュメントについて適合率と再現率で評価したところ, 
理想訳と比べて約62\%の精度を得て, 
対訳辞書のすべての訳語候補を用いる場合と比べて12\%, 機械
翻訳による訳語選択と比べて6\%高い精度を得ることができた.","This paper proposes a method to translate query terms for
cross-language information retrieval (CLIR).  CLIR is generally
performed by query translation and information retrieval (IR). CLIR is
less precise than IR because of query term translation ambiguities,
especially in Japanese and English CLIR. We developed
Double MAXimize criteria
based on comparable corpora (DMAX), which is an equivalent
translation selection method for machine translation (MT) , by using
term co-occurrence frequency in comparable corpora. Though a term
should be translated into one word for MT, a query term should be
translated into several appropriate terms for CLIR. This paper
describes a generalized query term selection model, the GDMAX for CLIR.
In this model, a source query is represented in the vector form of the
term co-occurrence frequency in source corpora. Translation queries are
searched by vector similarity calculation between a source query and a
target query represented by the co-occurrence frequency in comparable
target corpora.  GDMAX was evaluated by using 
TREC6 (Text Retrieval
Conference) English data and 15 Japanese queries. GDMAX queries had
approximately 62\% accuracy of human queries, and
6\% higher accuracy than machine translation queries and 12\% higher
accuracy than bilingual dictionary-based queries.","['はじめに', 'キュエリタームの訳語選択', 'GDMAX 訳語選択法', '実験・評価', '考察', '関連研究', 'おわりに']",,,,,,,,
V05N04-06.tex,点字翻訳ボランティアのための\\対話型分かち書き支援システム,"Japanese Sentence Segmentation System \\ for Translating
Japanese into Braille","日本語文書を点字に翻訳する問題をとりあげ, 分かち書きのため
の規則を分類, 整理して知識ベース化し, システムが判断し難い箇所のみを選
択的にユーザに提示する対話型の分かち書き支援システムを提案する. このシ
ステムでは文法情報を含む大規模な辞書の代わりに見出し語のみからなる小規
模なテーブルを用いることにより, 辞書構築の手間を削減した. 従来より日本
語を点字に翻訳するシステムは過去にいくつか提案され市販されているが, 処
理は一括して行われ, ユーザの介入する余地はない. システムが誤って翻訳し
た箇所については点字翻訳ボランティアが全文を見直す必要があり, 実際には
利用し難いのが現状である. ここでは分かち書きの規則を知識ベース化してア
ルゴリズムから独立させ, システムとユーザが協調することによって, 日本語
点字翻訳のための分かち書きを対話的に行うシステムについて述べる. 本シス
テムで情報処理関連のテキストを処理し, その有効性を確認した.","Many Japanese sentence segmentation algorithms have been proposed to
translate Japanese into English or to query databases.  Those methods
use a huge dictionary including word representation, readings, and
grammar references which require considerable time and work.
Since Braille needs only blanks and phonetic information, we do not
have to check grammatical combination of words.
We propose a new system to segment the Japanese sentence in order to
translate Japanese into Braille.  Our methods uses a knowledge base
which categorizes Japanese sentence segmentation rules.
Segmentation rules for translating into Braille are heuristic,
ambiguous and complicated.  Software is available but the user
interface is not very good and volunteers rarely use it.  So we
provide a user interface for checking the position of ambiguous
segmentation.  In this way, the users' workload is reduced since it is
no longer necessary to check all parts of the sentences.
In our method, only a few small tables 
including words with the segmentation patterns
are necessary.  Our knowledge
base does not need any grammatical information, but utilizes surface
information such as Kanji, Hiragana, Katakana, and other character types.
The accuracy of segmentaion is 98.0\% -  a higher rate than that
found in usual methods.","['はじめに', '分かち書きの規則と問題点', 'システム構成と分かち書き手順', '実験', 'おわりに']",,,,,,,,
V05N04-07.tex,意味的類似性を用いた音声認識正解部分の特定法と\\正解部分のみ翻訳する音声翻訳手法,"Correct part extraction from speech recognition results using semantic dictance calcuation and speech translation\\ 
by translation of extracted parts only","音声対話および音声翻訳システムを実現するためには, 自由発話文
の音声認識誤り文に対する解析誤りの問題を解決する必要がある. その解決のた
めに, 文法以外の制約を積極的に用いて認識誤り文から正しく認識された部分を
特定するしくみを新たに導入し, 特定された部分, 或は, 特定されなかった部分
を修復しながら, 文を解析することが必要となる. 本論では, 予め学習された話
し言葉の表現パターンと入力文における表現パターンとの意味的類似性を用いて, 
認識結果文から正しく認識された部分を特定する手法を提案する. さらに, 本正解部分特定法を音声翻訳システムに導入し, 音声認識結果の正解部
分のみを部分翻訳するシステムを作成した. このシステムを用いて正解部分特定
法の効果を評価し, その結果から次の効果を確認した. 本正解部分特定法により
特定された部分の信頼性は高く, 特定した部分の96\%が実際に正解部分であった. 
また, 特定された部分のみを提示することにより, 誤り文をそのまま誤った意味
に理解してしまう割合を半分以上軽減することができた. さらに, 特定された正
解部分のみを部分翻訳した結果, 従来翻訳できなかった誤り文の約７割に対して, 
正しいかもしくは部分的に正しく意味を理解できる翻訳結果を得ることができた.","This paper proposes a method for extracting the correct parts
from speech recognition results by using an example-based approach for
parsing those results that include several recognition errors. Correct parts are extracted using two factors: (1) the semantic distance
between the input expression and example expression, and (2) the
structure selected by the shortest semantic distance. The examination results showed that the proposed method is able to
efficiently extract the correct parts from speech recognition
results. About ninety-six percent of the extracted parts are
correct. The results also showed that the proposed method is effective
in understanding misrecognition speech sentences and in improving speech
translation results. The misunderstanding rate for erroneous sentences
is reduced about half. Sixty-nine percent of speech translation results
are improved for misrecognized sentences.","['はじめに', '正解部分特定法', '評価', '考察', 'まとめ']",,,,,,,,
V05N04-08.tex,日韓機械翻訳システムの現状分析および開発への提言,Analysis of Current Commercial Japanese to Korean Machine Translation Systems and Suggestions for Future Development,"\quad
本技術資料は, 現在入手可能な日韓機械翻訳システムを対象に翻訳品質の評価を行い, 
日韓機械翻訳システムの現状および技術水準を把握, 今後の研究方向についてのいくつかの提言を行うことを目的とする. 
現在, 韓国国内で発表, あるいは発売されている日韓機械翻訳システムの中で, 入手可能な四つの製品に対して
ユーザサイドからの翻訳品質の分析と言語学的な解決範囲を把握するための対照言語学的誤謬分析を行う. 
さらに, \cite{choiandkim","\quad
This Report describes the status and performance of current Janpanese-to-Korean Machine Translation Systems. 
And some suggestions for developing the better systems are made. This result is made by analyzing the latest version 
of four commercial Japanese-to-Korean Machine Translation Systems in Korea which have been produced until Feburary 1997. 
Declarative evaluation is executed in the view of\break 
user side to measure the translation quality. Typological evaluation is tried  
to probe the linguistic coverage of current commercial systems. Operational evaluation is performed in the view of user interface. 
And progress evaluation is executed by comparing the result with the result reported at \cite{choiandkim","['まえがき', '韓国国内の日韓機械翻訳システム', '評価方法の考察', '評価の準備および方法', '評価の実施および結果分析', '結論']",,,,,,,,
V06N01-01.tex,決定木学習による日本語対話文の格要素省略補完,Case Ellipsis Resolution in Japanese Dialogues via Decision-Tree Learning,"機械翻訳では目的言語で必須格となる格の人称と数を補う必要がある。本論文
では、省略補完知識の決定木による表現、及び帰納的に機械学習することによっ
て日本語対話文の格要素省略を補完する手法を提案する。本研究では形態素分
割され、品詞、省略情報が付与された任意のコーパスとシソーラスのみを用い
て行なう。決定木学習には、内容語の意味属性、機能語の出現、言語外情報の
3種類の属性を使用する。未学習文に対してテストを行なった結果、ガ、ヲ、
ニの三つの格で照応的な省略の補完を十分な精度で行なうことができた。また
ガ格とニ格に対しては人称と数の補完にも有効であることを確認した。ガ格に
関して、処理の有効性を学習量、話題依存性、使用属性との関係の三点から実
験し、以下の知見が得られた。(1)当該問題に対する決定木学習量は全体とし
て$10^4\sim10^5$事例で十分である。この時の補完精度の上限は$80\%
\sim 85\%$と予想される。(2)対話の話題が既知もしくは予測可能な時は、そ
の話題のみのコーパスによる学習が最善である。話題が未知の場合は、可能な
限り広範な話題に対して学習するのが最も効果的である。(3)学習量増加に伴
い、決定木には機能語などの話題に依存しない属性が多く採用される。","A decision-tree learning approach to ellipsis resolution that appears
in Japanese dialogue is proposed.  The method has a high flexibility
since it only requires any dialogue corpus with part-of-speech and
ellipsis taggings and any thesaurus.  We provide three kinds of
attributes to machine learning: semantic category of content words,
functional words, and exophoric information. By the open tests against
Japanese dialogue corpus in the topic of travel arrangement, it is
proven that the proposed method performs satisfactory resolution
accuracy against `ga' (subject) and `ni' (indirect object) cases.  The
following findings are also obtained in the discussion: (1) learning
topic should be restricted if it is expected, otherwise widely-learned
decision tree may perform best.  (2) as a decision tree learns more,
it tends to use more general attributes such as functional words.  The
problem of data size relative to decision-tree training is also
discussed and found that resolution accuracy of proposed method may be
saturated in $10^4\sim10^5$ samples. Resolution accuracy of $80\% \sim
85\%$ is expected at the highest.","['はじめに', '日本語における格要素の省略現象', '決定木を用いた省略補完', '実験', '議論', '結論', '決定木の例', '決定木学習に使用した発話の例']",,,,,,,,
V06N01-02.tex,長さ可変文脈を用いたマルチニューロタガー,A Multi-Neuro Tagger\\ Using Variable Lengths of Contexts,"本稿は最長文脈優先に基づいて長さ可変文脈で品詞タグづけを行うマルチニュー
ロタガーを提案する.
マルチニューロタガーはそれぞれ長さの異なる文脈を入力とした複数の
ニューラルネット（それぞれをシングルニューロタガーと呼ぶ）とそれらの出力
を選別
する最長文脈優先セレクターで構成される.
個々のニューラルネットの訓練は
それぞれ独立に行なわれるのではなく, 短い文脈での訓練結果（訓練で獲得した
重み）
を長い文脈での初期値として使う. その結果, 訓練時間が大幅に短縮でき, 複数
のニューラルネットを用いても訓練時間はほとんど変わらない.
タグづけにおいては, 目標単語自身の影響が最も強く, 
前後の単語もそれぞれの位置に応じた影響を与えていることを反映させるために,
入力の各構成部分は情報量最大を考慮して訓練デー
タから得られるインフォメーションゲイン（略してIGと呼ぶ）を影響度として重
み付けられる.
その結果, 更に訓練時間が短縮され, タグづけの性能が改善される. 計算機実験
の結果, マルチニューロタガーは, 8,322文の小規模タイ語コーパスを訓練に用いることによ
り, 未訓練タイ語データを94\%以上の正解率でタグづけすることができた.
この結果は, 固定長さを文脈としたどのシングルニューロタガーを
用いた場合よりも優れ, マルチニュー
ロタガーはタグづけ過程において動的に適切な長さの文脈を見つけていることを
示した.","This paper presents a multi-neuro tagger that uses variable
lengths of contexts for part of speech (POS) tagging based on longest
context priority. The tagger is constructed using multiple neural
networks, all of which can be regarded as single-neuro taggers with
fixed but
different lengths of contexts in inputs, and the longest context
priority based selector.
Because the trained results (weights) of the taggers with shorter
lengths
of contexts can be used as initial weights for those with longer lengths
of contexts,
the training time for the latter ones can be greatly reduced and
the cost to train a multi-neuro tagger is almost the same as that to
train a single-neuro tagger.
In tagging, given that the target word is more relevant
than any of the words in its context and the words in context may have
different relevances, each element of the input is
weighted by its relevance with information gain.
Computer experiments show that the multi-neuro tagger
has a correct rate of over 94\% for tagging untrained data
when a small Thai corpus with 8,322 sentences
that we have on hand
is used for training.
This result is better than any of the results obtained using the
single-neuro taggers,
which indicates that the multi-neuro tagger can dynamically find
a suitable length of contexts in tagging.\bigskip","['はじめに', '品詞タグづけ問題', 'インフォメーションゲイン（IG）', 'マルチニューロタガー', '実験結果', '結び']",,,,,,,,
V06N01-03.tex,表題へのつながりに基づく文の重要度評価,Evaluation of Importance of Sentences based on Connectivity to the Title,"本稿では，表層的な情報を手がかりとして文と文のつながりの強
さを評価し，その強さに基づいて重要な文を選び出す手法を提案する．
文の重要度の評価に際して，表題はテキスト中で最も重要な文であり，
重要な文へのつながりが強い文ほど重要な文であるという仮定を置き，
文から表題へのつながりの強さをその文の重要度とする．
二つの文のつながりの強さは，人称代名詞による前方照応と，同一辞書
見出し語による語彙的なつながりに着目して評価する．
平均で29.0文から成る英文テキスト80編を対象とした実験では，
文選択率を25\%に設定したとき，従来手法による精度を上回る再現率78.2\%，
適合率57.7\%の精度を得，比較的短いテキストに対して提案手法が有効である
ことを確認した．","This paper proposes a method of selecting important
sentences from a text based on the evaluation of the connectivity
between sentences by using surface information.
We assume that the title of a text is the most concise statement
which expresses the most essential information of the text, and
that the closer a sentence relates to an important sentence,
the more important this sentence is.
The importance of a sentence is defined as the connectivity between
the sentence and the title.
The connectivity between two sentences is measured based on
correference between a pronoun and a preceding (pro)noun, and 
on lexical cohesion of lexical items.
In an experiment with 80 English texts, 
which consist of an average of 29.0 sentences, the proposed method has
marked 78.2\% recall and 57.7\% precision, with the selection ratio
being 25\%.
The recall and precision values surpass those achieved by conventional
methods, which means that our method is more effective in abridging
relatively short texts.","['はじめに', 'テキストの結束を維持する手段', '文の重要度の評価', '実験と考察', 'おわりに']",,,,,,,,
V06N01-04.tex,,The Concept of Sensitive Word in Chinese - A Survey in a Machine-Readable Dictionary,,"In Machine Translation (MT), using compound words or phrases often makes the translation process easier. For 
example, the phrase \ebox{59-2-1.eps","['Introduction', 'Ambiguities in segmentation', 'Approaches to Chinese segmentation', 'Sensitive words and their impact on MT', 'Conclusions and Future work']",,,,,,,,
V06N02-01.tex,単語単位による日本語言語モデルの検討,A Word-based Japanese Language Model,"日本語では単語の境界があいまいで，活用等のルールに基づいて定義された単位である
形態素は必ずしも人が認知している単語単位や発声単位と一致しない．
本研究では音声認識への応用を目的として
人が潜在意識的にもつ単語単位への分割モデルとその単位を用いた
日本語の言語 ({\it N","This paper deals with a word-based language model of Japanese.
In Japanese, word boundaries are not stable and grammatical units do not
necessarily coincide with human intuition. For accurate segmentation
it is therefore necessary to create a vocabulary set that covers human
utterance units.
In our word-segmentation method, a model of word boundary is described by morphological
parameters (i.e. part of speech), which are learned by comparing
results of human segmentation with those of Japanese morphological
analyzer. Then by using pseudo-random number and the model,
it is determined whether each morpheme transition is a word
boundary. As a result, we obtain a vocabulary set and learning data
for Japanese language model automatically. According to our experiments using
articles from three newspaper and appended texts in network-based forums,
about 44,000 words cover 94-98\% of all words in the test data, and the average numbers of
words per sentence are 12-19\% smaller than those of morphemes.
The parameters of word segmentation
model and language model are quite different in newspaper articles and forum's texts.
However,
the difference does not exist in the probabilities of common events, but in the
kinds of events. Therefore the language
model, which was created from newspaper articles and forum's text, gave
the satisfactory results for both test set.","['はじめに', '単語単位への分割', '形態素解析プログラムの変更', '分割モデルの作成と分割過程', '語彙とコーパス', '単語単位による言語モデル', 'おわりに']",,,,,,,,
V06N02-02.tex,単語知識を必要としない高精度な言語モデル,Precise Language Models \\ Requiring No Lexical Knowledge,"本論文では，知識に依存しない，\mbox{高い曖昧性削減能力を持つ新しい言語モデル
を提案","This paper proposes a novel, knowledge-free language model with great
ability in reducing ambiguity. This model is defined as $n$-gram of
string which is referred to ``superword,'' and belongs to a superclass
of traditional word or string $n$-gram models' class. The concept of
superword is based on only one principle---repetitionality in training
text. The probabilistic distribution of the model is learned through
the forward-backward algorithm. Experimental results showed that the
performance of superword model combined with character trigram model
was superior to the traditional word model based on morphological
analysis, and traditional string-based model.","['はじめに', 'superwordモデルの定式化', 'superwordモデルの学習法', '長さ制限の導入', '複合モデル', '評価実験', 'あとがき']",,,,,,,,
V06N02-03.tex,品詞および可変長形態素列の 複合Ｎ─ｇｒａｍを用いた日本語形態素解析,"Japanese Morphlogical Analysis using\\ 
Composite Part-of-speech and Morpheme Sequence N-gram","本論文では，
日本語連続音声認識用のN-gram言語モデルの学習に用いる形態素データを，
テキストデータから自動的に生成することを目的として，
品詞および可変長形態素列の複合N-gramを用い，
日本語テキストデータを自動的に形態素解析する手法を提案する．
複合N-gramは，品詞，形態素，形態素列を単位としたN-gramで，
少ないデータ量から高い予測精度を持つ言語モデルである．
また，品詞から未知語が出現する確率を定式化することにより，
未知語の形態素解析を行えるようにモデルの改良を行った．
形態素解析実験の結果，
複合N-gramの形態素同定率は最高99.17\%で，
従来のルールベースによる方法よりも正確に形態素の同定が行えることが判明し，
提案手法の有効性を確認した．
また，読みまで含めた評価を行った場合でも，
最高98.68\%の正解率が得られた．
未知語を含む文の形態素解析では，
全ての語いが辞書に登録されている場合と比較して
0.8\%程度の低下に抑えることができた．","In this paper, Japanese morphological analyzer is proposed
using composite part-of-speech(POS)
and morpheme sequence N-gram(Composite N-gram).
Composite N-gram is a N-gram type language model
whoes unit is POS class, morpheme and morpheme-sequence,
which can give an excellent prediction ability from small corpus.
In order to apply unknown words,
we improved the composite N-gram by considering the probability that
unknown word appears from POS class.
Experimental results showed that morpheme accuracy
using composite N-gram reached a maximum of 99.17\%,
which was better than using conventional rule based method.
Considering the pronounciation to the evaluation,
the accuracy was 98.68\%.
When applied to sentences including unknown words,
the fall of the morpheme accuracy was only about 0.8\%.","['はじめに', '日本語連続音声認識のための形態素解析', 'N-gram言語モデルによる形態素解析', '品詞と可変長形態素列の複合N-gram', '未知語を含んだ文の形態素解析', '評価実験', 'むすび']",,,,,,,,
V06N02-04.tex,多段解析法による形態素解析を用いた 音声合成用読み韻律情報設定法と その単語辞書構成,Reading-and-Prosodic Information Generation Using Morphological Analysis based on Multi-level Analysis Method and its Word Dictionary Components,"日本語テキスト音声合成において，自然で聞きやすい合成音声を出力するためには，
読み，アクセント，ポーズ等の読み韻律情報を正しく設定する必要がある．
本論文では，複合語等に対しては部分的に深い解析を行うことを特徴とする
多段解析法に基づく形態素解析を用いて，読み韻律情報を設定する方法，および，
読み韻律情報を設定するために用いる単語辞書情報について述べる．
本方式の主な特徴は，
形態素解析における読み韻律情報付与に対応した長単位認定，
複合語内意味的係り受け情報を用いたアクセント句境界設定，
文節間係り受け情報を用いず，複合語内等の局所構造，およびアクセント句
境界前後単語の品詞情報等から得られるアクセント句結合力を用いて
段階的にポーズを設定する多段階設定法に基づくポーズ設定である．
本方式をニュース文章を対象に2名の評価者により評価した結果，
クローズ評価で95\%，オープン評価で91\%の精度（評価者2名の平均）で，
読み韻律情報を正しく設定でき，その有効性が確認できた．","In order for Japanese text-to-speech synthesis to provide highly 
natural synthesized speech,it is necessary to correctly generate 
reading-and-prosodic information, 
that is, information about readings, accents, pauses, and so on.
This paper describes a method of generating reading-and-prosodic information 
that uses morphological analysis based on the multi-level analysis method, 
which deeply analyses compound words and heteronyms; also described is 
 the word dictionary information used in the method.
The main characteristics of this generation method are: 
(1) long unit word recognition in the morphological analysis to cope with 
generating reading-and-prosodic information, 
(2) accentual phrase assignment using 
semantic dependent relationships in compound words, 
(3) pause insertion based on multi-level assignment using 
local structures in compound words and connected power of 
accentual phrases instead of dependent relationships of syntactic phrases.
In an evaluation for news-texts, this method generated reading-and-prosodic 
information with 95\% accuracy for closed data and 91\% 
accuracy for open data. 
These results show the effectiveness of this method.","['はじめに', 'テキスト音声合成の流れ', '形態素解析における読み韻律情報設定のための特徴', '読み韻律情報設定のための単語辞書情報', '読み韻律情報の設定法', '評価', 'おわりに']",,,,,,,,
V06N02-05.tex,発話単位の分割または接合による \\ 言語処理単位への変換手法,"Transformation into Meaningful Chunks by \\ Dividing or
Connecting Utterance Units","自然で自発的な発話を対象とする音声翻訳ないし音声対話システムへの入力と
しての発話は文に限定できない．一方，言語翻訳処理における処理単位は文で
ある．話し言葉における文に関して，計算機処理から見て十分な知見は得られ
ていないので，文の代わりに「言語処理単位」と呼ぶことにする．まず，一つ
の発話を複数の言語処理単位に分割したり，複数の発話をまとめて一つの言語
処理単位に接合する必要があることを，通訳者を介した会話音声データを使っ
て示す．次に，ポーズと細分化された品詞の $N$-gram を使って，発話単位か
ら言語処理単位に変換できることを実験により示す．","The utterance units that serve as input to speech translation and/or
spoken dialogue systems that handle spontaneous speech are not always
sentences.  However, the processing units of language translation are
sentences.  Since we do not have enough knowledge about the sentences
of spoken languages, we use the term ``meaningful chunks'' instead of
sentences.  First, using conventionally interpreted dialogue data, we
show that utterance units sometimes need to be divided into several
meaningful chunks, and sometimes need to be connected to make up a
single meaningful chunk.  Next, we propose a method of transforming
from utterance units to meaningful chunks based on pause information
and the $N$-gram of fine-grained part-of-speech subcategories.  We
have conducted experiments and have confirmed that our method yields
good results.","['まえがき', '発話単位から言語処理単位への変換の必要性', '発話単位から言語処理単位への変換手法', 'むすび']",,,,,,,,
V06N02-06.tex,音声認識用言語モデルのための\\ タスク適応化と定型表現の利用,A task adaptation method and use of idiomatic expression of stochastic language model \\for speech recognition,"本研究では大規模コーパスが利用可能な新聞の読み上げ音声の認識のための精度
の良い言語モデルの構築を実験的に検討した．N-gram言語モデルの改善を目指し，
以下\mbox{の3つの点に注目した","In this paper, we describe a method that constructs language models
using a task-adaptation strategy and idiomatic expressions of news
articles.  To build an effective N-gram based language model, it should
be noted that the training data must be prepared as much as possible.
However, for a given task/topic, it is very \mbox{difficult to","['はじめに', '言語モデルの評価基準', '言語モデルの適応化', '定型表現', 'まとめ']",,,,,,,,
V06N02-07.tex,テキスト音声合成における係り受け解析結果を \\ 用いたポーズ挿入処理,Pause Insertion which uses the results of \\ Grammatical Dependency Analysis within \\ a Text-to-Speech Conversion System,"我々は，テキスト音声合成システムのポーズ挿入精度向上のために，言語処理部に構文解析処理を導入し，一文全体の係り受け解析結果を利用したポーズ挿入処理を試みた．本稿では，この結果について報告する．
テキストを音声に変換して出力する際には，その内容を感覚的，意味的に捉え易くするために，テキスト中の適当な位置に適当な長さのポーズを与える必要がある．
ポーズ位置やポーズ長の設定に，対象文の係り受け解析結果が有効な手がかりになるとの知見が得られているが，従来は実施上の都合から，語彙情報の利用や，局所的なテキスト解析による方法が代用されていた．
そこで本稿では言語処理部に軽量かつ高速な構文解析系を導入し，一文全体の係り受け解析のシステム上での実現を試みた．ポーズ挿入の生じ易さの指標としてポーズ挿入尤度を設け，係り受け情報に着目したポーズ挿入規則に基づき，全文節境界にポーズ挿入尤度を設定する．尤度の高い境界から基本ポーズ長レベルを設定した後，各境界に対してアクセント結合処理および呼気段落に基づく閾値による調整を行なう．
実際にテキスト音声合成システムに実装し，形態素解析と隣接間係り受け処理のみ実装しているテキスト音声合成ソフトウェアパッケージとの比較実験を行なったところ，ポーズ挿入精度の大幅な向上が得られ，その効果を確認することができた．","This is a practice of method which uses results of grammatical dependency analysis within a text-to-speech conversion system to insert pauses. 
In order to understand the meaning of text which is converted into speech, the appropriate insertion of pauses into phrase boundaries should be required. 
In previous studies, several approaches using the results of simple analyses such as  morphological analysis or analysis of adjoining phrases to determine the position and length of pauses was proposed.
In those studies, we often found the speech had inappropriately determined pause positions. 
In the present method, we introduce a quick Japanese parser into the text analysis, determine the distance and relationship between dependent phrases, and use this information to determine the position and length of pauses. 
The distance and relationship between dependent phrases is translated into the cost of pause insertion. 
Each phrase boundary has a cost of pause insertion, and appropriate pauses are inserted according to the costs. 
To test the validity of our method, we implemented it in a text-to-speech conversion system, and compared the proposed method with a previous pause insertion method which used simple analyses such as morphological analysis or analysis of adjoining phrases. 
The results confirmed that our proposed method fulfilled our expectations.","['はじめに', '構文解析処理の導入', 'システムへの実装', '発音記号生成実験', 'おわりに']",,,,,,,,
V06N03-01.tex,,Empirical Support for New Probabilistic Generalized LR Parsing,,"This paper shows the empirical results of our probabilistic
  GLR parser based on a new probabilistic GLR language model (PGLR)
  against existing models based on the same GLR parsing framework,
  namely the model proposed by Briscoe and Carroll (B\&C), and two-level
  PCFG or pseudo context-sensitive grammar (PCSG) which is claimed to be
  a context-sensitive version of PCFG. We evaluate each model in
  character-based parsing (morphological and syntactic analysis) tasks,
  in which we have to consider the word segmentation and multiple
  part-of-speech problems. Parsing a sentence from the morphological
  level makes the task much more complex because of the increase of
  parse ambiguity stemming from word segmentation ambiguities and
  multiple corresponding sequences of parts-of-speech. As a result of
  the well-founded probabilistic nature of PGLR, the model accurately
  incorporates probabilities for word prediction, by way of encoding
  pre-terminal n-gram constraints into LR parsing tables. The PGLR model
  empirically outperforms the other two models in all measures, on
  experimentation with the ATR Japanese corpus.  To examine the
  appropriateness of PGLR using an LALR table, we test the PGLR model
  using both an LALR and CLR table. The results show that parsing with
  the PGLR model using LALR table returns the best performance in
  parse accuracy, parsing time and memory space consumption.","['Introduction', 'Probabilistic Models', 'Experimental Results', 'Discussion', 'Comparative Results for LALR and CLR Table-based PGLR', 'Conclusion']",,,,,,,,
V06N03-03.tex,複数の表層的手がかりを統合したテキストセグメンテーション,Text Segmentation with Multiple Surface Linguistic Cues,"一般に、テキストは複数の文から形成されており、文間には何らかの意味的なつ
ながりがある．テキスト中の意味的にまとまったある範囲が，談話セグメントや意
味段落と呼ばれる一貫性のある談話の単位を構成する．また，談話セグメント間の
関係によってテキスト全体の談話構造が形成される．こうしたことから，セグメン
ト境界の検出は，テキスト構造解析の第一歩であると考えられる．テキスト中には，
セグメント境界の検出に利用できる多くの表層的手がかりが存在する．本稿では，
複数の表層的手がかりを組み合わせて日本語テキストのセグメント境界を検出する
手法について述べる．セグメント境界の検出は，複数の手がかりのスコアを基に各
文間のセグメント境界への成り易さあるいは成り難さを表す文間スコアを計算する
ことで行われる．文間のスコアは，各手がかりのスコアに重要度に応じた重みをか
け，この重み付きスコアを足し合わせることにより計算する．本稿では，各手がか
りへの重み付けを人手によらず，訓練データを用いた統計的手法により自動的に行
う手法について述べる．また複数の手がかりの中で，実際にセグメント境界の検出
に有効な手がかりだけを選択することで訓練データへの過適合を避ける手法につい
ても述べる．","In general, a text consists of multiple sentences, and there are some
semantic relations among them. A certain range of sentences in a text,
is  widely assumed to form a coherent unit which is usually called a
discourse segment. While sentences in a segment have semantic relations
with each other, segments in a discourse have some relations with each
other. The global discource structure of a text can be constructed by
relating the segments with each other. Therefore, identifying the segment
boundaries is a first step to recognize the structure of a text. There are
many surface linguistic cues which help for identifing text segmentations
in a text. In this paper, we describe a method for identifying segment
boundaries of a Japanese text with the aid of multiple surface linguistic
cues, though our experiments might be small-scale. We calculate a weighted
sum of the scores for all cues that reflects their contribution to
identifying the correct segment boundaries. We also present a method of
training the weights for multiple linguistic cues automatically without 
the overfitting problem.","['はじめに', 'テキストセグメンテーションに使用する表層的手がかり', '複数の手がかりへの自動的な重み付け', '有効な手がかりの自動選択', '実験', 'おわりに']",,,,,,,,
V06N03-04.tex,文末から解析する統計的係り受け解析アルゴリズム,Statistical Dependency Analysis using Backward Beam Search,"係り受け解析は日本語文解析の基本的な方法として認識されている．
日本語の係り受けは，ほとんどが前方から後方であるため，
解析は文末から文頭の方向へ解析を進める事は効率的であり，
これまでもルールベースの解析手法ではいくつかの提案がある．
また，統計的文解析は英語，日本語等の言語を問わず数多くの提案があり，
その有効性が確認されている．
本論文では，上記の二つの特徴を兼ね備えた日本語文係り受け解析を
提案し，その実験結果を示し，有効性を実証する．
システムの精度は，正しい文節解析ができた所から開始した場合，
京大コーパスを使用した実験で係り受け正解率が87.2\%，
文正解率が40.8\%と高い精度を示している．
ビームサーチのビーム幅を調整した実験では，ビーム幅を小さくする
事による精度の劣化が認められなかった．
実際にビーム幅が1の際に得られた結果の95\%は
ビーム幅20の時の最良の結果と同一であった．
また，N--best文正解率を見た時には，Nが20の時には
78.5\%という非常に高い結果を示している．
解析速度は，解析アルゴリズムから推測される通り，
文節数の2乗に比例し，平均0.03秒(平均文節数10.0)，
最長文である41文節の文に対しては0.29秒で解析を行なった．","Dependency analysis is regarded as a standard method
of Japanese syntactic analysis.
As dependencies normally go from left to right,
it is effective to parse from right to left, 
as we can analyze predicates first.
There have been several proposals for such methods 
using rule based parsing.
In this paper, we will propose a Japanese dependency analysis
which combines right to left parsing and a statistical method.
It performs a beam search, an effective way of limiting the
search space for right to left parsing.
We get a dependency accuracy of 87.2\% and 
a sentence accuracy of 40.8\%
using the Kyoto University corpus.
Varying the beam search width,
we observed that the best performances were achieved when
the width is small.
Actually, 95\% of the sentence analyses obtained with
beam width=1 were the same as the best analyses with
beam width=20.
The N--best sentence accuracy for N=20 was 78.5\%.
The analysis speed was proportional to the square of 
the sentence length (number of segments), 
as predicted for the algorithm.
The average analysis time was 0.03 second (average sentence length
was 10.0) and it took 0.29 second to analyze
the longest sentence, which has 41 segments.","['はじめに', '最大エントロピー法の利用', '解析アルゴリズム', '実験結果', 'まとめ']",,,,,,,,
V06N03-06.tex,語彙的連鎖に基づくパッセージ検索,Passage-Level Document Retrieval \hspace*{17mm,"計算機上の文書データの増大に伴い，膨大なデータの中から
ユーザの求める文書を効率よく索き出す文書検索の重要性が高まっている．
伝統的な検索手法では，文書全体を1つのまとまりとして考え検索要求との
類似度を計算する．しかし，実際の文書，特に長い文書では様々な話題が存
在し，文書中の各部分によって扱われる話題が異なる場合も多く見られる．
そのため，最近の文書検索では，ユーザの入力した検索要求と関連の高い文
書の一部分を取り出して類似度を計算するパッセージレベルの検索が注目さ
れている．パッセージ検索におけるパッセージとは，文書中で検索要求の内容と強
く関連する内容を持つ連続した一部分のことを言う．
パッセージ検索では，このパッセージをどのように決定するかが問題となる．
良いパッセージを決定するためには，パッセージ自体が意味的なまとまりを
形成し，パッセージの位置やサイズが検索要求や文書に応じて柔軟に設定さ
れる必要があると考えられる．
本稿では，文書中の文脈情報である語彙的連鎖を利用し，検索要求と文書の
適切な類似度を計算できるパッセージ決定手法について述べる．また，このパッ
セージを使用し，検索精度を向上させる検索手法について述べる．","The importance of document retrieval systems which can retrieve relevant
documents for user's needs is now increasing with the growing availability
of full-text documents.
In the traditional document retrieval, each document is 
treated as a single unit. However, since long documents tend to contain
various topics, the passage-level retrieval has been received more
attention in the recent document retrieval.
The passage can be considered as a sequent part of the document
which contains a content related to a content of the query.
In the passage retrieval, it is a problem how to decide the passages. 
It is considered that the passages which form coherent semantic units can
effectively improve the accuracy. Furthermore, it may be also 
effective that the size and position of passage can be flexibly changed 
determining on the query and the document.
In this paper, we describe a better method for calculating the passages
that uses lexical chains.
We will also present a passage-level document retrieval method which
improves the accuracy.","['はじめに', 'パッセージレベルの文書検索', '語彙的連鎖の計算', '語彙的連鎖に基づくパッセージ検索', '実験', 'おわりに']",,,,,,,,
V06N04-01.tex,"日本語の照応関係理解に関する一考察\\—「主題」（Topic）が果た
す役割を中心に—",Anaphoric Resolution in Japanese Sentences: Topic Assignment Strategy,"日本語の照応関係理解のプロセスにおいて，どのようなストラテジーが関与
しているのかについて，言語心理学的実験を通して考察した．実験１では，自
己のペースによる読解課題およびプローブ認識課題を用いて，日本語の主語を
表す「が」と主題を表す「は」の違いが照応関係理解に影響を及ぼすかどうか
について調査した．その結果，「は」でマークされた名詞句で読解時間がかか
る傾向が見られ，それを照応表現の指示対象として優先する傾向が見られた．
また，プローブ認識課題では，主題を表す「は」の影響が見られ，目的語名詞
句よりも主語名詞句をプローブ語として呈示した場合の方が判断時間が速い傾
向が見られた．このように，主題の影響が見られたことから，「主題割当方略」
とでも言うべきストラテジーが利用されていることが分かった．実験２では，
英語の実験に基づいて提案されている「主語割当方略」や「平行機能方略」
と呼ばれるストラテジーが日本語の照応理解にも利用されるのかどうかにつ
いて調査した結果，parallelな構造をもつ文では，平行機能方略が用いられ
ることが分かった．さらに実験３では，これら２つのストラテジーおよびそ
の他のストラテジーと主題割当方略との相互関係について調査を行った．そ
の結果，日本語の照応関係理解のプロセスでは，これらのストラテジー競合
する場合，主題割当方略が優先的に利用されることが分かった．このことは，
日本語が「主題卓立言語」としての性質を持っていることを示している．","This paper describes three psycholinguistic experiments on
anaphoric resolution during sentence comprehension in Japanese. 
   In Japanese language, there is a postposition ``wa'' which signifies
``topic'' and ``ga'' which signifies ``(grammatical) subject''. Experiment 1 
investigated the influence of the difference between ``wa'' and ``ga'' to
assign ambiguous pronouns. In a self-paced reading paradigm, reading
times were longer subject noun phrase than object noun phrase,
irrespective of the difference of postposition, and there were more
assignment to the topic noun phrase(Topic-NP: ``NP+wa'') than subject
noun phase(Subject-NP: ``NP+ga''). In a probe recognition task, reaction 
times (RTs) were faster subject(S-probe) than object(O-probe), and RTs 
for S-probe were faster Topic-NP sentence than Subject-NP sentence.
Overall, the influence of topic was observed, thus suggesting that the 
topic assignment strategy (TAS) were utilized during the assignment of 
pronouns in Japanese.
   Experiment 2 investigated the influence of another heuristic
strategies which have been proposed to account for the assignment of
pronouns in sentences in English: the subject assignment strategy
(SAS) and the parallel function strategy (PFS). Furthermore,
Experiment 3 investigated to distinguish between the heuristic
strategies: TAS, SAS, and PFS.  In both Experiment 2 and 3, there was
a strong preference assigning a pronoun to the preceding Topic-NP,
thus showing TAS was predominantly used over other heuristic
strategies. These findings support that Japanese has a linguistic
nature of a topic prominent language.","['はじめに', '人間の照応関係理解', '主題割当方略 (Topic Assignment Strategy）', '主語割当方略および平行機能方略', '発見的ストラテジー間の相互関係', 'まとめと今後の課題']",,,,,,,,
V06N04-02.tex,音声対話システムのための対話の認知プロセスモデル,Cognitive Process Modeling of Dialogue for Spoken Dialogue Systems,"本稿では，音声を用いて人間と機械が対話をする際の対話過程を，
認知プロセスとしてとらえたモデルを提案する．対話システムをインタラクティ
ブに動作させるためには，発話理解から応答生成までを段階的に管理する発話
理解・生成機構と，発話列をセグメント化し，焦点および意図と関連付けて構
造的にとらえる対話管理機構とが必要である．さらに，入力に音声を用いた音
声対話システムでは，音声の誤認識によるエラーを扱う機構を組み込む必要が
ある．本稿で提案するモデルは，発話理解・生成機構における各段階での処理
を具体化し，それらと対話管理機構とのやりとりを規定することによる統合的
な認知プロセスモデルとなっている．それらの処理の中に，音声の誤認識によっ
て生じ得るエラーを具体的に記述し，その対処法を網羅的に記述している．こ
のモデルを実装することによって，ある程度のエラーにも対処できる協調的な
音声対話システムの実現が期待できる．","In this paper, we propose a cognitive process model of
spoken dialogue. In order to make an interactive dialogue system, we
need two management processes: one is an understanding process which
manages the subprocess of utterance understanding through response
generation; the other is a dialogue management process which
aggregates the utterances to the discourse segment and manages focus
and intentions of dialogue. Furthermore, in applying the model to
spoken dialogue systems, we have to deal with misunderstandings caused
by speech recognition errors. Our model specifies the cognitive
process of whole dialogue understanding process and stipulates the
interaction between understanding process and dialogue management
process. We also specify the recovering method from communication
errors into this cognitive process.  Therefore, our model is suitable
for implementing cooperative spoken dialogue systems.","['まえがき', '対話処理の認知プロセスモデル', '会話レベルの管理機構', '問題解決機構', '対話モデルの処理例', 'むすび', '述語の定義']",,,,,,,,
V06N04-03.tex,感情表出文,Exclamatory Sentence,"本稿は，「まあ，嬉しい！」のような発話者が感情を思わず口にした「感情表出文」とはどのようなものか，感情主の制約のあり方と，統語的特徴から分析し
たものである． 感情述語の人称制約は２種類のムードに関わる問題である．一つは，「述べ立
てのムード」，もう一つは「感情表出のムード」である．前者のムードを持つ
「述べ立て文」に生じる人称制約は語用論的なものであり，一人称感情主の場合
が多いが，条件が整えば他の人称も可能である．一方，感情主が一人称以外では
あり得ないような人称制約を持つタイプの文がある．これを，感情表出のムード
を持つ「感情表出文」と定義した． その上で，感情表出文の統語的特徴について検討した結果，感情表出文は，述
語が要求する感情主や感情の対象といった意味役割を統語的に分析的な方法では
言語化しない，すなわち述語一語文であるという事実を明らかにした． 一語文では，言語文脈上に意味役割の値を参照することができないため，発話現
場に依存して決めるしか方法がなく，感情主は発話現場の発話者，感情の対象は
発話時の現場のできごとに自動的に決まる．よって，一語文は感情表出文のムー
ドに適合する．一方，意味役割を言語化した文は，意味役割を発話現場に依存す
る必要がないため，発話現場に拘束されない．こうしたことから，感情表出文は
述語一語文でなければならないと結論づけた．","This paper attempts to elucidate the nature of ‘emotive sentences’ like Maa, ureshii! ’Wow, how happy (I am)!’ in which the speaker 
involuntarily expresses his emotion, with special focus on the 
manifestation of experiencer subjects. The grammatical person of the experiencer subject that an emotive 
predicate takes is constrained by the mood of the sentence in which it 
appears.  On the one hand, if an emotive predicate appears in a sentence 
in ‘the mood of the speaker's statement’, the person of its subject is 
only pragmatically controlled.  While the first person is most often 
found, other persons are also allowed in certain pragmatic contexts.  On 
the other hand, the first person is the only possibility in sentences in 
‘the mood of involuntary expression of an emotion’. Furthermore, several syntactic tests reveal that predicates of sentences 
in the latter mood do not syntactically manifest either the experiencer 
or the source of an emotion.  In other words, emotive predicates solely 
constitute sentences in this particular grammatical mood. Put differently, since neither the experiencer nor the source of an 
emotion is realized in a one-word emotive sentence, the values of these 
implied arguments must be inferred from the context of utterance of that 
sentence.  Consequently, the experiencer is automatically understood as 
the speaker, and the source as a concurring event in the same context.","['まえがき', '感情主の人称制約と文のムード', '感情表出文の統語的特徴', '感情表出文はなぜ一語文か', 'むすび']",,,,,,,,
V06N04-04.tex,日本語の指示詞における\\直示用法と非直示用法の関係について,On the Relation between the Deictic Use and the Non-deictic Use\\ of the Japanese Demonstratives,"日本語の指示詞の3系列（コソア）は，いずれも直示用法とともに非直示用法を
持つ．本稿では「直示」の本質を「談話に先立って話し手がその存在を認識
している対象を，話し手が直接指し示すこと」ととらえ，ア系列およびコ系列
では直示・非直示用法にわたってこの直示の本質が認められるのに対し，
ソ系列はそうではないことを示す．
本稿では，ア系列の非直示用法は「記憶指示」，すなわち話し手の
出来事記憶内
の要素を指し示すものであり，コ系列の非直示用法は「談話主題指示」，すなわち
先行文脈の内容を中心的に代表する要素または概念を指し示すものと考える．
「記憶指示」も「談話主題指示」も上記の直示の本質を備えている上に，
ア系列およびコ系列の狭義直示用法において特徴的な話し手からの遠近の対立
も備えているという点は，ア系列およびコ系列の非直示用法がともに
直示用法の拡張であることを示唆している．
さらにさまざまなソ系列の非直示用法を検討した上で，ソはコ・アとは異なって，
本質的に直示の性格が認められないことを論じる．非直示用法のソ系列は
話し手が談話に先立って存在を認めている要素を直接指すためには用いられず，
主に言語的な表現によって談話に導入された要素を指し示すために用いられる．
またソが，「直示」によっては表現できない，分配的解釈や，いわゆる代行用法
等の用法を持つことも，ソがアやコと違って非「直示」的であるという主張と
合致する．","This paper addresses the status of deixis and anaphoricity in Japanese
demonstrative system. It has traditionally been claimed that the Japanese
demostrative prefixes {\it ko","['はじめに', '指示詞表現の意味論的・談話理論的機能', 'ア系列とソ系列', 'コ系列とソ系列', 'ソ系列特有の用法', 'ソ系列の直示用法', 'まとめと今後の課題']",,,,,,,,
V06N04-05.tex,,Formal Semantics of Dialogues\\Based on Belief Sharing\\and Observational Equivalence of Dialogues,,"This paper shows that there is a direct connection between dialogues and belief sharing.
This connection is shown by proving a correspondence between {\it observational equivalence","['Introduction', 'Dialogues as Belief Sharings', 'Hyper-Discourse Representation Theory', 'Comparison with the Other Formal Semantics', 'Conclusion']",,,,,,,,
V06N04-06.tex,創発的な対話に関するコーパスの構築,Construction of Corpus for Emergent Dialogue,"我々が日常行っているおしゃべりのように明確な目標の定まっていない
対話では，話者は事前に対話戦略を立てることができない．
そのような対話では話者は，その場で断片的に
思い付いたことを発話し（即興性），相互に触発されて
新しい考えが浮き上がり（創造性），全体で一つの対話を作っていく．
我々は，即興性と創造性を備えた対話を創発的な対話と呼び，
このような対話を収録し，対話コーパスとして整備することを考えた．
対話の収録には，解き方や正解かどうかの判定が明確になっていない課題を
二人で協調して解く課題を用い，「画像と音声を用いた対話環境」と
「音声だけの対話環境」の２つの条件で実験を行った．
収録した対話には，相手の立場を尊重し互いに良い関係を作っていく
ことを目的とした共話現象や，さまざまな同意表現の使われ方が
観察され，これまでのような目的指向対話には見られない特徴のある
対話コーパスが得られた．","In this paper, we show our corpus for emergent dialogues. 
Emergent dialogues have two features, improvisation and creativity. 
Improvisation means we have to achieve our aim while we are looking for 
an action required on the spot each other during a dialogue. 
Although we try to have a dialogue based on a prior plan about 
the content of the dialogue, we would fail the dialogue. 
Creativity means we sometimes conceive a new idea after we hear 
our partner's utterance. To convey the idea to our partner have 
influence on our partner's thought. 
These two features are important in human spontaneous dialogues. 
Our aim is to construct a dialogue management model for emergent 
dialogue, and to implement it on a computer. 
We conducted an experiment using tasks in order to collect emergent 
dialogues. The solutions of the tasks and their correct answers are not 
clear. We used two experimental settings, ``visual and audio'' and 
``audio only''. We report features in relation to how turn-taking 
were operated and how agreement expressions were used to come 
to mutual agreement in the dialogue.","['はじめに', '従来研究との比較', '協調作業対話実験の方法', '収録データ', '収録対話の特徴', '考察', 'おわりに']",,,,,,,,
V06N05-01.tex,音節連鎖特性に着目した対話文の言い直し表現の抽出法,A Method to Detect Self-repair Syllable Strings in Spontaneous Speech using Markov Model,"会話文では，言い直しなどの冗長な表現が含まれ，解析を困難にしている．
本論文では，言い直し表現は繰り返し型が多く，また，
文節境界に挿入されやすいことに着目して，
べた書きで音節標記された会話文を対象に，
これを抽出する方法を提案した．提案した方法は，
言い直しを含んだべた書き音節列をマルコフ連鎖モデルを用いて文節単位
に分割する処理と，それによって得られた文節境界を手がかりに
文節間の音節列の類似性を評価して言い直し音節列を抽出する処理の
２つの処理から構成される．具体的には，第１の処理では，
言い直しの表現を含む文節境界の推定に適した文節境界推定法を提案し，
第２の処理では，文節境界の使い方の異なる３つのマッチングの方法を
提案した．また，これらの2つの方法を組み合わせたときの言い直し表現
の抽出精度を計算によって推定すると共に，
その結果を総合的な実験結果と比較して提案した方法の効果を評価した．
ATRの「旅行に関する対話文」のコーパス（その内，言い直しは106個所）を用
いて実験評価した結果によれば，言い直し表現の抽出精度は
第２の処理の方法に強く依存し，再現率を重視する場合は，
再現率80.2％（その時，適合率84.2％），また，適合率を重視する場合は，
適合率94.9％（その時，再現率52.8％）の精度が得られることが分かった．","This paper proposes a method to detect self-repair strings included
in spontaneous speech by Markov models of syllables.
These strings  are assumed to be represented with syllable strings
obtained correctly by acoustic processing.
 The method comprises the following two steps:
 The first step is to determine the provisional bunsetsu boundaries
 of a non-segmented syllable sentence with self-repair strings.
 We improved the method which has been proposed to find the
 provisional  bunsetsu boundaries of correct sentences
 by Markov models, to be applicable to sentences with self-repair.
 The second step is to detect self-repair strings,
 which are inserted in the location of bunsetsu boundaries.
 In this step, we proposed three methods of pattern matching
 to detect these strings.
 This method is applied to detect self-repair strings in
 ATR dialogue corpus. It is confirmed that the method is
 effective to detect self-repair strings inserted in bunsetsu
 boundaries.","['はじめに', '言い直し表現の特徴と抽出の方針', '言い直し表現の抽出法', '言い直し表現の抽出精度の推定', '実験結果と考察', 'あとがき']",,,,,,,,
V06N05-02.tex,比喩理解における顕著な属性の発見手法,A Method for Finding Salient Features in Metaphor Understanding,"本論文では，二つの名詞概念からなる比喩表現における顕著な属性を
自動的に発見する手法を提案する．
まず，概念から連想される属性について調べる連想実験を行い，
次に，その結果に基づく属性の束を作ってSD法の実験を行う．
そして，SD法実験の評定値をパラメータとして用いる
ニューラルネットワークを使用して，二つの概念に共通で
しかも顕現性の高い属性を抽出する．
この手法では多数の属性間の顕現性に関する数値的な順序づけが行われるので，
多様な概念の組み合わせを含む
「TはVだ」の形の比喩表現に対して適用できる．
ここで，Tは被喩辞，Vは喩辞である．
本手法を用いたシステムによる実行例を示し，その有効性を検証す
る．","In this paper, we propose a method which finds
salient features automatically in a metaphorical expression consisting of 
two noun concepts. First, we prepared a bundle of features by
human association experiments on the concepts, 
and, using the bundle, we implemented
SD(Semantic Differential) Method experiments
to evaluate the features.
Then, we extracted common salient features by using
a new neural network mechanism where the result of 
the SD Method experiments were used for the parameters of the mechanism.
Since this mechanism 
can be applied to any pair of concepts to form a sentence ``T is V'',
saliency of features which are common to the T and V is evaluated
quantitatively.
We show examples calculated by the system to
verify its effectiveness.","['はじめに', '関連研究', '認知心理実験', '比喩の特徴発見手法', '実行例', 'おわりに']",,,,,,,,
V06N05-03.tex,論文間の参照情報を考慮した サーベイ論文作成支援システムの開発,Towards Multi-paper Summarization Using Reference Information,"\quad 
本稿では，データベースから関連する論文を自動的に収集し，人間が特定分野の
サーベイ論文作成する作業を支援するシステムを示す．本研究では，サーベイ論
文作成支援の際，論文の参照情報に着目する．論文の参照情報とは，論文中でそ
の論文と参照先論文との関係について記述されている箇所(参照箇所)から得られ
る情報のことで，参照先論文の重要点や，参照元と参照先論文間の相違点を明示
する有用な情報が得られる．サーベイ論文作成には2つの処理(1)特定分野の論文
の収集(2)論文間の相違点の検出が必要であると考えられる．本研究では参照情
報を利用することでこれらの処理が部分的に実現可能であることを示す．具体的
には，ある論文が他の論文を参照する時の参照の目的を，cue wordを用いて解析
し，論文の参照・被参照関係にリンク属性(参照タイプ)を付与する．結果として，
参照箇所抽出ではRecall 79.6\%，Precision 76.3\%の精度が得られた．また，
参照タイプ決定では83\%の精度が得られた．これらの参照タイプを利用し，ある
特定分野の論文を自動的に収集するのに近い処理が可能になった．また，ユーザ
に論文間の参照関係を表すグラフ，グラフ中の個々の論文のアブストラクト，論
文間の相違点の記述された参照箇所を提示するシステムを構築した．このシステ
ムを利用することで特定分野の論文が自動収集され，また収集された論文集合の
論文間の相違点が明らかにされるため，参照情報がサーベイ論文作成の支援に有
用であることが示された．","\quad In this paper, we present a system to support writing
a survey of the specific domain. In this system, we use reference
information. Reference information includes the reference relationships
between papers and the information which can be derived from the
description around the citation, and be useful for understanding the
difference between the referring and referred papers. To write a survey,
at least two processes are necessary. One is to collect papers of some
domain. Another is to make clear the differences between papers. We
think the reference information is useful for these two
processes. Firstly, we try to extract a fragment of texts where the
author describes about a referring paper. We call the fragment
``Reference Area''. Secondly, we attempt to analyze the purpose of
reference. We divide that into three categories(we call these categories
``Reference Types''), and develop the method to determine the type by
using cue words．As a result, we got the recall of 79.6\% and the
precision of 76.3\% in reference area extraction, and the accuracy of
83\% in reference type decision. Making use of these reference types，we
can collect a set of papers in the same domain. Finally, we build up a
system to display the reference graph of the papers. With the system,
abstracts and reference areas of papers can be seen. Users of this
system can easily collect papers of some specific domain, and also can
understand the differences between the related papers.","['はじめに', 'サーベイ論文作成', 'サーベイ論文作成における参照情報の利用', 'サーベイ論文作成支援システム', '実験', 'サーベイ論文作成支援システムの構築', 'おわりに']",,,,,,,,
V06N05-04.tex,構成素境界解析を用いた多言語話し言葉翻訳,Using Constituent Boundary Parsing\\for Multi-lingual Spoken-language Translation,"表層パタンの照合を行なう構成素境界解析を提案し，構成素境界解析と用
例利用型処理を組み合わせた変換主導型機械翻訳の新しい実現手法が多言語話し言葉翻訳に有効
であることを示す．
構成素境界解析は，変項と構成素境界より成る単純なパタン
を用いた統一的な枠組で，多様な表現の構文構造を記述できる．
また，構成素境界解析は，チャート法に基づくアルゴリズムで
逐次的に入力文の語を読み込み，解析途中で候補を絞り込みながらボトムアップに構文構造
を作り上げることにより，効率的な構文解析を可能にする．
構成素境界解析の導入により，変換主導型機械翻訳は
構文構造の記述力，構文解析での曖昧性爆発といった，
頑健性や実時間性の問題を解決することができた．
さらに，構成素境界解析と用例利用
型処理は単純で言語に依存しない手法であり，
多言語話し言葉翻訳へ適用するための汎用性を高めることができた．
旅行会話を対象とした日英双方向と日韓双方向の話し言葉翻訳の評価実験の結果により，
本論文で提案する変換主導型機械翻訳が，多様な表現の旅行会話文を話し手の意図が理
解可能な結果へ実時間で翻訳できることを示した．","We propose a method called constituent boundary parsing which uses pattern
matching on the surface form. The new version of
Transfer-Driven Machine Translation (TDMT) combining constituent
boundary parsing and example-based processing is effective for
multi-lingual spoken-language translation. 
Constituent boundary parsing consistently describes
the syntactic structures of various expressions
with surface patterns
consisting of variables and constituent boundaries.
In constituent bound-\break
ary parsing, input words are read in a left-to-right fashion, 
and the best \mbox{syntactic","['はじめに', 'TDMTの枠組', '構成素境界パタン', '構成素境界解析', '用例利用型処理', '解析途中での構文構造候補の絞り込み', '多言語話し言葉翻訳の評価実験', 'おわりに', '評価文に対するTDMTシステムの翻訳実行例']",,,,,,,,
V06N05-05.tex,放送ニュース文を対象とした効果的類似用例検索法,An Efficient Way of Gauging Similarity between Long Japanese News Expressions,"著者らは用例提示型の日英翻訳支援システムを開発している．こ
の中には利用者が入力する日本語の表現に類似する表現を検索して，検索結果を含ん
だ日本語文とその対訳を表示する機能がある．著者らの日本語データベースの
文は平均長が 88.9 文字と長い．このように長い用例を対象に類似検索を行
う場合，キーワードによる AND 検索は適切ではない．
なぜなら用例が長いため 1 文中に同一キーワードが複数回出現する場合があり，
これが原因で不適切な用例を検索しやすくなるからである．
これに対して著者らは入力キーワードの語順とその出現位置の間隔を考慮した
検索手法を提案する．これによって構文解析を行うことなく構文情報を反映した検索を
行うことができる．本稿では従来の AND 検索と提案手法を使った評定者に
よる主観評価実験を報告する．この中で，提案手法の有効性が統計的に有
意となったことを示す．また，検索時間の増加は 1.3 倍であった．","We are developing a Japanese--to--English Translation Aid system for
news translators. The system consists of a voluminous bilingual news
database whose sentences are properly aligned across languages
beforehand, and a similar expression search engine. A user can find
past translation examples of input Japanese with the system.
Similar expression search engines like the one in this paper have
usually employed an AND retrieval technique that uses keywords in the
input expression, to measure the similarity between the input and the
target by the number of shared keywords.
In many cases of applying such search engines to our database,
however, a number of spurious search results have been produced as a
consequence: the sentences have been quite long (88.9 Japanese
characters on average) and a single sentence has often contained
identical keywords many times.
In this paper, we propose adding two constraints to the AND retrieval
technique: the order and positions (deviations) of keywords. We
enhance AND retrieval allowing it to be able to reflect some syntactic
similarity by this inexpensive modification.
We will show, through a set of experiments, that the proposed method
significantly improves the level of user satisfaction in search results in
a statistical sense, with only a 1.3--fold increase in the search time.","['はじめに', '用例提示システムの概要', 'AND 検索の問題点', '提案手法', '検索実験', '関連研究', 'おわりに']",,,,,,,,
V06N06-01.tex,テキスト自動要約に関する研究動向(巻頭言に代えて),Automated Text Summarization: A Survey,"本稿では，これまでの(主に領域に依存しない)テキスト自動要約手法を概観す
る．特に重要箇所の抽出を中心に解説する．
また，これまでの手法の問題点を上げるとともに，
最近自動要約に関する研究で注目を集めつつある，いくつかのトピックについ
てもふれる．","In this article, we try to survey the state of the art of (mainly
domain-independent) automated text summarization techniques.
We also try to provide readers with the limitations of current
technology, and some new trends in the research field.","['はじめに', '重要文抽出による要約手法', '抽象化，言い換えによる要約手法', 'ユーザに適応した動的な要約手法', '複数テキストを対象にした要約手法', '文中の重要箇所抽出，不要箇所削除による要約手法', '要約の表示方法について', '要約の評価方法について', 'おわりに']",,,,,,,,
V06N06-02.tex,５Ｗ１Ｈ情報抽出・分類によるテキスト要約,"Text Summarization based on\\ Information Extraction and
  Categorization\\ Using 5W1H","オフィス業務においては，大量の関連情報から，特定のイベント
  についての経過や状況を把握するために，要約や抄録の生成が求めらている．
  本論文では，複数の文書から抄録や要約をロバストに生成する手法として，
  あるイベントに関する時間的経緯を抄録として生成するエピソード抄録と，
  大量の情報を大局的に把握するための要約文を生成する鳥瞰要約を提案する．
  エピソード抄録では，あるイベントを表す 5W1H (だれが，なにを，いつ，
  どこで，どうした) が含まれる文書を検索し，そのイベントに関する時間的
  経緯を抄録として生成する．鳥瞰要約は，文章中の 5W1H 要素を，シソーラ
  スを用いてそれらの上位概念で置き換えることで，要約文を生成する．新聞
  記事 10,000 件とセールスレポート 2,500 件を対象として適用し，その効
  果を確認した．","In an office, it is necessary for understanding the
  temporal transition and the \mbox{overall","['はじめに', 'オフィス業務で求められる要約・抄録', 'エピソード抄録・鳥瞰要約', '新聞記事とセールスレポートへの適用', 'おわりに']",,,,,,,,
V06N06-03.tex,重複部・冗長部削除による複数記事要約手法,Multiple Articles Summarization by Deleting Overlapped and Verbose Parts,"複数の関連記事に対する要約手法について
述べる．記事の第一段落を用いて，
その重複部・冗長部を削除することにより複数の関連記事を
どの程度要約できるかを明確にすることを目的とする．
さらに，重複部・冗長部を特定，削除する処理をヒューリスティックスにより
実現する手法を提案する．
まず，新聞記事における推量文の一部は重要度が低いと考えられ，
これを文末表現ならびに手掛り語で特定し，削除する．次に，詳細な住所の表現は
記事の概要を把握するためには不必要であり，これも削除する．
さらに，導入部と呼ぶ部分を定義し，導入部内の名詞と動詞が
他記事の文に含まれるならば導入部は重複しているとし，削除する．
また，頻繁に出現する人名・地名に関する説明語句，
括弧を用いた表現について，他記事との重複を調べる．
重複している部分は，1つを残し他は削除する．
提案手法を計算機に実装し，実験を行った．
その結果，27記事群に対して
各記事の第一段落を平均要約率82.1\%で要約することができた．
さらに，実験結果のうち6記事群を用いて評価者11人に対して
アンケートを行い評価した．
アンケートの内容は，要約文章において冗長に感じる箇所，
ならびに削除部分を含めた元記事において重要と考えられるが
削除されている箇所を指摘する，である．
アンケート調査の結果，本手法による要約がおおむね自然であることを
確認した．
また，本手法によって削除された部分がおおむね妥当であることが
明らかになった．","In this paper, we attempt to summarize multiple Japanese articles into 
one document.  Our summarization method deletes verbose parts 
and overlapped parts in the input texts.  
We defined an introduction part to detect overlapped part, and if
nouns and verbs in an introduction part were included in some 
other article, then the introduction part is considered to be an 
overlapped part.  This paper focuses on the following five points 
to sum up: 
guess sentences, noun modifiers, expressions using parenthesis, 
detailed expressions of address and the introduction part.  
We have implemented a prototype system of summarization and 
experimented on the system using 27 groups of articles.  As a result of 
the experiments 82.1\% compression ratio on the average was achieved.
In addition, we evaluated this method using 6 groups of articles by 
obtaining information by means of questionnaires to 11 examinees.
The result of evaluation showed us that the summarizations were 
almost always natural and deleted parts were also appropriate.","['はじめに', '関連研究', '要約手法', '評価実験', '議論', 'おわりに', '要約例', '評価実験のアンケートで用いた6記事群の概要']",,,,,,,,
V06N06-04.tex,ニュース番組における字幕生成のための\\文内短縮による要約,A Summarization Method by Reducing Redundancy of Each Sentence for Making Captions of Newscasting,"ニュース原稿を1文ごとにそれぞれ要約する手法について報告する．
1文が長く，1記事中の文数の少ないニュース原稿に対して
文を抽出単位とする要約手法を用いることは，
大きく情報が欠落する可能性があり，適切でない．
そこで，本要約手法では修飾部および比較的冗長と考えられる部分
を削除することにより，1文ごとの要約を行う．また，1文を部分的
に削除する際に構文構造が破壊されることを防ぐため，ニュース文
要約に特化した簡易構文解析手法を利用している．
字幕文は，画面上を一方的に流されるという性質から，適切な長さ
に要約されている必要があり，読みやすく，原稿の情報が正確に伝
わり，冗長さが解消されている必要がある．このため，被験者32名
に対し，本手法による要約文についてのアンケートを行うことによ
り，自然さ，忠実さ，非冗長さの3つの視点から評価を行った．そ
の結果，理想的な要約を100\%とした場合で，自然さ81.5\%，忠実
さ74.3\%，非冗長さ83.3\%という評価値を得た．","We propose and evaluate a method for summarizing each sentence in TV
news texts written in Japanese. It is not appropriate to select important
sentences for abstracting news texts, because a news text consists of
only a few and long sentences. Then, we try to reduce redundant parts,
which consist of modifier etc., of each sentence.
We use a simple parsing method specialized for news texts
so that the syntactical structure is not destroyed.
As audiences cannot read repeatedly, a summary must be shortened moderately.
It must also be
easy to read, containing important information, and reduced its redundancy.
Therefore, we evaluate this summarizing method by obtaining information
by means of questionnaires to 32 examinees.","['はじめに', 'ニュース文要約', '本要約手法の構成', '簡易構文解析', '削除文節選択', '評価', '考察', 'おわりに', '謝辞', '原稿5', '原稿9']",,,,,,,,
V06N06-05.tex,語彙的結束性に基づく話題の階層構成の認定,Thematic Hierarchy Detection of a Text using Lexical Cohesion,"語彙的結束性に基づき、文章中の話題の階層的な構成を自動認定する手法を提
案する。語の繰り返しだけを手がかりに、文章全体の数分の1程度の大きな話
題のまとまりから、段落程度の小さな話題のまとまりまで、話題の大きさ別に
認定し、次に、大きな話題に関する境界と小さな話題に関する境界を対応づけ
ることで、話題の階層構成を求める手法である。この手法は、複数の話題に関
する文章が混在している集合的な文書の要約作成を目的に考案したものである。
白書のような数十頁の報告書の骨子を把握したい利用者にとっては、1/4程度
にまとめた通常の要約では長過ぎて役に立たないことがある。また、新聞の連
載記事を要約する場合、関連する記事をまとめて要約した方がよい場合なども
考えられる。よって、利用目的に応じて適切な粒度の話題を抽出する技術が重
要となる。提案手法を使えば、指定した程度の大きさの話題のまとまりを認定
できるので、要約の単位として適した大きさの話題のまとまりを抽出し、それ
ぞれを要約することで、粒の揃った話題を含む要約が作成できる。本文では、
提案手法の詳細を説明するとともに、長めの報告書と、新聞の連載記事を集め
た文書などを対象とした話題構成認定実験により、提案手法の有効性と認定精
度を示す。","This paper presents an algorithm for detecting the thematic
  hierarchy of a text with lexical cohesion measured by term
  repetitions.  It detects topic boundaries separating thematic
  textual units of different sizes, from those just smaller than the
  entire text to those of about a paragraph in size.  It produces a
  thematic hierarchy by correlating topic boundaries of larger and
  smaller these textual units.  It is intended to be used to summarize
  a long text, especially a collective one that is aggregated of
  several parts concerning different topics, such as long reports or
  serialized columns in a newspaper.  It is required for the
  summarization of such a collective document that topics of
  appropriate grading be extracted according to the size of the
  summary to be output.  The algorithm can extract a thematic textual
  unit of arbitrary size so that a well-balanced summary can be
  generated that includes topics of appropriate grading by summarizing
  every thematic textual unit of appropriate size. This paper
  describes the algorithm in detail, and shows its features and
  accuracy based on experiments using test data consisting of a long
  technical survey report, eight series of newspaper columns, and
  twelve economic reports.","['はじめに', 'Hearstの手法を用いた章・節レベルの話題境界の認定', '語彙的結束性に基づく話題の階層構成の認定手法', '提案手法の評価', '結論', '話題境界候補区間認定原理の補足', '実験文書中の見出しの例']",,,,,,,,
V06N06-06.tex,文章の構造化による修辞情報を\\利用した自動抄録と文章要約,"Extract Generation using Rhetorical\\ Information
by Text Structuring and its Extension to Summary Generation","本論文では，
重回帰分析にもとづいた文章構造解析を利用した自動抄録手法とその評価，およ
び文章要約への展開について述べる．文章構造の解析は，文章中の様々な特徴を
パラメタとした判定式や局所的な言語知識により，文章セグメントの分割統合を
進めて構造木を作るものである．得られた文章構造上の各種特徴をもとに，さら
に文章抄録の観点から選択されたパラメタを加えて，文抽出のための判定式を作
る．本研究では被験者5人にのべ350編の新聞社説の抄録調査を実施し，これを基
準に，重回帰分析によりパラメタの重みを決定し判定式を得，また，本方式を評
価する．また，自動生成された抄録文に対して，照応情報の欠落による文章の首
尾一貫性の低下を避けるための補完や，論旨を損なわない冗長な表現の削除を行
なうことで要約文章を生成する手法を紹介する．","This paper presents a method of automatic extract generation from text
by using the rhetorical structure of the text which is built by 
discriminants trained by the multiple regression analysis.  We also show
an extension to the summary generation. The way of our method of text
structuring is to divide or to combine text segments one after another
according to the discriminant with various parameters for superficial
features of sentences. Another discriminant to extract important
sentences has parameters selected from the point of view of text
extraction along with several features on the text structure (rhetorical
structure). In the experiment, five examinees select important sentences
of newspaper editorials from  total 350 articles.  The results are used
to the calculation of weights of parameters by the multiple regression
analysis, and also to the evaluation of accuracy of our method. We also
show an extension to summary generation in which sentences or phrases
are restored not to decrease the coherency of text caused by deletion of
referred sentences, and in which redundant expressions independent to
the context are deleted.","['はじめに', '分割と統合による文章構造解析', '文章抄録', '実験と検討', 'おわりに']",,,,,,,,
V06N06-07.tex,短文分割の自動要約への効果,Partitioning long sentences\\for text summarization,"TVニュース原稿は，新聞記事に比べて１記事中の文数が少なく，１文当たりの
文字数も多い．このため，自動要約としての重要文抽出を行うと，文単位で選
択が行われる為，情報の欠落が大きい．本論文では，記事中に現れる長文を分
割出来る条件を設定し，条件に合う場合は，短い文に分割するという処理（短
文分割処理）を行った結果が自動要約の基本的技術にどれだけ影響・効果があ
るのかを調べた．短文分割は，基本的に，動詞，形容動詞と述語名詞の連用文
節を分割の対象とした．また，分割の自動要約に対する影響については，評価
の尺度として，各文の重要度による順位付けと文字数圧縮（不要部分削除）を
用いた．
文順位付けの評価では，テキスト中の各文を人手及びシステムによって，その
重要度に応じて順位を付けたものを対象とした．人手により重要と判断された
文が，短文分割により分割された場合に，その分割された文は，どのような順
位となると判断されるのかを調べた．その結果，短文分割により分割された重
要文は，分割後の順位差において「３」以上離れる場合のほうが，順位差が生
じない場合，つまり順位差が「１」の場合より多くあり，短文分割の効果が見
られた．
次に，記事中の重要文だけではなく全部の文を対象として，人手とシステムに
よる順位付けについて短文分割前後での変化をスペアマンの順位相関関係係数
を用いて比較した．
その結果，短文分割をすることにより，スペアマンの係数が０．０３１８〜０．
０６５増加し，文の順位が，人とシステムにおいてより近いものになることが
判明した．
最後に，文字数圧縮での評価では，不要部分を特定し，文字列を削除または言
い換えを行う文字数圧縮処理において，短文分割を行う前後での変化を調査し
た．短文分割により削除される文字数は増え，文字数圧縮後の文字数を元記事
の文字数で割る圧縮率において，２．７１％〜２．７８％減少することが判明
し，短文分割が文字数圧縮に良い効果があることが分かった．","It is known that there are fewer sentences and the sentences are
longer in a TV news text compared with those in a newspaper article.
If we would like to summarize such TV news texts by selecting
important sentences, since each sentence is rather long, we end up
with losing a good amount of information by omitting a whole sentence.
Therefore, we adopt a method in which we partition a long sentence
into shorter sentences before summarization. To evaluate how the
partitioning affects text summarization, we select two basic measures
for text summarization, and examine how they vary before and after the
partitioning of long sentences.  The two measures are first ranking of
sentences in the text by their importance, and second, the number of
characters removed from the text by applying the same set of rules for
shortening and deleting the text.
All the sentences in the text are ranked by their importance by hand
and by sentence extraction system.  First, we examine how the ranks of
the sentences judged important by human vary before and after the
partitioning.  We found that there are more partitioned important
sentences whose difference in ranking is greater or equal to three (3)
than those whose difference in ranking is one (1).   This suggests
that the partitioning is good for sentence extraction.
Then, we compare the rankings of the human and the system for all the
sentences in the text using Spearman's rank correlation coefficient.
We found that the coefficient increases between 0.0318 and 0.065,
which means the rankings of the human and the system for the
partitioned texts are more similar that those for the original texts.
Lastly, we investigate how the partitioning affects the shortening the
text.  Here we found that the number of characters that are deleted
increases for the partitioned texts and a compaction ratio (the number
of characters of the shortened text divided by the number of
characters in the original text) decreases by 2.71 percent to 2.78
percent.  It shows that the partitioning long sentences makes
shortening method work better.","['はじめに', '原稿', '短文分割', '自動要約への影響', 'おわりに']",,,,,,,,
V06N07-01.tex,形態素解析結果から過分割を検出する統計的尺度,Statistical Measure for Detecting Over-Segmentations in Results of Japanese Morphological Analysis,"本稿では，形態素解析の結果から過分割(正解が分割していないと
  ころを形態素解析システムが分割している個所)を検出するための統計的尺
  度を提案する．もし，形態素解析の結果から過分割を検出できれば，それを
  利用して形態素解析結果の過分割を訂正する規則を作成できるし，人手修正
  済みのコーパスで除去しきれていない過分割を発見し取り除くこともできる
  ため，そのような尺度は有用である．本稿で提案する尺度は文字列に関する
  尺度であり，文字列が分割される確率と分割されない確率との比に基づいて
  いて，分割されにくい文字列ほど大きな値となる．したがって，この値が大
  きい文字列は過分割されている可能性が高い．本稿の実験では，この尺度を
  使うことにより，規則に基づく形態素解析システムの解析結果から，高精度
  で過分割を検出できた．また，人手で修正されたコーパスに残る過分割も検
  出できた．これらのことは，提案尺度が，形態素解析システムの高精度化に
  役立つこと，及び，コーパス作成・整備の際の補助ツールとして役立つこと
  を示している．","This paper proposes a statistical measure for detecting
  over-segmentations, which are errors in segmentation where a
  morphological analyzer segments places which should not be
  segmented, in results of Japanese morphological analysis.  Such a
  measure is useful because we can use detected over-segmentations for
  creating error correction rules or for removing remaining errors in
  manually debugged corpora.  The measure proposed in this paper is
  based on the ratio of the probability of a whole string to that of
  the string being segmented into two parts.  Therefore, the value of
  the measure is high when a given string is rarely segmented into two
  parts. Consequently, a string rated high by the measure is likely to
  contain over-segmentations.  In the experiments, the measure
  detected over-segmentations in the results of rule-based
  morphological analyzers very precisely and it also detected
  remaining over-segmentations in manually debugged corpora.  These
  results show that the proposed measure is useful for developing high
  quality Japanese morphological analyzers and for
  developing/debugging corpora.","['はじめに', '過分割を検出する統計的尺度', '実験', '考察と今後の課題', 'おわりに']",,,,,,,,
V06N07-02.tex,コーパスからの日本語従属節係り受け選好情報の抽出\\およびその評価,Extraction of Preference of Dependency between Japanese Subordinate Clauses\\ from Corpus and its Evaluation,"日本語の長文で一文中に従属節が複数個存在する場合，それらの節の間の係り
受け関係を一意に認定することは非常に困難である．また，このことは，
日本語の長文を構文解析する際の最大のボトルネックの一つとなっている．
本論文では，大量の構文解析済コーパスから，統計的手法により，
従属節節末表現の間の係り受け関係を判定する規則を自動抽出する手法を提案する．
統計的手法として，決定リストの学習の手法を用いることにより，
係り側・受け側の従属節の形態素上の特徴と，
二つの従属節のスコープが包含関係にあるか否かの間の因果関係を分析し，
この因果関係を考慮して，従属節節末表現の間の係り受け関係判定規則を学習する．
また，EDR日本語コーパスから抽出した係り受け情報を用いて，
本論文の手法の有効性を実験的に検証した結果について述べる．
さらに，推定された従属節間の係り受け関係を，
統計的文係り受け解析において利用することにより，
統計的文係り受け解析の精度が向上することを示す．","Dependeny analysis of Japanese subordinate clauses is one of the most difficult
phases in the syntactic analysis of Japanese long sentences.
This paper proposes a\break
 corpus-based method of learning preference rules of 
deciding dependency relation of Japanese subordinate clauses.
We utilize morphological cues included in the subordinate clauses and
statistically estimate the co-relation of those cues and dependency relation
of Japanese subordinate clauses.
Especially, we exploit scope embedding preference of subordinate clauses
as a useful information source for disambiguating
dependencies between subordinate clauses.
In the experimental evalution on EDR Japanese parsed corpus,
we discover that there exist several morphological cues that are quite
effective in deciding dependency relation of Japanese subordinate clauses.
We also show that the estimated dependencies of subordinate clauses 
successfully increase the accuracy of 
an existing statistical dependency analyzer.","['はじめに', '従属節の階層的分類を用いた係り受け解析', 'コーパスからの従属節係り受け選好情報の抽出', '決定リストを用いた従属節係り受け解析', '実験および評価', '文係り受け解析における従属節係り受け選好情報の評価', 'おわりに']",,,,,,,,
V06N07-03.tex,日本語文と英語文における統語構造認識と\\マジカルナンバー７±２,"Magical Number Seven Plus or Minus Two:\\ 
Syntactic Structure Recognition\\ 
in Japanese and English Sentences","George A. Miller は人間の短期記憶の容量
は７±２程度のスロットしかないことを提唱
している．本研究では，京大
コーパスを用い
て日本語文の各部分において係り先が未決定
な文節の個数を数えあげ，その個数がおおよそ
７±２の上限9程度でおさえられていたことを報告
した．また，英語文でも同様な調査を行ない
NP程度のものをまとめて認識すると仮定した場合
７±２の上限9程度でおさえられていたことを確認した．
これらのことは，文理解における情報の認知単位として
日本語で文節，英語ではNP程度のものを仮定すると，
Miller の7±2の理論と，言語解析・生成において
短期記憶するものは7±2程度ですむという Yngve の主張を
整合性よく説明できることを意味する．","George A. Miller insisted that 
human beings have only 
seven chunks in short term memory plus or minus two. 
We counted the number of 
bunsetsus whose modifiees were not recognized 
in each step when investigating 
the dependencies from the beginning of Japanese sentences   
by using the Kyoto University corpus, 
and we report that the number was roughly lower 
than nine, the upper bound of seven plus or minus two. 
We also investigated English sentences, 
and we got a result similar to Japanese 
when we assumed that human beings 
recognize 
a series of words such as a noun phrase (NP) 
as a unit. 
This indicates that 
if we assume that human beings' cognitive unit 
in Japanese and English are 
bunsetsu and NP respectively, 
we can accept Miller's theory.","['はじめに', '短期記憶と７±２', '日本語文での調査報告', '英語文での調査報告', 'おわりに']",,,,,,,,
V06N07-04.tex,局所的要約知識の自動獲得手法,A new approach to acquiring linguistic \\knowledge for locally summarizing \\Japanese news sentences,日本語ニュースを局所的要約する際に必要となる要約知識を，コーパスから自動獲得する手法について述べる．局所的要約とは注目個所の近傍の情報（局所的情報）を用いて行なう要約をいう．局所的情報には注目個所そのものやその前後の単語列などがある．本手法では要約知識として置換規則と置換条件を用い，これらを原文−要約文コーパスから自動獲得する．はじめに原文中の単語と要約文中の単語のすべての組み合わせに対して単語間の距離を計算し，ＤＰマッチングによって最適な単語対応を求める．その結果より，置換規則は単語対応上で不一致となる単語列として獲得する．一方，置換条件は置換規則の前後ｎグラムの単語列として獲得する．原文と要約文にそれぞれＮＨＫニュース原稿とＮＨＫ文字放送の原稿を使って実際に要約知識を自動獲得し，得られた要約知識を評価する実験を行った．その結果，妥当な要約知識が獲得できることを確認した．,"This paper proposes a new approach to acquiring linguistic knowledge for local context-based summarization. Our summarization method can transform characters, words, and Bunsetsu-phrases to the shorter ones by using linguistic information on some words to be summarized and some words located before and after the summarized words. Our linguistic knowledge for summarization, which is composed of transformation rules and transformation conditions, is automatically acquired from Japanese news corpus. In our corpus, original articles and the human-summarized ones are collected from NHK news text and NHK teletext respectively. The proposed method analyzes original news sentences and the summarized ones by Japanese morphological analyzer, and aligns original words with the summarized words by DP matching based on distances between both of the words. Transformation rules are acquired as the result of the difference. Transformation conditions are extracted as n-gram words located near a transformation rule. We acquired linguistic knowledge from NHK news corpus and obtained a high accuracy rate as a result of a series of experiments to evaluate the linguistic knowledge.","['はじめに', '大域的要約と局所的要約', '原文−要約文コーパス', 'コーパスからの要約知識の自動獲得', '実験', 'おわりに']",,,,,,,,
V06N07-05.tex,文字クラスモデルによる日本語単語分割,A Japanese Word Segmenter by a Character Class Model,"日本語処理において，単語の同定，すなわち文の単語分割は，最も
基本的かつ重要な処理である．
本論文では，日本語文字のクラス分類により得られた文字クラスモデルを
用いる新しい単語分割手法を提案する．
文字クラスモデルでは，推定すべきパラメータ数が文字モデルより
少ないという大きな利点があり，文字モデルより頑健な推定を可能とする．
したがって，文字クラスモデルを単語分割へ適用した場合，
文字モデルよりもさらに頑健な未知語モデルとして機能することが
期待できる．
文字クラスタリングの基準はモデルの推定に用いるコーパスとは別に
用意したコーパスのエントロピーであり，探索方法は貧欲アルゴリズムに
基づいている．このため，局所的にではあるが最適な文字のクラス分類が
クラスの数をあらかじめ決めることなく得られる．
ATR 対話データベースを用いて評価実験を行った結果，
文字クラスモデルを用いた提案手法の単語分割精度は文字モデルによる精度
より高く，特に，文字クラスを予測単位とする可変長 $n$-gram クラスモデルでは
オープンテストにおいて再現率 96.38\%，適合率 96.23\% の高精度を達成した．","Word segmentation, which segments an input sentence into words,
is the most fundamental process of Japanese language processing.
In this paper, we present a new method for Japanese word segmentation
based on a character class model.
The character class model is more robust than a character-based model
because the number of parameters of the character class model
is fewer than that of a character-based model.
The measurement for Japanese character clustering
is the entropy on a corpus different from the corpus for model estimation
and the search method is based on the greedy algorithm.
For this reason, this clustering method gives us
an optimum character classification without giving the number of classes.
As the result of experiments on the ADD (ATR Dialogue Database) corpus,
the proposed Japanese word segmenter using
the character class model marked a higher accuracy
than a character-based model.
In particular,
the proposed method using a variable-length $n$-gram class model
achieved 96.38\% recall and 96.23\% precision for open text.","['はじめに', '文字モデルに基づく単語分割法', '日本語文字のクラスタリング', '文字クラスモデルに基づく単語分割法', '評価実験', 'おわりに']",,,,,,,,
V06N07-06.tex,連体形形容詞に先行する格助詞「が」「の」格の\\係りに関する体系的分析,"Analysis on Dependency of GA/NO-particles\\ preceding adjective 
  in adnominal form","長文の係り解析の精度向上は,自然言語処理において重要な課題の一つである.我々はすで
に,連体形形容詞周りの「が」「の」格に関して,以下の３つのパターンに分類される７つ
の係りを規定するルールを見つけだした.
\begin{itemize","Improving the accuracy of dependency analysis in long Japanese sentences is a big 
problem in the natural language analysis.  Concerning the form of 
``$<$noun-1$>$ + $<$adjective$>$ + $<$noun-2$>$"",
we found the seven effective dependency rules which are 
classified into three following patterns in the previous paper. 
\begin{itemize","['はじめに', '提案済ルールの概説とその問題点', '形容詞選定と利用したコーパス', '抽出済みルールの検証', '新規ルールの追加', '適用後の係り解釈精度と現行システムとの比較', 'まとめ']",,,,,,,,
V07N01-01.tex,ソフトウエアの要求獲得会議での\\言い直しに注目した要求獲得方法論,"A Requirements Capturing Method Focusing\\ in the Corrected
Words\\ in the Software Requirements Capturing Meeting.","ソフトウエアの要求獲得会議では会議参加者の関心のあることをきちんと堀起
こすことが重要である．関心のあることを堀起こすためには会議参加者の無意
識の部分を知る方法が考えられる．無意識の兆候としては古来から言い直しが
挙げられている．言い直しを利用するとしても，言い直しを解釈するやり方は
高度の技術を要する．そこで本研究では，言い直しを解釈しないで利用する方
法を考えることにする．そこでまず，言い直した語と言い直された語との間で
どちらに関心が高いかを調べた．その結果，言い直された語の中にも話し手の
関心が高いものが見受けられた．そこで，言い直された語を抽出して，次の会
議の話題展開に用いる方法論を考案した．","In software requirements capturing meeting, it is important to find
out interesting items of clients. There is a method of finding out
interesting items by knowing their structure of unconsciousness. There
are some indications of unconsciousness. For example, there are some
mistakes in speaking. To interpret mistakes in speaking seems to need
a high technic. We aim at the method of not interpreting the mistakes
or corrects in speaking. We experimented the relationship between
interests and corrects in speaking. We found the case that the speaker
was interested in the corrected word. We made a requirements capturing
method to find the topics in the next meeting extracting the corrected
words.","['はじめに', '言い直しと関心の高さ', '要求獲得オフライン法での利用', '言い直しを利用した関心事項の抽出法', '例題', 'おわりに']",,,,,,,,
V07N01-02.tex,,,,,"['まえがき', '決定木', '短文分割への決定木の応用', '決定木の生成と分割点の推定実験', '決定木の枝苅り', 'むすび']",,,,,,,,
V07N01-03.tex,機械学習手法を用いた名詞句の指示性の推定,Machine learning approach to estimating\\ a referential property of a noun phrase,"言語処理の研究には名詞句の指示性の推定という問題がある．
名詞句の指示性とは，名詞句の対象への指示の仕方のことで，
指示性としては総称名詞句，定名詞句，不定名詞句の三種類が主に設けられている．
この指示性は，日英機械翻訳の冠詞生成や同一名詞句の照応解析に役に立つ．
名詞句の指示性の先行研究では，
表層の手がかり語を利用した規則を利用して解析し，
複数の規則が競合した場合は規則により各指示性に得点を付与し
合計点の最も大きい指示性を解としていた．
この規則に記述している得点は人手で付与しており，
人手の介入が大きい方法となっていた．
本稿では，この人手で付与していた得点部分を
機械学習の手法で自動調節することで，
人手のコストを削減することに成功したことについて記述している．","The referential properties of noun phrases are useful for 
article generation in Japanese-English machine translation and 
in anaphora resolution in Japanese same noun phrases 
and are generally classified into 
generic noun phrases, definite noun phrases and indefinite noun phrases. 
In the previous work, 
an estimation of referential properties was done by 
developing rules that used clue words. 
If two or more rules were in conflict with each other, 
the category having the maximum total score given by the rules 
was selected as the desired category. 
The score given by each rule was established by hand, 
so the manpower cost was high. 
This paper describes a machine learning method 
that reduces the amount of manpower needed to adjust these scores.","['はじめに', '名詞句の指示性の分類', '名詞句の指示性の推定方法', '実験と考察', 'おわりに', '最大エントロピー法(文献\\protect\\cite{uchimoto:nlp99']",,,,,,,,
V07N01-04.tex,意味ソートmsort\\--- 意味的並べかえ手法による辞書の構築例と\\タグつきコーパスの作成例と情報提示システム例 ---,"Meaning Sort MSORT\\--- Three Examples: Dictionary Construction,\\ Tagged Corpus Construction,\\ and  Information Presentation System ---","本稿では単語の羅列を意味でソートすると
いろいろなときに便利であるということについて記述する．
また，この単語を意味でソートするという考え方を示すと同時に，
この考え方と辞書，階層シソーラスとの関係，
さらには多観点シソーラスについても論じる．
そこでは単語を複数の属性で表現するという考え方も示し，
今後の言語処理のためにその考え方に基づく辞書が必要であることに
ついても述べている．
また，単語を意味でソートすると便利になるであろう
主要な三つの例についても述べる．","It is often useful to arrange words in their meanings' order 
that is obtained by using a thesaurus. 
In this paper, we introduce a method of 
arranging words semantically, 
and show the relationship between this method and 
some types of dictionaries and thesauruses. 
We also examine an ideal dictionary that could be 
used for future natural language processing. 
Finally, we describe three main examples 
of this method.","['はじめに', '意味ソート', '意味ソートの仕方', '意味ソートの諸相', '意味ソートの三つの利用例', 'おわりに']",,,,,,,,
V07N02-01.tex,複合語の分野連想語の効率的決定法,An Efficient Method of Determining Field \\ Association Terms of Compound Words,"人間は文書全体を読むことなしに，代表的な単語を見るだけで，＜政治＞や＜スポーツ＞などの分野を認知できることから，文書断片内の数少ない単語情報から分野を的確に決定するための分野連想語の構築は重要な研究課題である．しかし，文書から連想語を抽出する場合，複合語の冗長な連想語が多く存在する．本論文では，事前に分野体系が定義され，各分野に文書データが構築されている場合において，複合語の分野連想語を効率的に決定する手法を提案する．本手法では，連想分野を特定する範囲に応じて連想語を五つの水準に分類し，まず複合語以外の単語（短単位語と呼ぶ）の連想語候補を決定し，人手で修正を加える．次に，この短単位語の連想情報を利用して，膨大な数になる複合語の連想語候補を自動的に絞り込む．収集された180分野の学習データ（42メガバイト，15,435ファイル）に対して提案手法を適用した結果，88,782個の候補が8,405個（候補数の約9\%）の連想語に絞り込まれ，再現率0.77以上（平均0.85），適合率0.90以上（平均0.94）の高い抽出精度が得られた．また，連想語を利用した264種類の断片文書の分野決定実験より，複合連想語と短単位連想語による正解率は90\%以上となり，短単位連想語のみの場合より約30\%向上することが分かった．","Although there are many kinds of research about text classification based on term information in the whole text, humans can recognize the field of a text by finding a small number of  specific words in it.   In this paper, such terms are called a field association (FA) term that can be directly related to the field of a text.  It is possible to collect single-word FA terms because the number is finite, but there are some difficulties: how to select useful compound FA terms from a huge number of combinations of single-word FA terms.     For FA terms, five association levels are defined and two kinds of ranks based on stability and inheritance are presented.   Redundant candidates of compound FA terms can be removed remarkably by using the level and the rank.   From the simulation results of 180 fields' Japanese text files, it turns out that the total number 88,782 of candidates for compound FA terms can be reduced to 8,405 which is about 9\% to the original and that recall and precision are more than 0.77 and 0.90, respectively.   From the experimental results of field determination using FA terms for 264 fragments of texts, it is shown that the accuracy by the presented method attains  more than 90\% , and that is about 30\% higher than the case where only single-word FA terms are used.","['はじめに', '分野連想語の水準', '分野連想語候補の決定手法', '複合語の分野連想語の決定', '分野連想語の構築実験結果と評価', 'むすび', '分野体系と学習データの情報']",,,,,,,,
V07N02-02.tex,英字新聞記事見出し翻訳の自動前編集による改良,Improvement of Translation Quality of English Newspaper Headlines by Automatic Preediting,"英字新聞記事の見出しは通常の文の表現形式とは異なる特有の形式
をしているため，従来の英日機械翻訳システムによる見出しの翻訳の品質はあま
り高くない．
この問題に対して本研究では，見出しを通常の表現形式に書き換える自動前編
集系を既存のシステムに追加することによる解決を目指している．
見出しを通常の表現形式に書き換えれば，より品質の高い翻訳が，システムの
既存部分にほとんど変更を加えることなく得られる．
例えば``Sales up sharply in June''という見出しは通常のシステムには受理
されない可能性が高いが，``Sales {\it were","Since the headlines of English news articles have a characteristic
style, different from the styles which prevail in ordinary sentences, it 
is difficult for MT systems to generate high quality translations for 
headlines.
We try to solve this problem by adding to an existing system a
preediting module which rewrites the headlines to ordinary expressions.
Rewriting of headlines makes it possible to generate better translations 
which would not otherwise be generated, with little or no changes to the 
existing parts of the system. 
While most MT systems would not probably accept, for example, the
headline ``Sales up sharply in June'', they would be able to generate
a satisfactory translation of the expression ``Sales {\it were","['はじめに', '英々変換系', '英字新聞記事見出しの調査', 'be動詞補完規則の記述', '実験と考察', 'おわりに']",,,,,,,,
V07N02-03.tex,新聞の用字の面による変動と時系列変動,"Page-type and time-series variations \\ 
of a newspaper's character occurrence rate","1991年から1997年までの毎日新聞7年分の電子テキスト（約3.4億文字）
を対象に，使用されている文字種すべて（5,726；空白文字を除く）
について，その出現率（出現頻度）が，面種（e.g., 解説面，スポーツ面，
社会面），月次，年次の3つの要因に関して，どの程度まで系統的な変動を
示すかを検討した．5,726文字種のうち，16の面種間による出現率の差は
69.2\%で，月次による出現率の差は20.3\%で，年次による出現率の差は
43.9\%で認められた．低出現率の文字（0.001‰未満）を除いた2,732
文字種では，さらに変動は顕著で，面種差は98.4\%で，月次差は
33.5\%で，年次差は76.0\%で認められた．このように，紙面の種類と
時系列によって，新聞の文字使用が系統的に変動することが，広範に
確認された．こうした語彙表現に関わる変動現象は，大量のテキストに
基づいて文字や単語の計量を行うような研究ではあまり関心が払われて
こなかったが，変動のもつ規則性は，それ自体，精細な分析の対象と
なりうるものである．","Using 1991--1997 Mainichi Shimbun Newspaper's CD-ROMs containing 
about 340 million characters, nature of character use was explored.  
Significant differences of mean occurrence rates among 16 
page-types (e.g., editorial, sports, local) were observed 
in 69.2 \% of 5,726 character types that covered all the cases 
in the corpus except a space character.  
Similarly, 20.3 \% of those showed significant 
month-level (seasonal) variations of occurrence rates, 
and 43.9 \% showed significant year-level variations (trends).  
Limited to frequent 2,732 character types that severally 
accounted for more than 0.001 ‰ of the corpus, these tendencies became 
more clearly: rates of character types that showed significant 
variations of occurrence rates by page-types, month-levels, 
and year-levels were 98.4, 33.5, and 76.0 \% respectively.  
These results suggest that there could be a vast range of 
systematic variations in lexical use, which have been 
overlooked in simple summing-up of mass corpuses.","['はじめに', '研究1：面種変動の分析', '研究2：時系列変動の分析', '全体的考察', 'おわりに']",,,,,,,,
V07N02-04.tex,最大エントロピーモデルと\\書き換え規則に基づく固有表現抽出,Named Entity Extraction \\Based on A Maximum Entropy Model and Transformation Rules,"本論文では，ME(最大エントロピー)モデルと書き換え規則を用いて
固有表現を抽出する手法について述べる．
固有表現の定義はIREX固有表現抽出タスク(IREX-NE)の定義に基づくものとする．
その定義によると，固有表現には一つあるいは複数の形態素からなるもの，
形態素単位より短い部分文字列を含むものの2種類がある．
複数の形態素からなる固有表現は，
固有表現の始まり，中間，終りなどを表すラベルを40個用意し，
各々の形態素に対し付与すべきラベルを推定することによって抽出する．
ラベルの推定にはMEモデルを用いる．
このMEモデルでは学習コーパスで観測される素性と
各々の形態素に付与すべきラベルとの関係を学習する．
ここで素性とはラベル付与の手がかりとなる情報のことであり，
我々の場合，着目している形態素を含む前後2形態素ずつ
合計5形態素に関する見出し語，品詞の情報のことである．
一方，形態素単位より短い部分文字列を含む固有表現は，
MEモデルを用いてラベルを決めた後に書き換え規則を適用することによって抽出する．
書き換え規則は学習コーパスに対するシステムの解析結果とコーパスの
正解データとの差異を調べることによって自動獲得することができる．
本論文ではIREX-NE本試験に用いられたデータに対し我々の手法を適用した
結果を示し，さらにいくつかの比較実験から
書き換え規則と精度，素性と精度，学習コーパスの量と精度の関係を明らかにする．","This paper describes a system for extracting named entities. 
The system is based on a ME (maximum entropy) model 
and transformation rules. 
Eight types of named entities are defined by IREX-NE, 
and each named entity consists of one or more morphemes, 
or it includes a substring of a morpheme. 
We define 40 named entity labels, which are at the beginning, 
the middle, or the end of a named entity, 
and extract a named entity which consists of one or more morphemes 
by estimating the labels according to the ME model. 
The trained ME model detects the relationship between features 
and named entity labels assigned to morphemes. 
The features are clues used for estimating labels. 
We use information about lexical items and parts-of-speech 
as features in the target morpheme. 
We also use information 
about lexical items and parts-of-speech in four morphemes, 
two on the left and two on the right of the target morpheme, 
as features. 
After estimating the named entity labels according to the ME model, 
we extract a named entity, which includes a substring of a morpheme, 
by using transformation rules. 
These rules are automatically acquired by investigating the difference 
between named entity labels in a tagged corpus and those extracted by 
our system from the same corpus without tags. 
This paper also 
evaluates the relationships between transformation rules and accuracy, 
between features and accuracy, and between the amount of 
training data and accuracy by conducting several comparative experiments.","['はじめに', '固有表現抽出アルゴリズム', '実験と考察', 'まとめ']",,,,,,,,
V07N02-05.tex,統計的手法による換喩の解釈,Statistical Approach to \\ the Interpretation of Metonymy,"本稿では，換喩を統計的に解釈する方法を述べた．換喩と
  は，喩える言葉(喩詞)と喩えられる言葉(被喩詞)との連想に基づい
  た比喩である．たとえば，「漱石を読む」という換喩は，「漱石の
  小説を読む」というように解釈できる．この場合，喩詞である「漱
  石」と被喩詞である「(漱石の)小説」との間には，「作者-作品」と
  いう連想関係が成立する．本稿では，以下の方針で換喩を解釈する
  ことを試みた．
  \begin{enumerate","This paper describes a statistical approach to the
interpretation of metonymy.  In metonymy, the name of one
thing ({\it the source","['はじめに', '換喩の種類と対象とする換喩', '喩詞と関連する名詞群を求めるときに使う共起関係', '被喩詞らしさの統計的尺度', '実験', '考察', 'おわりに']",,,,,,,,
V07N02-06.tex,,"The Exploration and Analysis of
\\Using Multiple Thesaurus Types
\\for Query Expansion in Information Retrieval",,"This paper proposes the use of multiple thesaurus types for query expansion
in information retrieval. Hand-crafted thesaurus, corpus-based
co-occurrence-based thesaurus and syntactic-relation-based thesaurus
are combined and used as a tool for query expansion.  A simple word
sense disambiguation is performed to avoid misleading expansion terms. 
Experiments using TREC-7 collection proved that this method could
improve the information retrieval performance significantly. Failure
analysis was done on the cases in which the proposed method fail to
improve the retrieval effectiveness. We found that queries containing
negative statements and multiple aspects might cause problems in the
proposed method.","['Introduction', 'Method', 'Experimental Results', 'Discussion', 'Failure Analysis', 'Combining with relevance feedback', 'Conclusions and Future Work', 'Acknowledgments']",,,,,,,,
V07N02-07.tex,位置情報と分野情報を用いた情報検索,Information Retrieval Using Location\\ and Category Information,"われわれの情報検索の方法では基本的に，
確率型手法の一つの Robertson の2-ポアソンモデルを用いている．
しかし，この Robertson の方法では
検索のための手がかりとして当然用いるべき
位置情報や分野情報などを用いていない．
それに対し
われわれは位置情報や分野情報などをも用いる枠組を考案した．
IREXのコンテストでは，
この枠組に基づくシステムを二つ提出していたが，
記事の主題が検索課題に関連している記事のみを正解とする
A判定の精度はそれぞれ 0.4926 と 0.4827 で，
参加した15団体，22システムの中では最もよい精度であった．
本論文ではこのシステムの詳細な説明を行なうとともに，
種々のパラメータを変更した場合の詳細な対照実験を記述した．
この対照実験で位置情報や分野情報の有効性を確かめた．","Robertson's 2-poisson model to retrieve information 
does not use location information and category information. 
We constructed a framework using 
location information and category information 
in a 2-poisson model. 
We submitted two systems based on this framework  
at the IREX contest. 
Their precisions in the A-judgement measure were 
0.4926 and 0.4827, 
the highest values among the 15 teams and 22 systems 
that participated in the IREX contest. 
This paper describes our systems and 
comparative experiments when various parameters are changed. 
These experiments 
confirmed the effectiveness of location information and category information.","['はじめに', '情報検索の方法', 'IREX コンテストの本試験に提出した二つのシステムの説明とその実験結果', '各種の情報・手法の評価実験', 'おわりに']",,,,,,,,
V07N03-01.tex,痕跡処理のためのGLR法の拡張,Gap Handling Extension of \\ GLR Parsing Method,"本論文では，GLR法に基づく痕跡処理の手法を示す．痕跡という考え方は，チョ
ムスキーの痕跡理論で導入されたものである．痕跡とは，文の構成素がその文
中の別の位置に移動することによって生じた欠落部分に残されると考えられる
ものである．構文解析において，解析系が文に含まれる痕跡を検出し，その部
分に対応する構成素を補完することができると，痕跡のための特別な文法規則
を用意する必要がなくなり，文法規則の数が抑えられる．これによって，文法
全体の見通しが良くなり，文法記述者の負担が軽減する．GLR法は効率の良い
構文解析法として知られるが，痕跡処理については考慮されていない．本論文
では，GLR法に基づいて痕跡処理を実現しようとするときに問題となる点を明
らかにし，それに対する解決方法を示す．主たる問題は，ある文法規則中の
痕跡の記述が，その痕跡とは関係のない文法規則に基づく解析に影響を与え，
誤った痕跡検出を引き起す，というものである．本論文で示す手法では，この
問題を状態の構成を工夫することで解決する．","We propose an approach to gap handling, which is based on GLR parsing
method. The idea of gaps has been introduced in Noam Chomsky's
transformational grammar. A gap occurs in a sentence when a
constituent of the sentence transfers to another position of the
sentence. In cases where a parser finds a gap in a sentence and
fills it with the corresponding constituent, it is easier to write and
maintain grammar because additional grammar rules for gaps are
needless. GLR parsing method has no gap handling, although it is known
as an efficient parsing method. We make it clear what prevents GLR
parsing method from handling gaps, and give a solution to gap handling
in GLR parsing method. The main problem is that a gap described in a
grammar rule has bad effect on parsing with other grammar rules (LR
items) in the same state. We solve this problem by splitting a state
into more than one state.","['はじめに', 'GLR法', '文法記述形式', '痕跡検出', '状態の構成', 'slash項に基づく状態遷移', '複合名詞句制約', '動作例', 'おわりに']",,,,,,,,
V07N03-02.tex,日本語連体修飾要素の多義解消に関する \\ 語彙意味論的検討,"Lexical Semantics to Disambiguate \\  Polysemous Phenomena 
  of Japanese \\ Adnominal Constituents","日本語連体修飾要素に関する言語現象を取り扱うことができるような辞書記述を
  実現するため，生成的辞書理論を用いた連体修飾要素の形式的記述法の検討を行っ
  た．問題となる現象の解決法を「静的な曖昧性解消」と「動的な曖昧性解消」に
  分類した．静的な曖昧性解消は辞書中の語彙情報を用いて行うことができるが，
  動的な曖昧性解消には知識表現レベルでの推論が必要となる．","We exploit and extend the Generative Lexicon Theory to develop a formal
  description of adnominal constituents in a lexicon which can deal with 
  linguistic phenomena found in Japanese adnominal constituents.
  We classify the problematic behavior
  into ``static disambiguation'' and ``dynamic disambiguation'' tasks.
  Static disambiguation can be done using lexical information in a
  dictionary, whereas dynamic disambiguation requires inferences at the
  knowledge representation level.","['はじめに', '日本語連体修飾要素の用法の分類', '日本語の連体修飾要素の役割の分類', '日本語連体修飾要素の形式的取り扱い', 'おわりに']",,,,,,,,
V07N03-03.tex,言語に依存しない形態素解析処理の枠組,Framework for \\Language Independent Morphological Analysis,"形態素解析処理において，
  日本語などのわかち書きされない言語と英語などのわかち書きされる言語では，
  形態素辞書検索のタイミングや辞書検索単位が異なる．
  本論文ではこれらの言語で共通に利用できる形態素解析の枠組の提案と，
  それに基づいた多言語形態素解析システムを実装を行った．
  また，日本語，英語，中国語での解析実験も行った．","This paper takes up the problem of tokenization and part-of-speech
tagging of segmented and
non-segmented languages, and proposes a simple framework that enables
an efficient and uniform treatment of tokenization for both types of
languages.  We also reports a language independent morphological
analysis system based on the proposed idea, and shows running systems
for three different languages, English, Japanese and Chinese.","['はじめに', '言語に依存しないトークン認識と辞書検索', 'コンポーネント化と実装', '関連研究', 'おわりに']",,,,,,,,
V07N03-04.tex,派生文法に基づく日本語動詞句のウイグル語への翻訳,"Verbal Phrase Generation \\based on 
Derivational Grammar \\in Japanese-Uighur Machine Translation","日本語とウイグル語は共に膠着語であり，
構文的に類似した点が多い．
したがって，日本語からウイグル語への機械翻訳においては，
形態素解析によって得られた各単語を逐語翻訳する
ことにより，ある程度の翻訳が可能となる．
しかし，従来の日本語文法は動詞が活用することを前提としていたため，
ウイグル語への翻訳の前に，
動詞の活用処理が必要であった．
本論文では，
日本語，ウイグル語を共に派生文法で記述することにより，
日本語の活用処理を不要とすると同時に，両言語間の形態論的類似点を
明確にし，単純でかつ体系的な機械翻訳が可能になることを示す．
しかし，日本語とウイグル語との間の文法的差異から，
単純な逐語翻訳では
不自然な翻訳となる場合がある．
本論文では，単語間の接続関係を考慮した訳語置換表を用いる
ことによりこの問題を解決し，より自然な翻訳を実現した．
さらに，この手法に基づく日本語--ウイグル語機械翻訳システム
を作成した．このシステムでは，
日本語形態素解析システムと
ウイグル語整形システムを，それぞれ独立のモジュール
として構成している．
この設計は，他の膠着語間における翻訳にも応用可能
であると考えられる．
また，実験によりその翻訳精度を評価した．
本論文では，特に両言語において文の中心的役割を果たす動詞句の翻訳
について述べる．","Since both Japanese and Uighur languages are 
{\it agglutinative languages","['はじめに', '日本語--ウイグル語逐語翻訳', '派生文法による日本語動詞句の記述', '派生文法によるウイグル語動詞句の記述', '派生文法を用いた逐語翻訳', '逐語翻訳における問題点', '訳語置換表の導入', '機械翻訳システムの実現', '翻訳実験の評価と検討', 'おわりに']",,,,,,,,
V07N03-05.tex,ターム間の意味的関連性に基づくタームリストの翻訳多義解消,Disambiguation in Termlist Translation based on Semantic Proximity,"文章に付与されたキーワード集合のような内容語（ターム）
の並びを
「タームリスト」と呼ぶ．本論文では，翻訳先言語のコーパス
のみを用いて，各タームに対する訳語候補のなかから最適なものを
選択する「翻訳多義解消」の新たな方法を提案する．
  本手法では，各タームに対する訳語候補を一つずつ集めてできる
組み合わせのうち，含まれる訳語同士の意味的関連性が最も高い組
を選択する．単語間の意味的関連性の尺度は各単語が翻訳先言語
コーパスにおいてどの程度近い文脈に出現するかを数値化したも
のである．翻訳実験の結果，翻訳多義のある
単語に対する平均正解率77.4\%を達成した．","This paper presents a method for resolving ambiguity in 
translating a list of content words (termlist), such as a 
query for information retrieval or a keyword list 
that represents a document. The entire translation begins 
with retrieving a potential translation alternatives for each 
input word from a bilingual dictionary. Our disambiguation 
method, then, chooses one translation for each input word in 
such a way that the resulting list of translations are most 
coherent or related. The {\em relatedness","['はじめに', 'タームリスト翻訳における多義解消タスク', '翻訳多義解消モデル', 'アルゴリズム', '評価実験', '考察', 'おわりに', 'SVDによる共起行列の縮退', '不等式(4)の証明']",,,,,,,,
V07N04-01.tex,アロー論理によるアスペクトの解析,Aspectual Information \\ Represented by Arrow Logic,"本稿の目的は自然言語の時間の相 -アスペクト- に対し，個別
の言語から独
立した論理的意味を与えることである．近年のアスペクトの意味論では，ア
スペクトは共通のイベント構造に対してその異なる部位にレファランスを与
えるものであると説明される．本稿でもまずこの理論を概観するが，その際
アスペクトの形式化において常に問題となる用語および概念の混乱を，本稿
で扱う範囲で整理・統合する．次に従来的な点と区間の論理に代わってアロー
論理を導入し，アローを両端が固定されない向きをともなった時間区間であ
るとしてアスペクトの解析に適用する．アローを含む動的論理はそれ自身既
にアローとその端点の関係が含まれる上に，論理の側で順序や包含の関係が
表現できるために，従来的な述語論理によるアスペクトの形式化に比べては
るかに簡潔な表現が可能である．さらにアローと時点の関係をつけることで，
時間的に束縛されていない事象の原型がアスペクトを伴う表現にシフトされ
る過程を動的な推論規則として説明できる．","The objective of this paper is to give formal semantics to aspects,
independent of individual natural languages. Recent aspect theories
explain that aspects are different ways of viewing the common event
structure. However, the theories are still informal and not
expressed in precise logical formalisms, and in addition, aspectual
terms are different from one theory to another. We first survey
those theories and give a united view to terms.  Thereafter, we
propose to adopt {\it arrow logic","['はじめに', '言語学的背景', 'アロー論理と状況推論', 'アスペクトの形式化', 'おわりに']",,,,,,,,
V07N04-02.tex,計算機処理のための韓国語言語体系と形態素処理,Language System and Morphological Processing Technique for Korean Computational Processing,"韓国語の言語処理，特に韓国語を原言語もしくは目的言語とする機械翻訳にお
ける，韓国語の言語体系と形態素処理手法を提案する．本論文の韓国語体系の
特徴は，機械処理を考慮した体系であるという点にある．すなわち，形態素解
析の解析精度や機械翻訳における品詞設定の必要性に応じて，韓国語各品詞に
対して仕様の検討を行ない，設計を行なった．また分かち書きや音韻縮約といっ
た韓国語の特徴をどのように機械処理すべきかについても述べる．韓国語形態
素解析では，品詞と単語の混合n-gramによる統計的手法を基本としながら，韓
国語固有の問題に対しては残留文字などの概念を導入するなどして独自の対応
を施した．以上の品詞体系と形態素解析エンジンによって，単語再現率99.1\%，
単語適合率98.9\%，文正解率92.6\%という良好な解析精度が得られた．また韓
国語生成処理では，特に分かち書き処理についてどのような規則を作成したの
かについて提案を行なう．以上の形態素体系と処理の有効性は，機械翻訳シス
テムTDMTの日韓翻訳，韓日翻訳部に導入した際の翻訳精度という形で文献
\cite {古瀬99","A morpheme and part-of-speech system for Korean natural language
  processing, or machine translation in particular, is proposed in
  this paper.  We designed this language system for easier computer
  processing.  It is important to attain satisfactory performance when
  we segment and tag input Korean strings.  There is also under- and
  over-classification in a linguistic part-of-speech system for
  machine translation.  Thus we defined an original part-of-speech
  system, which is demonstrated in this paper with some examples.  We
  based our morphological analysis on the mixed n-gram statistics of
  both parts-of-speech and words.  We tuned up this engine to the
  Korean language for proper characteristics.  Experiments have proven
  that our engine has 99.1\% word recall, 98.9\% word precision, and
  92.6\% sentence accuracy, for unseen Korean strings.  In language
  generation, spacing rules are proposed for Korean using our
  part-of-speech system.  We have proven the appropriateness of our
  morpheme system in the performance of machine translation for both
  Japanese-Korean and Korean-Japanese, as shown in (Furuse, Yamamoto,
  and Yamada, 1999).","['はじめに', '計算機処理から見た韓国語の特徴', '韓国語形態素体系', '韓国語品詞体系', '韓国語形態素解析', '韓国語生成', '結論']",,,,,,,,
V07N04-03.tex,語彙的連鎖に基づく要約の情報検索タスクを用いた評価,"Evaluation of Summaries Based on Lexical \\Chains 
Using Information Retrieval Task","電子化テキストの増大にともない，テキスト自動要約技術の重要性
が高まっている．近年，情報検索システムの普及により，検索結果
提示での利用が，要約の利用法として注目されている．要約の利用
により，ユーザは，検索結果のテキストが検索要求に適合している
かどうかを，素早く，正確に判定できる．
一般に情報検索システムでは，ユーザの関心が検索要求で表わされる
ため，提示される要約も，元テキストの内容のみから作成されるもの
より，検索要求を反映して作成されるものの方が良いと考えられる．
本稿では，我々が以前提案した語彙的連鎖に基づくパッセージ抽出
手法が，情報検索システムでの利用を想定した，
検索要求を考慮した要約作成手法として
利用できる
ことを示す．
語彙的連鎖の使用により，検索要求に関連するテキスト中のパッセー
ジを要約として抽出できる．
我々の手法の有効性を確かめるために，情報検索タスクに基づいた
要約の評価方法を採用し，10種類の要約作成手法による実験
を行なう．
実験結果によって，我々の手法の有効性が支持されることを示す．
また，評価実験の過程で観察された，タスクに基づく評価方法に関
する問題点や留意すべき点についても
分析し，報告する．","The importance of the automatic summarization research is now
increasing with the growing availability of on-line documents. In
information retrieval systems, summaries can be used as the display of
the retrieval results,
in order for users to quickly and accurately judge the relevance of
the documents which are returned as a result of the users' query.
Here, rather than producing a generic summary, the summary that reflects
the user's topic of interest expressed in the query would be considered as 
more suitable. This type of summary is often called `query-biased
summary'.
In this paper, we show that our previously proposed passage extraction
method based on lexical chains can be 
used 
 to produce better
query-biased summaries for information retrieval systems.
To evaluate the effectiveness of our method, a task-based evaluation
scheme is adopted. The results from the experiments 
support 
that query-biased
summaries by lexical chains outperform others in the accuracy of subject's 
relevance judgments.
Furthermore, to establish a better evaluation methodology, we also
investigate
and describe the problems that arise from the experimental design.","['はじめに', '語彙的連鎖型パッセージ抽出法に基づく要約', '評価実験', '結果と考察', 'おわりに']",,,,,,,,
V07N04-04.tex,動詞型連体修飾表現の``$N_1のN_2$''への言い換え,"Paraphrasing a Japanese verbal noun phrase \\into an expression 
``$N_1$ {\it no","動詞を含む連体修飾表現を``$N_1のN_2$''という表現に言い換える
 手法を提案する. 動詞を含む連体修飾節は, 各文を短縮する既存の
 要約手法において, 削除対象とされている. ところが, 連体修飾節
 の削除によって, その名詞句の指示対象を同定することが困難にな
 る場合がある. それを表現``$N_1$の$N_2$''に言い換えることで, 
 名詞句の意味を限定し, かつ, 字数を削減することが可能である. 
 言い換えは, 動詞を削除することによって行う. 表現``$N_1のN_2$''
 では, 語$N_1$と$N_2$の意味関係を示す述語が省略されている場合
 がある. この省略されうる述語を,削除可能な動詞として2種類の方
 法により定義した. 一方では, 表現``$N_1のN_2$''の意味構造に対応する
 動詞を, シソーラスを用いて選択した. また, 他方では, ある語から
 連想される動詞を定義した. ただし, コーパスから, 名詞とそれが係る動詞
 との対を抽出し, 共起頻度の高いものを, 名詞から動詞が連想可能
 であると考えた. 
 これらの削除可能な動詞を用いた言い換えを評価したところ,
 再現率63.8\%, 適合率61.4\%との結果を得た. さらに, 言い換え可能
 表現の絞り込みを行うことによって適合率は 82.9\%に改善すること
 が可能であることを示す.","The purpose of this paper is to propose a method of paraphrasing a
Japanese verbal noun phrase into a noun phrase in the form of ``$N_1$
{\it no","['はじめに', '連体修飾表現の言い換え', '本手法の構成', '削除可能な動詞', '言い換え可能表現の絞り込み', '評価', '考察', '関連研究', 'おわりに']",,,,,,,,
V07N04-05.tex,頑健な英日機械翻訳システム実現のための\\原文自動前編集,Automatic Preediting of English Sentences for a Robust English-to-Japanese MT System,"本稿では，従来の機械翻訳システムの構文解析能力を越える倒置や
挿入などを含む文に対して頑健な処理を実現するための一手法として，形態素解
析と簡単な構文解析によって得られる情報に基づいて原文を書き換える自動前編
集手法を示す．
原文書き換え系を既存システムに追加することによって，
1) より品質の高い翻訳がシステムの既存部分にほとんど変更を加えることなく
得られるようになるだけでなく，
2) 構文解析の負担が減少するためシステム全体としての効率化が実現できる．
実際，提案手法を我々の英日機械翻訳システムPower E/Jに組み込み，新聞
記事を対象として実験を行なったところ，
1) 書き換え規則が適用された330文の78.8\%にあたる260文の翻訳品質が改善さ
れ，
2) 書き換えを行なった場合の翻訳速度は行なわない場合の速度の1.12倍にな
った．","As a means of allowing for robust processing of such linguistic 
phenomena as inversion, ellipsis, parenthesis and emphasis, which are liable to
prevent a syntactic parser from generating appropriate syntactic structures,
this paper shows a method of automatically preediting sentences, based on 
information obtained by morpholexical and simple syntactic analysis.
Addition of a preediting module to the existing system
makes it possible
1) to generate better translations, which would not otherwise be generated,
with little or no changes to the existing parts of the system, and
2) to reduce the load of syntactic analysis, thus enhancing the efficiency 
of the whole system.
We have incorporated the proposed method into our English-to-Japanese
machine translation system Power E/J, and carried out an experiment with 
sentences in news wire articles.
The incorporation of the preediting module has satisfactorily 
1) improved the quality of translations for the 260 sentences out of 
rewritten 330 ones (78.8\%), and
2) marked up the speed of 1.12 times as fast as the system without the 
module.","['はじめに', '書き換え対象文', '原文書き換え系', '実験と考察', '関連研究との比較', 'おわりに']",,,,,,,,
V07N04-06.tex,係り受け関係を用いた重複表現削除,Reduction of overlapping expressions using dependency relations,"字幕生成のためのニュース文要約のような報知的要約で
は，原文の情報を落とさないことが望まれる．本論文では，このよ
うな原文の情報を極力落とさない要約手法の一つとして，重複部削
除による要約手法について議論する．テキスト内に，同一の事象を
表す部分が再度出現したならば，その部分を削除することによって
冗長度を減少させ，情報欠落を可能な限り回避した要約を行う．
事象の重複を認定するために，係り受け関係のある2語が一つの
事象を表していると仮定し，2語の係り受け関係の重複を事象の重複
と認定する．また，2語の係り受け関係を用いて重複部を削除するだ
けでは，読みやすく，かつ，自然な要約文を生成することができない．
そのために考慮すべきいくつかの情報について議論する．以上の方
法のうち，実装可能な部分を計算機上に実装し，評価実験を行った．
人間による削除箇所と本手法による削除箇所とを比較したところ，
再現率81.0\%，適合率85.1\%の結果を得た．","Informative summaries are used as substitutions for original
  texts. It is necessary for those for captions of newscasting, in
  particular, to keep the original information as much as possible.
  This paper discusses a summarization method of reducing overlaps to
  generate informative summaries.  Deleting a part of the text which
  refers to the same content as some other part can reduce redundancy
  and avoid lack of information.  In order to recognize overlaps, we
  utilize a pair of dependent words.  Deletion of overlaps only using
  a pair of words having a dependency sometimes makes a summary
  unnatural and difficult to read.  Therefore, what should be
  considered to cope with the problems is described.  We compared
  summaries of TV news texts independently generated by our method and
  by human for evaluation.  The experimental results show that the
  precision attained 85.1\% and the recall attained 81.0\%, respectively.","['はじめに', '重複部の認定', '重複部の削除', '計算機上での実現', '評価', '考察', '関連研究', 'おわりに']",,,,,,,,
V07N04-07.tex,"日英機械翻訳システムTWINTRANの
\\言語知識と翻訳品質の評価",Linguistic Expertise in the Japanese-to-English MT System TWINTRAN and Evaluation of Translation Results,"本稿では，日英機械翻訳システムTWINTRANの言語知識(辞書と規則)
と翻訳品質の評価結果について述べる．
TWINTRANは次のような設計方針に基づくシステムである．
1) 翻訳対象と翻訳方向を日本語テキストから英語テキストへの翻訳に固定し，
日本語の解析知識の記述を，日本語文法だけでなく英語文法も考慮に入れて行な
う．
2) 解釈の曖昧性の解消は，各規則に与えた優先度に基づいて解釈の候補に優
劣を付け，候補の中から最も優先度の高い解釈を選択することによって行なう．
3) 動詞の主語や目的語など日本語では任意的だが英語では義務的である情報を
得るために照応解析を行なう． NTT機械翻訳機能試験文集を対象として行なったウィンドウテストでは，我々の
評価基準で，試験文集の73.1\%の文が合格となり，試験文集全体の平均点も合格
点を上回る結果が得られた．","In this treatise we set out to describe the linguistic expertise
(dictionary and rules) embodied in the Japanese-to-English MT system 
TWINTRAN, and the evaluation of the translation results. 
TWINTRAN is based on the following design policy:
1) The translation equivalents and the direction of the translation
process are strictly monodirectional, from Japanese to English.
The analysis of the Japanese input is not confined to Japanese grammar
but also anticipates at every step the possible English translation.
2) Disambiguation is based on prioritisation of each rule, where
each rule contains a priority value and the highest aggregate priority
candidate is selected.
3) Verb complements are screened for acceptability not only in the input
Japanese but also in the output English, and anaphora resolution
is used for arriving at the optimum result. In the window test we have carried out, based on 
NTT's functional MT test set,
applying our evaluation procedure, 73.1\% of the corpus was acceptable
and the corpus average was above the point of acceptability.","['はじめに', '辞書', '形態素解析', '構文解析', '英語必須補足語の補完と関係節の処理', '共起的意味・照応解析', '動詞型に基づく変換', '個別の変換情報に基づく変換', '生成', '翻訳実験', 'おわりに']",,,,,,,,
V07N04-08.tex,コーパスからの語順の学習,Word Order Acquisition from Corpora,"本論文では，日本語の語順の傾向をコーパスから学習する手法を提案する．
  ここで語順とは係り相互間の語順，
  つまり同じ文節に係っていく文節の順序関係を意味するものとする．
  我々が提案する手法では，文節内外に含まれるさまざまな情報から
  語順の傾向を自動学習するモデルを用いる．
  このモデルによって，それぞれの情報が語順の決定にどの程度寄与するか，
  また，どのような情報の組み合わせのときにどのような傾向の語順になるか
  を推測することができる．
  個々の情報が語順の決定に寄与する度合は
  最大エントロピー(ME)法によって効率良く学習される．
  学習されたモデルの性能は，そのモデルを用いて語順を決めるテストを行ない，
  元の文における語順とどの程度一致するかを調べることによって
  定量的に評価することができる．
  正しい語順の情報はテキスト上に保存されているため，
  学習コーパスは必ずしもタグ付きである必要はなく，
  生コーパスを既存の解析システムで解析した結果を用いてもよい．
  本論文ではこのことを実験によって示す．","In this paper we propose a method for acquiring word order from corpora. 
  We define word order as the order of modifiers or 
  the order of bunsetsus which depend on the same modifiee. 
  The method uses a model which automatically discovers what the tendency of 
  the word order in Japanese is by using various kinds of information in 
  and around the target bunsetsus. 
  It shows us to what extent each piece of information contributes to 
  deciding the word order and which word order tends to be selected 
  when several kinds of information conflict. 
  The contribution rate of each piece of information in deciding word order 
  is efficiently learned by a model within a maximum entropy (ME) framework. 
  The performance of the trained model can be evaluated 
  by checking how many instances of word order selected by the model 
  agree with those in the original text. 
  A raw corpus instead of a tagged corpus can be used to train the model, 
  if it is first analyzed by a parser. This is possible because text in the 
  corpus is in the correct word order. In this paper, we show that this is
  indeed possible.","['はじめに', '語順の学習と生成', '実験と考察', 'まとめ']",,,,,,,,
V07N04-09.tex,複数決定木を用いた入力誤りに頑健な省略補完手法,"Multiple Decision-Tree Strategy for Ellipsis\\ Resolution
    with Input-Error Robustness","日本語は主語などの要素がしばしば省略されるため，これらの補完は対話処
  理において重要である．さらに音声対話処理においては，実際に対話を処理
  する際に入力となるのは音声であり，一部誤りを含んだ音声認識結果が処理
  対象となるため，言語処理部においても不正確な入力に対する頑健性が要求
  される．このため，入力の一部に誤りのある状況下における格要素補完問題
  を考え，以前に提案した決定木を使用した補完手法を改良したモデルを提案
  する．このモデルは，複数の決定木を使用することで複数解候補を出力し，
  その中から学習時の終端節点事例数によって解の選好を行なうことで入力誤
  りに対する頑健性を強化した．音声認識の実誤りと人工的な誤りの2種類で
  評価実験を行なった結果，提案手法が誤りを含む入力に対し頑健であること
  を確認した．また人工的な問題に対するシミュレーションの結果，本提案手
  法は問題非依存であり，入力誤りの多さに応じた決定木の組み合わせでモデ
  ルを構成することで有効に機能することが明らかとなった．","In Japanese spoken language processing, the subject and other cases
  are often omitted.  Several approaches to resolve such an ellipsis
  have been proposed so far, but none of them have considered
  robustness against noisy input.  It is important to also have some
  robustness in an ellipsis resolution module, since the inputs of the
  process are the results of a speech recognition module, which may
  have some recognition errors in spoken dialogue processing.  We thus
  propose a robust model of ellipsis resolution which utilizes a
  multiple decision tree (MDT) model.  Experimental results have
  proven its robustness, and have also shown that the model is
  task-independent and works more effectively if we provide decision
  trees with the number of attributes corresponding to the amount of
  noise.","['はじめに', '関連研究', '主語補完手法', '複数決定木モデル', '主語補完実験', 'シミュレーション', '結論と今後の課題']",,,,,,,,
V07N04-10.tex,テキストデータを使った音声認識誤りの訂正,Correction of Speech Recognition Error using Text Data,"認識誤りに起因して，音声翻訳の性能(品質)が劣化するという問題がある．認
識結果の正解部分のみを翻訳する手法が提案されているが，翻訳されない部分に
関する情報は失われてしまう．我々はこの問題を解決するため，次のような手順
からなる誤り訂正の手法を提案する．(1) 訂正の必要性の判断および誤り部分
の特定を行う．(2)認識結果の誤り近傍に関して音韻的に近い用例をテキスト
データ中から検索し，訂正候補の生成を行う．(3)訂正候補の妥当性を意味と
音韻の両方の観点から判断し，最も妥当なものを選択する．提案手法を音声翻
訳システムに組み込み，旅行会話を対象として評価した．認識結果の単語誤り
率で$2.3\%$の減少，翻訳率で$5.4\%$の増加が得られ，提案手法の有効性が示
された．","Because of recognition errors, the performance (quality) of speech
translation is degraded. Previously, we proposed a method that used
only reliable parts of the recognition result for the
translation. However, in this method, non-translated parts are omitted
even if useful information exist in these parts. To overcome this
problem, we propose an error correction method which is composed of
the following steps: (1) The necessity of correction is judged and
only utterances of the recognition results with ``potentially''
recoverable erroneous parts are retained. (2) The example utterances
that have phonetically similar parts to the ones retained in the step
(1) are retrieved from a text corpus, and correction hypotheses are
created. (3) The reliability of the correction hypotheses is judged
according to both semantic and phonetic point of view and the most
reliable one is selected. The error correction method was incorporated
into a speech translation system, and evaluated for speech inputs in
travel conversations. As the results, the word error rate was reduced
by 2.3\%, and the acceptable translations rate was increased by
5.4\%.","['はじめに', '提案手法', '日英音声翻訳における誤り訂正実験', '入力会話を含まないデータベースでの日英音声翻訳実験', 'おわりに']",,,,,,,,
V07N04-11.tex,複数決定リストの順次適用による文節まとめあげ,Bunsetsu Identification with \\ Sequential Use of Plural Decision Lists,"近年の高度情報化の流れにより，自動車にも種々の情報機器が搭載
  されるようになり，その中で音声認識・合成の必要性が高まってい
  る．本研究は音声合成を行うための日本語解析の中で基本となる，
  文節まとめあげに関する研究報告である．
  従来の文節まとめあげは，人手規則による手法と機械学習による手
  法の二つに大きく分けられる．前者は，長年の努力により非常に高
  い精度を得られているが，入力データ形式が固定であるために柔軟
  性に欠け，人手で規則を作成・保守管理するため多大な労力を要し，
  車載情報機器へ実装するには問題が大きい．また後者は，それらの
  問題に柔軟に対処できるが，精度を向上させるためにアルゴリズム
  が複雑化しており，その結果開発期間が延長するなどの問題が生じ，
  車載情報機器には不向きである．
  そこで本研究は，決定リストを用いる手法を発展させ，複数の決定
  リストを順に適用するだけという非常に簡明な文節まとめあげの手
  法を提案する．決定リストの手法は非常に単純であるが，それだけ
  では高い精度が得られない．そこで，決定リストを一つではなく複
  数作成し，それぞれのリストを最適な順序に並べて利用することに
  より精度向上を図った．この結果，京大コーパスの最初の10000文
  を学習コーパス，残りの約10000文をテストコーパスとして実験を
  行ったところ，非常に簡明な手法ながら，99.38\%という高い精度
  を得られた．","Recent information-oriented society becomes to need Car-Multi-Media
  systems. In the systems, speech recognition and synthesis systems are
  also necessary. We aimed to improve Bunsetsu Identification which
  is important for them.
  There are two types of traditional Bunsetsu Identification methods:
  one is a method which uses handmade rules and the other is a method 
  which uses machine learning.
  The former has high accuracy rate, but there are some problems
  especially for Car-Multi-Media systems.
  For example, the method is not flexible because it needs fixed inputs,
  and the method needs a lot of efforts to keep identification rules
  because all rules are made by hand.
  The latter is robust for these problems, but the algorithms are
  much more complex to improve accuracy, so there are some problems for
  Car-Multi-Media systems.
  Therefore, we propose a new method that uses plural decision lists
  sequentially. The Decision List method is very simple, but
  it does not have very high accuracy rate. Then, we use not 'one'
  decision list but 'plural' decision lists 'sequentially'.
  We made some experiments using 10,000 sentences as a training corpus,
  and 10,000 sentences as a test corpus in Kyoto-University-Corpus.
  As the result, the accuracy rate was 99.38\%.","['はじめに', '従来の研究', '本研究の文節まとめあげの手法', '実験と考察', 'おわりに']",,,,,,,,
V07N04-12.tex,手指動作記述文間の類似性に基づく類似の 動作特徴を含む手話単語対の抽出方法,"An Extraction Method of Similar Signs Based\\ on
Similarity between Manual Motion\\ Descriptions","手話言語は，主に手指動作表現により単語の表出・受容を行う
視覚言語としての側面を持つ．
そのため，手話単語を構成する手指動作特徴の要素（例えば，手の形，
手の位置，手の動き）の一部を変更することで別の手話単語を構成
できる特徴がある．
特に，手指動作特徴の一つの要素だけが異なる単語対を手話単語の
最小対と呼ぶ．
また，手指動作特徴の類似性が意味の類似性を反映している場合がある．
このように，類似の手指動作特徴を含む手話単語対は
意味関係を内包する可能性があるなど，手話単語の分類を行うための
重要な手がかりの一つとなると考える，
すなわち，類似の動作特徴を含む手話単語対は言語学的に重要であるばか
りでなく，手話単語の検索処理や登録・編集処理機能を実現する上でも
重要な知識データと捉えることができる．
本論文では，類似した手指動作特徴を含む手話単語対を与えられた手話
単語の集合から抽出する方法として，市販の手話辞典に記述されている
手指動作記述文間の類似性に着目し，この手指動作記述文間の類似性を
手話単語間の手指動作特徴の類似性と捉え，手指動作記述文間の類似度
計算に基づき最小対を抽出する方法を提案する．
実験により，提案手法の妥当性を示す結果が得られた．","Sign language has an interesting characteristic that a change in part
of the manual motion properties in hand-shape, location, movement often
results in changing the meaning of signs.
It is particularly called a minimal pair that the difference of properties
between two signs is only one feature element.
In building an electronic sign dictionary system,
a couple of signs with similar manual motion properties play an important
role in the retrieval, registration and synthesizing mechanism.
This paper proposes a method for extracting a couple of signs with similar
manual motion properties from a given set of signs.
The method is based on the similarity between two signs, which is derived
from the longest common subsequence(LCS) between manual motion descriptions(MMDs).
It can be considered that a MMD represents information extracted from a series
of motions of a sign.
By computing the feature vectors of $n$ properties from MMDs and
plotting them in the $n$-dimensional Euclidean space, an angle between two
vectors can be considered as the similarity between two signs.
However, when the feature vector can be considered as a string of MMD,
the similarity can be obtained by string matching between the two MMDs.
The results of evaluation experiments show the applicability of the proposed
method.",[],,,,,,,,
V07N04-13.tex,自動要約のための文重要度の比較,"Empirical Comparison of Sentence Importance\\ 
  Measures for Automatic Text Summarization","本稿では，重要文抽出によるテキスト自動要約のための各
  種重要度を比較した．特に，タイトルとの類似度の高い文から抽出
  するという要約方法を想定し，各種の類似度を比較した．類似度と
  しては，共起関係を利用する方法と利用しない方法とを試みた．そ
  の結果，共起関係を利用する方法の方が高精度な要約が作成できた．
  また，要約の手法としては，他に，本文の先頭数文を抽出する方法
  と，単語の重要度の総和を文の重要度とする方法も試みたが，これ
  らの方法よりも，タイトルとの類似度に基づく方法の方が高精度で
  あった．これらの結果は，共起関係を利用した類似度が自動要約に
  有効であることを示している．","The effectiveness of various statistical measures of
  sentence importance was compared for automatic text
  summarization done by extracting important sentences. We
  focused on comparing various measures of sentence similarity
  on the assumption that important sentences in an article are
  similar to the title. Two types of similarity measures were
  compared: one uses word co-occurrence statistics and the
  other does not. The former proved superior to the latter. 
  Other automatic text summarization methods, such as
  extracting the leading part of an article, or extracting
  sentences with important words, proved inferior to the
  similarity-based method. These results show that similarity
  measurement using word co-occurrence statistics is effective
  for automatic text summarization.","['はじめに', '文の重要度の定義', '実験', 'おわりに']",,,,,,,,
V07N05-01.tex,後方文脈を考慮した係り受けモデル,Dependency Model Using Posterior Context,"係り受け解析は日本語解析の重要な基本技術の一つとして認識されている．
  依存文法に基づく日本語係り受け解析では，文を文節に分割した後，
  それぞれの文節がどの文節に係りやすいかを表す係り受け行列を作成し，
  一文全体が最適な係り受け関係になるようにそれぞれの係り受けを決定する．
  本論文ではそのうち，係り受け行列の各要素の値を計算するためのモデル
  について述べる．
  アプローチとしては，主にルールベースによる方法と統計的手法の
  二つのものがあるが，我々は利用可能なコーパスが増加してきたこと，
  規則の変更に伴うコストなどを考慮して，統計的手法をとっている．
  統計的手法では行列の各要素の値は確率値として計算される．
  これまでよく用いられていたモデル(旧モデル)では，
  その確率値を計算する際に，着目している二つの文節が係るか係らないか
  ということのみを考慮していた．
  本論文では，着目している二つの文節(前文節と後文節)だけを考慮するのではなく，
  前文節と前文節より文末側のすべての文節との関係(後方文脈)を考慮するモデルを
  提案する．
  このモデルをME(最大エントロピー)に基づくモデルとして実装した場合，
  旧モデルを同じくMEに基づくモデルとして実装した場合に比べて，
  京大コーパスに対する実験で，全く同じ素性を用いているにもかかわらず
  係り受け単位で1\%程度高い精度(88\%)が得られた．","Dependency structure analysis is
  one of the basic techniques in Japanese sentence analysis, 
  and the Japanese dependency structure is usually represented
  by relationships between phrasal units called `bunsetsu.'
  This analysis is a two-step procedure, and 
  the first step is to prepare a dependency matrix 
  in which each element represents how likely it is 
  that one bunsetsu depends on another.
  The second step of the analysis finds an optimal set of dependencies 
  for the entire sentence. 
  In this paper we discuss a model used in the first step,
  a model for estimating dependency likelihood.
  There are two approaches to estimating the dependency likelihood: 
  rule-based, and statistical. We take the statistical approach 
  because electrically available corpora are getting large, 
  and changing hand-crafted rules is costly. 
  In our approach the value of each element in a dependency matrix 
  is an estimated probability. 
  A statistical model (here called the ``old model'') 
  that considers only the relationship between two bunsetsus 
  when estimating those probabilities was earlier proposed. 
  In this paper we propose a new model 
  that estimates dependency likelihood by considering 
  not only the relationship between two bunsetsus but also the relationship 
  between the left bunsetsu and all of the bunsetsus to its right. 
  Our implementation of this model is based on the ME (maximum entropy) model. 
  When tested with the Kyoto University corpus 
  the dependency accuracy obtained with this model was 88\%, 
  which is about 1\% higher than that obtained with the old model 
  even using exactly the same features.","['はじめに', '係り受け確率モデル', '実験結果', 'おわりに']",,,,,,,,
V07N05-02.tex,HPSGにもとづく日本語文法について ---実装に向けての精緻化・拡張---,On HPSG-Based Japanese Grammar ---Refinement and Extension ~~~~~~~~~~~~~~~~~ for Implementation---,"自然言語処理において，言語の諸相を見据えた宣言的な文法にもとづく構文解
析は必須である．
実用的な文法体系を構築するため，我々は最近の主辞駆動句構造文法の成果を
実装することにより，NAIST JPSGという単一化にもとづいた日本語句構造文法
を構築した．
NAIST JPSGでは，日本語の様々な特徴の検討を経て，それらの間の規則性を局
所的な制約として記述することで，文法の原理，スキーマおよび素性が設計さ
れている．
また，個別言語の現象として，格助詞の分布・サ変動詞構文の意味的局所性・
連体修飾節における格を伴った修飾句の係り先について，それらの語彙情報に
関心を持って我々は分析した．
(i) 格助詞が現れるか否かは，言語的対象を説明する素性体系の一部である型
階層化された格素性にもとづいて説明できる．
(ii) サ変動詞構文は形態的には複雑な構造でありながら意味的には単純な関
係を含んでいるが，そのような不整合は語彙記述および単一化といった一般的
な機構によって解消される．
(iii) 連体修飾に関してコーパスを調査することで，いくつかの修飾--被修飾
の関係は述語的な形態素を導入することにより削減することができる．","A parser based on declarative grammar that deals with various aspects
of language is indispensable for natural language processing.
For constructing a practical grammar system, we develop
unification-based Japanese phrase structure grammar, NAIST JPSG, which
is an implementation of ideas from recent developments in Head-driven
Phrase Structure Grammar.
The principles, schemata and features are designed through considering
various aspects of Japanese and describing regularities among them
as a set of local constraints.
We then devote our discussion to the analysis of language-specific
phenomena, the distribution of case particles, 
the thematic locality of {\it sa-hen d\^o-si\/","['はじめに', '基本的な枠組と実装に向けての理論の精緻化・拡張', '語彙記述の設計と素性・原理の相互作用', 'コーパス調査にもとづく，分析と語彙記述の設計', 'おわりに']",,,,,,,,
V07N05-03.tex,音声合成におけるポーズ制御のための 決定リストを用いた局所係り受け解析,Partial Dependency Structure Analysis Using Decision Lists for Pause Insertions of Speech Synthesizers,"音声合成におけるポーズ制御において重要となる局所係り受け解析に関し，
決定リストを用いる方法を開発し，ポーズ挿入の正解率をF値で評価したとこ
ろ90.04\%を得た．本係り受け解析は決定リストを用いているので，
使用するメモリの容量と
処理速度に関して設定を容易に変更できるという特徴を有している．
このため，用途に応じてメモリ容量，計算速度が選択できる．
計算速度を重視し，メモリ容量を12Kバイトと小さくした場合，
文あたりのポーズ設定処理の時間7msec（PentiumIII450MHz），F値85\%となり，
音声合成システムへの実装が現実的であることがわかった．
係り受け解析結果に基づくポーズ挿入位置制御規則を作成し，聴取実験によっ
て性能を確認した．
その結果，係り受け距離のみに基づく制御で，85\%程度のポーズ挿入位置が
挿入適当という結果であった．
さらに，句読点や，ポーズの連続などの要因を取り入れて規則の精緻化を行っ
た結果，91\%程度のポーズ挿入位置が挿入適当という結果が得られた．","We developed a method which used decision lists for partial dependency 
analysis which was important for a pause control of a speech synthesis.
As the result of its evaluation using F measure as the accuracy of
pause insertions, it was 90.04\%. 
This method based on decision lists has a good point about flexibility
adjusting the memory usage and the processing speed.
In the case of small 12K Bytes memory and high speed,
the processing time to set pauses of a sentence is 7msec
(PentiumIII450MHz) and F measure is 85\%. These results indicate that
this method is applicable for the actual speech synthesis systems.
We made rules for pause insertions and confirmed the performance
by subjective evaluations.
As the result, the appropriate rate was about 85 \% by the control
derived only by dependency distances and the appropriate rate was
about 91 \% by the control derived additional factors such as punctuation marks and succession of pauses.","['はじめに', '決定リストを用いた局所解析', 'ポーズ挿入規則', '考察', 'まとめ']",,,,,,,,
V07N05-04.tex,３つ以下の候補から係り先を選択する係り受け解析モデル,A Statistical Japanese Dependency Analysis Model with Choice Restricted to at Most Three \\ Modification Candidates,"本稿では、日本語係り受け解析のための統計的手法について述べる。
この手法は、統計値の計算方法が従来の手法と異なる。
従来の手法では、２つの文節間が依存関係にある確率をそれぞれの文節の組に対して計算するが、
本研究で提案する「３つ組／４つ組モデル」は、
係り元の文節と係り先の文節の候補となる全ての文節に関する情報を確率の条件部として、
ある文節が係り先として選択される確率を求める。
なお、係り先の候補は、HPSGに基づいた文法及びヒューリスティクスによって高々３つに絞られる。
確率の推定には最大エントロピー法を用いており、
我々の構文解析器はEDRコーパスに対して文節正解率88.6$\%$という
高い解析精度を達成した。","This paper describes a statistical method for Japanese dependency analysis.
The method differs from conventional statistical models in the way of 
calculating statistical values.
The conventional models calculate the probability of a correct dependency 
between two {\it bunsetsu","['はじめに', '関連研究', '本研究の手法', '実験結果', '考察', 'まとめ']",,,,,,,,
V07N05-05.tex,自然言語解析のためのMSLRパーザ・ツールキット,MSLR Parser Tool Kit --- Tools for \\ Natural Language Analysis,"本論文では，我々が現在公開している自然言語解析用ツール
「MSLRパーザ・ツールキット」の特徴と機能について述べる．
MSLRパーザは，一般化LR法の解析アルゴリズムを拡張し，
日本語などの分かち書きされていない文の形態素解析と構文解析を
同時に行うツールである．
MSLRパーザを用いて解析を行う際には，
まずLR表作成器を用いて，文法と接続表からLR表を作成する．
このとき，LR表作成器は，接続表に記述された
品詞間の接続制約を組み込んだLR表を生成する．
このため，接続制約に違反する解析結果を受理しないLR表が
作られるだけでなく，LR表の大きさを大幅に縮小することができる．
次に，MSLRパーザは，作成されたLR表と辞書を用いて
辞書引きによる単語分割と構文解析を同時に行い，
その結果として構文木を出力する．
さらに，MSLRパーザは，
文中の括弧の組によって係り受けに関する部分的な制約が与えられた文を
入力とし，その制約を満たす構文木のみを出力する機能を持つ．
また，文脈依存性を若干反映した言語モデルのひとつである
確率一般化LRモデル(PGLRモデル)を学習し，
個々の構文木に対してPGLRモデルに基づく生成確率を計算し，
解析結果の優先順位付けを行う機能も持つ．","In this paper, we describe a tool kit for natural language analysis, the
MSLR parser tool kit. The `MSLR parser' is based on the generalized LR
parsing algorithm, and integrates morphological and syntactic analysis
of unsegmented sentences. The `LR table generator' constructs an LR
table from a context free grammar and a connection matrix describing
adjacency constraints between part-of-speech pairs. By incorporating
connection matrix-based constraints into the LR table, it is possible to
both reject any locally implausible parsing results, and reduce the size
of the LR table. Then, using the generated LR table and a lexicon, the
MSLR parser outputs parse trees based on morphological and syntactic
analysis of input sentences. In addition to this, the MSLR parser
accepts sentence inputs including partial syntactic constraints denoted
by pairs of brackets, and suppresses the generation of any parse trees
not satisfying those constraints. Furthermore, it can be trained
according to the probabilistic generalized LR (PGLR) model, which is a
mildly context sensitive language model. It can also rank parse trees in 
order of the overall probability returned by the trained PGLR model.","['はじめに', 'LR表作成器', 'MSLRパーザ', 'おわりに', 'MSLRパーザによる解析例']",,,,,,,,
V08N01-01.tex,ランダム・プロジェクションによる\\ベクトル空間情報検索モデルの次元削減,"Dimensionality Reduction of Vector Space \\ Information Retrieval Model
Based on \\ Random Projection","ベクトル空間モデルは情報検索における代表的な検索モデルである．
ベクトル空間モデルでは文書を索引語の重みベクトルで表現するが，
文書ベクトルは一般に要素数が非常に多く，スパースなベクトルに
なるため，検索時間の長さや必要なメモリの量が大きな問題となる．
本論文では，この問題を解決するため，ベクトル空間モデルにおける
ベクトルの次元圧縮を行う手法としてランダム・プロジェクションを
用いた検索モデルを提案する．
その有効性を評価するために，評価用テストコレクションである 
MEDLINEを利用して，検索実験を行った．
その結果，ランダム・プロジェクションはLSI(Latent Semantic Indexing)
に比べ高速で，かつ同等な検索性能を持つ次元圧縮手法であることが確認された．
また，ランダム・プロジェクションで次元圧縮に必要な行列を得るために，
球面 $k$ 平均アルゴリズムで得られる概念ベクトルの利用を提案する．
同様に検索実験を行った結果，任意のベクトルを用いた検索性能に比べ改善され，
概念ベクトルが検索性能の向上に有効であることが確認された．","Vector space model is a conventional information retrieval model,
in which text documents are represented as high-dimensional and 
sparse vectors using words as features in a multidimensional space.
These vectors require a large number of computer resources and 
it is difficult to capture underlying concepts referred to by the terms.
In this paper, we present a technique of an information retrieval model
using a random projection to project document vectors 
to a low-dimensional space as a way of solving these problems.
To evaluate its efficiency, we show results of retrieval experiments 
on the MEDLINE test collection.
Experiments show that the proposed method is faster 
than LSI(Latent Semantic Indexing) and efficient close to the LSI.
In addition, we propose to apply a concept vector, 
which random projection needs for dimensionality reduction, 
produced by a spherical $k$-means algorithm.
A result of this evaluation shows that the concept vector captures 
the underlying concepts of the corpus effectively.","['はじめに', 'ランダム・プロジェクションによるベクトルの次元圧縮', '概念ベクトルを用いたランダム・プロジェクション', '実験', '実験結果および考察', 'おわりに']",,,,,,,,
V08N01-02.tex,,An Agent-based Parallel HPSG Parser\\ for Shared-memory Parallel Machines,,"We describe an agent-based parallel HPSG parser that
operates on shared-memory parallel machines.  It efficiently parses
real-world corpora by using a wide-coverage HPSG grammar.  The
efficiency is due to the use of a parallel parsing algorithm and the
efficient treatment of feature structures.  The parsing algorithm is
based on the CKY algorithm, in which resolving constraints between a
mother and her daughters is regarded as an atomic operation.  The CKY
algorithm features data distribution and granularity of parallelism.
The keys to the efficient treatment of feature structures are i)
transferring them through shared-memory, ii) copying them on demand,
and iii) writing/reading them simultaneously onto/from memory.  Being
parallel, our parser is more efficient than sequential parsers.  The
average parsing time per sentence for the EDR Japanese corpus was 78
msec and its speed-up reaches 13.2 when 50 processors were used.","['Introduction', 'Parallel Substrate for Typed Feature Structures (PSTFS)', 'Parsing Algorithms', 'Performance Evaluation', 'Discussion', 'Conclusion']",,,,,,,,
V08N01-03.tex,\bf WFSTに基づく確率文脈自由文法およびその 拡張文法の高速EM学習法,Efficient EM learning of probabilistic CFGs and their extensions by using WFSTs,"現在，統計的言語モデルの一クラスとして確率文脈自由文法 (PCFG)
が広く知られている．また，括弧なしコーパスからPCFGを訓練する方法
として Inside-Outside (I-O)アルゴリズムが知られてきた．
I-O アルゴリズムは PCFG 用に効率化を施した EM
(expectation-maximization) アルゴリズムだが，
依然その計算速度に問題があることが知られている．
本論文では，文法構造があらかじめ与えられていることを前提に，
訓練過程を構文解析とEM学習に分離した高速 EM 学習法を提案する．
その中間データ構造にパーザが生成する WFST
(well-formed substring table) を用いる．
例えば，一般化LRパーザを用いると事前コンパイル・ボトムアップ
探索による効率性，および Chomsky 標準形を要求しないという一般性を
引き継ぐことができる．
一方EM学習では，WFSTのコンパクトさを利用して
効率的なパラメタ推定が行なわれる．
推定結果は I-O アルゴリズムで得られるものと一致する．
更に，文脈依存性を取り入れたPCFGの拡張モデルに対する多項式オーダの
EM学習法を示す．また，ATR対話コーパスを用いて実験を行ない，
訓練時間が大幅に短縮されていることを確認した．","Probabilistic context-free grammars (PCFGs) are a widely-known class
of statistical language models. The Inside-Outside (I-O) algorithm
is also well-known as an efficient EM algorithm tailored for PCFGs.
Although the algorithm requires only inexpensive linguistic resources,
there remains a problem in its efficiency.
In this paper, we present a new framework for
efficient EM learning of PCFGs in which the parser is separated from
the EM algorithm, assuming the underlying CFG is given.
A new EM procedure exploits the compactness
of WFSTs (well-formed substring tables) generated by the parser.
Our framework is quite general in the sense 
that the input grammar need not to be in Chomsky normal form
(CNF) while the new EM algorithm is equivalent to the I-O algorithm
in the CNF case.
In addition, we propose a polynomial-time EM procedure for
CFGs with context-sensitive probabilities, and report
experimental results with ATR corpus
and a hand-crafted Japanese grammar.","['はじめに', '準備', '提案手法', '訓練時間に関する実験', 'PCFGの拡張文法のEM学習', '関連研究', 'まとめ', 'グラフィカルEMアルゴリズムの正当化', 'Stolcke の方法との対応']",,,,,,,,
V08N01-04.tex,情報検索システムの統計的手法による特徴と精度の分析,Analysis of the Characteristics and the Efficiency of Information Retrieval Systems by Statistical Method,"本論文では，IREXワークショップにおける情報検索課題(IR)の本試験の結果，
および，参加したすべてのIRシステムについてのアンケートをもとに，
平均適合率，再現率・適合率曲線を直線回帰させた傾きと切片が
システムに用いられた手法とどのような相関関係をもっているのかを調査し，
それぞれの手法がシステムの性能に与える影響の大きさを示した．
その結果，多くの手法について，再現率0.0での適合率の値と適合率の減少量に
トレードオフの関係が存在し，検索システムに用いる手法の選択の難しさが現れた．
また，NARRATIVEタグの使用有無により，同様に相関関係を調査し，
NARRATIVEタグの有効性とシステムの性能に与える影響の大きさを示した．
その結果，NARRATIVEタグを利用する場合，それに適した有効な手法を
選択することが重要であることが分かった．","In evaluating the effectiveness of information retrieval (IR) and
extraction system, the most common method is to compare two retrieval
methods and decide if one system measurably achieves better results than the
other.  However, it is difficult for researchers to compare more than
two retrieval methods because there are many participants in IR task
in IREX workshop.
In this paper, we evaluate the characteristics and the effectiveness
of the IR systems using a statistical method based on the results
of the IR formal run and questionnaires of systems.  Comparisons of
systems deal with effects on the performance such as indexing, 
querying and retrieval model.
The results confirm the effectiveness of this evaluation method because 
phrases relates to the performance better than words.
There is a trade-off relation between the precision value at 0.0 and 
decrease rate in many systems and this result indicates the difficulty 
of the choice of techniques in system.
We also evaluate correlations between the efficiency and the characteristics 
of the systems with both a short and long versions of the topics.
A result of this evaluation shows that it is important 
to select effective methods for the long version of topics.","['はじめに', '再現率と適合率の関係', '評価実験', 'まとめ']",,,,,,,,
V08N01-05.tex,,TRUCKS: A Model for Automatic\\Multi-Word Term Recognition,,"This paper examines the use of linguistic techniques in the area of
automatic term recognition. It describes the TRUCKS model, which makes
use of different types of contextual information - syntactic, semantic,
terminological and statistical - seeking particularly to identify
those parts of the context which are most relevant to terms. From an
initial corpus of sublanguage texts, this identifies, disambiguates
and ranks candidate terms. The system is evaluated with respect to the
statistical approach on which it is built, and with respect to its
expected theoretical performance. We show that by using deeper forms of
contextual information, we can improve on the extraction of multi-word
terms. The resulting list of ranked terms is shown to improve on that produced by
traditional methods, in terms of precision and distribution, while the
information acquired in the process can also be used for a variety of
other applications, such as disambiguation, lexical tuning and term clustering.","['Introduction', 'Term Recognition and Disambiguation', ""Contextual Information: a Term's Family and Friends"", 'The SNC-Value', 'Using Similarity for Term Sense Disambiguation', 'Evaluation', 'Conclusions', 'Appendix - Examples of disambiguation']",,,,,,,,
V08N01-06.tex,最大エントロピーモデルに基づく形態素解析 \q\p ---未知語の問題の解決策---,Morphological Analysis Based on A Maximum Entropy Model --- An Approach to The Unknown Word Problem ---,"形態素解析は日本語解析の重要な基本技術の一つとして認識されている．
形態素解析の形態素とは，単語や接辞など，文法上，最小の単位となる要素
のことであり，形態素解析とは，与えられた文を形態素の並びに分解し，
それぞれの形態素に対し文法的属性(品詞や活用など)を決定する処理のことである．
近年，形態素解析において重要な課題となっているのは，
辞書に登録されていない，あるいは学習コーパスに現れないが
形態素となり得る単語(未知語)をどのように扱うかということである．
この未知語の問題に対処するため，これまで大きく二つの方法がとられてきた．
一つは未知語を自動獲得し辞書に登録する方法であり，
もう一つは未知語でも解析できるようなモデルを作成する方法である．
ここで，前者の方法で獲得した単語を辞書に登録し，
後者のモデルにその辞書を利用できるような仕組みを取り入れることができれば，
両者の利点を生かすことができると考えられる．
本論文では，最大エントロピー(ME)モデルに基づく形態素解析の手法を提案する．
この手法では，辞書の情報を学習する機構を容易に組み込めるだけでなく，
字種や字種変化などの情報を用いてコーパスから未知語の性質を学習することも
できる．我々はこの手法により未知語の問題が克服される可能性が高いと考えている．
京大コーパスを用いた実験では，再現率95.80\%，適合率95.09\%の精度が得られた．","Morphological analysis is one of the basic techniques 
used in Japanese sentence analysis.
A morpheme is defined as the minimal grammatical unit 
such as a word or a suffix. 
Morphological analysis is the process segmenting a given sentence into 
a row of morphemes and assigning to each morpheme grammatical attributes 
such as a part-of-speech (POS) and an inflection type. 
Recently, one of the most important issues in morphological analysis 
has become how to deal with unknown words, or words which are not found 
in a dictionary or a training corpus. So far, there have been mainly 
two statistical approaches for coping with this issue. 
One is the method of acquiring unknown words from corpora 
and incorporating them into a dictionary. 
The other is the method of estimating a model 
which can recognize unknown words correctly. 
We would like to be able to make good use of both approaches.
If words acquired by the former method could be added to a dictionary 
and a model developed by the latter method 
could consult the amended dictionary, 
then the model could be the best statistical model 
which has the potential to overcome the unknown word problem. 
In this paper, we propose a method 
for Japanese morphological analysis based on a maximum entropy (M.E.) model. 
This method uses a model which can not only consult a dictionary 
with a large amount of lexical information but also 
recognizes unknown words by learning certain characteristics. 
We focused on the information such as what types of characters are used 
in a string in order to learn these characteristics. 
The model has the potential to overcome the unknown word problem.  
The recall and precision of the identification of a morpheme segment
and its major parts-of-speech were 95.80\% and 95.09\%, respectively, 
when using the Kyoto University corpus.","['はじめに', '形態素モデル', '実験と考察', '関連研究', 'まとめ']",,,,,,,,
V08N01-07.tex,多義解消のための構造規則の生成方法と 日本語名詞句への適用,"Automatic Knowledge Acquisition for \\Disambiguation
and Its' Application to\\ Japanese Noun Phrases","自然言語処理では，処理の過程で，さまざまな解釈の曖昧さが生じる．この曖昧
さを解消するのに必要な知識を記述するため，対象とする表現を部分的な表現の
組に還元せず，一体として捉える方法として，言語表現とその解釈の関係を変数
とクラスの組からなる構造規則として表現し，学習用標本から半自動的に収集す
る方法を提案した．この方法は，パターン化された表現の変数部分を表すのに文
法属性体系と意味属性体系を使用しており，$N$個の変数を持つ表現パターンに対
して，一次元規則から$N$次元規則までの規則と字面からなる例外規則を合わせて
$N+1$種類の構造規則が順に生成される点，また，各規則は，その生成過程にお
いて，各属性の意味的な包含関係を用いて容易に汎化される点に特徴がある．本
方式を「$AのBのC$」の型の名詞句に対する名詞間の係り受け解析規則の
生成に適用した結果では，変数部分を意味属性で表現した構造規則の場合，1
万件の学習事例から，一次元規則198件，二次元規則1480件，三次元規則136件
が得られ，それを使用した係り受け解析では，約$86\%$の解析精度が得られるこ
とが分かった．また，変数部分を文法属性で表した規則と意味属性で表した規
則を併用する場合は，解析精度は，$1〜2\%$向上することが分かった．この値は，
2名詞間の結合強度に還元して評価する方法($72\%$)より約$15\%$高い．この種の
名詞句では，人間でも係り先の判定に迷うような事例が$10\%$近く存在すること
を考慮すると，得られた規則の精度は，人間の解析能力にかなり近い値と
言える．","In order to represent the knowledge for resolving the syntactical and
semantic ambiguities, structural rules and their generalization methods
were proposed. In this method, the structural rules are composed of
structure definition part and class definition part. The former is
written by the set of any of almighty symbol, syntactic attributes,
semantic attributes and word itself. From the view point of the number
of parameters used for defining the expression structures, the rules are
classified into one-dimensional rules, two-dimensional rules and so on,
and automatically generated in this order from examples. Prominent
feature of this method is in generalization methods. The generated rules
are furthermore generalized based on the upper to lower relation of
semantic attributes or syntactic attributes to reduce the number of
rules without decreasing the performance.This method was applied to
generate the dependency rules for Japanese expressions of ""$A$ no $B$ no $C$""
which are known as 
the most popular noun phrases which have hard ambiguities to be resolved. As
     the result, it was found that structural rules can easily be
     obtained by this method. The experimental results showed that the
     dependency relations can be determined at the accuracy of $86\%$ by
     the rules obtained by this method. This rate is not so low compared to
     human ability for this kind of ambiguous noun phrases.","['はじめに', '構造規則の記述方法', '構造規則生成の基本的な考え方', '構造規則の生成手順', '名詞句への適用例', 'あとがき']",,,,,,,,
V08N01-08.tex,対話者の社会的役割を利用した訳し分け手法,Transfer Method Using\\ Dialogue Participants' Social Role,"音声翻訳を介した対話をより自然なものにするためには, 原言語を解析するだ
けでは取得困難な『言語外情報』を利用することが有効である. 例えば, 『対
話者の社会的役割』を使用した翻訳は対話をより自然にする. 本論文では, 特
にこの『対話者の社会的役割』に着目し, この役割情報を利用して, 適切な丁
寧度の翻訳にする手法を提案する. 既存の変換ルールや辞書にこの役割情報に
応じた修正を加えることによって訳を変える. 実際に英日翻訳における変換ルー
ルや辞書に『対話者の社会的役割』に応じたルールやエントリーを登録し, そ
の際に参照していない未訓練の23会話(344発声)を使って実験をした.
その結果, 丁寧表現にすべき発声に対して, 再現率が65\%, 適合率が86\%
であった. したがって, 本手法は, 音声翻訳を使って自然な対話を行うために
は効果的であり実現性も高い. さらに, 対話者の性別情報など他の言語外情報
や英日以外の言語対に対する本手法の適用可能性についても考察する.","This paper proposes to improve translation quality by using
information on dialogue participants that is easily obtained from
outside the translation component. If we wish to make a conversation
smooth with the dialogue translation system, it is important to use
not only linguistic information, which comes from the source language,
but also extra-linguistic information, which is shared between the
participants of the conversation. We incorporated the participants'
social role into transfer rules and dictionary entries. We conducted
an experiment with 23 unseen dialogues (344 utterances) using an
English-to-Japanese translation. The experiment demonstrated that
recall and precision for expressions which should be polite, are 65\%
and 86\%, respectively. Thus, our simple and easy-to-implement method
is effective, which is a key technology enabling smooth conversation
with a dialogue translation. Additionally, this paper discusses the
useful information such as the participants' gender, and how our
method could apply information on dialogue participants to other
language pairs.","['はじめに', '『話し手の役割』と『丁寧さ』', '話し手の役割情報を翻訳知識に組み込む方法', '実験', '考察', 'おわりに']",,,,,,,,
V08N02-01.tex,決定リストを弱学習器としたアダブーストによる日本語単語分割,Japanese word segmentation by Adaboost using the decision list as the weak learner,"本論文では決定リストを弱学習器としたアダブーストによる日本語単語分割法を提案する．
日本語単語分割は，入力文の各文字の間に単語区切りを置くか置かないかの問題とみなすことで，
分類問題として定式化できる．
この分類問題を決定リストを利用して解くことで単語分割が行える．
ここでは決定リストで利用する属性に辞書情報を含めない．
そのためここでの単語分割は未知語の問題を受けないという長所がある．
更に単語分割を分類問題として解く場合，近年研究の盛んなアダブーストの手法を適用できる．
アダブーストを用いることで，決定リストの精度を高めることができる．
実験では，京大コーパス（約4万文）を利用して決定リストを作成した．
この決定リストによる単語分割の正解率は 97.52\% であった．
この値は、同じ訓練データから構築したtri-gram モデルに基づく
単語分割法での正解率 92.76\% を
大きく上回った．またアダブーストを利用することで精度が 98.49\% にまで向上させる
ことができた．また作成した単語分割システムは未知語の検出能力が高いことも確認できた．","In this paper, we propose the new method of Japanese word segmentation
by Adaboost using the decision list as the weak learner.
The word segmentation is regarded as the classification problem of 
judging whether the word boundary exists between two characters or not.
By solving the problem by the decision list method,
we can conduct Japanese word segmentation.
Our method has the advantage not to suffer the unknown word problem
because we do not use dictionary information as an attribute of our decision list.
Moreover, by taking this approach we can use Adaboost which is actively researched in 
the machine learning domain recently.
Adaboost improves the precision of our decision list.
In experiments, we built the decision list through Kyoto University
Corpus (about 40K sentences).
The precision of this decision list was 97.52\%.
This values was much higher than the precision of character based tri-gram model, 92.76\%.
By using Adaboost method, our precision was improved to 98.49\%.
Furthermore, our word segmentation system was excellent in detecting unknown words.","['はじめに', '決定リストによる単語分割', 'アダブーストの利用', '実験', '考察', 'おわりに']",,,,,,,,
V08N02-02.tex,,Balancing up Efficiency and Accuracy in\vspace*{1.5mm,,"This research looks at the effects of segment
            order and segmentation on translation retrieval performance
            for an experimental Japanese-English translation memory
            system. We implement a number of both bag-of-words and
            segment order-sensitive string comparison methods, and test
            each over character-based and word-based indexing.  The
            translation retrieval performance of each system
            configuration is evaluated empirically through the notion of
            segment edit distance between the translation output and
            model translation. Our results indicate that character-based
            indexing is consistently superior to word-based indexing in
            terms of raw accuracy, although segmentation does have an
            accelerating effect on TM search times in combination with a
            number of retrieval optimisation techniques. Segment
            order-sensitive approaches are demonstrated to generally
            outperform bag-of-words methods, with 3-operation edit
            distance proving the most effective comparison method. We
            additionally reproduced the same basic results over
            alphabetised data as for lexically differentiated data
            containing kanji characters.","['Introduction', 'Segmentation and segment order', 'String comparison methods', 'Evaluation', 'Concluding remarks']",,,,,,,,
V08N02-03.tex,常識的判断のための概念間の関連度評価モデル,Measuring Degree of Association between Concepts for Commonsense Judgements,コンピュータに人間のような常識的判断を行わせるための主要素は，概念ベースおよび概念間の関連性に基づく概念連鎖機能であると考えられる．概念ベースは，自動学習などにより恒常的に拡張・精錬を行わなければならないために，その構造はできるだけ単純なものが望ましい．本論文では，概念間の関連度を評価するための新しい手法を提案している．従来の手法では，概念はその１次属性のベクトルモデルとして表現され，関連度はベクトル間の内積により求められている．そのような従来手法では，各１次属性をカテゴリーに変換しなければならないためシソーラスなどのカテゴリーデータベースが必要となる．提案手法では，関連度をカテゴリーを利用せず概念連鎖により求めている．約４万の概念よりなる概念ベースを用いた実験により，提案手法はベクトル内積を用いる方法に比べ正解率の面でやや優れる上に，概念知識の追加/変更が容易で利用を通じての質の向上が図れることを示した．,"It is thought that the main elements of commonsense judgement similar to human beings are a concept-base and the association mechanism based on the association between concepts. It is expected that the structure of the concept-base be as simple as possible since the concept-base has to be expanded and refined automatically by automated learning. This paper proposes a new method of measuring the degree of association between concepts. In the conventional method, a concept is expressed by first attributes vector model, and the degree of association between concepts is derived from an inner product of vectors. In this model, since each first attribute must be converted to its category, a category database such as a thesaurus is required. By the proposed method, the degree of association is derived using the chain of concepts without category. By experimental results using the concept-base, which consists of about 40,000 concepts, it is shown that the proposed method outputs the closer degree of association to that decided by human judgement than the conventional method.","['はじめに', '概念ベースの構造', '概念関連度の評価モデル', '評価実験と考察', 'おわりに']",,,,,,,,
V08N03-01.tex,文節重要度と係り受け整合度に基づく日本語文簡約アルゴリズム,A Japanese Sentence Compaction Algorithm Based on Phrase Significance and Inter-Phrase Dependency,"これまで主に検討されてきた文書要約手法は，文集合から重要文を抽出するもの
である．
この方法によれば，段落などを要約した結果として誤りのない文の
集合が得られる．
しかし，目的によっては更に要約率を上げるため，または段落などの単位での要
約が不適当であるときなど，一文毎の簡約が必要となる場合がある．
このような文書要約手法では，簡約文が日本語として自然な文であることが重要
ppである．
そこで本論文では，文の簡約を「原文から，文節重要度と文節間係り受け整合度
の総和が最大になる部分文節列を選択する」問題として定式化し，それを解くた
めの効率の良いアルゴリズムを提案する．
本稿の定式化では簡約文の評価に文節間の係り受け整合度が用いられていること
から，簡約結果は適切な係り受け構造を持つことが期待できる．
したがって本手法を用いることにより，自然で正確な簡約文を高速に生成できる
可能性がある．
このアルゴリズムを実用するには，文節重要度と係り受け整合度の適切な設定が
不可欠であるが，本稿ではこれらについては議論せず，アルゴリズムの導出と計
算効率，実装法などに重点を置いて報告する．","Conventional methods for text summarization are mostly based 
 on the idea of selecting important sentences from a set of 
 given sentences such as a paragraph or a whole text. 
Those methods have a merit that each selected sentence remains 
 unchanged and is thus correct. 
However, it is sometimes necessary to shorten each sentence, 
 when a higher compaction rate is required, or when a 
 paragraph-by-paragraph summarization is not adequate. 
In such sentence compaction, it is important that a 
 shortened sentence is natural as a Japanese sentence. 
In this paper, the sentence compaction problem is formulated as 
 ``a problem of selecting a subsequence of phrases from a given sentence 
 that maximizes the sum of phrase significance scores and inter-phrase 
 dependency scores.''
Then, an efficient algorithm to solve this problem is proposed.
Since this method takes inter-phrase dependency into account, 
 a shortened sentence is expected to be grammatically correct 
 and natural.
This paper is focused on the derivation, computational complexity, 
 and implementation issues of the algorithm, and will not discuss 
 the matter of how to define the phrase significance score and the 
 inter-phrase dependency score, though it will be a crucially important 
 matter in practical applications.","['はじめに', '問題の定式化', '再帰式とアルゴリズム', '簡約例', 'おわりに', '再帰式の証明']",,,,,,,,
V08N03-02.tex,手指動作記述文間の類似性に基づく手話単語の分類方法,A Clustering Method of Signs Based on Similarity between Manual Motion Descriptions,"手話は視覚言語としての側面を持つため，手話単語の語構成（造語法）に
おける特徴の一つとして「写像性」が挙げられる．
例えば，日本手話の日本語単語見出し「家」に対する手話表現は，
屋根の形を視覚的に写像して
いる．すなわち，手話表現が概念特徴の一部を視覚的に模倣している点である．
一般に，概念特徴は定義的特徴と性格的特徴に分類される．
ここで，定義的特徴とは，ある概念の定義に不可欠な特徴素の集合であり，
性格的特徴とは概念を間接的に特徴付ける特徴素の集まりを指す．
例えば，「家」に対する手話表現は，定義的特徴としての特徴素からの写像と
捉えることができる．
一方，「破産」に対する手話表現は，比喩的な表現「家が潰れる」という
概念の間接的な記述，すなわち，性格的特徴を視覚的に写像し
「家」の手話表現を提示した後に，両手を付け合わせる表現で定義されている．
すなわち，一義的には、双方の単語間に概念の類似性はみられないものの，
手指動作特徴の類似性という観点からみると「家」の派生語と捉える
ことができる．
また，日本語との言語接触により，日本語の単語見出しの構成要素を借用した
複合表現（例えば，「青森」は「青い」と「森」から成る．）で構成される単語が
少なくない．この借用も広義の写像性と捉えることができる．
このように，
手指動作特徴の類似性により手話単語を分類することは，
手指動作特徴が担う概念特徴と造語法との関係を明らかにする重要な手がかり
の一つを提供できると考える．
また，手話単語を対象とする電子化辞書システムなどにおいては，
手指動作特徴を検索キー
とする類似検索機構を実現する上での有益な知識データと捉えることができる．
本論文では，与えられた手話単語の有限集合を手指動作特徴間の類似性に基づき
分類する方法として，市販の手話辞典に記述されている手指動作記述文間の類似
性に着目した手法を提案する．本手法の特徴は，手指動作記述文間の類似度を
求め，集合の要素間の同値関係により単語集合を同値類に分割する点にある．
実験により，提案手法の妥当性を示す結果が得られた．","Since sign language is a kind of visual language,
there is ``iconicity'' as salient visual characteristics of
the word formation.
That is, iconicity in sign language refers to a visual
resemblance between signs and the things they stand
for (i.e. the meanings). 
The property of the meaning can be divided into the definition
and characteristic features.
For example, a sign for ``house'' provides a direct representation
that both hands outline the shape of the roof of a house; 
there is a direct relation between the meaning of sign and
a visual characteristic of what it presents as the definition features.
However, a sign for ``bankruptcy'' provides an indirect representation
that both hands touch each other after the `house', which
is derived from the causal relationship such that the house is
destroyed by bankruptcy as the characteristic features.  
Although their words don't resemble in the meanings,
there is similarity between their manual motion properties, that is,
it can be considered that the `bankruptcy' is a derivation of the `house'. 
By being in contact with Japanese, furthermore, signs are often
formed by borrowing from a part of the elements of word formation.
For example, a sign ``{\it Ao-mori",[],,,,,,,,
V08N03-03.tex,文字間統計情報に基づく口語文字列の自動抽出,Automatic Extraction of Oral Expressions\\Based on Letter Cooccurrence Statistics,"統計情報に基づく自然言語処理が盛んになる中で，訓練データとしてのコーパスの影響は非常に大きい．
生コーパスをそのまま利用する場合には，コーパスの取得が容易であるため，目的に合ったドメインのコーパスを大量に入手できるという利点がある．
しかし，生コーパスは人間の言語の性質上，未登録語や未知の言い回し，非文とされるような文の出現等を多く含むことがほとんどであり，これらが処理の精度の低下を招くという問題がある．
特に，口語表現の処理は，電子メールでの利用等利用頻度の高いものであるにも関わらず，十分に研究されているとは言い難い．
本稿では，生コーパスに含まれる未知の語句および言い回しに着目し，電子メール文書内に出現する意味のある文字列を自動的に抽出する実験を行なった結果について報告する．
本システムは事前に与えられた電子メール文書中の各文字の共起確率を利用して，テストコーパスとして与えられた電子メール文書から意味のある文字列を抽出し出力する．
本システムを利用することで，同じテストコーパスを既存の形態素解析ツールで解析した結果未登録語として処理された文字列の 69.06\% を抽出することに成功した．","Researches based on statistical information have been more significant in the field of natural language processing.
The use of raw corpora is fascinating, as it is easy to obtain a certain amount of non-tagged texts.
However raw corpora often contain unknown words and phrases, and this causes low accuracy of the experiments.
Colloquialism has not been worked enough because of this problem, though the processing of colloquialism is strongly required for the emails and other tasks.
In this paper we propose a simple method to obtain domain-specific sequences from unrestricted texts using statistical information only.
Our method needs a non-tagged training corpus.
We use the statistical information drawn from the training corpus to extract semantic character sequences automatically.
We had experiments on sequence extraction on email texts, and succeeded in extracting significant semantic sequences in the test corpus.
The sequences our system salvaged contain casual terms, proper nouns, and sequences with representation change such as pronunciation extension.","['はじめに', '日本語における語句抽出', 'システム概要', '実験', 'まとめ']",,,,,,,,
V08N03-04.tex,,A trainable method for pronominal anaphora \\ resolution using shallow information,,"We propose a corpus-based approach to anaphora resolution of Japanese pronouns
         combining a machine learning method and statistical information. First,
         a decision tree trained on an annotated corpus determines the 
         coreference relation of a given anaphor and antecedent candidates
         and is utilized as a filter in order to reduce the number of potential
         candidates. In the second step, preference selection is achieved by taking into
         account the frequency information of coreferential and non-referential pairs
         tagged in the training corpus as well as distance and counting features
         within the current discourse.","['Introduction', 'Corpus-based anaphora resolution', 'Evaluation', 'Feature dependency', 'Related research', 'Conclusion']",,,,,,,,
V08N03-05.tex,英日機械翻訳における代名詞翻訳の改良,Improvement of Translation Quality of Pronouns in an English-to-Japanese MT System,"代名詞を含む英文を日本語として適格で自然な文に翻訳するために
は，英語の代名詞を日本語の代名詞としてそのまま表現せず，ゼロ代名詞化した
り他の表現に置き換えたりする必要がある．
ゼロ代名詞化に関しては，人手で記述された規則による方法が既に提案されてい
る．
本稿では，
1) ゼロ代名詞化に加え，他の表現に置き換えるべき場合も扱い，
2) 規則を人手で記述するのではなく，決定木学習によって自動的に学習する方
法を示す．
学習に利用する属性は，ゼロ代名詞化に関してこれまでに解明されている
言語学的制約や，ゼロ代名詞の復元に関する工学的研究で着目された手がかりを
参考にして選択した．
提案手法を我々の英日機械翻訳システムPower E/Jによる訳文に対して適用した
ところ，
ゼロ代名詞化するか否かの判定を行なう場合の精度が79.9\%，
ゼロ代名詞化するか否かに加え他の表現に書き換えるか否かの判定も行なう場合
の精度が72.2\%となり，人手で記述された規則の精度に近い精度が得られた．
また，選択した属性には，書き換え精度を低下させる属性は含まれておらず，
ゼロ代名詞化に関する言語学的制約だけでなく，
ゼロ代名詞の復元に関する手がかりも利用できることが明らかになった．","In order to translate sentences of English containing 
pronouns into natural and suitable Japanese, it is frequently necessary 
either to eliminate pronouns or to turn them into some other expressions. 
As for eliminating unwanted pronouns, a set of manually-written rules 
has already been presented.
In this article we propose to
1) offer a way of substituting unwanted pronouns for other expressions
as well as eliminating them, and
2) use a decision tree learning algorithm to learn rules automatically 
from a corpus, without requiring human intervention.
The features used for learning are selected from the linguistic 
constraints we have so far understood which apply on zero 
pronominalisation, and from the clues which have been used for anaphora 
resolution of zero pronouns in the engineering studies.
Having applied the proposed method to the translation results of our
English-to-Japanese machine translation system {\it Power E/J","['はじめに', '代名詞の日英対照比較', '決定木学習の利用', '正解付きコーパスの作成', '着目した属性', '実験と考察', 'おわりに']",,,,,,,,
V08N03-06.tex,,Resolving Overlapping Ambiguities and Selecting Correct Word Sequence in Chinese Using Internet Corpus,,"We propose an effective method for resolving overlapping ambiguities
found in sentential analyses of Chinese. It detects the ambiguities
by a FBMM scanner, resolves them by using the relevancy value($RV$),
a statistical measure for word co-occurrences taken from textual data
on the Internet, and selects the correct word sequence for the sentence
being analyzed. We use contextual information also when $RV$s are
considered not sufficient to resolving the ambiguities and choosing
the correct word sequence. An experiment for selecting the desired
sequences shows a success rate of about 85\%. This result is convincing
and far better than those in other comparable studies.","['Introduction', 'Ambiguities in Chinese Word Segmentation', 'Disambiguation and Selection of Correct Word Sequence', 'Experimental Results', 'Conclusion']",,,,,,,,
V08N03-07.tex,日本語--ウイグル語機械翻訳のための格助詞の変換処理,Translation of Case Suffixes\\ on Japanese-Uighur Machine Translation,"日本語とウイグル語は共に膠着語であり，
語順がほぼ同じであるなどの構文的類似性が見られる．
そのため，
日本語--ウイグル語機械翻訳においては，
日本語文を形態素解析した後，逐語訳を行うだけでも
ある程度の翻訳が可能となる．
これは，名詞や動詞などの自立語の文中での役割が
助詞，助動詞といった付属語によって示されており，
そうした付属語においても，
日本語とウイグル語との間で対応関係があるからである．
特に名詞に接続する格助詞は，文中での他の語との関係を
決めるという，言語構造上重要な機能を持っている．
そのため，格助詞を正しく翻訳できなければ，
違和感のある翻訳文になるだけでなく，ときには
致命的に誤った意味となる翻訳文を生成することがある．
そこで，本論文では，日本語--ウイグル語機械翻訳における
格助詞の取り扱いについて論じる．
まず，計算機用日本語基本動詞辞書IPALを用いて
動詞と格助詞の使われ方を調べるとともに，
それぞれの格助詞の機能に対応するウイグル語格助詞を決定する．
さらに，この調査結果から作成した動詞の格パターンを
利用して
複数の格助詞の訳語候補の中から，適切な訳語を選択する
手法を提案する．
また，本提案手法に対する評価実験では，
環境問題関連の新聞社説3編の日本語138文を対象にし，
我々が本論文で提案するアプローチに基づいて実験を行った．
その結果，99.3\%の
正解率を得ることができた．",Japanese and Uighur languages are {\it agglutinative languages,"['はじめに', '日本語とウイグル語の格助詞', '動詞辞書調査に基づく日本語--ウイグル語の格助詞の対応付け', '両言語間の機械翻訳における格助詞の変換処理', '実験と評価', 'おわりに', 'ウイグル語文字体系']",,,,,,,,
V08N04-01.tex,拡張言語行為論による了解の分析 -- あいづち「はい」による了解の程度と過程 --,Analyzing the Uptake with an Extended Speech Act Theory -- The Degree and the Process of the Uptake through Chiming-in ``Hai'' --,{\bf 了解,"It was pointed out by Austin that the linguistic phenomenon of the
``uptake'' was important for an analysis of the speech act in
conversation. The ``uptake'', however, has not been analyzed
sufficiently. This paper has analyzed the ``uptake'' in terms of
pragmatics. We have extended the speech act theory by Austin and
Searle in order to analyze the ``uptake'' in terms of pragmatics, and
then proposed a framework of the extended speech act theory. It has
the following features:
\begin{itemize","['まえがき', '関連研究', '言語行為論', '拡張言語行為論', '拡張言語行為論の枠組みによる分析法', '「はい」による了解応答の分析', 'むすび']",,,,,,,,
V08N04-02.tex,統計的手法による分野非依存のテキスト分割,A Statistical Approach to \\ Domain Independent Text Segmentation,"複数のトピックからなる文章を，それぞれのトピックに切り分けるこ
とをテキスト分割と呼ぶ．テキスト分割は，情報検索や要約のための
基本技術として有用である．本稿では，分割確率最大化という観点か
らテキスト分割を定式化した．その定式化の特色の一つは，テキスト
内の単語しか，確率推定に利用しないことである．そのため，提案手
法は，任意の分野のテキストに対して適用できる．提案手法の有効性
は二つの実験により確認された．まず，実験1では，公開データに対し
て提案手法を適用することにより，提案手法の分割精度が従来手法の
分割精度よりも優れていることが示された．次に，実験2では，長い文
書の元々の章や節の構造と提案手法による分割結果とを比較した結果，
厳密な一致のみを正解とする場合，章には0.37, 節には0.34の割合で
一致し，±1行のずれを許容する場合，章には0.49,節には0.51の割合
で一致した．これらのことは，提案手法が，テキスト分割に対して有
効であることを示している．","A text is usually composed of multiple topics. Segmenting such
a text into coherent topics is useful both for information
retrieval and for automatic text summarization. This paper
proposes a statistical method that selects the segmentation of
the highest probability among possible segmentations as the
best segmentation of the given text. Since the method estimates
probabilities of segmentations from the given text, it does not
need training data. Therefore, it can be applied to any text in
any domain. The effectiveness of the method was confirmed
through two experiments. The first experiment evaluated the
accuracy of the method by using publicly available data. The
experimental results showed that the accuracy of the proposed
method is at least as good as that of a state-of-the-art text
segmentation system. The second experiment compared the
segmentations done by our method with those of original
segments in relatively long documents. When we compared our
system's segmentations with chapters in the documents, the
accuracy was 0.37 on the condition that we regarded only exact
matches as correct matches. If we regarded ±1 line differences
as correct then the accuracy was 0.49. When we compared our
system's segmentations with sections, the accuracies were 0.34
and 0.51, respectively.  These results show that our method is
effective for domain independent text segmentation.","['はじめに', 'テキスト分割のための統計的モデル', '最適分割を選択するアルゴリズム', '実験', '考察と今後の課題', 'おわりに']",,,,,,,,
V08N04-03.tex,概念間距離の定式化と既存電子化辞書との比較,"Construction of Associative Concept Dictionary with Distance Information, and Comparison with Electronic Concept Dictionary","コンピュータで言語処理を行なうとき，構文解析や意味解析だけでなく人間が持
つ一般的な知識や当該分野の背景的知識などの情報が必要になる．本研究では，
人間の持つ知識を調べるため連想実験を行ない連想概念辞書として構造化した．
連想実験では，小学生の学習基本語彙中の名詞を刺激語とし，刺激語と「上位概
念，下位概念，部分・材料，属性，類義語，動作，環境」の7つの課題から連想
語を収集する．従来の電子化辞書は木構造で表現され，概念間の距離は階層の枝
の数を辿る回数をもとに計算するなど構造に依存したものであったが，連想概念
辞書では連想実験から得られるパラメータをもとに，線形計画法によって刺激語
と連想語の距離を定量化した．また距離情報を用い，「果物」「野菜」「家具」
などの日常頻出語を中心として3〜4階層をなす刺激語の連想語(上位/下位概念)
のつながりを調べた．この連想概念辞書とEDR電子化辞書，WordNetの比較を，上
位/下位階層をなす概念間の距離を求めることで行なった．連想概念辞書と
WordNetは，ある程度近い概念構造を持っており，一方EDRは他の2つとは異なる
特徴の構造を持っていることがわかった．","Background knowledge concerning to the input text is
necessary when a computer tries to understand the text as well as
syntactic and semantic information about it. This paper presents a
method to construct an associative concept dictionary using large-scale
association experiments. The dictionary includes semantic and contextual
information about the stimulus words. In the association experiments,
100 stimulus words from the textbook of Japanese language used in
elementary schools are given to subjects. They are requested to make
association from the stimulus words about 7 tasks for each word.  The
tasks, for example, are higher level concepts, lower level concepts,
actions, situations and so on. Conventional concept dictionaries have
tree structures to express its hierarchical ones. Distances between
concepts are calculated using number of links between the concepts. This
paper shows a way to formulate the distance between concepts by using a
linear programming method. Its parameters, especially frequency of the
associated word and associated order of the word, are found significant
for the distance calculation. By comparing the associative concept
dictionary with EDR concept dictionary and WordNet using the distance
information, it is found that the dictionary is more similar to WordNet
than EDR.","['はじめに', '連想実験システム環境', '連想実験の集計結果', '線形計画法による概念間距離の計算式の決定', '距離情報を用いた概念階層の特徴', '距離を用いた既存の電子化辞書との比較', '観点の違いについて', 'おわりに']",,,,,,,,
V08N04-04.tex,英日機械翻訳における自然な和文生成のための 英語名詞句の書き換え,"Automatic Rewriting
\\of English Deverbal Noun Phrases
\\to Generate More Natural Translations
\\by English-to-Japanese MT Systems","英日機械翻訳システムによる翻訳に対して感じる不自然さの原因の
一つとして，動詞的意味を含む英語の名詞句がそのまま日本語でも名詞句として
訳されているということがある．
この不自然さを解消するために本稿では，動詞的意味を含む名詞句を文に近い形
式に書き換える自動前編集方法を示す．
動詞的意味を含む名詞句のうち，属格名詞とof前置詞句の両方を修飾句として持
つ名詞句を主な対象として実験を行なった．
提案方法によって書き換えた名詞句を含む文を我々のシステム
Power E/Jで処理し，書き換えを行なわない場合の翻訳と比較したところ，
67.3\%の文においてより自然な翻訳が得られた． 
従来，この不自然さの問題に対しては，システム内部の変換過程で対処されるこ
とが多かった． 
従来の方法に比べて，前編集による方法の利点は，特定のシステムへの依存性が
低く，実践上の適用範囲が広いことである．
実験を通じて，市販されている幾つかのシステムにおいても，書き換えによって
より自然な翻訳が得られることを確認した．","One of the major factors causing unnatural translation in
English-to-Japanese MT systems is the literal translation of 
verb-derived nominal constructions. 
This paper shows a method of automatically rewriting the nominalization
(the packed form) into the one less packed, leading to the generation of 
a more natural and suitable Japanese.
We have carried out an experiment mainly centered upon nominal 
constructions where the head deverbal noun is pre-modified by a genitive 
noun and post-modified by an ``of'' prepositional phrase.
Having combined the proposed method with our system {\it Power E/J","['はじめに', '書き換えの分類', '名詞句の書き換え', '実験と考察', '関連研究', 'おわりに']",,,,,,,,
V08N04-05.tex,日本語読み上げ文の係り受け解析における\\韻律的特徴量の有効性,"Effectiveness of Prosodic Features in Dependency
\\Analysis of Read Japanese Sentences","韻律には発話が文字化されると失われてしまう情報が含まれているが，
そのような情報は発話文の構文解析に有効である可能性がある．
我々のグループでは，以前の研究で12種類の韻律的特徴量を取り上げ，それらと
係り受け距離の関係を表現する統計モデルを構成した．そして，
そのモデルを組み込んだ係り受け解析器を用い，韻律情報が実際に読み上げ文
の係り受け解析に有効であることを示した．本研究では新たな特徴量を加えて
24種類の韻律的特徴量を取り上げ，有効な特徴量を広い範囲で探索した．
また, 統計モデルを特徴量の現実の分布によりよく当てはまるように
修正した．その結果，
ATR 503文データベースを用いたオープン実験において，韻律的特徴量を
用いることにより，係り受け解析の文正解率が 21.2\%向上した．これは, 
我々のグループの以前の実験における向上率より4.0ポイント高い．特徴量の中で
ポーズ長はクローズド実験においてもオープン実験においても非常に有効であっ
たが，これと併用したときの，ピッチやパワー，話速等に関連する他の特徴量
の有効性はオープン実験においてはあまり明らかでなかった．","Prosody contains information that is lost when utterances are
transcribed into letters or characters. Such information may be
useful for syntactic analysis of spoken sentences. In our previous work,
we took up 12 prosodic features, and made a statistical model to
represent the relationship between those features and dependency
distances. Then, using a dependency analyzer that incorporates the
model, we have shown that prosodic information is in fact effective
for dependency analysis of read Japanese sentences. In the present
work, we employed 24 features including new ones, and conducted an
extensive search for effective ones. Also, the statistical model
was modified to better
fit the actual distributions of the feature values. 
As a result, in open experiments
using the ATR 503-sentence database, the correct parsing rate was
improved by 21.2\%  with the use of the prosodic features. This figure 
is 4.0 points higher than the improvement in the previous experiment 
of our group.
 Among the features, the duration of pause was definitely
effective in both the open and the closed experiments, while
the effectiveness of
other features related to the pitch, the power, and the speaking 
rate, when used together
 with the duration of pause, was not clear in the open experiments.","['まえがき', '係り受け解析', 'データベース', '韻律情報', 'ペナルティ関数', '韻律的特徴量の有効性', 'あとがき']",,,,,,,,
V09N01-01.tex,用言と直前の格要素の組を単位とする\\格フレームの自動構築,"Case Frame Construction by Coupling \\ the Predicate and its
Closest Case Component","本稿では，生コーパスから格フレームを自動的に構築する手法を提案する．格
 フレームの自動構築における最大の問題は，用言の用法の多様性をどのように
 扱うかということである．本研究では，用言と直前の格要素の組を単位として
 コーパスから格要素と用言の用例を収集することにより，用言の用法の多様性
 を扱う．さらに，用法に違いはないが，直前の単語が異なるために別の格フレー
 ムになっているもののクラスタリングを行う．得られた格フレームを用いて格
 解析実験を行い，その結果を考察する．","This paper describes a method to construct a case frame dictionary
 automatically from a raw corpus. The main problem is how to handle the
 diversity of verb usages. We collect predicate-argument examples, which
 are distinguished by the verb and its closest case component in order
 to deal with verb usages, from parsed results of a corpus.
 Furthermore, we cluster and merge predicate-argument examples which do
 not have different usages but belong to different case frames because
 of different closest case components. We also report on an experimental
 result of case structure analysis using the constructed case frame
 dictionary.","['はじめに', '格フレーム構築の種々の方法', '関連研究', '用例の収集', '用例格フレームの作成', '必須格の選択', '作成した格フレーム辞書', '解析実験', 'おわりに']",,,,,,,,
V09N01-02.tex,手指動作記述文間の類似性に基づく 手話単語の検索方法,A Retrieval Method of Signs Based on Similarity between Manual Motion Descriptions,"手話は聴覚障害者と健聴者との重要なコミュニケーション
手段の 1つであり，手話を学習する健聴者の数も年々増加
する傾向にある．この様な背景から，近年，手話の学習支
援システムや手話通訳システムなどの研究が各所で盛んに
行われている．
特に，これらの自然言語処理システムの知識辞書となる手
話電子化辞書の構築は重要な課題であり，手話側から対応
する日本語ラベルを効率良く検索する手段の実現は，日本
語と手話との対訳辞書の検索機能として，必要不可欠な要
素技術といえる．
従来の検索方法の多くは，手話単語の手指動作特徴を検索
項目とし，検索条件を詳細に設定する必要があった．その
ため，初心者には満足する検索結果を得ることが難しいと
いう問題点が指摘されている．この主な原因の 1つは，検
索条件の複雑さや検索項目間の類似性から選択ミスが生じ
やすく，結果として，利用者の要求に適合する検索結果が
出力されないという問題点にある．
本論文では，市販の手話辞典に記載されている手指動作記
述文に着目した検索方法を提案する．
本手法の特徴は，検索キーとして入力された手指動作記述
文と類似の手指動作記述文を検索辞書から検索し，対応す
る手話単語の日本語ラベルを利用者に提示する点にある．
すなわち，手話単語の検索問題を文献検索問題と捉えたア
プローチといえる．
実験の結果，本手法の妥当性を示す結果が得られた．一方，
実験により明らかになった問題点の 1つとして，手指動作
記述文で表現された手指動作の一部に曖昧さがあることが
分かった．この問題を含め，本手法の問題点と今後の課題
について，例を示しながら詳細に議論する．","Sign language is an important path for us to communicate
with hearing impaired people. Therefore, learners has been
increasing in recent years.
There are several researches into learning aid systems
and electronic dictionaries for sign language.
Especially, when users want to look up the Japanese word
labels corresponding with manual motion properties,
most of previous retrieval methods are necessary to set
various and many retrieval conditions in detail.
There is a serious problem that it is hard for beginners
to look up the most appropriate sign by the setting
wrong conditions.
To overcome this problem, this paper proposes a method
which uses a different approach from
the previous methods.
The point of the method is based on the similarity between
manual motion descriptions(MMDs) appeared in ordinary sign
dictionaries.
By computing the similarity between an inputted MMD as
a query and the MMDs in the database, 
retrieval results are outputted in similarity order.
The retrieval results formed by the similarity
can be considered as a set of signs that are similar to
each other.
As an interesting point, a subject of sign retrieval can
be considered as the document retrieval.
The results of evaluation experiments show the applicability
and usability of the proposed method.
We also discuss a problem that there are ambiguous MMDs
by demonstrating examples.",[],,,,,,,,
V09N01-03.tex,システム知識制限下での効率的音声対話制御法,"Efficient Spoken Dialogue Control \\ 
under System's Limited Knowledge","本稿では，音声対話システムがシステム知識として保有するデータベースの内容
に依存して，できるだけ短い対話でユーザの必要とする情報を伝達するためのデュ
アルコスト法と呼ぶ対話制御法を提案する．音声対話システムは，音声認識誤り
のために，ユーザ要求内容を確定することを目的とした「確認対話」を実施する
必要がある．長い確認対話は対話の円滑な流れを阻害するので，確認対話は簡潔
であることが望ましい．ユーザは対話時点でのシステム知識の内容を知らないの
で，システムが詳しい情報を保有していない事柄に関して詳細な情報を要求する
場合が頻繁に起きる．そのような場合にも，従来法ではユーザ発話内容を逐一確
認するので，無駄な確認が増えてしまうという問題があった．この問題を解決す
るために，確認コストと情報伝達コストと呼ぶ2 種類のコストを導入する．確認
コストは確認対話の長さであり，音声認識率に依存する．情報伝達コストは，確
認対話でユーザ要求を確定した後，ユーザに情報を伝達する際のシステム応答の
長さであり，システム知識の内容に依存する．デュアルコスト法は，この2 つの
コストの和を最小化することにより対話を制御する方法であり，従来法が避ける
ことができない無駄な確認対話を回避しながら，短い対話でユーザ要求に応じた
情報を伝達することができる．","We present a dialogue control method called the ``dual-cost method'',
by which a spoken dialogue system conveys information relevant to a
user request by a concise dialogue within the confines of the system's
knowledge stored in its database.  Due to speech recognition errors, a
system has to carry out a ``confirmation dialogue'' to clarify the user
request. A confirmation dialogue should be concise since a lengthy one
destroys the flow of the overall dialogue. There are cases where the
user request is beyond the system's knowledge since a user does not
know what knowledge the system has. In such cases, conventional
methods have a problem of invoking unnecessary confirmations since
they attempt to confirm the whole contents of the request. To resolve
this problem, we introduce the notions of confirmation cost and
information transfer cost. The confirmation cost is the length of a
confirmation dialogue and depends on the speech recognition rate.  The
information transfer cost is the length of a system response and
depends on the system's knowledge.  The dual-cost method controls a
dialogue based on the minimization of these two costs and can avoid
unnecessary exchanges, which are inevitable in conventional methods.","['はじめに', '音声対話システムの対話制御', 'デュアルコスト法', '評価', 'おわりに']",,,,,,,,
V09N01-04.tex,正誤判別規則学習を用いた\\複数の日本語固有表現抽出システムの出力の混合,Learning to Combine Outputs of \\Multiple Japanese Named Entity Extractors,"本論文では，日本語固有表現抽出の問題において，複数のモデルの出力を混合す
る手法を提案する．一般に，複数のモデル・システムの出力の混合を行なう際に
は，まず，できるだけ振る舞いの異なる複数のモデル・システムを用意する必要
がある．本論文では，最大エントロピー法に基づく統計的学習による固有表現抽
出モデルにおいて，現在位置の形態素が，いくつの形態素から構成される固有表
現の一部であるかを考慮して学習を行なう可変(文脈)長モデルと，常に現在位置の形態
素の前後数形態素ずつまでを考慮して学習を行なう固定(文脈)長モデルとの間のモデル
の挙動の違いに注目する．そして，複数のモデルの挙動の違いを調査し，なるべ
く挙動が異なり，かつ，適度な性能を保った複数のモデルの出力の混合を行なう．
次に，混合の方式としては，複数のシステム・モデルの出力(および訓練データ
そのもの)を入力とする第二段目の学習器を用いて，複数のシステム・モデルの出
力の混合を行なう規則を学習するという混合法(stacking法)を採用する．第二段目
の学習器として決定リスト学習を用いて，固定長モデルおよび可変長モデルの出
力を混合する実験を行なった結果，最大エントロピー法に基づく固有表現抽出モ
デルにおいてこれまで得られていた最高の性能を
上回る性能が達成された．","In this paper, we propose a method for learning a classifier which
combines outputs of more than one Japanese named entity extractors.  
The proposed combination method belongs to the family of {\it stacked generalizers","['はじめに', '日本語固有表現抽出', '最大エントロピー法を用いた固有表現抽出', '正誤判別規則学習を用いた複数システム出力の混合', '実験および評価', '関連研究', 'おわりに']",,,,,,,,
V09N01-05.tex,最大エントロピー法を用いた対訳単語対の抽出,Extracting Bilingual Word Pairs \\ with Maximum Entropy Modeling,"機械翻訳などの多言語間自然言語処理で用いられる対訳辞書は現在，人手によっ
て作成されることが多い．しかし，人手による作成には一貫性・網羅性などの
点で限界があることから対訳コーパスから自動的に対訳辞書を作成しようとす
る研究が近年盛んに行われている．
本論文では，最大エントロピー法を用いて対訳コーパス上に対訳関係の確率モ
デルを推定し，自動的に対訳単語対を抽出する手法を提案する．
素性関数として共起情報を用いるモデルと品詞情報を用いるモデルを定義した．
共起情報により対訳関係にある単語の意味を制約し，品詞情報により対訳関係
にある単語の品詞を制約する．
本手法の有効性を示すために日英対訳コーパスを用いた対訳単語対の抽出実験
を行い，本論文で提案した手法が従来の手法よりも精度・再現率において優れ
た結果となり，また，テストコーパスによる実験では学習コーパスに出現しな
かった単語対に関しても学習データに現れたものとほぼ同等の精度・再現率で
抽出できることを示した．","Translation dictionaries used in multilingual natural
  language processing such as machine translation have been made
  manually, but a great deal of labor is required for this work and it
  is difficult to keep the description of the dictionaries
  consistent. Therefore, researches of extracting bilingual word pairs
  from parallel corpora automatically become active recently.
  In this paper, we propose a learning and extracting method of
  bilingual word pairs from aligned parallel corpora with the maximum
  entropy modeling.
  We define a probabilistic model of bilingual word pairs and four
  types of feature functions which express statistical and linguistic
  properties such as co-occurrence information and morphlogical
  information. Co-occurrence information restricts the sense of
  words. Morphlogical information restricts the part-of-speech of
  words.
  Experiment results in which Japanese and English parallel corpora
  are used show that our method performs better than the previous
  methods and can extract the bilingual word pairs which do not appear
  in the training corpus with almost the same accuracy as the appeared
  pairs due to the property of the maximum entropy modeling.","['はじめに', '最大エントロピー法', '最大エントロピー法による対訳単語対の抽出', '実験と考察', 'おわりに']",,,,,,,,
V09N01-06.tex,日英機械翻訳のための日本語抽象名詞の文法的・意味的用法の分類,Classification of Syntactic and Semantic Usage of Japanese Abstract Nouns and Their Translations,"日英機械翻訳において，翻訳が難しいと見られる抽象名詞，「の」，「こと」，
「もの」，「ところ」，「とき」，「わけ」の６種類を対象に，文法的用法と意味
的用法を分類し，英語表現との対応関係を検討した．具体的には，名詞「の」は，
意味的に他の抽象名詞に置き換えられる場合（交替現象）の多いことに着目して，
置き換え先となる抽象名詞の種類と置き換え可能となる条件について検討した．次
に，置き換え後の５種類の抽象名詞の用法を「語彙的意味の用法」，「文法的意味
の用法」に分け，このうち，「文法的意味の用法」を，さらに，「補助動詞的用法」
と「非補助動詞的用法」に分類した．さらに，これらの分類を詳細化し，英語表現
との対応関係を「日英対応表」にまとめた．交替現象の解析精度と「日英対応表」
の精度を調べるため，新聞記事から抽出した抽象名詞の用例に適用した結果では，
「の」の交替現象の解析精度は，97％，「日英対応表」の平均カバー率は89％，平均
正解率は73％であった．","Translation of highly abstract nouns has been one of the most
difficult problems in Japanese to English machine translation. In order
to develop the translation rules, meanings and usage of typical Japanese
abstract nouns of ""no"", ""koto"", ""mono"", ""tokoro"", ""toki"", ""wake"" were
studied and syntactic and semantic usage were classified.  First, taking
notice of the semantic equivalency of ""no"" and other abstract nouns,
exchange rules for noun ""no"" was studied. Next, semantic usage and
syntactic usage of these abstract nouns were analyzed and translations
for them were summalized. The results were applied to 741 expressions extracted from newspaper
and translation quality was evaluated. The results showed that the
quality for exchange rules was 96.9％ and the classification accuracy of
5 abstract nouns was 73％ in average. The results are very useful to
improve the translation quality of abstract nouns.","['まえがき', '抽象名詞とその用法について', '抽象名詞「の」の交替現象の解析', '抽象名詞の用法と英語表現の対応関係', '評価と考察', 'おわりに']",,,,,,,,
V09N02-01.tex,意味グラフのマッチングによる 事故問い合わせ文からの判例検索システムJCare,Judicial CAse REtriever JCare from Japanese Query Sentences based on Semantic Graph Matching,意味解析を用いた情報検索の一手法を提案し，「判例」を検索対象とし日本語文章で記述した「問い合わせ文」を検索質問とした検索システム{\bf JCare,"We propose the technique to realize the information retrieval based on the semantic analysis and actually develop the judicial case retrieval system called JCare. It accepts a query written in Japanese sentences and retrieves a judicial case containing the sentences describing the similar situation specified by the query. It first transforms both a judicial case and the query into semantic graphs that have nodes representing the meaning of word and arcs representing the relations (deep case) between the words. Next, it calculates the similarity between the case and the query by searching the maximum common parts that are topologically equivalent. The graph matching is speeded up by separating each semantic graph into sub-graphs based on the ``View'' point about a judicial case.","['はじめに', '意味グラフ', 'JCareの概要', 'Viewへの分割', 'Viewごとのマッチング手法', 'まとめ']",,,,,,,,
V09N02-02.tex,主体と動詞の属性に基づく複文の連接構造の解析,"A Model for Analyzing Structures of Coherence \\ Relations
Using Features of Verbs and Subjects",本論文は，動詞と主体の属性を用いて複文の連接関係の関係的意味を解析し，この関係的意味を用いて連接構造を解析するモデルについて述べる．従来，複文の従属節間の連接構造解析は，接続の表現を階層的に分類し，その階層的な順序関係による方法が用いられてきた．しかし，接続の表現には曖昧性があり，同じ接続の表現でも意味が違うと係り方が違う．このため，本論文では，動詞と主体の属性を用いて，連接関係の意味を求め，この連接関係の関係的意味を，連接関係の距離によって分類する．この分類を用いて連接構造を解析する方法を用いた．動詞の属性として，意志性，アスペクト・ムード・ヴォイス，意味分類などを用いた．主体の属性として，主節と従属節の主体が同一かどうか，無生物主体かどうかを採用した．このモデルを実際の用例により評価した結果98.4\%の精度が得られた．接続の表現の階層的分類を使用したモデルに同じ用例を適用したところ97.0\%の精度が得られたので，本論文のモデルを使用することにより誤り率が約半分に改善された．,"The model proposed in this paper analyzes meanings of coherence relations using features of verbs and subjects in Japanese complex sentences, and then analyzes structures of coherence relations using the meanings of coherence relation. Dependency structures of Japanese complex sentences are usually analyzed using hierarchical classification of conjunctions and conjunctive particles. But, conjunctions and conjunctive particles usually have multiple senses and are ambiguous. If a conjunction or conjunctive particle in a subordinate clause has a different sense, the subordinate clause may modify a predicate in a different clause. So, the model analyzes the coherence relations between subordinate clauses and a main clause using the features of verbs and subjects, and defines the meanings of coherence relations. Then the meanings of coherence relations are classified according to distance of coherence. The model uses this classification of coherence relations to analyze the structures of coherence relations. Volition, aspect, mood, voice and semantic category etc. are used as the features of verbs, and animate or inanimate etc. is used as the features of subjects. The model is evaluated by examples from actual documents, and shows 98.4\% accuracy. Since the model using the classification of conjunctions and conjunctive particles shows 97.0\% accuracy with the same examples, the model proposed in this paper, decreases the error rate by half.","['はしがき', '動詞と主体の属性と連接関係の関係的意味', '接続の表現の意味による距離', '連接関係の意味による連接構造の解析モデル', '連接構造解析モデルの評価結果', 'むすび']",,,,,,,,
V09N02-03.tex,コーパス中の一対多関係を推定する問題における類似尺度,"A Similarity Measure for Estimation of\\ 
One-to-Many Relationship in Corpus","{
本論文では, コーパスから事象間の一対多関係を推定する問題を考える. 
これまでにコーパスから事象間の関係を推定することが
多く研究されている. 一般に, この問題に対する解決法
の多くは, コーパスを構成する文書における事象の共起に基づき, 
暗黙的に事象間の関係は一対一関係であることを想定している. 
しかし, 実際には, 事象間の関係は一対多関係である場合があり, この特徴のためにいくつか
の工夫が必要である. 
本論文では, コーパス中の一対多関係を推定するために補完類似度を利用することを提案する. 
この尺度は本来文字認識システムのために開発され, テンプレートの文字の
パターンにオーバーラップしたパターンがある条件で有効であることが知られてい
るが, これまでテキスト処理に利用されたことはなかった. 
この補完類似度の一対多関係を推定する能力を評価するために, 地名(都道府県市郡名
)を対象事象とした実験において, 
平均相互情報量, 自己相互情報量, 非対称平均相互情報量, $\phi$相関係数, 
コサイン関数, ダイス相関係数, 信頼度との性能比較を行う. 
実験では, 三種類のコーパスを用いる. 一つ目は実際に地名間にある一対多関係か
ら合成する人工的なデータ集合である. 二つ目も実際の関係から合成するが, 
誤った関係を導く少量の要素も含むデータ集合である. 
三つ目は現実の新聞記事コーパスから得られるデータ集合である. 
これらの評価実験において, 補完類似度がもっとも優れており, 
補完類似度は一対多関係の推定問題に対して有効であることを示す.","{
In this paper, we consider the estimation of the one-to-many relationship
between entities in corpus. 
Many works have been done to estimate the relationship between entities
from corpus. 
Generally speaking, the most common method is based on the co-ocurrence
of entities in a document of corpus, and this method implicitly assumes
that the relationship is one-to-one mapping. 
The real relationship may sometimes be one-to-many relationship, and
need some consideration for this property. 
We propose to use CSM(Complementary Similarity Measure) to
detect this relationship.
This measure is originally developed for character recognition system,
and is known to work well for overlapped patterns with template pattern, 
but is rarely used for text processing. 
We have compared CSM with other similarity measures, including three kinds
of mutual information, $\phi$ coefficient, cosine, 
dice coefficient, and confidence. 
We choose the names of prefectures and cities as the entities, which has
real one-to-many relationship. 
For the evaluation, we have used three kinds of corpora. 
The first one is a synthesized from real relations. 
The second one is also synthesized from relations but it contains
an element of false relation. 
The third one is compiled from actual newspaper corpus. 
We have found that CSM is the best similarity measure for this
experiment and works well for one-to-many relationship.","['はじめに', '問題定義', '評価対象となる類似尺度', '評価実験', '考察', '関連研究', 'おわりに']",,,,,,,,
V09N02-04.tex,,A Method for Similarity-based\\　　Lexical Disambiguation,,"This paper describes a method for word sense disambiguation using a 
similarity metric.  In this method, we first obtain context-similarity vectors 
for the senses of a polysemous word using a corpus and also define the context 
representation for the same polysemous word appearing in text.  We then 
calculate distributional matrix between each context-similarity vector and the 
context representation for the word to be disambiguated.  Finally, comparing 
the values of distributional matrices, we select the sense with the highest 
value as the meaning of the polysemous word.  An experiment with 682
instances for 10 polysemous words shows that we are able to disambiguate at 
a rate of almost 92\%.","['Introduction', 'Word Sense Disambiguation', 'Measurement of Similarity', 'A Method of Word Sense Disambiguation', 'Experiment and Result', 'Evaluation', 'Conclusion']",,,,,,,,
V09N02-05.tex,diff を用いた言語処理\\ --- 便利な差分検出ツール mdiff の利用 ---,"NLP using DIFF\\ --- Use of convenient tool\\ for detecting differences, MDIFF ---","差分検出を行なう diff コマンドは言語処理の研究において
役に立つ場面が数多く存在する．
本稿では，diff を使った言語処理研究の具体的事例として，
差分検出，書き換え規則の獲得，データのマージ，最適照合の例を示す．
diff コマンドは UNIX で標準でついているため，
これを用いることは極めて容易である．
本稿は言語処理の研究を行なう上で diff コマンドが
実用的でありかつ有用であることを示すものである．",{\it Diff,"['はじめに', 'diff と mdiff', '差分検出，および，書き換え規則の獲得', 'データのマージ，および，最適照合', 'おわりに', 'mdiff の構成方法および使い方']",,,,,,,,
V09N03-01.tex,ベイズ統計の手法を利用した決定リストのルール信頼度推定法,Estimating reliability of rules in decision lists \\ using Bayesian learning,"統計的クラス分類器としての決定リストは，近年自然言語処理における
様々な分野でその有効性を示している．決定リストを構成する上で最も
重要な問題の一つは，ルールの信頼度の算出法である．決定リストを
用いた多くの研究では，最尤推定法と簡単なスムージングにより
信頼度を算出しているが，理論的な根拠に欠け推定精度も高くないという問題がある．
そこで本論文では，ベイズ学習法を利用してルールの信頼度を算出する手法を示す．
さらに，証拠の種類ごとに異なる事前分布を利用することで，
より正確な信頼度の推定が可能になり，決定リストの性能が向上する
ことを示す．
本手法の有効性を確かめるために，語義曖昧性解消の問題に決定リスト
を適用して実験を行なった．英語に関しては Senseval-1 のデータを
用い，日本語に関しては疑似単語を用いた．
その結果，ベイズ学習による信頼度推定手法が，ルールの確率値の推定
精度を高め，決定リストの分類性能を向上させることを確認した．","The decision list algorithm is one of the most successful algorithms for
classification problems in natural language processing.
The most important part of the decision list algorithm is
the calculation of reliability for each rule, hence the
estimation of probability for each contextual evidence.
However, the majority of research efforts using decision lists
do not think much of the estimation method.
We propose an estimation method based on Bayesian learning which
gives well-founded smoothing and better use of 
prior information on each type of contextual evidences.
Experimental results obtained using Senseval-1 data set and Japanese 
pseudowords show that our method makes probability estimation more
precise, leading to improvement of classification performance
of the decision list algorithm.","['はじめに', '決定リスト', 'ベイズ学習によるルール確率値の推定', '事前分布の利用による確率値の正確な推定', '実験 \\label{sc:experiments', 'おわりに']",,,,,,,,
V09N03-02.tex,ドメイン固有の文字列情報の組み込みによる\\形態素解析処理の精度の向上,The Use of Domain-Specific Statistical Data\\for Japanese Morphological Analysis,"辞書ベースの自然言語処理システムでは辞書未登録語の問題が避けられない．
本稿では訓練コーパスから得た文字の共起情報を利用する手法で辞書未登録語の抽出を実現し，辞書ベースのシステムの精度を向上させた．
本稿では形態素解析ツールをアプリケーションとして採用し，処理時に統計情報を動的に利用することによって形態素の切り分けの精度を上げる手法と，統計情報を利用して事前に辞書登録文字列を選別し必要なコスト情報を補って辞書登録を行なう手法との 2 つのアプローチを提案し，さらにこの 2 つの手法を組み合わせてそれぞれの欠点を補う手法を提案する．どちらも元のツールの改変を行なうものではなく，統計情報の付加的な利用を半自動的に実現するもので，元のツールでは利用できない辞書未登録語の抽出に対象を絞ることで精度の向上を図る．
実験の結果，動的な統計情報の利用のシステムが未知語の認識に，辞書登録システムが切り分け精度の向上に有効であることが示され， 2 つのシステムを適切に組み合わせることによって訓練コーパスのデータで認識可能な辞書未登録語をほぼ完全に解決できた．さらに複合語の認識も高い精度で実現することができた．","We propose two methods for the recognition of unknown strings in dictionary-based natural language processing systems.
One method is for the dynamic use of statistical information during processing, and the other is for obtaining meaningful strings which should be added to the dictionary.
Both methods are based on statistical information drawn from a training corpus, and there is no need for part-of-speech tagging or other preprocessing of the training corpus.
We applied our methods to a Japanese morphological analysis system and had good results in reduction of unknown words and over segmentation.","['はじめに', '語句抽出', 'システム概要', 'システム M: 茶筌への組み込み', 'システム D: 辞書への組み込み', 'システム M+D: 辞書登録と切り分け処理の併用', '実験および考察', '結論']",,,,,,,,
V09N03-03.tex,連用修飾表現の省略可能性に関する知識の獲得,Knowledge Acquisition about the Abbreviation Possibility of Verb Phrases,"文内要約の一要素技術として，
連用修飾表現の省略可能性に関する知識を獲得する手法を提案する．
具体的には，
省略できる可能性のある連用修飾表現を含む節に対して，
同一の動詞をもち，かつ，格助詞出現の差異が認められる
節をコーパスから検索し，検索された節対から省略可能な連用修飾表現を認定する．
また，連用修飾表現の内容および前後の文脈を考慮して，重要な情報が多く含まれている連用修飾表現に対しては省略可能と認定できる可能性を低く，
逆に，認定対象としている連用修飾表現に，それより以前の文に存在する情報が含まれている場合に対しては，省略可能と認定できる可能性が高くなるような工夫を施した．
本手法によって省略可能と認定された連用修飾表現を評価したところ，
適合率78.0\%，再現率67.9\%との結果を得た．
また，本手法を，格フレーム辞書によって動詞に対する任意格として記述される格要素を，
省略可能な連用修飾表現として認定する手法と比較した．
その結果，適合率，再現率ともに比較手法より良好な結果を得ることができ，提案手法の有効性を確認した．","This paper proposes a method of acquiring knowledge about the abbreviation possibility of verb phrases.
In a certain clause containing a verb and including verb phrases,
the proposed method extracts some clauses which contain the same verb and have different case postpositional particles from a large corpus. 
Then, our method 
recognizes verb phrases possible to be abbreviated by comparing the verb phrases with the verb phrases contained in the extracted clauses.
In our method, the verb phrases containing important piece of information 
is hard to recognize as being possible to be abbreviated, 
and the verb phrases containing information which appear in previous sentences is easy to recognize as being possible to be abbreviated.
The evaluation of our method by experiments shows that the precision is 78.0\% and the recall is 67.9\%.
We compare our method with the method which recognizes verb phrases possible to be abbreviated by recognizing optional case elements described in a case frame dictionary as being possible to be abbreviated.
By the evaluation results, we conclude that our
method outperforms the method which recognizes verb phrases possible to be abbreviated by using a case frame dictionary.","['はじめに', '提案手法', '手法の実装', '評価実験', '格フレームによる手法との比較実験', '結び']",,,,,,,,
V09N03-04.tex,確率モデルを用いた日本語ゼロ代名詞の照応解析,Japanese Zero Pronoun Resolution \\using a Probabilistic Model,"日本語では，読み手や聞き手が容易に推測できる語は頻繁に省略される．これ
らの省略を適切に補完することは，自然言語解析，とりわけ文脈解析において
重要である．本論文は，日本語における代表的な省略現象であるゼロ代名詞に
焦点を当て，確率モデルを用いた照応解析手法を提案する．本手法では，学習
を効率的に行なうため，確率モデルを統語モデルと意味モデルに分解する．統
語モデルは，ゼロ代名詞の照応関係が付与されたコーパスから学習する．意味
モデルは，照応関係が付与されていない大規模なコーパスを用いて学習を行な
い，データスパースネス問題に対処する．さらに本手法では，照応解析処理の
精度を高めるために確信度を定量化し，正解としての確信が高いゼロ代名詞の
み選択的に結果を出力することも可能である．新聞記事を対象にした照応解析
実験を通して本手法の有効性を示す．","In Japanese, entities which can easily be predicted are often omitted.
Identifying appropriate antecedents associated with those ellipses,
which is termed ``anaphora resolution'', is crucial in natural
language processing, specifically, a discourse analysis. This paper
proposes a probabilistic model to resolve zero pronouns, which are one
of the major ellipses in Japanese. Our proposing model can be
decomposed into two models associated with syntactic and semantic
properties, so as to optimize a parameter estimation. A syntactic
model is trained based on corpora annotated with anaphoric
relations. However, a semantic model is trained based on a large-scale
unannotated corpora to counter the data sparseness problem. We also
propose a notion of certainty to improve the accuracy of zero pronoun
resolution. We show the effectiveness of our method by way of
experiments.","['はじめに', '本研究で提案するゼロ代名詞の照応解析手法', '評価実験', '関連研究との比較', 'おわりに']",,,,,,,,
V09N03-05.tex,自動文節対応付けを用いた要約中の文再構成操作の調査,Sentence Reconstruction in Summary Generation: An Investigation using Automated Alignment,"本研究では，
要約文とその要約文を作成するために使用された表現を含む原文とを
自動的に対応付ける手法を用いて，人間が要約文を作成する上で，
要約元となった原文をどのように再構成するかを調査した．
対応付けに用いた手法は， かかり受け構造の解析結果を利用し，
要約文とその対応文との間の対応付けを文節単位で行う．
また，要約文1文に対して，要約元文章中の複数文を対応付けすることを許して対応付けが可能である．
調査した対象は，複数の作業者が新聞の社説を要約したデータである．
このデータに対して，対応付け手法を実際に適用した．
対応付けの結果，要約元文章で用いられていなかったり，
元文章でかかり受け関係がなかった表現が要約文に用いられていた場合に，
それらの表現を構成する文節は未対応となる．
そこで，そのような要約文中で未対応になった文節がどのように生成されたかを，計算機でも処理可能な操作を主眼に分類・整理して考察した．
その結果，要約原文のかかり受け構造は，要約文においても保存されることが多く，要約文に新しく出現する表現の多くは，複数の原文から１つの要約文を作成する文結合操作と，単文節を中心とした言い換え操作により生成されることがわかった．","In this paper, we investigate operations in summary generation. 
In order to align a summary expression with the corresponding original expression in source text,
 we introduce an automated algorithm based on dependency structure of sentences.
Our algorithm detects not only one-to-one sentence alignment,
 but also one-to-many sentence alignment.
We apply the algorithm to human made natural summaries,
 and analyze the results of the alignment.
As a result of the analysis,
 we find most of the summary expressions are kept their dependency structure in original sentences
 and confirm one of the operation called ''sentence combination'',
 in which more than two source sentences are used to generate a summary sentence,
 plays an important role in summary generation.
Furthermore,
 we characterize operations and paraphrasing that cover most summary generation.","['はじめに', '要約における文再構成操作', 'かかり受け構造を用いた自動的対応付け', '対応付け手法の適用と要約操作', 'まとめと今後の課題']",,,,,,,,
V09N03-06.tex,複数の対話エージェントを導入した 情報検索の対話モデル,A Dialogue Model for Information Retrieval with Multiple Dialogue Agents,"本稿では複数の対話エージェントを導入する効率的な情報検索の対話モデルを採用する．
情報検索という複雑な対話に対して，万能の対話エージェントを用意することは，
現状では困難である．そこで，以下の三つの局面で，対話エージェントを
切り替えることによって，ユーザは円滑な情報検索対話を進めることができる．
\begin{itemize","In this paper, we described a natural language dialogue model for 
information retreival with multiple dialouge agents. 
In the complex dialogue for information retrieval, it is difficult to realize
an effective dialogue with the almighty dialogue agent. Therefore, we propose
a dialogue model which mekes users proceed dialogue fluently 
in the following three situations by changing dialogue agents:
\begin{itemize","['はじめに', '自然言語を用いた情報検索の対話例と問題点', '複数の対話エージェントの導入した対話モデル', '複数の対話エージェントの導入した対話モデルの評価', 'おわりに']",,,,,,,,
V09N03-07.tex,要約の内的(intrinsic)な評価法に関する いくつかの考察 -- 第2回NTCIR ワークショップ 自動要約タスク(TSC)を基に --,"Some Examinations of Intrinsic Methods for Summary Evaluation Based on Text Summarization Challenge(TSC), a Subtask of NTCIR Workshop 2","\quad 
システムの出力した要約そのものを評価する方法は，一般に内的な評価と呼ばれ
ている．これまでの典型的な内的な評価の方法は，人手で作成した抜粋と要約シ
ステムの出力との一致度を，F-measure等の尺度を用いて測ることで行われてき
た．しかし，F-measureは，テキスト中に類似の内容を含む文が複数存在する場
合，どちらの文が正解として選択されるかにより，システムの評価が大きく変化
する，という問題点がある．本研究では，この問題点を解消するいくつかの評価
方法をとりあげ，その有用性に関する議論を行う．F-measureの問題点を解消す
る評価方法の1つにutility に基づく評価があるが，この方法では評価に用いる
データ作成にコストがかかるという問題がある．本研究では，あるテキストに関
する複数の要約率のデータを用いることで，疑似的にutilityに基づく評価を実
現する方法を提案する．提案する評価方法を，第2回NTCIR ワークショップ自動
要約タスク(TSC)のデータに適用し，有用性に関する調査を行った結果，提案方
法は，F-measureの問題点をある程度改善できることが確認された．次に，
F-measureの問題点を解消する他の評価方法の一つであるcontent-basedな評価を
取り上げる．content-basedな評価では，指定された要約率の正解要約を一つだ
け用意すれば評価可能であるため，utilityに基づく評価に比べ，被験者への負
荷が少ない．しかし，この評価方法で2つの要約を比較する場合，どの程度意味
があるのかについては，これまで十分な議論がなされていない．そこで，
pseudo-utilityに基づく評価と同様にTSC のデータを用い，content-based な評
価の結果を被験者による主観評価の結果と比較した結果，2つの要約が
content-basedな評価値で0.2 以上の開きがあれば，93\%以上の割合で主観評価
の結果と一致することが分かった．","\quad
Evaluation methods whose targets are system outputs (summaries)
themselves are often called ``intrinsic methods''. Computer-produced
summaries have been traditionally evaluated by comparing with
human-written summaries using the F-measure. But, the F-measure has the
following problem: the F-measure is not appropriate when alternative
sentences are possible in a human-produced extract. For example, when
there are two sentences 1 and 2, and sentence 1 is in a human-produced
extract, if a system chooses sentence 2, it obtains lower score, even if
sentences 1 and 2 are interchangeable. In this paper, we examine some of
the evaluation methods devised to overcome the problem. Several methods
that devised to overcome the problem have been proposed. Utility-based
measure is one of them. However, the method requires a lot of effort for
humans to make data for evaluation. In this paper, we first propose
pseudo-utility-based measure that uses human-produced extracts at
different compression ratios. In order to evaluate the effectiveness of
pseudo-utility-based measure, we compare our measure and the F-measure
using the data of Text Summarization Challenge(TSC), a subtask of NTCIR
workshop 2, and show that pseudo-utility-based measure can resolve the
problem. Next, we focus on content-based evaluation. Though it is
reported that content-based measure is effective to resolve the problem,
it has not been examined from a viewpoint of comparison of two extracts
that are produced from different systems. We evaluated
computer-produced summaries of the TSC by the content-based measure, and
compared the results with a subjective evaluation. We found that the
evaluation by the content-based measure matched those by humans in 93\%
of the cases, if the gap in the content-based scores between two
abstracts is more than 0.2.","['序論', '関連研究', 'pseudo-utilityに基づく評価', '評価方法の分析', '結論と今後の課題']",,,,,,,,
V09N04-01.tex,検索結果表示向け文書要約における 情報利得比に基づく語の重要度計算,A Term Weighting Method based on Information Gain Ratio for Summarizing Documents retrieved by IR Systems,"本稿では，情報検索の結果として得られた文書集合中の各々の文書を要約す
る一手法を提案する．この場合の要約の質は，検索質問-要約文書間の
関連性判定が，検索質問-原文書の間の関連性判定に一致する度合で評価され
るので，検索を考慮した要約が必要となる．
検索質問により語の重みにバイアスを与え，語の重要度を求める従来手法とは異なり，
我々の方法では，
検索された文書間の表層的類似性を適切に説明する語に高い重みを付与する．
具体的には，検索文書集合に階層的クラスタリングを適用することにより，
文書間の類似性構造を抽出するとともに，各クラスタにおける各語の出現確率から，その構造
を説明するのに寄与する単語により高い重みを与える．
我々は，その重みづけに情報利得比を用いることを提案する．
そして，この語の重み付けに基づき重要文抽出方式による検索文書要約システムを実装した．
このシステムを評価型情報検索ワークショップであるNTCIR2におけるText Summarization Challengeの情報検索タスクにより評価した結果，
関連性判定において検索質問バイアス付きTF方式，リード文方式によるベースライン手法ならびに，他参加システムよりも，良好な結果を示した．","This paper proposes a new term weighting method for summarizing
documents retrieved by IR systems. Unlike query-biased summarization
methods, our method utilizes not the information of query, but the
similarity information among original documents by hierarchical
clustering. In order to map the similarity structure of the
clusters into the weight of each word, we adopt the information gain
ratio (IGR) of probabilistic distribution of each word as a term weight.
If the amount of information of a word in a cluster increases
after the cluster is partitioned into sub-clusters, we may
consider that the word contributes to determine the structure of
the sub-clusters.  The IGR is a measure to express the degree of such
contribution. We show the effectiveness of our method based on
the IGR by comparison with other systems in Text Summarization Challenge
 of NTCIR2.","['はじめに', '検索文書集合中の語に対する情報利得比に基づく重みづけ', '重要文抽出に基づく要約文書生成', '評価', '考察', '関連研究', 'おわりに', 'NTCIR2 TSC Task B に参加した他システムの概要']",,,,,,,,
V09N04-02.tex,テキスト要約の複数の正解に基づいた評価,Evaluating Text Summarization\\Using Multiple Correct Answer Summaries,"本稿では，要約手法として複数の正解に基づく評価法の提案を行なった．従来
のテキスト要約の評価方法では唯一の正解を用いるが，テキストによっては観
点の異なる正しい要約が複数存在する場合もあり，評価の信頼性が保証されな
いという問題があった．我々は，自動評価の信頼性を高めるため，特に重要文
抽出法に焦点を当てて複数の正解に基づく評価方法を検討した．提案手法では，
複数の正解と評価対象の要約を共にベクトルで表現し，複数の正解の線形結合
と評価対象の要約との内積の最大値を評価値とする．提案手法の検証のために，
NTCIR-2要約データ中の4記事に対して，要約者7名で要約の作成を行なった．
正解の要約間の一致度に基づく品質評価の結果，提案手法の評価の正解として
用いるのに十分な品質が得られなかったが，要約の比較から，照応関係，結束
性等，元テキスト中の構造を損なわないように要約する共通の法則性が見出さ
れ，今後要約の正解を作成する上で有用な知見を得た．提案手法の有効性を検
証する予備実験として，異なる幾つかの自動要約手法と複数正解との一致度に
基づく評価を行なった．正解ごとに評価の高い自動要約手法が異なるという傾
向が見られ，複数の正解を用いることで評価対象の要約との相性によらない評
価結果を得るという提案手法の前提を裏付ける結果を得た．","We proposed an evaluation method based on multiple correct
answer summaries. Conventional evaluation methods had reliability
problem due to adopting single model answer while multiple correct
answer summaries may exist from various points of view. We aimed to
increase the reliability of automatic evaluation, and focused on an
evaluation method using multiple answer summaries. In our method, we
introduced linear combinations of answer summaries, all denoted by
vectors, and calculated its maximum value of the scalar product for
the answers and the target summary. To verify the reliability of our
method, 7 people created summaries for 4 newspaper articles in NTCIR-2
summarization test collection data. However, low agreement among these
answer summaries showed these data inadequate to be used as answers
for the evaluation method. These summaries showed some tendency of
keeping the text configurations due to anaphoric relations and
sentence cohesions. Those findings will be valuable in creating model
summaries. To verify the feasibility of the evaluation method, some
automatic methods were evaluated using the multiple correct summaries. 
Most feasible method was varied according to each correct summary.
\vspace{6mm","['はじめに', '唯一の理想的な要約を正解とする評価法の問題', '複数の正解に基づく評価手法', '提案評価手法の予備実験', 'おわりに']",,,,,,,,
V09N04-03.tex,重要語句抽出による新聞記事自動要約,"Summarizing Newspaper Articles Using \\ 
Extracted Informative and Functional Words","本論文では，格フレーム辞書を用いて原文の重要語句を抽出し，
抽出された語句を再構成することにより
要約文を生成する新聞記事要約手法を提案する．
この手法に基づいて新聞記事自動要約システムALTLINEを試作し，
人手要約との比較により評価を行なった．
この結果，提案手法によって
人間の要約に匹敵する要約文が生成できることが分かった．","We propose a new method of summarizing newspaper articles that
extracts using a case-frame dictionary, 
important words and phrases from original articles 
and generates a summary by reconstructing those
extracted words and phrases.
The number of sentences in the generated summary can be
controlled by users from one to a few sentences.
We have also developed the prototype summarization system ALTLINE
and evaluate the system by comparing generated summaries 
to human-produced summaries.
This evaluation result shows that 
the ALTLINE was ranked near middle among all the human subjects,
proving that the system summaries obtained comparable to human summaries.","['はじめに', '要約方式', '要約システムの実装', '被験者を用いた要約実験', '被験者結果を用いたALTLINEの評価', '考察', 'おわりに', '付録: 原文(新聞記事)の例', '付録: 単語リスト', '付録: 被験者の要約結果']",,,,,,,,
V09N04-04.tex,句表現要約手法に基づく要約システムの開発と評価,Development and Evaluation of \\ a Summarization System based on \\ Phrase-representation Summarization Method,検索結果のふるいわけに適した要約生成手法を開発した．多くの要約システムでは重要文選択という手法を採用しているが，この方法による要約は長く複雑な文になりがちである．我々が開発した句表現要約手法は， 短い句を列挙することで，そのような長い文を読む際に生じる負荷を軽減する．各句は，(1)係り受け解析により単語間の関係を抽出，(2) 係り受け関係からコアになる関係を選択，(3) 句に意味のまとまりを持たせるのに必要な関係を付加，(4) このようにして作られたグラフから表層句を生成，という手順で作られる．この手法の効果を評価するため，タスクベース評価法の改良を行った．この方法では，検索が必要になった背景を含めたタスクの詳細まで規定すること，ひとつの要約を10名の評価者で評価して個人差の影響を少なくすることにより，正確性を増している．また，適合性の評価に複数のレベルを設けることで，様々な状況における適合率・再現率の評価を可能にした．この方法で評価したところ，句表現要約が情報検索結果のふるいわけに最も適していることがわかった．この結果は，生成された句が比較的短く，文書中の重要な概念を広くカバーするということから得られたものと考えられる．,"We have developed a summarization method that creates a summary suitable for the process of sifting information retrieval results.  Conventional methods extract important sentences to produce summaries that tend to be long and complex.  We have developed the phrase-representation summarization method that constructs short phrases to reduce the burden of reading such long sentences.  Each phrase is constructed by (1) dependency analysis to extract the relations between words, (2) selection of the core relation from dependency relations, (3) adding relations necessary for the unity of the phrase's meaning, and (4) generation of the surface phrase from the constructed graph.  To evaluate the effectiveness of this method, we have developed an improved task-based evaluation method of summarization, the accuracy of which is increased by specifying the details of the task including background stories, and by assigning ten subjects per summary sample. The method also serves precision/recall pairs for a variety of situations by introducing multiple levels of relevance assessment. The method is applied to prove that phrase-represented summary is most effective to select relevant documents from information retrieval results. The result comes from the fact that the constructed phrases are rather short but cover more important keywords.","['はじめに', '句表現要約の概念', 'アルゴリズム', '実現と応用', '評価', '関連研究', 'おわりに', '関係の種類', '補完規則', '低減率の効果']",,,,,,,,
V09N04-05.tex,テキスト自動要約に関する最近の話題,New Topics on Automated Text Summarization,"本稿では，1999年の解説の後を受
け，テキスト自動要約に関する，その後の研究動向を概観する．
本稿では，その後の動向として，特に最近注目を集めている，以下の3つの話
題を中心に紹介する．
\begin{enumerate","In this article, we try to survey the current trends in the field of
automated text summarization, especially concentrating on the
following three topics: researches on producing more natural summaries
in single document summarization, farther activation of researches on
multi-document summarization, and more variety of summarization inputs
in the researches.","['はじめに', 'より自然な要約作成に向けて', '複数テキストを対象にした要約手法', '要約対象の幅の広がり', 'おわりに']",,,,,,,,
V09N05-01.tex,Support Vector Machine を用いた Chunk 同定,Chunking with Support Vector Machines,"本稿では，Support Vector Machine (SVM)に基づく一般的な chunk 同定手法
を提案し，その評価を行う．SVMは従来からある学習モデルと比較して，
入力次元数に依存しない高い汎化能力を持ち，Kernel 関数
を導入することで効率良く素性の組み合わせを考慮しながら分類問題を学習することが可能である． 
SVM を英語の単名詞句とその他の句の同定問題に適用し，
実際のタグ付けデータを用いて解析を行ったところ，従来手法に比べて高い精度を示した．
さらに，chunk の表現手法が異なる複数のモデルの重み付き多数決を行うこと
でさらなる精度向上を示すことができた．","In this paper, we apply Support Vector Machines (SVMs) to identify 
English base phrases (chunks). It is well-known that SVMs achieve high
generalization performance even using input data with a high dimensional feature
space. Furthermore, by introducing the Kernel principle, SVMs can
carry out training with smaller computational cost independent of
the dimensionality of the feature space.
In order to improve accuracy, we also apply majority
voting with 8 SVMs which are trained using distinct chunk representations.
Experimental results show that our approach achieves better accuracy
than other conventional frameworks.","['はじめに', 'Support Vector Machine', 'SVM に基づく Chunk 同定', '実験と考察', 'まとめ']",,,,,,,,
V09N05-02.tex,構文解析と融合した階層的句アライメント,Hierarchical Phrase Alignment \\ Harmonized with Parsing,"本稿では，機械翻訳知識の自動獲得を目的とした，2言語の対訳文の階層的句
アライメントについて提案する．従来提案されてきた句アライメント方法は，
いずれも構文解析結果を取得したのちに，部分木同士の対応をとるものであっ
た．本稿で提案する方式は，構文解析器が持つ部分解析結果を句対応スコアと
呼ぶ構造類似性評価尺度で評価し，前向きDP後ろ向きA*アルゴリズムを用いて
最適な組み合わせを探索する．この方式を用いることにより，実験では従来手
法に比べ2倍の同等句を得ることができ，そのときの精度の低下はほとんどな
いことが観察された． \indent また，本提案方式は単語アライメントを用いる．この単語レベルの対
応は，内容語のみでなく，機能語間対応を含めた方が句アライメント精度が向
上する．その一般形として，本方式に適合した単語アライメントは，再現率重
視のものが望ましいことを併せて示す．","In this paper, we propose a hierarchical phrase alignment method that
aims to acquire translation knowledge.  Previous methods utilize the
correspondence of sub-trees between bilingual parsing trees after
determining the parsing result.  The method described in this paper
combines partial tree candidates, and selects the best sequence of
partial trees.  Then, a structural similarity measure (called a
`phrase score') is used for evaluation. A forward DP backward $A^{*","['はじめに', '階層的句アライメントの基本方式', '句アライメントと構文解析の融合', '単語アライメントに要求される機能', '実験・評価', '関連研究', 'まとめ']",,,,,,,,
V09N05-03.tex,文字列を$k$回以上含む文書数の計数アルゴリズム,"Counting documents that contain substrings\\ 
 more than $k$ times","この論文で計算するものは，ある文字列を$k$回以上含む
ドキュメントの総数($df_k$)である．全ての部分文字列に対してこれらの
統計量を保存する場合$O(N^2)$の表が必要となり，コーパスの大きさを
考えると，この表は実用的でなく，通常の計算機では実際に作ることは
難しい．しかし，$k=1$の場合，Suffix Array，文字列のクラス分けを
利用して，統計量をクラス毎に保存することで，これを$O(N)$の表に
できるという報告がある\cite{DF1","The statistics we compute is $df_k$: the number of documents 
which contain certain strings more than $k$ times. We can hardly 
keep the statistics of all substrings because we need $O(N^2)space$ 
where $N$ is the size of corpus. Yamamoto et al. show that it is 
possible to produce a table for $k=1$ in $O(N)space$ using Suffix Array 
and the concept of ""class of string"". However, this method cannot 
solve the problem where $k \geq 2$. We present an algorithm that can 
be used for $k \geq 2$ and we can compute the statistics by using 
the table.
In this report, we explain $df_k$ and compare the proposed algorithm 
with simple methods. This algorithm takes $O(N \log N)time$ and 
$O(N)space$ to produce the table and $O(\log N)time$ to obtain 
statistics from the table.","['はじめに', '記号の定義', 'Suffix Array', '文字列のクラス分け', 'クラスの階層関係', '重複条件付きドキュメント頻度の計測における問題点', '出現場所の重複条件', '重複条件付き文字列頻度', '重複条件付き文字列頻度とドキュメント頻度の関係', '重複度判定のためのデータ構造', 'クラス検出のアルゴリズム', '単純な重複条件付き文字列頻度の計数', '重複条件付き文字列頻度の計数', 'クラスの発見と頻度計算', '実行例', '実行時間の計測', 'そのほかの応用', 'まとめ']",,,,,,,,
V09N05-04.tex,確率的判定尺度を用いた比喩性検出手法,A Method of Metaphoricity Detection\\ using Probabilistic Measurements,"本論文では，テキスト中に出現する比喩表現を認識するために，確率的な尺度を用いて，
概念(単語)間の比喩性を検出する手法について述べる．比喩性を検出するための確率的な
尺度として，``顕現性落差""と``意外性""を設定する．``顕現性落差''は，概念対を比較
したときに，クローズアップされる顕現特徴の強さをはかる尺度であり，概
念同士が理解可能か否かの判断に用いる．``顕現性落差''は，確率的なプロトタイプ概念記述を用い
て，概念の共有属性値集合が持つ冗長度の差で定量化する．``意外性''は，概念の組み合
わせがどれほど稀であるかをはかる尺度であり，概念同士が例示関係であるか否かの判断
に用いる．``意外性""は，単語間の意味距離を用いて定量化する．二つの尺度を併用する
ことによって，比喩関係を持つ概念対，すなわち，比喩性の判定が可能となる．二つの尺
度を計算するために，コーパス中から抽出した語の共起情報を利用して知識ベースを利用
する．両尺度を用いた比喩性検出手法を検証するために，1年分の新聞記事コーパスから
構築した知識ベースと，比喩関係・例示関係・無意味の各単語対が混在するデータ100組
を用いて，単語対の判別実験を行った．その結果， 70\%以上の適合率で比喩関係単語対
が判別できることがわかり，本手法の有効性が確認できた．","We propose a method to detect metaphoricity between words with 
probabilistic measurements.  In order to detect metaphoricity, we have 
introduced two probabilistic measurements: ``$salience$ $gap$"" and ``$novelty$.""  
The salience gap measures strength of closed-up property set between a 
concept pair and has contribution to separate concept pairs into 
anomalous and others.  The measurement can be computed by 
probabilities of properties in each concept representation.  The novelty 
measures how surprisingly a concept combination is, and contributes to 
extract anomalous relation from concept pairs.  The measurement can 
be calculated using word similarity.  Using both measurements, 
concept pairs can be classified into metaphorical, literal and anomalous.  
For the evaluation of our metaphoricity detection model, we have used 
one-year newspaper articles and 100 sets of word combinations 
including three kinds of relations: metaphorical, literal and anomalous.  
In the experimental results, precision attained 70 percent for dividing 
metaphorical word pairs from others.   It can be considered that 
performance of our method is useful.","['まえがき', '比喩性の尺度', '顕現性落差の定量化', '意外性の定量化', '評価', '考察', 'むすび']",,,,,,,,
V09N05-05.tex,連想システムのための概念ベース構成法 --属性信頼度の考え方に基づく属性重みの決定,A Method of a Concept-base Construction for an Association System: Deciding Attribute Weights Based on the Degree of Attribute Reliability,"自然言語の意味を理解するコンピュータの実現には，入力された語から関連の
強い語を導き出す連想システムが必要と考える．本研究の目的はこのような連
想システムの主要要素である概念ベースの構築である．我々の開発した連想シ
ステムは電子化辞書から作られた概念ベースと，語間の関係の深さを定量化す
る関連度計算アルゴリズムから構成される．概念ベースでは語の意味を語の持
つ意味の特徴を表す語（属性）とその語に対する重要性を表す重みの集合で定
義している．本研究においては，概念を概念ベースによって定義される語の連
鎖としてモデル化している．機械構築された最初の概念ベースは不適切な属性
が多く，重みの信頼性も低い．本稿ではこの機械構築された概念ベースを出発
点とし，雑音属性を除去し，より適切な重みを付与するために，属性信頼度の
考えに基づく新しい精錬を提案している．さらに，人間の感覚による評価とテ
ストデータの関連度を用いた実験によって提案方式の有効性を示した．","To realize computers understanding natural language needs
an association-system which outputs words strongly related to input
words. This study aims to construct a concept-base which is a main
element of the association-system.  In the concept-base, the meaning
of a word is defined by a set of an attribute expressing the feature
of a word and the weight representing the importance to the word.  In
our study, we model concepts as a chain of words defined by the
concept-base. The first concept-base automatically constructed
contains not a few unsuitable attributes and, therefore, the
reliability of weights is also questionable.  Making the automatically
constructed concept-base a starting point, we are aiming to achieve a
new refining method based on the reliability of attributes so that
noises will be removed and more appropriate weight will be gained.
Moreover, this paper shows effects of the proposed method by
presenting an evaluation by human senses and an experiment that
utilizes the degree of association in test data.","['はじめに', '概念の定義と概念ベース', '関連度の定義', '概念ベースの精錬', '評価実験と考察', 'おわりに']",,,,,,,,
V09N05-06.tex,日中機械翻訳におけるとりたて表現の翻訳について \\ --「も」，「さえ」，「でも」--,"A Translation Method of Expressions Containing \\ the ``Toritate'' Words ``mo, sae, demo'' \\ in Japanese-Chinese Machine Translation","「も，さえ，でも$\cdots$」などのとりたて詞による表現は日本語の機能語の中でも特有な一族である．その意味上と構文上の多様さのために，更に中国語との対応関係の複雑さのために，日中機械翻訳において，曖昧さを引き起こしやすい．現在の日中市販翻訳ソフトでは，とりたて表現に起因する誤訳（訳語選択，語順）が多く見られる．本論文では，とりたて詞により取り立てられる部分と述語部の統語的，意味的な特徴，更に中国語側での取り立てられる部分の統語的意味的な特徴によって，とりたて詞の意味の曖昧さを解消する手順を提案した．また，とりたて詞に対応する中訳語の位置について，訳語の文法上の位置に対する約束と，取り立てられる部分の中国語側での成分などから特定する手順を提案した．またこれらの手順を，「も，さえ，でも」の三つのとりたて詞をそれぞれ含む100文に対して手作業で検証した．正訳率はすべて80\,\%以上となり，本手法の有効性が示された．","Words such as ``mo'', ``sae'' and ``demo'' are particular function words in Japanese, and are known as “toritate” words. They have a variety of syntactic and semantic uses, and complicated corresponding relations to Chinese leads to ambiguities in Japanese-Chinese machine translation. Thus the use of current commercially available machine translation software results in numerous mistranslations of these words, in terms of vocabulary selected and word order determination. In this paper, we propose a method for disambiguating the meaning of expressions containing the ``toritate'' words ``mo'', ``sae'' and ``demo'' by referring to the following syntactic and semantic features: (1) the features of the scope of the ``toritate'' word (it may be NP or VP), (2) the features of the predicate that are related to the ``toritate'' word, and (3) the features of the corresponding Chinese word for the Japanese scope of the ``toritate'' word. The positions of these ``toritate'' words in Chinese are determined according to their syntactic rules as well as the grammatical role of the scope of the ``toritate'' words in Chinese. We evaluated our translation algorithm manually using 100 example sentences each for ``mo'', ``sae'' and ``demo''. The translation accuracy for each of these words was over 80\,\%, indicating that our method provides a more accurate Japanese-Chinese translation than currently commercially available translation software.","['はじめに', 'とりたて表現の特徴と中国語との対応関係', '取立て表現の中国語への翻訳', '3つのとりたて詞の翻訳手順', '翻訳規則の評価と問題', 'おわりに']",,,,,,,,
V09N05-07.tex,自然言語処理技術を用いた大会プログラム作成支援について,Supporting Conference Program Production Using Natural Language Processing Technologies,"2000年言語処理学会第\ 6回年次大会プログラムの作成において，
言語処理技術を適用し，大会プログラムを自動作成することを
試みた．
本稿では，第\ 5回大会のデータを利用して，大会プログラム作成のために
行なった一連の実験について説明する．その結果に基づき，実際に第\ 6回の
大会プログラムを作成した手続きについて報告する．大会プログラム作成に
キーワード抽出および文書分類の言語処理技術は十分に利用でき，事務手続き
の効率化に貢献できることを報告する．また，大会終了後のアンケート
調査の結果を示し，参加者からの評価についても報告する．","We applied natural language processing technologies
to automatically produce a program for the sixth annual meeting of the
Association for Natural Language Processing. In this paper, we 
describe experiments used to automatically generate 
the program using the fifth annual meeting data. 
We produce the sixth annual meeting program on the basis of the experiments. 
We report the process of making the sixth annual meeting program 
in practice and show to what extent the natural language processing 
technologies are efficient for this task. Furthermore, we show the results 
of a questionnaire targeting the participants of the sixth annual meeting.","['はじめに', '大会プログラム作成実験', '第\\ 6回大会プログラム作成', 'アンケート結果', '考察', 'おわりに']",,,,,,,,
V10N01-01.tex,日本語固有表現抽出の難易度を示す指標の提案と評価,Analysis on Difficulty Indices \\ for Japanese Named Entity Task,"本論文では，固有表現抽出の難易度をテストコーパスから評価
する指標を提案する．固有表現抽出システムの性能は客観的な指標
によって評価される．しかし，システムの出力に対する評価だけで
は，あるコーパスに対する固有表現抽出がどのように難しいのか，
どのような情報がそのコーパスに対して固有表現抽出を行なう際に
有効なのかを知ることは難しい．本論文で提案する指標は，個々の
システムの出力に依存することなく，複数のコーパスについて統一
的に適用できる．指標の有効性は固有表現抽出システムの性能評価
と比較することで検証される．さらに固有表現のクラス間における
難易度の比較や，有用な情報の違いについても議論する．","We propose indices to measure the difficulty of the named entity (NE)
task by looking at test corpora, based on expressions inside and outside
the NEs.  These indices are intended to estimate the difficulty of each
task without actually using an NE system and to be unbiased towards a
specific system.  The values of the indices are compared with the
systems' performance in Japanese documents. We also discuss the
difference between NE classes with the indices and show useful clues
which will make it easier to recognize NEs.","['はじめに', '\\label{section:FT', '\\label{section:TI', '\\label{section:CW', '個々のシステムとの相関', '結論']",,,,,,,,
V10N01-02.tex,出現頻度と連接頻度に基づく専門用語抽出,Term Extraction Based on Occurrence and Concatenation Frequency,"本論文では，専門用語を専門分野コーパスから自動抽出する方法の提案と実験的評価を報告する．本論文では名詞(単名詞と複合名詞)を対象として専門用語抽出について検討する．基本的アイデアは，単名詞のバイグラムから得られる単名詞の統計量を利用するという点である．より具体的に言えば，ある単名詞が複合名詞を形成するために連接する名詞の頻度を用いる．この頻度を利用した数種類の複合名詞スコア付け法を提案する．NTCIR1 TMREC テストコレクションによって提案方法を実験的に評価した．この結果，スコアの上位の1,400用語候補以内，ならびに，12,000用語候補以上においては，単名詞バイグラムの統計に基づく提案手法が優れていることがわかった．","In this paper, we propose a new idea of automatically recognizing
domain specific terms from monolingual corpus. The majority of domain
specific terms are compound nouns that we aim at extracting. Our idea
is based on single-noun statistics calculated with single-noun
bigrams. Namely we focus on how many nouns adjoin the noun in question
to form compound nouns. In addition, we combine this measure and
frequency of each compound nouns and single-nouns, which we call FLR
method. We experimentally evaluate these methods on NTCIR1 TMREC test
collection. As the results, when we take into account less than 1,400
 or more than 12,000 highest term candidates, FLR method performs best.","['はじめに', '用語抽出技術の背景', '単名詞の連接統計情報の一般化', '実験および評価', 'おわりに']",,,,,,,,
V10N01-03.tex,類似学習例の除外とRocchioフィードバックを 弱学習アルゴリズムとするAdaBoostによる レレバンスフィードバックの精度向上,Removing Similar Documents from Training Samples and Applying Rocchio feedback as week learner of AdaBoost for Improving Relevance Feedback,"レレバンスフィードバックは検索者が与えた検索条件を利用してシステムが選択する文書(サンプル文書)について，検索者が必要文書と不要文書を選択し，フィードバックすることで，より正確な文書検索を実現する手法である．
レレバンスフィードバックによる検索精度はフィードバックの対象となるサンプル文書の選択方法によって異なる．通常のレレバンスフィードバックでは検索要求との関連が最も強いと推定される文書をサンプルとするレレバンスサンプリングが用いられるが，これに対して必要文書か不要文書かを分類するのが難しい文書をサンプルとするuncertaintyサンプリングが提案され，より高い検索精度が得られると報告されている．
しかしいずれのサンプリング手法も複数の類似した文書をサンプルとして選択することがあるため，検索精度が十分に向上しない恐れがあった．
本稿ではレレバンスサンプリングおよびuncertaintyサンプリングを改良する手段として
unfamiliarサンプリングを提案する．unfamiliarサンプリングは既存のサンプリング手法において，新たにサンプルとして加える候補と既存のサンプルの文書間距離を評価し，既存サンプルの最近傍であればサンプルから排除する．
この処理により，既存サンプルと類似した文書が排除されることにより検索精度が向上される．レレバンスフィードバックを用いた文書検索においては，少数のサンプル文書で高い精度を得ることが重要になる．本稿ではAdaBoostにおいてRocchioフィードバックを弱学習アルゴリズムとして用いる手法を提案し，これをRocchio-Boostと呼ぶ．
NPLテストコレクションを用いた実験の結果，unfamiliarサンプリングによる
サンプリング手法の改良とRocchio-Boostにより従来のRocchioフィードバック
とレレバンスサンプリングに対して平均適合率を6\,\%程度向上できることが分
かった．","Relevance feedback is a method to achieve accurate information retrieval
through the application of both evaluated relevant and irrelevant sample documents which are chosen based on an initial query by the system.
Retrieval accuracy achieved by relevance feedback changes according to
the sample selection methodology for user judgement. Relevance sampling, a method which is often utilized for choosing samples, asks users to label the sample documents which are classified as most likely to be relevant. On the other hand, uncertainty sampling, a method which selects samples according to an unclear classification system, has been reported to be more effective than the original sampling method. However, both sampling methods may select multiple numbers of similar documents, and thus in both cases, the retrieval accuracy should be improved. 
In this paper, we propose `unfamiliar sampling', a method which evaluates the distance between each pair of documents and removes a candidate for sampling if it is the nearest neighbor of any sample that has already been selected. With this procedure, not only multiple numbers of similar documents are not used for sampling, but, moreover, retrieval accuracy improves.
 In applying relevance feedback for document retrieval, it is important to achieve high retrieval accuracy with a small number of sample documents. 
Also, in this paper, we propose `Rocchio-Boost', which applies Rocchio feedback as a weekly learner of AdaBoost. Furthermore, we show that it can achieve high retrieval accuracy. Empirical results on NPL test collection show that the proposed methods improve the average precision of retrieval by 6\% over the original relevance sampling and Rocchio feedback.","['はじめに', '従来のレレバンスフィードバック技術', 'Rocchio-Boost', '既存のサンプリング手法', 'unfamiliarサンプリング', '実験', '実験結果', 'まとめ']",,,,,,,,
V10N01-04.tex,情報検索のための表記の揺れに寛容な類似尺度,An IR Similarity Measure which is Tolerant for Morphological Variation,"本論文では，情報検索のための表記の揺れに寛容な類似尺度を提案する． 
情報検索において，検索対象となるデータがさまざまな人によって記述されたものであ
るため，同じ事柄であっても表記が異なり，入力した文字列で意図した情報を得る
ことができない場合がある．人間ならば，表記が多少異なっていて（表記の揺れ
があって）も柔軟に対応し，
一致していると判断できるが，計算機はこの柔軟性を備えていない． 
表記の揺れに対応することができる尺度として編集距離が知られているが，
実際にこの尺度を単純に類似尺度に変換したものを用いて情報検索を
行ってみたが，性能がでなかった．そこで，本論文では，この単純な類似尺度を
情報検索に適した表記の揺れに寛容な類似尺度に拡張することを試み，
その結果，この拡張によって検索性能が向上したことを示す．
さらに，提案する類似尺度を組み込んだ情報検索システムを構築し，
多くの情報検索システムに用いられている一般的な類似尺度と
同等以上の検索性能を実現できたことを示す．","In this paper, we propose a measure for information retrieval (IR). 
This measure is tolerant for morphological variation. 
When various persons describe the data to retrieve, their notations may 
vary even if the data describe the same topic. 
This variation prevents system to retrieve all of relevant documents for 
 the input sentence. 
Although human can handle this variation, computers usually can not
 handle this. 
Edit distance is a well-known measure that can
cope with this variation. 
We have used this measure for information retrieval and found that 
its precision is poor. 
Therefore, we propose to modify this similarity measure to be suitable 
for information retrieval. 
We show that this extension improves the performance. 
We also compared the proposed similarity measure with the popular 
similarity measures used in many information retrieval systems.","['はじめに', '編集距離の類似尺度への変換とその拡張', '本論文で用いる重み', '実験の概要', '編集類似度の性質および検索性能の検証', '基本的な類似尺度との検索性能比較', '考察', 'まとめ']",,,,,,,,
V10N01-05.tex,機械学習による複数文書からの重要文抽出,Machine Learning Approach to Multi-Document Summarization,"近年，インターネットや大容量の磁気デバイスの普及によって，大量の電子化文
書が氾濫している．こうした状況を背景として，文書要約技術に対する期待
が高まってきている．
特に，ある話題に関連する一連の文書集合をまとめて要約することが可能とな
れば，人間の
負担を大きく軽減することができる．
そこで本稿では，特定の話題に直接関連する文書集合を対象とし，
機械学習手法を用いることによって重要文を抽出する手法を提案する．重要文
抽出の手法としては近年，自
然言語処理研究の分野でも注目されている機械学習手法の1種である Support
Vector Machine を用いた手法を提案する．
毎日新聞99年1年分より選んだ
12話題の文書集合を用意し，それぞれの話題から総文数の10\,\%，30\,\%，
50\,\%の要約率に応じて人手により重要文を抽出した正解データセットを異なる
被験者により3種作成した．
このデータセットを用いて評価実験を行った結果，提案手法の重要文抽出精度
は，Lead手法，TF$\cdot$IDF手法よりも高いことがわかった．また，従来
より複数文書要約に有効とされる冗長性の削減が，文を単位とした場合には，
必ずしも有効でないこともわかった．","Due to the rapid growth of the Internet and the emergence of low-price
and large-capacity storage devices, the number of online documents is 
exploding.
Automatic summarization is the key handling this situation.
The cost of manual work demands that we be able to summarize a 
document set related to a certain event.
This paper proposes a method of extracting important sentences from 
document sets.
The method is based on Support Vector Machines, a technology that is 
attracting attention in the field of natural language processing.
We conducted experiments using three document sets formed from twelve 
events published in the MAINICHI newspaper of 1999.
These sets were manually processed by newspaper editors.
Tests using this corpus show that our method has better performance 
than either the Lead-based method or the TF$\cdot$IDF method.
Moreover, we clarify that reducing redundancy is not always effective 
for extracting important sentences from a set of multiple documents 
taken from a single source.","['はじめに', '対象とする複数文書', 'Support Vector Machine に基づく複数文書からの重要文抽出手法', '評価実験', '冗長性削減の有効性', 'まとめ']",,,,,,,,
V10N01-06.tex,サポートベクトルマシンを用いた中国語解析実験,Performance Evaluation of Chinese Analyzers with Support Vector Machines,"現在入手可能な解析器と言語資源を用いて中国語解析を行った場合にどの程
度の精度が得られるかを報告する．解析器としては，サポートベクトルマシン
(Support Vector Machine) を用いた YamCha を使用し，中国語構
文木コーパスとしては，最も一般的な Penn Chinese Treebank を使用した．
この両者を組み合わせて，形態素解析と基本句同定解析 (base phrase
chunking) の2種類の解析実験を行った．形態素解析実験の際には，
一般公開されている統計的モデルに基づく形態素解析器 MOZ との比較実験も行っ
た．この結果，YamChaによる形態素解析精度は約88\%でMOZよりも4\%以上高いが，
実用的には計算時間に問題があることが分かった．また基本句同定解析精度は約
93\%であった．","We will report performances of currently and publicly available Chinese
analyzers and resources.  We use YamCha, a tool based on Support Vector
Machines, and the Penn Chinese Treebank as a language resource.
Combining these two, we measure the performances of Chinese analysis,
i.e., word segmentation, part-of-speech tagging, and base phrase
chunking.  In the experiment of word segmentation and part-of-speech
tagging, we also report the performance of MOZ, a statistical
morphological analyzer, which is also available to the public.  We found
that the accuracy of morphological analysis using YamCha attains around
88\%, which is over 4\% higher than that of MOZ, although it is
computationally very expensive.  We also found that the accuracy for
base phrase chunking is approximately 93\%.","['はじめに', '中国語解析のための言語資源と解析環境', 'YamCha による形態素解析', 'YamChaによる基本句同定解析', 'まとめ', 'CTB のタグセット', '本報告で用いた言語情報資源']",,,,,,,,
V10N02-01.tex,日本語レシピ文における時間的関係構造の自動生成,Automatic Generation of Event Structure for Japanese Cooking Recipes,"本稿の目的は日本語の料理レシピ文における各事象の時間構造を特定
し，隣接する事象間の時間関係を明確化することである．レシピ文は時間に沿った
作業のシーケンスを述べたものであり，事象間の時間関係を示す典型でありながら，
常識を排除して機械的に文章を読むと時間関係の復元が困難である問題があげられ
る．本研究の試みはアスペクト，すなわち各事象の時間的側面に着目し，そこから
文章全体の時間関係を再構築することである．本稿ではイベント構造の概念を用い
たアスペクト理論を用いることにより，アスペクトクラスを達成相，完成相，完了
相，進行相の4つの型に分類する．さらに事象の隣接関係を明確化するために完成
相，完了相の細分化を試みる．この細分化により進行や完了の関係，並行動作関係，
終了時や開始時の前後動作関係を解析することが可能となった．またアスペクトを
補助する情報として副詞句，省略動作，並行関係に着目し，事象の時間的な隣接関
係を簡潔に表現することによって，文章全体の時間的な意味を限定した．以上の結
果に基づき，料理レシピ文における時間的関係構造の自動生成システムを設計した．","The objective of this paper is to analyze the temporal
structure of a sequence of sentences. As the target of this analysis, we
consider cooking recipes. The recipes are considered to be typical
examples that prescribe the temporal relations of affairs. However, it
seems very difficult to understand the temporal relations without our
common knowledge. In order to do this analysis, we utilize aspectual
information of each activity. We reclassify aspects, considering the
affairs which are specific to cuisine, and define subclasses of perfective
aspect. In addition, to enhance the adequacy of analysis, we consider
adverbial information, elliptical expressions, and concurrent
operations. we design and implement the automatic generation system of
`time map' for the cooking recipes.","['はじめに', 'アスペクト理論', '日本語レシピ文における時間的関係構造の提案モデル', '時間構造の自動生成システム', 'おわりに']",,,,,,,,
V10N02-02.tex,表層表現に着目した自由回答アンケートの\\意図に基づく自動分類,"Classification of Open-Ended Questionnaire \\Texts based on
Surface Expressions","自由記述形式のアンケート調査の回答は，選択型回答のアンケー
トと異なり，回答者の自由な意見を集約できる効果があるため社会的にも注目
されている．アンケート調査（質問紙調査法）について研究されてきた社会学・
心理学の分野では，アンケートの回答分類はコーディングと呼ばれ，選択型回
答・自由回答ともに人手で分析・分類されることが多い．特に自由回答のコー
ディングには多大なコストがかかるうえに，人の判断による作業は主観的な分
類結果を招くという懸念もある．このような背景から，本研究では言語処理の
要素技術であるテキスト分類の技術を取り入れアンケート回答の自動分類を行
うことで，その結果を自由回答のコーディングに活用するためのコーディング
支援を試みた．テキストの分類には，学習アルゴリズムのひとつである最大エ
ントロピー法を用いている．分類にあたり，まずはテキストへのタグ付与実験
をもとに意図タグの決定を行った．これらの意図タグを付与した意図タグ付き
正解データを作成し，このデータを訓練データとしてN-gram抽出を行い，各タ
グに特徴的な表現を取り出した．この表現を素性とし，訓練データに対して最
大エントロピー法を用いて学習し，分類を行った結果，約8割弱の分類精度が
得られた．この手法によって，自由回答テキストに対して回答者の意図を反映
した分類を行うことができた．これにより，回答を一件ずつ読みながら類似の
内容を持つ回答を探すという，自由回答の人手による分類コストを軽減するこ
とができた．また，辞書を用いる形態素解析を使わずに，最大エントロピー法
による素性と意図タグの学習を行うことで，「です」「ません」「べき」「必
要」「図る」「化」など断片的な情報が意図タグ付与に効果的であることが明
らかになった．","While the open-ended questionnaire method is a good means
to collect free expressions of opinion, the analysis of collected
questionnaires is usually done manually, and thus is
costly. Furthermore, the results derived from such humans' judgments
tend to lack objectivity.  Given this background, we are exploring
computational approaches to the automatic classification of collected
open-ended questionnaires. This paper reports the results of our
preliminary experiments, where we used the maximum-entropy model for
questionnaire classification. The results show that our method works
well for extracting discriminative linguistic expressions for each
response type such as proposal, demand, approval, opposition, etc.,
and can produce questionnaire clusters analogous to those produced by
humans.","['はじめに', '自由回答とは', '意図タグの作成', '最大エントロピー法（ME法）を用いた学習および分類実験', '考察', 'おわりに']",,,,,,,,
V10N02-03.tex,``名詞Aのような名詞B''表現の比喩性判定モデル,The Metaphorical Judgment Model for ``Noun B like Noun A'' Expressions,"我々は文章中に現れる比喩表現，その中でも直喩・隠喩的な比喩について，その認識・抽出を目的として研究を進めている．
本論文では，``名詞Aのような名詞B''表現について，名詞の意味情報を用いたパターン分類によって比喩性を判定し，比喩表現については喩詞（喩えるもの）と被喩詞（喩えられるもの）とを正確に抽出できるモデルを提案する．
この表現には比喩（直喩）とリテラル（例示など）の2つの用法があり，また比喩であっても名詞Bが被喩詞ではない場合がある．
我々はそれらを機械的に判定するために，これまでに行ってきた構文パターンやシソーラスを用いて喩詞と被喩詞の候補を抽出する手法を発展させ，名詞Aと名詞Bの意味情報やその関係に従って``名詞Aのような名詞B''表現を6つのパターンに分類し，比喩性を判定し喩詞と被喩詞を特定するモデルを構築した．
このモデルを日本語語彙大系の意味情報を利用して実装し，新聞記事データを用いて検証したところ，得られたパターン分類結果（比喩性判定結果）と人間のそれとが一致する割合は，学習データについては82.9\,\%（未知語データを除く），評価用データについては72.7\,\%（同）であり，比喩性判定モデルの全体的な処理の流れは実際の文章中の比喩表現認識に有効であることを示した．
また，比喩語という比喩性を決定づける語についてもその効果を示すことができ，モデルへの組み込みの可能性を示唆した．","We have been studying for the automatic recognition and extraction of metaphor expressions in practical sentences.
This paper introduces our metaphorical judgment model for ``Noun B like Noun A'' expressions.
``Noun B like Noun A'' expressions are classified into two usages; simile and literality.
To automatically judge whether a phrase is simile or literality, ``Noun B like Noun A'' expressions were classified into six patterns depending on the semantic information of a noun, and the metaphorical judgment model was constructed based on these patterns.
When the ``Noun B like Noun A'' expressions from newspaper articles were judged by the model, and its judgment was compared with the correct judgment, it was approximately 80\,\% correct.
Thus, the model was found to be effective in the recognition of metaphor expressions in real-life situations.","['はじめに', ""``名詞Aのような名詞B''表現のパターン分類"", '比喩性判定モデルの提案', 'コーパスでの検証', '比喩語の存在', 'おわりに']",,,,,,,,
V10N02-04.tex,古典の総索引からの品詞タグ付きコーパスの作成,"A Transformation of Concordance Data on
\\Japanese Classics
to Corpus Tagged with\\Part-of-Speech","全単語の出現箇所を与える総索引は日本の古典の研究の補助として用いられている．品詞タグ付きコーパスはコンピュータを用いた自然語研究の手段として重要である．しかし日本語古典文に関する品詞タグ付きコーパスは公開されていない．
そこで総索引を品詞タグ付きコーパスに変換する方法を検討した．使用した総索引は本文編と索引編とから成り，後者は単語の仮名／漢字表記・品詞情報を見出しとし，その単語の本文での出現行番号のリストを与える．変換機能には活用表の知識のみを保持した．ある単語の部分文字列が他の単語の表記と一致し，両者が同一行に出現することがあり得る問題に対し，一種の最長一致法を用いた．索引の見出しの漢字表記が送り仮名等の仮名文字を含まないため，照合条件を緩める先読み法を用いた．照合失敗部や索引自体の誤りへの対処のため，変換の不完全部分を示す印を出力し人手で検査・修正した．
以上の結果，約15万単語の古典文の品詞タグ付きコーパスを得た．","A `sou-sakuin' is a kind of concordance, which gives an alphabetical list of all words used in a book
 and shows all positions where each word can be found.
It is useful as a tool for researching Japanese classics. 
A corpus with part-of-speech tags, 
which gives a collection of sentences  
and their part-of-speech data, 
is useful as a tool for natural language processing.  
 However, there is no such corpus for Japanese classics.
 Thus, we try to transform `sou-sakuins' 
 into corpora with part-of-speech tags. 
 Each `sou-sakuin' we used consists of two parts:
 a text part and an index part. 
 The index part consists of records,
 each of which has a headword 
 (Kana-string, Kanji-string and part-of-speech data on each word)
 and an inverted list, which gives line numbers of text part 
 where the word is found.
 In transformation program, we only use inflection tables 
 for inflective words. 
 We adopt a kind of longest-match method 
 to resolve the problem of occurring of two or more words in a same text line,
 one of which is sub-string of another word.
 We also adopt a kind of look-ahead method for
 the headword's Kanji string which consists of only Kanji characters
 even if the corresponding text string of the word consists
 of Kanji and Kana characters. 
As a result, we got corpora on Japanese classics having about 150,000 words.","['はじめに', '総索引と品詞タグ付きコーパス', '総索引から品詞タグ付きコーパスへの変換', '変換結果と検討', 'むすび']",,,,,,,,
V10N02-05.tex,Lexical Functional Grammarに基づく 実用的な日本語解析システムの構築,Constructing a practical Japanese Parser based on Lexical Functional Grammar,"本稿では，Lexical Functional Grammar (LFG)に基づいた実用的な日本語文解析
システム構築に向けての日本語LFG文法記述の詳細とシステムの評価について述
べる．本稿で述べる日本語LFG文法は，(1)解析対象が口語的・非文法的文であっ
ても解析可能な高いカバー率を持つ，(2)言語学的に精緻な文法規則を持ち豊富
な意味情報を含\break
むf-structureを出力可能とする，(3)f-structureの持つ言語普
遍性の特徴を活かすため他言語のLFG文法と高い整合性・無矛盾性を保つ，の3点
を特徴とする．自然言語の文法記述を完全に体系的・手続き的に進めることは困
難であり，本稿で述べる文法記述においても経験的なものに依存する面は大きい．
しかしながら，OTマークを利用して段階的に解析を行う手法によって，例外的な
文法・語彙規則が解析結果に及ぼす悪影響を減じ，文法の大規模化に伴う記述の
見通しの悪さを軽減することが可能となった．さらに，部分解析機能の導入によっ
て，口語的・非文法的文への対処が可能となった．マニュアル文のような文法に
則った文と，お客様相談センター文のような口語的な文の両者を対象に解析実験
を行い，日本語LFGに基づくシステムとしてはこれまでにない，95\,\%以上の解析カ
バー率が得られていることを確認した．また，マニュアル文を対象に解析精度測
定のための評価実験を行い，係り受けの再現率・適合率共に平均値で約84\,\%，上限
値で約92\,\%の値が得られていることが確認できた．","This paper describes a Japanese computational grammar on the basis of
Lexical Functional Grammar toward a practical parser with deep analysis
for Japanese sentences.  The Japanese grammar is characterized by (1)
broad coverage even for colloquial or ungrammatical sentences, (2)
linguistic preciseness to output f-structures with rich information, and
(3) consistency with grammars for other languages.  It is a difficult
task to develop a computational grammar systematically or in a
procedural way, and our grammar writing also depends on our own
experience.  However, a gradual analysis method using Optimality Theory
marks can prevent exceptional rules from causing unexpected analysis
results, and a fragment analysis method makes it possible to deal with
colloquial or ungrammatical sentences.  These two methods give a clear
perspective on writing the grammar.  We conducted experiments to
evaluate our parser with grammatical text (manual text) and colloquial
and ungrammatical text (“Voice of Customer” text).  The coverage was
over 95\,\%, and the predicate dependency accuracy was 84\,\%.","['はじめに', 'Lexical Functional Grammar', 'Parallel Grammar Project', 'システム構成', '日本語LFG文法記述', 'システムの評価', '今後の課題', 'おわりに']",,,,,,,,
V10N02-06.tex,単語意味属性を使用したベクトル空間法,Vector Space Model bsased on \\Semantic Attributes of Words,"従来，ベクトル空間法において，ベクトルの基底数を削減するため，ベクトル
の基軸を変換する方法が提案されている．この方法の問題点として，計算量が
多く，大規模なデータベースへの適用が困難であることが挙げられる．
これに対して，本論文では，特性ベクトルの基底として，単語の代わりに単語の
意味属性（「日本語語彙大系」で規定された約2,710種類）を使用する方法を提
案する．この方法は，意味属性間の包含関係に基づいた汎化が可能で計算コスト
もきわめて少なく，容易にベクトルの次元数を圧縮できることが期待される．ま
た，単語の表記上の揺らぎに影響されず，同義語，類義語も考慮されるため，従
来の単語を基底とする文書ベクトル空間法に比べて，検索漏れを減少させること
が期待される．
BMIR-J2の新聞記事検索（文書数約5,000件）に適用した実験結果によれば，提案
した方法は，次元数の削減に強い方法であり，検索精度をあまり落とすことなく，
文書ベクトルの基底数を300〜600程度まで削減できることが分かった．また，単語を
基底とした文書ベクトルの方法と比べて高い再現率が得られることから，キーワー
ド検索におけるＫＷ拡張と同等の効果のあることが分かった．","In order to reduce the dimension of VSM (Vector Space Model) for
information retrieval and clustering, this paper proposes a new
method, Semantic-VSM, which uses the Semantic Attribute System defined
by ``A-Japanese-Lexicon'' instead of literal words used in conventional
VSM.
The attribute system consists of a tree structure with 2,710
attributes, which includes 400 thousand literal words. Using this
attribute system, the generalization of vector elements can be
performed easily based on upper-lower relationships of semantic
attributes, so that the dimension can easily be reduced at very low
cost. Synonyms are automatically assessed through semantic attributes
to improve the recall performance of retrieval systems.
Experimental results applying it to BMIR-J2 database of 5,079
newspaper articles showed that the dimension can be reduced from 2,710
to 300 or 600 with only a small degradation in performance. High
recall performance was also shown compared with conventional VSM.","['はじめに', '意味属性体系を基底とした文書ベクトル空間法', '必要最小限の意味属性の決定', '実験', '考察', '結論']",,,,,,,,
V10N02-07.tex,SVDPACKC とその語義判別問題への利用,Introduction of SVDPACKC and its application to word sense disambiguation problems,"本論文ではフリーの特異値分解ツール SVDPACKC を紹介する．
その利用方法を解説し，利用事例として語義判別問題を扱う．
近年，情報検索では潜在的意味インデキシング（Latent Semantic Indexing，LSI）
が活発に研究されている．
LSI では高次元の索引語ベクトルを
低次元の潜在的な概念のベクトルに射影することで，
ベクトル空間モデルの問題点である同義語や多義語の問題に対処する．
そして概念のベクトルを構築するために，索引語文書行列に対して特異値分解を行う．
SVDPACKC は索引語文書行列のような高次元かつスパースな行列に対して
特異値分解を行うツールである．
また LSI は，高次元の特徴ベクトルを重要度の高い低次元のベクトルに
圧縮する技術であり，情報検索以外にも様々な応用が期待される．
ここでは SVDPACKC の利用事例として語義判別問題を取り上げる．
SENSEVAL2 の辞書タスクの動詞 50 単語を対象に実験を行った．
LSI に交差検定を合わせて用いることで，最近傍法の精度を向上させることができた．
また最近傍法をベースとした手法は，一部の単語に対して
決定リストや Naive Bayes 以上の正解率が得られることも確認できた．","In this paper, we introduce a free software package SVDPACKC
computing the singular value decomposition (SVD) of large sparse matrics.
First we explain how to use it, and then
solve word sense disambiguation problems by using it.
In information retrieval domain, Latent Semantic Indexing (LSI) has
actively been researched.
LSI maps a high dimensional term vector to the low dimensional concept vectors
to overcome synonymy and polysemy problems over information retrieval 
using vector space model.
To build low dimensional concept vectors
LSI computes the SVD of term-document matrics.
SVDPACKC is a software tool to computes the SVD  of large sparse matrics
like term-document matrics.
LSI compresses a high dimensional future vector
to the low dimensional concept vectors, so has many applications 
besides information retrieval.
In this paper, we attack word sense disambiguation problems of 50 verbs in Japanese dictionary 
task of SENSEVAL2.
By using cross validation and LSI, we improved simple Nearest Neighbor method (NN).
And we showed that the methods based on NN achieve better precision than 
the decision list method and Naive Bayes method for some words.","['はじめに', 'LSI と 特異値分解', '特異値分解ツール SVDPACKC', '語義判別問題への利用', '考察', 'おわりに']",,,,,,,,
V10N03-01.tex,SENSEVAL-2 日本語辞書タスク,SENSEVAL-2 Japanese Dictionary Task,"SENSEVALは語義曖昧性解消を対象としたコンテストである．
本論文では，第2回SENSEVAL (SENSEVAL-2) における
日本語辞書タスクの概要について報告する．
日本語辞書タスクでは，
語の意味の区別(曖昧性)を岩波国語辞典によって定義した．
参加者には，岩波国語辞典，訓練データ，評価データの3つが配布された．
訓練データは，3,000個の新聞記事中の単語に
正しい語義を付与したコーパスである．
一方評価データは，参加者のシステムが語義を選択するべき
単語を含んだ新聞記事である．
評価単語の種類は，名詞50，動詞50，合わせて100個である．
また各評価単語毎に100ずつ語義を選択するとしたため，
評価単語の総数は10,000である．
正解データは，評価対象となる10,000個の単語について，
二名の作業者が独立に正しい語義を付与して作成した．
この際，二者の語義が一致した割合は0.863であり，
Cohenの$\kappa$ は0.657であった．
また，二者の語義が一致しなかった場合には，第三者が正しい語義を選んだ．
日本語辞書タスクには，3団体7システムが参加した．
ベースラインシステムのスコア(正解率)が0.726であるのに対し，
一番成績の良かった参加者のシステムのスコアは0.786であった．","SENSEVAL is an evaluation exercise for word sense disambiguation
programs.
This paper describes a Japanese dictionary task
in the second SENSEVAL (SENSEVAL-2).
This task defined word senses
according to a Japanese dictionary, Iwanami Kokugo Jiten.
Three data were distributed to the participants:
the Iwanami Kokugo Jiten, the training data and the evaluation data.
The training data was an word sense tagged corpus
made up of 3,000 newspaper articles,
while the evaluation data was newspaper articles
containing words of which participants' systems should determine
correct word senses.
The number of target words was 100, 50 nouns and 50 verbs.
One hundred instances of each target word were provided,
making for a total of 10,000 instances.
For constructing a gold standard data,
two annotators chose correct word senses for 10,000 instances
separately.
The inter-tagger agreement of two annotators was 0.863,
while Cohen's $\kappa$ was 0.657.
When word senses selected by two annotators didn't agree,
the third annotator chose the correct sense between them.
7 systems of 3 organizations participated
in a Japanese dictionary task.
The best score achieved by participants' systems was 0.786,
while the score of the baseline system was 0.726.","['はじめに', 'データ', '正解データの作成', 'コンテスト', 'おわりに', '評価単語', '1つのシステムだけが不正解となる事例', '一致率が高くシステムのスコアが低い単語の例']",,,,,,,,
V10N03-02.tex,,SENSEVAL-2 Japanese Translation Task,,This paper describes the {\sc Senseval-2,"['Introduction', 'Construction of the Translation Memory', 'Gold Standard Test Data and Evaluation of the Translations', 'Results', 'Conclusion']",,,,,,,,
V10N03-03.tex,文脈素性のベクタ空間モデルを用いた日英翻訳選択 {\large --- S{\normalsize ENSEVAL,Japanese-English Translation Selection Using Vector Space Model,{\sc Senseval,In the {\sc Senseval,"['はじめに', 'S{\\normalsize\\bf ENSEVAL', '文脈素性ベクタを用いた翻訳選択', 'S{\\normalsize\\bf ENSEVAL', '文脈素性種別と翻訳選択性能との関係', 'まとめ']",,,,,,,,
V10N03-04.tex,EM アルゴリズムを用いた教師なし学習の 日本語翻訳タスクへの適用,"Application of unsupervised learning using EM\\ 
algorithm to Japanese Translation Task","本論文では，Nigam らによって提案された EM アルゴリズムを利用した教師なし学習の手法を，
SENSEVAL2 の日本語翻訳タスクで出題された名詞の語義の曖昧性解消問題に適用する．
この手法は，ラベルなしデータをラベルを欠損値とする観測データ，
その観測データを発生させるモデルを Naive Bayes モデル，このモデルの未知パラメータを
ラベル\( c \)のもとで素性\( f \)が起る条件付き確率\( p(f|c) \)に設定して，
EM アルゴリズムを用いる．結果として，モデルの識別精度が向上する．
ここでは識別のための素性として，対象単語の前後数単語の原型や表記という簡易なものに設定した．
実験では，ラベル付き訓練データのみから学習した Naive Bayes の正解率が 58.2\,\%，
同データから学習した決定リストの正解率が 58.9\,\%（Ibaraki の公式成績）であったのに対し，
ラベル付き訓練データの他にラベルなし訓練データを用いた本手法では，
61.8\,\% の正解率を得た．また訓練データの一部の不具合を修正することで，
Naive Bayes の正解率を 62.3\,\% に改善できた．更に本手法によりそれを 
68.2\,\% に向上させることができた．","In this paper, we apply an unsupervised learning method using the EM
algorithm which Nigam et al. have proposed for text classification, 
to disambiguation problems involving noun meanings taken up 
in Japanese Translation Task of SENSEVAL2.
This method uses the EM algorithm, setting up hidden labels of 
unlabeled data as missing values of observational data,
the Naive Bayes model as the generating model, and 
the conditional probabilities \( p(f|c) \) (where \( f \) is a feature and \( c \) is a label)  
as parameters of the model. As the result, the learned classifier is improved.
In this study, we use only simple features for the  classification, 
which are some words surrounding a target word.
In the experiments, the precision of Naive Bayes classifier learned through
only labeled data was 58.2\,\%.  The precision of the decision list learned
through the same data was 58.9\,\%, which is the Ibaraki record in the Translation Task
contest.  
Our unsupervised learning method improved the precision to
61.8\,\% by using unlabeled data in addition to labeled data.  Furthermore, by
revising a small part of labeled data, the precision levels of the Naive Bayes
classifier and our unsupervised learning method were improved to 62.3\,\% and
68.2\,\% respectively.","['はじめに', 'Naive Bayes による多義語の曖昧性解消', 'EM アルゴリズムによる教師なし学習', '実験', '考察', 'おわりに']",,,,,,,,
V10N03-05.tex,日本語翻訳タスクへの帰納論理プログラミングの適用,Application of Inductive Logic Programming to Japanese Translation Task,"本論文では，SENSEVAL2 の日本語翻訳タスクに対して帰納論理プログラミング（Inductive Logic Programming, ILP）を適用する．
翻訳タスクは分類問題として定式化できるため，帰納学習の手法を利用して解決できる．
しかし翻訳タスクは新たに訓練データを作るのが困難という特異なタスクになっており，
単純に確率統計的な帰納学習手法を適用することはできない．
Translation Memory の例文だけ，つまり少ない訓練データのみを用いて，
どのように分類規則を学習すれば良いかが，翻訳タスク解決の 1 つの鍵である．
このために本論文では ILP を用いる．
ILP は確率統計的な帰納学習手法にはない特徴を有する．それは背景知識を容易に利用可能である点である．
背景知識とは訓練データには明示されない問題固有の知識である．
この背景知識によって訓練データが少ない場合の学習が可能となる．
ここでは ILP の実装システムとして Progol，背景知識として分類語彙表を
利用することで，翻訳タスクに対して正解率 54.0\,\% を達成した．
この値は，付加的な訓練データを用いない SENSEVAL2 参加の他システムと比較して優れている．","In this paper, we apply Inductive Logic Programming (ILP) to
Japanese Translation Task of SENSEVAL2.
Translation Task is regarded as a classification problem, and
can be solved by inductive learning methods.
However, we cannot use general statistical learning methods for this task,
because this task has the serious problem that it is hard to create training 
instances newly.
Therefore, the problem is how to learn a classifier from 
instances in Translation Memory, that is, small training data.
To overcome this problem, we use ILP
which can handle background knowledge in learning.
This is a big advantage over statistical learning methods.
Background knowledge means domain specific knowledge
which are not described in training data clearly.
Using background knowledge, we can learn rules through small training data.
In this paper, we used Progol as a ILP system, and `bunrui-goi-hyou' as background knowledge 
to achieve the precision 54.0\,\% for Translation Task.
This precision is superior to other systems in the contest which did not 
create new training instances.","['はじめに', 'ILP による多義語の曖昧性解消', '分類語彙表の利用', '実験', '考察', 'おわりに']",,,,,,,,
V10N03-06.tex,用例に基づく手法と機械学習モデルの\\組み合せによる訳語選択,Word Translation\\ by Combining an Example-Based Method and Machine Learning Models,"本論文では，機械翻訳における訳語選択の手法について述べる．
  我々のシステムは，入力文と対象単語が与えられたとき，
  翻訳メモリと呼ばれる対訳用例集合と入力文との類似度を求め，
  類似度が最大となる用例集合を用いて対象単語の訳語選択を行なう．
  類似度は，用例に基づく手法と機械学習モデルを用いて計算される．
  類似度の計算には，文字列の類似性や入力文における対象単語周辺の単語，
  入力文中の内容語とその訳語候補の対訳コーパスおよび日英の単言語コーパスに
  おける出現頻度などを考慮する．
  入力文と対象単語が与えられると，
  まず用例に基づく手法を適用し，類似した用例が見つからなかった場合に
  機械学習モデルを適用する．機械学習モデルは複数用意し，クロスバリデーション
  などにより単語毎に最適な学習モデルを選択する．
  本論文では，2001年の春に開催された単語の多義性解消のコンテスト
  第2回\sc{Senseval","We describe the method for word selection in machine translation. 
  Given an input sentence and a target word in the sentence, 
  our system first estimates the similarity 
  between the input sentence and parallel example sets called 
  ``Translation Memory.''  
  It then selects an appropriate translation of the target word 
  by using the example set with the highest similarity. 
  The similarity is calculated using an example-based method 
  and a machine learning model, which assesses the similarity 
  based on the similarity of a string, words to the left 
  and right of the target word in the input sentence, 
  frequencies of content words of the input sentence 
  and those of their translation candidates 
  in bilingual and monolingual corpora in English and Japanese. 
  Given an input sentence and a target word in the sentence,
  an example-based method is applied to them in the first step. 
  Then, if an appropriate example set is not found, a machine learning model 
  is applied to them. The most appropriate machine learning model is selected 
  for each target word from several machine learning models 
  by a certain method such as cross-validation on the training data. 
  In this paper, we show the advantage of our method 
  and also show that what kinds of information contributed 
  to improving the accuracy based on the results of the second 
  contest on word sense disambiguation, {\sc Senseval-2","['はじめに', '{\\sc Senseval', '訳語選択モデル', '実験と考察', '関連研究', 'まとめ']",,,,,,,,
V10N03-07.tex,SENSEVAL2J 辞書タスクでの CRL の取り組み\\ --- 日本語単語の多義性解消における種々の機械学習\\手法と素性の比較 ---,"CRL at Japanese dictionary-based task of\\ SENSEVAL-2\\ --- Comparison of various types of machine\\ learning methods
and features in Japanese\\ word sense disambiguation ---","本稿では，2001年に行なわれたSENSEVAL2 コンテストの
日本語辞書タスクでのわれわれの取り組みについて述べる．
われわれは機械学習手法を用いるアプローチを採用した．
この研究では数多くの機械学習手法と素性を比較検討し用いている．
コンテストには，我々は，サポートベクトルマシン法，シンプルベイズ法，
またそれらの組み合わせのシステム
二つの合計4システムを提出し，組合わせシステムが
参加システム中もっとも高い精度(0.786)を得た．
コンテストの後，シンプルベイズ法で用いていたパラメータを調節したところさらに
高い精度を得た．
現在もっとも性能の高いシステムは
二つのシンプルベイズ法を組み合わせたシステムであり，
その精度は 0.793 である．
また，本稿では素性を変更した実験もいくつか追加で行ない，
各素性の有効性，特徴を調査した．
その調査結果では文字列素性のみを用いても比較的高い精度が
得られるなどの興味深い知見が得られている．
また，関連文献も紹介し，今後の多義解消の研究のための
有益な情報を提供した．","This paper describes our work for 
the Japanese dictionary-based lexical-sample task of Senseval-2. 
In this work, 
we compared various types of machine learning methods and features. 
For the contest, 
we submitted four systems to the Japanese dictionary-based lexical-sample task of Senseval-2. 
They were i) a support vector machine method, 
ii) a simple Bayes method, 
iii) a method combining a support vector machine and simple Bayes method, and 
iv) a method combining two kinds of a support vector machine method and 
two kinds of a simple Bayes method. 
The combined methods produced the best precision (0.786) among all the systems 
submitted to the contest. 
After the contest, we tuned the parameter used in the simple Bayes method, and 
it obtained higher precision. 
The system which achieved the best precision now 
was the method combining the two simple Bayes methods 
and its precision was 0.793. 
In this paper, we discussed the results of experiments changing 
the features used and investigated 
the effectiveness and the characteristics of each feature. 
From these results, we obtained 
an interesting conclusion that 
we could obtained good precision 
when we only used string features, which 
are strings of 1-gram to 3-gram just before/after  
the analyzed morpheme. 
We also showed some related works 
that are useful for future work on word sense disambiguation.","['はじめに', '多義性解消の重要性', '問題設定', '機械学習手法', '素性(解析に用いる情報)', '実験', '関連文献', 'おわりに']",,,,,,,,
V10N04-01.tex,,Morpho-syntactic Rules for Detecting Japanese Term Variation: Establishment and Evaluation,,"In this paper, we describe a rule-based mechanism that detects Japanese 
term variations from textual corpora. The system operates on the basis 
of meta-rules that map syntactic and morpho-syntactic variants of terms 
to the original forms of terms. The framework used here has been 
successfully applied to such languages as English and French, and we 
show here that it also works well in detecting Japanese term variants, 
once we properly take into account specific characteristics of the 
Japanese language. We also discuss the potential of this work for 
IR-related applications.","['Introduction', 'General Framework for Term Variant Detection Using Fastr', 'Japanese Meta-rules', 'Experiments and Evaluations', 'Discussion']",,,,,,,,
V10N04-02.tex,音声対話システムにおける日本語自己修復の処理,Processing Japanese Self-correction\\ in Speech Dialog Systems,"音声対話システムが話し言葉に対応するためには，言い直し，助詞
落ち，倒置などの不適格性とよばれる現象に対処する必要がある．
これらの不適格性の中で特に問題となるのは，言い直しあるいは自
己修復と呼ばれている現象である．しかし，自己修復に関する既存
の手法は，自己修復を捉えるモデルと，その修正処理に問題点が
ある．本論文では，それらの問題点を改善した新しい手法を提案す
る．そして，提案手法を音声対話コーパスに適用した結果を基に，
提案手法の有効性と問題点について考察する．","Speech dialog systems need to deal with various kinds of ill-formed 
speech inputs that appear in natural human-human dialog.  
Self-correction (or repair) is a particularly problematic 
phenomenon. Although many methods of dealing with self-correction have 
been proposed, they have limitations in both detecting and correcting
this phenomenon. In this paper, we propose a new method overcoming 
ill-formedness of speech inputs.
We evaluate the proposed method using a speech dialog 
corpus and discuss its effectiveness and limitation.","['はじめに', '不適格性', 'パーザ', '自己修復の処理', '提案手法の評価と考察', 'おわりに']",,,,,,,,
V10N04-03.tex,,A Statistical Approach to Automatic Phonetic Transcription of Japanese Orthographic Words,,"We address the problem of automatically transcribing Japanese
orthographic words into symbols representing their
pronunciations. Such a function is necessary for commercial
continuous speech recognition systems since there are constant needs to create new
recognition lexica for new applications or purposes. Simple
look-up schemes are not adequate to deal with Japanese, while
methods based on morphological analysis require in-depth
linguistic knowledge and development effort. In this paper, we
propose a statistical approach which is based on an $N$-gram
language model. It is assumed that the pronunciation of a
character only depends on the previous one to two characters and
their pronunciations. Given an orthographic word, our method
outputs the most likely phonetic transcription. It is shown that
our approach provides superior performance to the public-domain
conversion tool KAKASI on ten out of twelve test sets.","['Introduction', 'Mathematical Formulation', 'Evaluations', 'Conclusions']",,,,,,,,
V10N04-04.tex,格フレームの対応付けに基づく用言の言い換え,Predicate Paraphrasing based \\ on Case Frame Alignment,"本稿では，国語辞典の見出し語を定義文の主辞で置き換えることによって用言
 の言い換えを行う方法を提案する．この際，見出し語の多義性解消，定義文中
 で主辞とともに言い換えに含むべき項の決定，用言の言い換えに伴う格パター
 ンの変換などを行う必要があり，これらを国語辞典の情報だけで行うことは不
 可能である．そこで，大規模コーパスから格フレームを学習し，見出し語と定
 義文主辞の格フレームの対応付けを行うことにより，これらの問題を解決する
 方法を考案した．220文に対する実験の結果，77\,\%の精度で日本語として妥当な
 用言の言い換えが可能であることがわかった．","This paper proposes a method of predicate paraphrasing using an ordinary
dictionary, which replaces a predicate with an equivalent word or phrase
 in its dictionary definition.  The ordinary dictionary does not contain
 sufficient information for three sub-tasks of predicate paraphrasing:
resolution of predicate sense ambiguity, extraction of the equivalent
 word or phrase from the definition, and proper transformation of case
 markers. To compensate for the insufficiency, we employ case frame
 alignment of two predicates (a headword and its equivalent predicate),
 which produces the predicate paraphrasing patterns.  The experimental
 result of paraphrasing 220 test sentences demonstrates the
 effectiveness of this method.","['はじめに', '国語辞典による言い換え', '格フレーム辞書の自動構築', '格フレームの対応付け', '実験', '考察', '先行研究', 'まとめ']",,,,,,,,
V10N04-05.tex,日本語--ウイグル語辞書の半自動作成と評価,"Semiautomatic Generation of Japanese-Uighur \\ Dictionary   
and Its Evaluation","著者らは，既存のウイグル語--日本語辞書を基にして，見出し語数約2万の
日本語--ウイグル語辞書を半自動的に作成した．
この辞書が日常よく使われる語彙をどの程度含んでいるかなどの特性を
調べるために，国立国語研究所の教育基本語彙6,104語のうちのより基本的とされている
2,071語，およびEDR日本語テキストコーパスの出現頻度上位2,056語に対し，
日本語--ウイグル語辞書の収録率を調査し，いずれについても約80\,\%の収録率
であることが分かった．未収録語について，逐一その理由を調べ，判明した種々の理由を
整理すると共に，それに基づいて未収録語を分類した．
その結果，辞書作成をする時に収録率を上げるために
注意すべき点などについていくつかの知見を得ることができた．
本論文では，それらについて述べる．","The authors have constructed semiautomatically a Japanese-Uighur dictionary 
consisting of about 20,000 items. This paper describes the process of generating 
our Japanese-Uighur dictionary from an available Uighur-Japanese one.
We have investigated the vocabulary of our dictionary and found it includes 
about 80\,\% of 2,000 high-priority words of Japanese. We have also investigated 
the reasons why each word of the remaining 20\,\% of 2,000 high-priority words is 
not included in our dictionary, and have classified the words not included to 
five groups according to the reasons we found through our investigation.","['はじめに', 'ウイグル語--日本語辞書の電子化', '日本語--ウイグル語電子辞書の自動生成', '日本語--ウイグル語辞書の翻訳システム用辞書への変換', '日本語--ウイグル語電子辞書の評価', 'まとめ']",,,,,,,,
V10N04-06.tex,サポートベクタマシンを用いた対訳表現の抽出,Extracting Word Sequence Correspondences Based on Support Vector Machines,"本論文では，機械学習の一手法であるサポートベクタマシンを用いて文対応
  付き対訳コーパスから対訳表現を抽出する手法を提案する．
  サポートベクタマシンは従来からある学習モデルに比べて汎化能力が高く過
  学習しにくいためにデータスパースネスに対して頑健であり，カーネル関数
  を用いることによって素性の依存関係を自動的に学習することができるとい
  う特徴を持つ．
  本手法では対訳モデルの素性として，対訳辞書による素性，語数による素性，
  品詞による素性，構成語による素性，近傍に出現する語による素性を使用し，
  サポートベクタマシンに基づく対訳表現の対応度を用いて対訳表現を抽出す
  る．
  既存の手法は対訳表現の対応度の計算に単語の共起関係を利用しているため
  にデータスパースネスに陥りやすく，低頻度の対訳表現の抽出は困難である
  のに対して，
  本手法は，訓練コーパスによって対訳モデルをあらかじめ学習する必要があ
  るが，一旦モデルを学習してしまえば低頻度の対訳表現でも抽出が可能であ
  るという特徴を持つ．","This paper proposes a learning and extracting method of
  bilingual word sequence correspondences from aligned parallel
  corpora based on Support Vector Machines (SVMs),
  which are robust against data sparseness because of high
  ability of generalization
  and can learn dependencies of features by using a kernel function.
  Our method learns a translation model using features such as
  translation dictionaries, the number of words, part-of-speech,
  constituent words and neighbor words,
  and extracts bilingual word sequence correspondences by using the
  correspondence level based on SVMs.
  Conventional methods cannot extract bilingual word sequence
  correspondences which appear infrequently because of data sparseness
  which is caused by correspondence levels based on word co-occurrences.
  Our method, however, can extract them
  by the model which has been already learned by training corpora.","['はじめに', 'サポートベクタマシン', 'SVM を用いた対訳表現の抽出', '実験および考察', '関連研究との比較', 'おわりに']",,,,,,,,
V10N04-07.tex,,The Dynamics of Morphemes \\in Japanese Terminology,"本論文では、日本語専門用語を構成する語基の役割を、語種（外来語と漢語・和語）
の観点から分析する。専門用語データにおいては、そして言語データでは、一般に、
常に標本中には現れていない要素が存在すると仮定せざるを得ないため、与えられ
たデータを静的に記述するだけでは十分ではない。本論文では、そこで、データに
基づき、標本量に応じた語基の変化を動的に追うために有用な、二項補間・補外の
枠組みを用いて、専門用語の構成における語種ごとの語基の役割を分析する。実際
の分析では、物理学、化学、農学、植物学、計算機科学、心理学の6分野の専門語
彙データを対象とする。分析の結果、全ての分野で、語彙量が増大すると外来語の
異なり比率が増大すること、農学を除いては、将来的に外来語の比率が漢語・和語
の比率より大きくなること、与えられたデータにおいては例外的に見えた計算機科
学がその点では農学を除く他の分野と同じ傾向を単に極端なかたちで実現している
に過ぎないこと、一方で、例外的なのは、農学分野における専門語彙構成であるこ
とが示された。","This paper quantitatively analyses the role of morphemes with respect to their
types of origin. Static quantitative analysis of a given data set is
not sufficient for this aim, as language data in general and terminological
data in particular have the specific characteristic of being ``incomplete"" in the
sense that many unseen elements are expected in the theoretical population.
Thus, the quantitative structure of morphemes in terminology should be analysed
dynamically, by observing the growth pattern of morphemes. In order to allow for
that, we use binomial interpolation and extrapolation. Results of analyses
of the terminologies of six different domains follow, revealing interesting
characteristics of the role of morphemes of different types of origin
that do not manifest themselves through static quantitative analysis.","['Introduction', 'The Terminological Data', 'Theoretical Model and the Status of Data', 'The Growth of Morphemes and the Roles of Morphemes', 'Conclusions']",,,,,,,,
V10N04-08.tex,大規模テキスト知識ベースに基づく自動質問応答 \\ ---ダイアログナビ---,Dialog Navigator : A Question Answering System \\ based on Large Text Knowledge Base,"本論文では，大規模テキスト知識ベースに基づく対話的自動質問応答システム
 「ダイアログナビ」について述べる．本システムは，2002年4月からWWW上で一
 般公開し，パーソナルコンピュータの利用者を対象としてサービスを行ってい
 る．実世界で用いられる質問応答システムにおいては，ユーザ質問の不明確さ
 や曖昧性が大きな問題となる．本システムは，「エラーが発生した」のような
 漠然とした質問について，対話的に聞き返しを行うことによってユーザが求め
 る答えにナビゲートする．聞き返しの方法としては，頻繁になされる漠然とし
 た質問に対する聞き返しの手順を記述した対話カードを用いる手法と，自動的
 に聞き返しの選択肢を編集して提示する手法を組み合わせて用いている．また，
 適切なテキストを正確に検索するために，ユーザ質問のタイプ，同義表現辞書
 や，日本語の文の係り受け関係などを利用している．","This paper describes a dialog based QA system, Dialog Navigator, which
 can answer questions based on large text knowledge base. This system is
 targeted at users of personal computers. We released the system on the
 WWW in April 2002. In real world QA systems, vagueness of questions is
 a big problem. Our system can navigate users to the desired answers
 using the following methods: asking users back with dialog cards, and
 description extraction of each retrieved text. Another feature of the
 system is that it retrieves relevant texts precisely, using question
 types, synonymous expression dictionary, and modifier-head relations in
 Japanese sentences.","['はじめに', 'ダイアログナビの構成', 'テキストの検索', 'ユーザのナビゲート', '評価', 'おわりに']",,,,,,,,
V10N04-09.tex,日中機械翻訳におけるテンス・アスペクトの処理,Handling of Tense and Aspect for Japanese-Chinese Machine Translation,本稿では，日本語のテンス・アスペクト表現を中国語に機械翻訳する手法を提案した．具体的には，日本語のテンス・アスペクト表現で主要な役割を果す「タ/ル/テイル/テイタ」を，両言語の文法特徴・共起情報，中国語述語の時間的性格を主要な手がかりとして，中国語のアスペクト助字(了/着/在/\kanji{001,"In this paper we propose a method for mechanical translation of tense and aspect expressions from Japanese into Chinese. We deal with the expressions of ‘タ/ta’, ‘ル/ru’, ‘テイル/teiru’ and ‘テイタ/teita’ that play an important role in Japanese tense and aspect expressions. Based on syntactic characteristics and co-occurring information of both Japanese and Chinese, and temporal feature of Chinese predicates, the method shows how to translate these Japanese expressions into Chinese aspectual particles such as ‘了/le’, ‘着/zhe’, ‘在/zai’, ‘\kanji{001","['はじめに', '日中両言語におけるテンス・アスペクト助辞の意味用法とその対照', '「タ/ル/テイル/テイタ」と中国語アスペクト助字の対応関係を定めるアルゴリズム', '評価', '終わりに']",,,,,,,,
V10N04-10.tex,日英新聞の記事および文を対応付けるための\\高信頼性尺度,Reliable Measures for Aligning \\ Japanese-English News Articles and Sentences,"大規模な日英対訳コーパスを作ることを目的として，1989
   年から2001年までの読売新聞とThe Daily Yomiuri とから日英記事
   対応と文対応とを得た．そのときの方法は，まず，内容が対応する
   日本語記事と英語記事とを言語横断検索により得て，次に，その対
   応付けられた日英記事中にある日本語文と英語文とをDPマッチング
   により対応付けるというものである．しかし，それにより対応付け
   られた記事対応や文対応には，間違った対応(ノイズ)が多く含まれ
   る．そのため，我々は，本稿において，そのようなノイズを避けて，
   正しい対応のみを得るための信頼性の高い尺度を提案し，その信頼
   性の評価をした．実験の結果，我々の提案した尺度を用いることに
   より，良質な記事対応や文対応が得られることがわかった．また，
   その数は，良質な記事対応は約4万7千であり，文対応は，1対1対応
   が約15万，1対1対応以外が約3万8千であった．これらは，現時点で
   一般に利用できる日英2言語コーパスとしては最大のものである．","We have aligned Japanese and English news articles
   and sentences, extracted from the Yomiuri and the Daily
   Yomiuri newspapers,  to make a large parallel corpus. We
   first used a method based on cross-language information
   retrieval to align the Japanese and English articles and then
   used a method based on dynamic programming (DP) matching to
   align the Japanese and English sentences  in these
   articles. However, the articles and sentences  included many
   incorrect alignments. To remove these, we propose two
   measures that evaluate the validity of the alignments.  Using
   these measures, we successfully extracted a valid
   correspondence of about 47 thousands article pairs, 150
   thousands 1-to-1 sentence pairs, and 38 thousands 1-to-many
   sentence pairs. We were therefore able to build the largest
   Japanese-English parallel corpus available to the public.","['はじめに', '対応付けに用いた日英新聞記事', '対応付けの方針', '記事対応付けの方法', '文対応付けの方法', '記事対応尺度と文対応尺度', '記事対応付けの精度', '記事対応付けの精度向上の可能性', '文対応付けの精度', '関連研究', 'データ公開', '今後の課題', 'おわりに']",,,,,,,,
V10N05-01.tex,自然言語の構文解析のためのLR解析表の圧縮法,A Method of LR Table Compaction \\ for Natural Language Processing,"LR構文解析法で利用するLR解析表のサイズを削減する新規の手法を提案する．
  提案法は，(1)従来のLR表縮小方法と同時に適用可能，(2)提案法によって作
  成されたLR表は従来のLR構文解析アルゴリズムでほぼそのまま利用可能，
  (3)解析結果や解析効率に影響を与えない，といった特徴を持つ．提案法を
  実際の自然言語処理用文法に適用したところ，元の文法のサイズによって，
  約60\,\%程度から，25\,\%程度まで，LR表が圧縮されることを確認した．","This paper presents a method to reduce the size of the parsing table
  used in the LR parsing algorithm. The proposed method has the
  following significant characteristics; (1) that it can be applied along
  with any other methods for the parsing table reduction already known
  today, (2) that the parsing tables constructed by it can be used in
  the existing LR parser without modification, and (3) that it does
  not affect the parsing results and the parsing efficiency. We
  applied the method to construct the reduced LR table from some
  existing grammars used for NLP, and compared the produced LR tables
  with the tables constructed by the ordinary method. Our method
  showed that  the produced tables had the sizes of between 60\,\% and
  25\,\% of their  original sizes according to the grammars.","['はじめに', 'LR解析表の圧縮', '実装と実験', '更なる圧縮のための改良手法', '関連研究', '結論', 'LR表作成手順', 'MSLR法への適用']",,,,,,,,
V10N05-02.tex,大語彙を対象とした音声対話インタフェースにおける \\自然な応答生成,A Spoken Dialogue Interface \\through Natural and Efficient Responses,"本稿は，「思い込み応答」戦略を取り入れた大語彙音声対話インタ
フェースを提案する．この戦略は，人間同士の対話において発話対象が広範囲に
及ぶ場合，聞き間違えにくい対象と間違えやすい対象が存在することに着目した
もので，聞き間違えやすい対象を誤認識しても利用者にストレスを与えないこと
を利用している．大語彙として16万種の個人姓に焦点を当て，音声認識精度と語彙網羅率の観点から，聞き間違えてはならない10,000種の思い込み対象を選択できた．更に，思い込みが外れた場合への対応として，思い込みの結果を利用者に応答として提示している時間を利用して，思い込み範囲外の残りの姓を対象とした裏認識処理を並行して進める仕組みを提案した．市販の認識エンジンを利用して，この仕組みと思い込み応答を組み合わせた個人姓確定インタフェースを実装した．思い込み応答は，現状の音声認識技術を用いたインタフェースにおいて，入力対象が大語彙であってもストレスを与えない結果を利用者に提示できる戦略であることを確認した．","This paper proposes a new dialogue control method with ``presuppositional responses'' to realize a large number of target words towards an efficient spoken dialogue interface. This strategy comes from  human characteristics in that people tend to presuppose the utterance to be familiar or frequently-spoken. The strategy is verified through huge data of human recognition of 160,000 sir names. We introduce  heuristics to determine what words are to be presuppositional; presuppositional words should cover as many frequently-used ones as possible, while they should be small for high-accurate speech recognition. We report a successful implementation of a dialogue interface using a conventional speech recognition device. We resolve the situations when speech recognition fails or when the corrent answer is not included in presuppositional words in order not to irritate the user with unnecessary or detoured questions. Realtime and natural responses are attained through parallel search of non-frequent words as well as presuppositional ones.","['はじめに', '大語彙音声対話インタフェースの課題', '思い込み戦略', '認識エンジンを用いた思い込み対象の分析', '大語彙インタフェースの実装', 'まとめ，及び今後の課題']",,,,,,,,
V10N05-03.tex,1次元自己組織化マップを用いた\\高次元データの高速近傍検索,"Efficient Multidimensional Indexing \\Using One-dimensional
Self-Organizing Maps","高次元空間における最近傍検索(nearest neighbor search)は，
マルチメディア・コンテンツ検索，データ・マイニング，パターン認識等の
分野における重要な研究課題の1つである．高次元空間では，ある点の最近点
と最遠点との間に距離的な差が生じなくなるという現象が起こるため，
効率的な多次元インデキシング手法を設計することが極度に困難となる．
本稿では，1 次元自己組織化マップを用いた近似的最近傍検索の手法を提案し，
提案した手法の有効性を類似画像検索と文書検索の2種類の実験により評価する．
自己組織化マップを用いて，高次元空間での近傍関係をできる限り保ちつつ，
高次元データを 1 次元空間へ配置し，1 次元マップから得られる情報で探索範囲
を限定することにより，きわめて高速な最近傍検索が可能となる．","Nearest neighbor search in high dimensional spaces is an interesting
and important problem which is relevant for a wide variety of applications,
including multimedia information retrieval,
data mining, and pattern recognition.
For such applications, the curse of high dimensionality
tends to be a major obstacle in the development of efficient indexing methods.
This paper addresses the problem of designing an efficient multidimensional indexing
structure for high dimensional nearest neighbor search.
More specifically, using self-organizing maps (SOM),
high-dimensional vector data are first transformed into one-dimensional units
while preserving the higher order topology by mapping similar data items
to the same or the neighboring unit.
Then, given a query vector, only data items whose location is
close to the unit location of the query are considered
as candidates.
Experimental results indicate that our scheme scales well
even for a very large number of dimensions.","['はじめに', '自己組織化マップを用いた最近傍検索', '実験結果', 'おわりに']",,,,,,,,
V10N05-04.tex,人間による翻訳文と機械翻訳文の語彙的差異の計量分析,Quantitative Analysis of Morpholexical Difference between Human-Translated and Machine-Translated Sentences,"本稿では，ニュース記事から無作為抽出した英文を英日機械翻訳シ
ステムで翻訳した結果と，これらの英文を人間が翻訳した結果を照らし合わせ，
両者の間にどのような違いがあるのかを計量的に分析した．
その結果，次のような量的な傾向があることが明らかになった．
(1) 人間による翻訳に比べ，システムによる翻訳では，英文一文が複数の訳文に
分割されにくい傾向が見られる．
(2) システムによる翻訳と人間による翻訳の間で訳文の長さの分布に統計的有意
差が認められる．
(3) 用言の連用形と連体形の分布に有意差が認められ，システムによる翻訳の
ほうが人間による翻訳よりも複雑な構造をした文が多いことが示唆される．
(4) 体言と用言の分布には有意差は認められない． さらに，動詞と名詞に関して比較検討を行ない，システムによる翻訳を人間によ
る翻訳に近づけるために解決すべき課題をいくつか指摘した．","This paper carries out a quantitative analysis of 
morpholexical difference between machine-translated Japanese sentences 
and human-translated ones, both of which are obtained from English 
sentences selected randomly from news articles.
The analysis gives the following results.
(1) A tendency to translate one English sentence into multiple Japanese 
sentences is less observed in machine translation than in human 
translation.
(2) Significant difference exists in the distribution of the sentence 
length between machine- and human-translated sentences.
(3) Significant difference in the distribution of the adverbial form and 
the attributive form of verbs and adjectives intimates that 
machine-translated sentences have more complex syntactic structure than 
human-translated ones do.
(4) No significant difference exists in the distribution between verbs, 
adjectives and nouns. A further investigation on verbs and nouns reveals what kind 
of technical challenges must be solved to improve the quality of machine 
translation up to the extent of human translation.","['はじめに', '調査方法', '結果と考察', 'おわりに']",,,,,,,,
V10N05-05.tex,用例ベース翻訳のための対訳文の句アライメント,Phrase Alignment for Example-Based Machine Translation,"用例ベース翻訳を実現するためには，大量の用例が必要である．
本研究は，対訳文を用例として利用できるようにするために，対訳文に対して句アライメントを行なう手法を提案する．
従来の句アライメントでは，語アライメントを得てから，その情報をもとに句アライメントに拡張する手法が方式が多かった．
本手法では基本句という文節に相当する単位を導入して，基本句間のアライメントを行なう．
実験を行なった結果，良好な結果を得た．","Example-based machine translation requires a large set of translation patterns.
In this paper, we propose a phrase alignment method that aims to acquire translation patterns from bilingual sentense pairs.
Most of previous methods employ word alingnment for phrase alingnment.
This method uses the basic-phrase as the unit of phrase alingnment, and estimates alignment between basic-phrases.
The experimental results show that this method performs well.","['はじめに', '提案手法', '実験と考察', 'まとめ']",,,,,,,,
V10N05-06.tex,異なるコーパスにおける重要文抽出の結果と素性の分析,Analysis of Evaluation Results and Features of \\ Sentence Extraction on Different Corpora,"本論文では，三種類の異なるコーパスに対する我々の自動要約システムの評価と，
その要約データの分析結果について述べる．我々は重要文抽出に基いた要約シス
テムを作成し，そのシステムを用いて日本語・英語双方の新聞記事を対象にした
要約評価ワークショップに参加し，良好な評価結果を得た．また日本語の講演録
を対象として重要文抽出データを人手によって作成し，そのデータに対して要約
システムの実験・評価を行った．さらにシステムの評価結果に加えて，重要文抽
出に用いられる主な素性の振舞い・素性の組合せによる重要文の分布の違いなど
を各々の要約データにおいて分析した結果を示した．","In this paper, we report evaluation results of our summarization system
and analysis of summarization data in three different types of corpora.
We have created our summarization system based on sentence extraction,
and applied the system to summarization for Japanese and English
newspaper articles and made excellent results.  We have also created
sentence extraction data from Japanese lecture speeches and evaluated
our system to them.  Besides evaluation results of our system, we showed
analysis of relationships between key sentences and features used in
sentence extraction, and distributions of key sentences with a
combination of features among these three different types of corpora.","['はじめに', '要約データ', '重要文抽出手法の概要', '各タスクの評価結果', '要約データの分析', 'おわりに']",,,,,,,,
V10N05-07.tex,初期質問文から蓄積された質問応答への効果的マッチング法,An Effective Matching to the Stored Q\&A data \\ using Initial Questions,カスタマサービスとして，ユーザから製品の使用方法等についての質問を受けるコールセンターの需要が増している．ユーザからの質問に的確に応答するためには，次々に開発される新製品の知識が必要となる．応対するオペレータは，過酷な業務のため定着率が低く，企業にとってもレベルの高い人材を継続して維持することは，人件費や教育などのコストがかかり，問題となっている． 本研究は，ユーザが自ら問題解決できるような，対話的ナビゲーションシステムを実現する基礎技術を開発することにより，コールセンターのオペレータ業務の負荷を軽減することを目的とする．Web上での質問応答システムにおいてユーザが初期に入力する自然言語による状況説明や質問文を分析したところ，20文字以下の質問文が7割を占めていた．一方，コールセンターでは，オペレータが，過去のユーザとのやり取りの結果を，質問と応答の要約文として蓄積している．そこで，本研究では，ユーザが初期に入力する20文字前後の比較的短い質問文を対象とし，その質問文から，コールセンターで蓄積した過去の質問の要約文を引き出し，それに予め付与された応答をそのまま回答する手法を採用する．しかし，ユーザの与える20文字以下の短い質問文と蓄積された要約文との単純なマッチングでは，多数の要約文が引き出されることが多いため，システムからユーザに新たなキータームの入力を促してユーザの意図する適切な要約文に速やかに到達できるような対話的ナビゲーション技術の開発が最も重要な研究課題となっている．対話的ナビゲーションを実現するために，ユーザが初期に入力した質問文中のどのようなタームが最適な要約文の検索に重要であるかを判定する方式として，入力した質問と要約文とのマッチングが成功したものから一定の基準によってタームを変更する方式（サクセスファクタ分析方式と呼ぶ）を開発した．この分析の結果から，主辞を修飾するタームをユーザの質問文に対して対話的に補うことがマッチングの精度に大きく影響し，極めて有効なことを実験的に明らかにした．,"As a part of customer service, there has been growing demand for call centers, whose function is to answer customer questions, such as usage inquiries for purchased products. In order to provide precise answers to the customer questions, frequently updated knowledge is required for newly developed products. The stressful nature of the work for the call center operators tend to discourage them to stay long with the center, and as a result, the companies are incurring increased personnel and training expenses for maintaining a group of highly skilled operators. This paper describes the basic technology employed in our interactive navigation system, designed to allow users to solve their own problems without operator intervention, and thus minimizing the operator work at the call centers. The system is intended to guide the users to the required Q\&A data expressed in natural language, stored within the call center database, where the natural language expressions or questions entered by the users are analyzed and used as the retrieval input. As a method for evaluating the importance in the task of query construction, of each term comprising the initial input question, we have developed a new method for altering the key-terms extracted from the set of initial questions matching the stored questions. We call this method the ``success factor analysis method"". It has been shown through our experiments that the term limiting or decomposing the heads of the sentence greatly influences the search accuracy, and hence, that the actual matching accuracy is substantially improved by empirically determining the priority of the terms and supplementing the query with these terms.","['はじめに', '研究概要', '関連研究', '実験方法と評価方法', '実験と考察', 'おわりに']",,,,,,,,
V10N05-08.tex,連想概念辞書の距離情報を用いた重要文の抽出,"Evaluating a Method \\ of Extracting Important Sentences \\ using
Distance between Entries \\ in an Associative Concept Dictionary",大量の文書情報の中から必要な部分を入手するために，自動要約技術などによって文書の量を制御し，短い時間で適確に内容を把握する必要性が高くなってきている．自動要約を行なうには文書中のどの箇所が重要なのかを判断する必要があり，従来の重要文の抽出方法には単語の出現頻度にもとづいた重要語の計算方法などがある．本論では連想概念辞書における，上位/下位概念，属性概念，動作概念などの連想関係を用いて文書中の単語の重要度を計算し重要文を抽出する手法を提案して有効性を示す．連想概念辞書は，小学校の学習基本語彙を刺激語とし大量の連想語を収集して構造化すると同時に，その連想語との距離が定量化されている．また既存の重要語抽出法と本手法での抽出結果とを，人間が行なった要約結果と比較することによって評価した．従来の手法に比べて連想関係を計算に含めることによって要約精度が人間の要約に近く，本手法によって改良されることがわかった．,"In this paper, we propose a method for calculating scores of
importance for sentences for text summarization purposes. In this
method, scores for sentences are calculated based on quantitative
distance information in an associative concept dictionary, which
includes about 160,000 associated concepts. Eight articles are used
to evaluate our method.The articles are chosen form Japanese elementary school textbooks because the dictionary was constructed using basic nouns in the textbooks. In order to evaluate the quality of the importance score ranking resulting from our method and other conventional methods using term frequency (tfidf), we carry out experiments where 40 human subjects chose the five most important sentences from each of the eight articles. The evaluation results show that sentences chosen by our method using association relationships is more comparable to those chosen by human subjects. The results show that summarization accuracy can be improved by applying our method.","['はじめに', '連想概念辞書の使用', '単語の重要度による重要文抽出', '本手法の評価', '今後の課題', 'おわりに']",,,,,,,,
V11N01-01.tex,Solving Ambiguities in Indonesian Words by Morphological Analysis Using Minimum Connectivity Cost,,,,"['Introduction', 'Characteristics of Bahasa Indonesia', 'Morphological Analysis System', 'Experiments and Results', 'Conclusions', 'Future Directions']",,,,,,,,
V11N01-04.tex,迂言表現と重複表現の認識と言い換え,Recognition and Paraphrasing of Periphrastic Phrases and Overlapping Phrases,,,"['はじめに', '迂言表現と重複表現の分類', '辞書定義文に基づく認識', 'パターンマッチング方式', '要素-辞書定義文マッチング方式', '迂言表現と重複表現の言い換え', '実験結果と考察', '関連研究', 'おわりに']",,,,,,,,
V11N01-05.tex,用例ベース翻訳のための日英アライメント確信度と日本語類似度を用いた訳語選択,Word Selection based on Source Language Similarity and Parallel Alignment Confidence,,,"['はじめに', '用例の作成', '用例ベース翻訳システム', '実験と考察', '関連研究', '結論']",,,,,,,,
V11N02-03.tex,講演の書き起こしに対する統計的手法を用いた文体の整形,"Automatic Transformation of Lecture Transcription into \\ 
Document Style using Statistical Framework","講演音声のような話し言葉の書き起こしや音声認識結果には、話し言葉特有の表現が
数多く含まれており講演録などのアーカイブとして二次利用しにくいため、文章として適した形態に
整形する必要がある。本稿では、統計的機械翻訳の考え方に基づいて
講演の書き起こしを整形された文章に自動的に変換する方法を提案する。
本研究で扱う処理は、フィラーの削除、
句点の挿入、助詞の挿入、書き言葉表現への変換、文体の統一である。
これらの処理を統合的に行うようにビームサーチを導入した。
実際の講演の書き起こしを用いた
定量的な評価により統計的な手法の有効性が示され、句点と助詞の
挿入に関して高い精度を得ることができた。","Transcriptions and speech recognition results of lectures include many expressions
peculiar to spoken language.
Thus, it is necessary to transform them into document style
for practical use of them.
We apply the statistical approach used by machine translation
to automatic transformation of the spoken language into document style sentences.
We deal with deletion of fillers, insertion of periods, insertion of particles,
conversion to written expressions and unification of the end-of-sectence style.
A beam search is introduced to apply these processings in an integrated manner.
Experimental evaluation using real lecture transcriptions
comfirms that the statistical transformation framework
works well and we achieved high recall and precision rates of period and particle insertion.","['緒論', '整形作業における処理', '統計的手法による文体の整形', '実験と評価', '結論']",,,,,,,,
V11N02-04.tex,直訳性を利用した機械翻訳知識の自動構築,Automatic Construction of Machine Translation Knowledge Using Translation Literalness,"機械翻訳知識を対訳コーパスから自動構築する際，コーパス中に存在する
翻訳の多様性に起因して冗長な知識が獲得され，誤訳や曖昧性増大の原因とな
る．本稿ではこの問題に対し，「機械翻訳に適した対訳文」に制限し，翻訳知
識自動構築を試みる．機械翻訳に適した対訳文の指標として直訳性を提案し，
これを測定する尺度として対訳対応率を定義した．
\\ \indent
この対訳対応率に従い，2つの知識構築法を提案する．第一は，翻訳知識構築
の前処理としての，直訳性を用いた対訳文フィルタリング，第二は対訳文を直
訳部／意訳部に分割し，部分に応じた汎化手法を適用する．これらの効果は，
自動構築した知識を用いた機械翻訳による，訳文の品質という観点で評価を行っ
た．その結果，後者の分割構築の場合で約8.6\%の入力文について翻訳品質が
向上し，直訳性を用いた機械翻訳知識構築は，翻訳品質向上に有効であること
が確認された．","When machine translation (MT) knowledge is automatically constructed
from bilingual corpora, redundant rules are acquired due to
translation variety. These rules increase ambiguity or cause incorrect
MT results. To overcome this problem, we constrain the sentences used
for knowledge extraction to ``the appropriate bilingual sentences for
the MT.'' In this paper, we propose a method using translation
literalness to select appropriate sentences or phrases. The
translation correspondence rate (TCR) is defined as the literalness
measure.
\\ \indent
Based on the TCR, two automatic construction methods are tested. One
is to filter the corpus before rule acquisition.
The other is to split the acquisition
process into two phases, where a bilingual sentence is divided into literal
parts and the other parts before different generalizations are
applied. The effects are evaluated by the MT quality, and about 8.6\% of
MT results were improved by the latter method.","['はじめに', '翻訳の多様性', '機械翻訳に適した対訳', '直訳性', '直訳性を用いた翻訳知識構築', '翻訳実験', 'おわりに']",,,,,,,,
V11N02-05.tex,人間による翻訳文と機械翻訳文における 動詞の馴染み度の比較分析,Comparative Analysis of Familiarity Rating of Verbs in Human-Translated and Machine-Translated Sentences,"機械翻訳システムによる翻訳を人間による翻訳に近づけるために取
り組むべき課題を明らかにしようという試みの一環として，本稿では，ニュース
記事から無作為抽出した英文を英日機械翻訳システムで翻訳した結果と，これら
の英文を人間が翻訳した結果を照らし合わせ，両者の間で使用されている動詞の
馴染み度の分布に違いがあるかどうかを計量的に分析した．
動詞の馴染み度を測る尺度としては，NTTの単語親密度データベースを利用した．
分析の結果，機械翻訳システムによる翻訳と人間による翻訳の間で単語親密度の
分布に統計的有意差は認められず，使用されている動詞の馴染み度に関しては両
者の間で違いがないということが示唆された．
従って，格要素などとの共起関係を考えず動詞だけに着目した場合，調査対象と
した機械翻訳システムでは動詞の翻訳品質は一定のレベルに達していると判断で
きる．","As part of an attempt of revealing what kind of technical 
challenges must be solved to improve the quality of machine translation 
up to the extent of human translation, this paper carries out a 
quantitative analysis of distribution of familiarity rating of verbs 
between machine-translated Japanese sentences and human-translated ones, 
both of which are obtained from English sentences selected randomly from 
news articles. 
The familiarity rating is measured based on the database of familiarity 
rating developed at NTT Communication Science Laboratories.
The analysis found that no significant difference exists in the 
distribution of familiarity rating of verbs between machine- and 
human-translated sentences.
This intimates that as far as concerning the translation of verbs alone, 
the quality of the investigated MT system has reached a fixed standard.","['はじめに', '分析方法', '分析結果', 'おわりに']",,,,,,,,
V11N03-01.tex,"自動構築した格フレーム辞書と先行詞の位置選好順序を用いた\\省略解
析","Zero Pronoun Resolution based on Automatically Constructed Case
Frames and Structural Preference of Antecedents",,,"['はじめに', '関連研究', '格フレーム辞書の自動構築とそれに基づく省略解析', '位置カテゴリの設定とその順序づけ', '分類器の利用', '先行詞同定処理', '実験', 'おわりに']",,,,,,,,
V11N03-04.tex,非線形な表現構造に着目した重文と複文の日英文型パターン化,"Japanese to English Sentence Pattern Generations \\ 
for Semantically Non-Linear Complex Sentences",,,"['はじめに', '文型パターン化の原則と方針', '文型パターン化の具体的方法', '重文と複文の文型パターン化', 'あとがき']",,,,,,,,
V11N04-04.tex,複合語の内部情報・外部情報を統合的に利用した訳語対の抽出,Integrated Use of Internal and External Evidence in the Alignment of Compound Words,"本稿では，機械翻訳システムの辞書に登録されておらず，かつ(対応
付け誤りを含む)対訳コーパスにおいて出現頻度が低い複合語を対象として，
その訳語を抽出する方法を提案する． 
提案方法は，複合語あるいはその訳語候補の内部から得られる情報と，複合語あ
るいはその訳語候補の外部から得られる情報とを統合的に利用して訳語対候補に
全体スコアを付ける．
全体スコアは，複合語あるいはその訳語候補の二種類の内部情報と
二種類の外部情報に基づく各スコアの加重和を計算することによって求めるが，
各スコアに対する重みを回帰分析によって決定する．
読売新聞とThe Daily Yomiuriの対訳コーパスを用いた実験では，全体スコアが
最も高い訳語対(のうちの一つ)が正解である割合が86.36\%，全体スコアの
上位二位までに正解が含まれる割合が95.08\%という結果が得られ，提案手法の
有効性が示された．","This paper proposes a method of extracting English compound 
words and their Japanese equivalents from a parallel corpus.
The aim of our research is to extract compound words which
are not listed in a dictionary of an English-to-Japanese MT
system and appear infrequently in a parallel corpus.
Our method makes its alignment on the basis of two kinds of external 
evidence provided by the context in which a bilingual pair appears, as 
well as two kinds of internal evidence within the pair.
Each kind of evidence is accompanied by a score, and the aggregate score 
is computed as a weighted sum of the scores.
The appropriate weights are estimated with the logistic 
regression analysis.
An experiment using a parallel corpus of Yomiuri Shimbun and The Daily 
Yomiuri satisfactorily found that 86.36\% of the extracted 
bilingual pairs with the highest scores and 95.08\% with the top two 
scores were judged to be correct.","['はじめに', '訳語対抽出処理の概要', '訳語対抽出に用いる制約条件と優先条件', '実験と考察', 'おわりに']",,,,,,,,
V11N05-02.tex,WWWを用いた書き言葉特有語彙から話し言葉語彙への用言の言い換え,"Paraphrasing Predicates from Written Language Specific
Vocabulary into Spoken Language Vocabulary Using the World Wide Web",,,"['はじめに', '関連研究', '用言の言い換え学習', 'WWWからの書き言葉コーパスと話し言葉コーパスの自動収集', '言い換えペアの選択', 'まとめ']",,,,,,,,
V11N05-03.tex,日本語言い換え処理を利用した日本語--ウイグル語対訳辞書の拡充,Expansion of a Japanese-Uighur Bilingual Dictionary\\ by Paraphrasing,"機械翻訳に対する要求の高まりに伴い，日本語や英語，韓国語といった
言語の翻訳に関する研究が進み，
実用的なシステムが構築されつつある．
その一方で，そうした研究があまり進んでいない言語が存在する．
こうした言語においては，
翻訳の要である対訳辞書の整備も遅れている場合が多い．
一般に対訳辞書の構築には高いコストが必要であり，
機械翻訳システムを実現する上での障害となっている．
しかし，人間が翻訳作業をする場合，
対訳辞書に記載がない単語を
別の表現に言い換えて辞書を引くことにより，
この問題に対処する場合がある．
本研究ではこの手法を模倣し，
未登録語を登録語に言い換えることにより対訳辞書を拡充する
ことを提案する．
本論文では，対訳辞書の拡充に必要な単語の言い換え処理を
収集段階と選抜段階の二つに分割し，
前者において語義文に基づく手法を，
後者において類似度に基づく手法を
それぞれ適用した．
また，類似度に基づく手法では，シソーラスにおける概念間の距離に加え，
単語を構成する漢字の語義を利用した．
これによって，語法や概念が近く意味的にも等価な言い換えを獲得できた．
さらに，獲得した言い換えを翻訳システムで翻訳して
日本語--ウイグル語対訳辞書への追加を試みたところ，
未登録語300語のうち，その68.3\%に対して利用可能な対訳が得られた．","In machine translation, the number of words in a bilingual dictionary has an important influence on the translation.
However, the development cost of such a dictionary is very expensive.
In this paper, we resolve this problem by paraphrasing a non-entry word into the entry words.
We divide the paraphrasing process into two steps: collecting and screening.
In the collecting step, we make paraphrasing expressions of an original word by using its lexical descriptions in a Japanese monolingual dictionary.
In the following screening step, we calculate the similarity between the original word and each of its paraphrasing expressions, and choose the best one.
We applied this method to our Japanese-Uighur bilingual dictionary.
As a result, for 68.3\% of non-entry words, the appropriate Uighur words were given.","['はじめに', '言い換え処理技術の分類', '対訳辞書拡充のための言い換え処理', '日本語--ウイグル語対訳辞書拡充実験', 'おわりに']",,,,,,,,
V11N05-05.tex,,Paraphrasing as Machine Translation,,,"['Introduction', 'Paraphrasing by Statistical Machine Translation', 'Data-Oriented Paraphrasing', 'Experiments', 'Application to Machine Translation Evaluation', 'Further Analysis', 'Conclusion']",,,,,,,,
V11N05-06.tex,言い換えの統一的モデル --- 尺度に基づく変形の利用 ---,Universal Model for Paraphrasing\\ --- Using Transformation Based on a Defined Criteria ---,"言い換えに関する研究は平易文生成，要約，質問応答と多岐の分野において重要
なものであるが，本稿では言い換えの統一的モデルとして，尺度に基
づく変形による手法を示し，
このモデルによって種々の言い換えを統一的に扱えることを示す．
このモデルでは，多様な言い換えの問題の違いを，尺度で表現することで，多様な言い換えを統一的に扱えるようになっている．
本稿では具体的にこのモデルで，
文内圧縮システム，推敲システム，文章語口語変換システム，RL発音回避システム，質問応答システムを
構築できることを示す．
本稿の言い換えの統一的モデルは，
システムの作成を効率的にしたり，
言い換えの原理を容易に理解させたり，
多様な新たな言い換えを思いつかせる効果があり，有益なものである．","Studies on paraphrasing are important in various research topics such
as sentence generation, summarization, and question-answering. A
universal model is described for paraphrasing that transforms
according to defined criteria. We show that by using different
criteria, we can construct different kinds of paraphrasing systems
including one for compressing sentences, one for polishing the
sentences up, one for transforming written language into spoken
language, one for transforming English words into synonyms with the same
meaing containing less ``l'' and ``r'' letters, and one for answering
questions. Our model efficiently constructs systems and produces
dynamic paraphrasing systems. It should prompt the creation of new
paraphrasing systems in the feature.","['はじめに', '言い換えの統一的モデル', '文内圧縮システムの場合', '推敲システムの場合', '文章語口語変換システムの場合', 'RL発音回避システムの場合', '質問応答システムの場合', '関連研究との対比', 'おわりに']",,,,,,,,
V11N05-07.tex,複数の辞書の定義文の照合に基づく同義表現の自動獲得,Automatic Paraphrase Acquisition Based on Matching\\ of Definition Sentences in Plural Dictionaries,"近年，言い換え表現の自動獲得の研究が重要視されつつある．
本稿では，複数の辞書を用意して，
それらにおける同じ項目の定義文を照合することにより，
言い換え表現の一種である同義表現を抽出することを試みた．
また，同義表現を抽出するための新しい尺度を提案し，
その尺度で抽出データをソートした結果の精度は，
一般によく行なわれる頻度だけでソートする方法による結果よりも高いことを確認した．
この尺度は，他の同義表現の抽出の研究にも利用できる有用なものである．
提案手法では，同義表現のみを正解とするとき，
上位500個で0.748，ランダムに抽出した500個で0.220の抽出精度であった．
また，誤りの多くのものは
包含関係や類義関係にある表現であり，
それらも正解と判断する場合は，
上位500個で0.954，ランダムに抽出した500個で0.722の抽出精度であった．","Studies on paraphrasing are important in various research topics 
such as sentence generation, summarization, and question-answering. 
Extracting automatic paraphrases 
by matching definitions of the same word in two dictionaries is described. 
A new method for extracting these paraphrases is also described. 
Higher precision was obtained 
than with the conventional method of using frequency. 
Our method can be applied to other studies on 
paraphrase extraction. 
The method obtained 
the precision rate of 0.748 in the top 500 data and 
that of 0.222 in the 500 data that were extracted randomly, 
when a synonym only was judged as a correct answer. 
It obtained 
the precision rate of 0.954 in the top 500 data and 
that of 0.722 in the 500 data that were extracted randomly, 
when a hypernym and a similar expression were also judged as correct answers.","['はじめに', '複数の辞書の照合に基づく同義表現の抽出方法', '比較手法', '実験', '関連研究', 'おわりに']",,,,,,,,
V11N05-08.tex,言い換え技術に関する研究動向,A Survey on Paraphrase Generation and Recognition,"意味が近似的に等価な言語表現の異形を言い換えと言う．言い換
え技術とは，所与の言語表現からその言い換えを生成する言い換え生成技術，
および所与の言語表現対が言い換え関係にあるか否かを判定する言い換え認識
技術の総称である．これらの技術は，機械翻訳の前編集や読解支援のための文
章簡単化，質問応答や複数文書要約など，様々な応用に貢献する応用横断的な
ミドルウェア技術になると期待されており，近年研究者の関心を集めてきた．
本論文では，こうした言い換え技術について，工学的研究を中心に近年の動向を
紹介する．具体的には，言い換えの定義や言い換え技術の応用可能性について
論じた後，構造変換による言い換え生成，質問応答・複数文書要約のための言
い換え認識に関する研究を概観し，最後に言い換え知識の自動獲得に関する最
新の研究動向を紹介する．","Paraphrases are alternative ways of conveying the same
content.  The language technology for processing paraphrases, namely,
paraphrase generation and paraphrase recognition, has drawn the
attention of an increasing number of researchers because of its
potential contribution to a wide variety of natural language
applications.  This survey paper overviews recent research trends in
paraphrase generation and recognition, and discusses
future prospects, addressing the issues of the definition of
paraphrases, transformation-based paraphrase generation, paraphrase
recognition in question answering and multi-document summarization,
and finally corpus-based knowledge acquisition.","['はじめに', '言い換えとは？', '言い換え技術の使い方', '言い換えの実現方法', '言い換え知識の獲得', 'おわりに', '語彙・構文的言い換えの分類']",,,,,,,,
V12N01-01.tex,大規模日本語文法の開発,Building a Large-Scale Japanese Grammar,"構文解析において，多様な言語現象を扱うためには大規模な文法が必要となる
  が，一般に人手で文法を開発することは困難である．一方，大規模な構文構造
  付きコーパスから様々な統計情報を取り出し，自然言語処理に利用する研究が
  多くの成果をあげてきており，構文構造付きコーパスの整備が進んでいる．こ
  のコーパスから大規模な文脈自由文法(CFG，以下，文法と略す)を抽出するこ
  とが考えられる．ところが，コーパスから抽出した文法をそのまま用いた構文
  解析では多数の解析結果(曖昧性)を作り出すことが避けられないことが問題で
  あり，それが解析精度の悪化や解析時間，使用メモリ量の増大の要因ともなる．
  効率的な構文解析を行うためには，曖昧性を増大させる要因を分析
  し，構文解析の段階では曖昧性を極力抑えるよう文法やコーパスを変更する必
  要がある．本論文では，構文解析で出力される曖昧性を極力抑えた文法を開発
  するための具体的な方針を提案し，その有効性を実験により明らかにしている．","Although large-scale grammars are prerequisite
  for parsing a great variety of sentences,
  it is difficult to build such grammars by hand.
  Yet, it is possible to derive a context-free grammar (CFG)
  automatically from an existing large-scale,
  syntactically annotated corpus.
  While seemingly a simple task,
  CFGs derived in such fashion have seldom been applied
  to existing systems.
  This is probably due to a great number of possible
  parse results (i.e. high ambiguity).
  In this paper, we analyze some causes of high ambiguity,
  and we propose a policy for building a large-scale Japanese CFG
  for syntactic parsing, capable of decreasing ambiguity.
  We also provide an experimental evaluation of the obtained CFG
  showing reduction in the number of parse results (reduced ambiguity)
  created by the CFG and the improved parsing accuracy.","['はじめに', '関連研究とその問題点', '大規模日本語文法の作成手順', '構文解析結果の曖昧性を増大させる要因', '文法，コーパスの変更方針', '評価実験', 'PGLRモデルによる解析結果を利用した係り受け解析', 'まとめ', '文法作成の出発点として使用したコーパス']",,,,,,,,
V12N01-04.tex,チャット対話における発言間の継続関係と応答関係の同定,Identifying Continuation and Response Relations between Utterances in Computer-Mediated Chat \\Dialogues,"本論文では，チャット対話の対話構造を解析する手法を提案し，そ
の実現可能性について論じる．まず，発言間の二項関係である継続関係と応答関
係に対話構造を分解し形式化する．継続関係とは，質問やそれに対する応答を構
成する同一話者による発言間の関係である(例えば，1つの質問を構成する2つの
発言「あなたは」と「学生ですか？」の関係，あるいは1つの応答を構成する2つ
の発言「はい」と「そうです」の関係)．応答関係とは，質問と応答のような異
なる話者による発言間の関係である．これらの関係に基き、発言をまとめあげる
ことで対話構造を解析する．本研究では，この問題をある発言とそれに先行する
発言との間に継続関係または応答関係があるか否かの2 値分類問題に分解し，コ
ーパスベースの教師あり機械学習を試みた．解析対象は，対話構造を付与したコ
ーパスである(2 人対話と3 人対話の合計69 対話，11905発言)．本手法による対
話構造全体の正解との一致率は，2人対話87.4\%，3人対話84.6\%であった．","This paper proposes a computational model for analyzing the 
communicative structure of computer-mediated chat dialogues, reporting 
the present results of our empirical evaluation. We first formalize 
communicative structure underlying chat dialogues by decomposing it into 
{\it continuation relations","['はじめに', '対話構造のモデル', '実験用コーパス', '提案手法', '評価実験', '関連研究', 'おわりに', '発言間の結束度']",,,,,,,,
V12N02-01.tex,機械学習とルールベースの組み合わせによる\\自動職業コーディング,"Automatic Occupation Coding with Machine Learning and
 Hand-Crafted Rules",,,"['はじめに', '関連研究', '職業コーディング', 'ルールベース手法', 'SVMによる方法', '実験', 'おわりに']",,,,,,,,
V12N02-05.tex,格フレーム辞書の漸次的自動構築,Gradual Fertilization of Case Frames,,,"['はじめに', '対象とする表現', '1次格フレーム辞書の構築とそれを用いた格解析', '高次格フレーム辞書の構築', '格フレーム辞書の後処理', '格フレーム辞書の評価', '関連研究', 'おわりに']",,,,,,,,
V12N02-10.tex,,Use of Multiple Documents as Evidence\\ with Decreased Adding\\ in a Japanese Question-answering System,,"We propose a new method of using 
multiple documents as evidence with decreased adding
to improve the performance of question-answering systems. 
Sometimes, the answer to a question may be found in multiple documents. 
In such cases, using multiple documents to predict answers
may generate better answers than using a single document. 
Our method therefore uses information from multiple documents, 
adding the scores of candidate answers 
extracted from various documents. 
However, because simply adding the scores can degrade the performance of 
question-answering systems, 
we add the scores with progressively decreasing weights to reduce the negative effect of simple adding. 
We carried out experiments using the Question-Answering Challenge (QAC) test collection.
The results showed that our method produced a statistically significant improvement, 
with the degree of improvement ranging from 0.05 to 0.14.
These results, and the fact that our method is simple and easy to use, 
indicate its potential feasibility and utility in question-answering systems. 
Experiments comparing our decreased adding method with
several previously proposed methods that use multiple documents
showed that our method was more effective than these other methods.","['Introduction', 'Use of Multiple Documents as Evidence with Decreased Adding', 'Question-answering Systems Used in This Study', 'Experiments', 'Discussion', 'Conclusion']",,,,,,,,
V12N03-03.tex,SVMおよびTransductive SVMを用いた製品スペック情報の抽出,Product Specification Extraction Using SVM \\ and Transductive SVM,"ネットワークの普及により，今までは紙面で伝えられていた情報の電子化が進んでいる．
本稿では，それら電子化された情報の一つである，製品のスペック情報の抽出について議論する．
現在，製品情報を収集し，利用しているポータルサイトが数多く存在するため，膨大なWebページの中から製品のスペック情報を的確に抽出することは，そのようなポータルサイトの自動構築のために大きな意義を持つ．
製品のスペック情報は，殆どの場合，表形式で記述されている．
Web上の表はHTMLの\verb+<+TABLE\verb+>+タグを用いて記述されるが，\verb+<+TABLE\verb+>+タグは表を記述する以外にも，レイアウトを整えたりする場合に頻繁に用いられる．
ある特定の領域においては，\verb+<+TABLE\verb+>+の70\%がレイアウト目的で使われているとの報告もある．
そのため，HTML文書中の\verb+<+TABLE\verb+>+タグが表なのか，それとも他の目的で使用されているのかを判別する必要がある．
提案手法では，Support Vector Machines (SVM) を用いて，Webページ中に存在する表領域が製品スペックかどうかの判定を行う．
Transductive SVM を用いて，訓練データの削減についても考察する．
パソコン，デジタルカメラ，プリンタの3種類の製品について，実験を行い，それぞれの製品について高い再現率と適合率を得た．
訓練データが少ない場合，Transductive SVM を用いた手法の方が，通常の SVM と比べ，精度が改善されることを確認した．","Tables are an efficient way to express relational information. Most of information about products is written in tabular form. Table (specification) extraction is a significant task to handle product information written in tabular form such as specifications.
We are developing a multi-specifications summarization system.
The specifications are written in \verb+<+TABLE\verb+>+ tags.
The presence of the \verb+<+TABLE\verb+>+ tags in an HTML document does not necessarily indicate the presence of specifications.
Less than 30\% of HTML \verb+<+TABLE\verb+>+ tags are real tables in one particular domain.
In this paper, we propose a method for specification extraction using SVMs.
To reduce the training data, we also evaluate this task by using transductive SVMs.
For PC, digital still camera and printer specifications, we evaluate the performance of SVMs and transductive SVMs. Experimental results show the effectiveness of our methods.","['はじめに', '関連研究', 'フィルタリング', '表領域抽出', '実験', 'おわりに']",,,,,,,,
V12N03-04.tex,交替の特徴調査と二言語結合価エントリ獲得への利用,"An Investigation into  the Nature of \\ 
Verbal Alternations and their Use in the Creation of Bilingual Valency Entries","本稿では、自動詞の主語が他動詞の目的語となる動詞の交替を対象とし、既
  存の結合価辞書における交替の選択制限の対応関係の調査や、2言語間の交替の
  比較などを行なう。更に、これらの調査結果に基づき、
  交替データを用いて比較的単純な置き換えにより既存の結合価辞書に新しい
  エントリを追加する方法を提案する。本稿では、交替の片側に対応するエン
  トリから、もう片側のエントリを獲得する。また、本提案手法では2言語の
  結合価エントリを同時に作成する。作成したエントリは、下位範疇化構造や
  選択制限、交替情報等の詳細な情報を持っている。本稿の実験の結果、対象
  とした交替を85.4\%カバーすることができた。また、翻訳評価の結果、本手
  法で作成したエントリによって、翻訳結果が32\%改善された。","In this paper we investigate the properties of Japanese and English
  transitive-intransitive alternations.  For Japanese alternations, we
  show that the selectional restrictions of alternating arguments are
  more similar than those for non-alternating arguments.  Across
  languages we show that there are four major strategies for
  translating alternating verbs.  Finally, we present a method that
  uses alternation data to add new entries to an existing bilingual
  valency lexicon. If the existing lexicon has only one half of the
  alternation, then our method constructs the other half.  The new
  entries have detailed information about argument structure and
  selectional restrictions.  In this paper we focus on one class of
  alternations, but our method is applicable to any alternation.  We
  were able to increase the coverage of the causative alternation to
  85.4\%, and the new entries gave an overall improvement in
  translation quality of 32\%.","['はじめに', '利用する言語資源', '\\soalt{', '結合価エントリの作成方法', '交替情報に基づく結合価エントリの獲得実験と評価', '議論と今後の課題', 'まとめ']",,,,,,,,
V12N03-05.tex,ゲーム理論による中心化理論の解体と実言語データに基づく検証,Dissolution of Centering Theory Based\\ on Game Theory and Its Empirical Verification,"中心化理論(centering theory)は，
注意の中心，照応，結束性の間の相互作用を説明する談話構造の理論である．
しかし，照応現象の背後にあるはずの基本原理を明らかにするものではない．
また，中心化理論で重要な役割を担う顕現性(salience)が，
客観的に計量可能な尺度として定式化されていないという問題もある．
一方，Hasidaら\citeyear{hasida1995,hasida1996","Centering theory is 
to explain relations among focus, anaphora, and cohesion.
However, it fails to address any general principle behind anaphora.
Moreover, although the salience of discourse entities 
plays a critical role in centering theory,
it is not defined as an objectively measurable quantity.
On the other hand, Hasida et al.\citeyear{hasida1995,hasida1996","['はじめに', '中心化理論の概略と問題点', '顕現性の定式化と計測', '意味ゲーム', '統計的検証', '考察', 'おわりに']",,,,,,,,
V12N03-07.tex,名詞格フレーム辞書の自動構築とそれを用いた名詞句の関係解析,"Automatic Construction of Nominal Case Frames and \\ 
its Application to Indirect Anaphora Resolution",,,"['はじめに', '名詞句「ＡのＢ」の意味解析', '名詞格フレーム辞書の構築', '名詞句の関係解析', '実験と考察', 'おわりに']",,,,,,,,
V12N03-09.tex,機械翻訳を介したコミュニケーションにおける利用者の機械翻訳システム適応の言語依存性,Language-Dependency in User's Adaptation for MT Systems in MT-mediated Communication,"本論文では，機械翻訳を介したコミュニケーションにおける利用者の機械翻訳システ
ムへの適応状況を分析し，機械翻訳を介した異言語間コミュニケーション支援の方向
性について論ずる．コミュニケーションの目的が明確で，利用者の機械翻訳への適応
が期待できる状況において，多言語機械翻訳を介したコミュニケーションを行う時，
利用者はどのような適応を行うのか，また，その適応の効果はどの程度のものなのか
を明らかにした．適応のための書き換えの方法は翻訳言語ペアに強く依存することが
分かった．日本語から英語への翻訳の場合，日本語と英語の概念間の食い違いを補う
ための語句の置き換えや言語表現習慣の違いを補う主語の補完などが多く観察された．
また，日本語や韓国語のように類似の言語では，それらの言語における適応の傾向が
似ていることが分かった．日本語から英語への翻訳のための適応は，英訳自体には効
果が大きいが，韓国語訳にはほとんど効果がなく，中国語訳への効果もそれほど大き
くはないことが分かった．","This paper analyzes the impact of user adaptation in MT-mediated communication.
It clarifies how the user adapts to machine translation and how effective the adaptation
is in terms of communication when the purpose of communication is clear. The
most common alterations and their effectiveness strongly depend on the translation
language pairs. In the case of Japanese-to-English translation, we observed two main 
alterations: replacing words or phrases to offset the difference in concepts between
Japanese and English and supplementing subjects to offset the difference in modes
of expression between Japanese and English. Since Korean and Japanese are similar
languages, Korean users exhibited similar adaptation tendencies. The adaptation
performed by Japanese users when referring to the English translation was very effective
in improving the quality of the English translations. However, it was not so
effective for Chinese and even less effective for Korean translations.","['はじめに', 'ICE2002', '利用者適応の傾向', '利用者適応の多言語翻訳への効果', '機械翻訳の品質と利用者適応やコミュニケーションとの関係', 'おわりに']",,,,,,,,
V12N04-03.tex,構造化された言語資料に対する全文検索システムの設計と実現,Design and Implementation of Full Text Search System for Structured Language Resources,"本論文では，構造化された言語資料の検索・閲覧を指向した全文検索システム
である『ひまわり』の設計，および，その実現方法を示す。ここで言う「構造
化された言語資料」とは，コーパスや辞書のように，言語に関する調査，研究
などに利用することを目的として，一定の構造で記述された資料一般を指す。
『ひまわり』は，言語資料の構造化形式の多様性と利用目的の多様性に対応し
た設計がなされている。構造化形式の多様性については，言語資料が XMLで構
造化されていることを想定して，XML 文書に対する全文検索機能を実現した。
全文検索に付け加えて，マークアップされている情報の抽出や抽出された情報
に基づく検索結果の制約を行うことも可能である。また，Suffix Array などの
索引を用いて，検索の高速化を図っている。一方，言語資料に適した検索式と
閲覧形式を柔軟に定義できるようにすることにより，利用目的の多様性に対処
した。閲覧形式は，KWIC表示機能を備えた表形式での閲覧を基本とし，ルビな
どの通常のテキストでは表現できない表示形式や音声，画像に対しては，XSL 
変換などを介して外部閲覧システムにデータを受け渡す方法を用いる。多様性
に対する『ひまわり』の有効性を検証するために，『分類語彙表』，および，
『日本語話し言葉コーパス』に『ひまわり』を適用し，定性的な評価を行うと
ともに，検索速度測定による定量的な評価を行った。","In this paper, we design and implement a full text search system
``Himawari''. Himawari is designed to handle various structures and
usages of language resources that are made to be used for language study
and research. For the variety of structure, Himawari has the ability to
search language resources structured by XML, extracting tagged
information that may be used to constrain the results. Himawari provides
some kind of indexes such as Suffix Array for the improvement of the
search process. To resolve the problem of the variety of usages, a query
and a method of reference for language resources can be defined by a
user as suitable for the target language resource. Search results are
displayed as a table including KWIC (KeyWord In Context), and can be
output to external reference system, for example, HTML browser, sound
player, when the result is not able to be displayed as text data. By
applying our system to a Japanese thesaurus ``Bunrui Goi Hyo'' and
``Corpus of Spontaneous Japanese'', the adaptability for the varieties
is verified and proved.","['はじめに', '前提条件', 'システムの概要', '検索方式', '評価', '関連研究との比較', 'おわりに']",,,,,,,,
V12N04-06.tex,原言語が異なる翻訳コーパスの定量的分析,Quantitative Analysis of Corpora with Different Source Languages,"日英パラレルコーパスにおける日本語と英語それぞれを原言語として翻訳した
2つの韓国語コーパスを用いて，原言語が翻訳に
及ぼす影響を調べた．
コーパスにはATRのBTEC(162,308文)を使った．
2つの韓国語コーパスは，日英パラレルコーパスからの翻訳であり，
内容は一致している．
それにも関らず，韓国語両コーパス間の同一文は3\%以下であり，
正書法が統一されていない点を考慮しても，同一または同一と
みなせる文は全体の8.3\%程度である．
本研究では，両コーパスにおける違いを原言語の影響と予想し，
分析した結果を報告する．","In order to investigate the effect of source language on
translations, we examine two variants of a Korean translation
corpus.  The first variant consists of Korean translations of
162,308 Japanese sentences from the ATR BTEC (Basic Expression Text
Corpus).  The second variant was made by translating the English
translations of the Japanese sentences into Korean.  We show that
the source language text has a large influence on the target text.
Even after normalizing orthographic differences, fewer than 8.3\% of
the sentences in the two variants were identical.  We describe in
general which phenomena differ and then discuss how our analysis can
be used in natural language processing.","['背景', '原言語が異なるコーパスの比較', '諸言語現象における原言語の影響', '2つのコーパスと自然言語処理', '結論', 'コーパス中の例文']",,,,,,,,
V12N05-02.tex,日英関連報道記事を用いた訳語対応推定,"Estimating Bilingual Term Correspondences\\ from Relevant
       Japanese-English News Articles","近年，ウェブ上の日本国内の新聞社などのサイトにおいては，日本語だけでなく英語で書
   かれた報道記事も掲載しており，これらの英語記事においては，同一時期の日本
   語記事とほぼ同じ内容の報道が含まれている．
   本論文では，これらの報道記事のページから，
   日本語で書かれた文書および英語で書かれた文書を収集し，
   多種多様な分野について，
   分野固有の
   固有名詞(固有表現)や事象・言い回しなどの
   翻訳知識を獲得する手法を提案する．
   本論文の手法には，
   情報源となるコーパスを用意するコストについては，
   コンパラブルコーパスを用いた翻訳知識獲得のアプローチと同等に小さく，しかも
   同時期の報道記事を用いるため，
   片方の言語におけるタームや表現の訳が
   もう一方の言語の記事の方に出現する可能性が高く，
   翻訳知識の獲得が相対的に容易になるという大きな利点がある．
   翻訳知識獲得においては，まず，
   報道内容がほぼ同一もしくは密接に関連した日本語記事および英語記事を
   検索する．
   そして，関連記事組を用いて二言語間の訳語対応を推定する．
   訳語対応を推定する尺度としては，
   関連記事組における訳語候補の共起を利用する方法を適用し，
   評価実験において文脈ベクトルを用いる方法と比較し，
   この方法が有効であることを示す．","This paper focuses on bilingual news articles on WWW news sites
  as a source for translation knowledge acquisition.
  We take an approach 
  of acquiring translation knowledge
  of domain specific named entities, event expressions, 
  and collocational expressions
  from the collection of bilingual news articles on WWW news sites.
  In this framework, pairs of Japanese and English news articles
  which report identical contents or at least closely related
  contents are retrieved.
  Then, a statistical measure is employed for the task of estimating
  bilingual term correspondences 
  based on co-occurrence of Japanese and
  English terms across relevant Japanese and English news articles.
  We experimentally show that 
  the proposed method is effective in estimating
  bilingual term correspondences from cross-lingually relevant news articles.","['はじめに', '言語横断関連報道記事検索', '日英関連報道記事における訳語対応の推定', '実験および評価', '関連研究', 'おわりに']",,,,,,,,
V12N05-03.tex,動詞待遇表現に対する丁寧さの印象に関する定量的分析 −接頭辞オを用いた表現と接頭辞ゴを用いた表現との比較−,"Quantitative Analysis regarding Impressions of Politeness of Verbal-Honorific Expressions: Comparison of Expressions Using Prefixes ``GO"" and ``O"" Honorific Expressions",日本語は，各品詞にわたって待遇表現が発達している．中でも動詞に関する待遇表現は多岐にわたるが，待遇表現：「接頭辞オ＋本動詞＋補助動詞(オ〜型表現)」，および「接頭辞ゴ＋本動詞＋補助動詞(ゴ〜型表現)」は，日本語の動詞待遇表現における主要な表現である．両表現の違いについては，オに続く本動詞が和語であり，ゴに続く本動詞が漢語であるということが，従来の言語学的研究で指摘されてきた．しかし，両表現の言語心理的な違いを定量的に調べた研究は，これまで殆どなかった． 今回，我々は，Scheffeの一対比較法を用いてオ〜型表現，およびゴ〜型表現に対して人々が感じる丁寧さの程度を数値化した上で，統計的検定を行って丁寧さの印象に関する両表現の違いを定量的に分析した．その結果，ゴ〜型表現はオ〜型表現に比べ，通常表現を待遇表現に変化させた場合に，通常表現からの丁寧さの変化量がより小さいことが分かった．そして，その原因として，待遇表現としての認識に関する両表現の違いが示唆された．,"Expressions of ``prefix O + main verb + auxiliary verb"" and ``prefix GO + main verb + auxiliary verb"" are important verbal-honorific expressions in the Japanese language. It has been pointed out in past linguistic researches that the difference between the two types of expressions is that the main verb after ``O"" is a Japanese word and the one after ``GO"" is a Chinese word. However, there have hardly been any quantitative researches made on the differences of the two expressions so far. In this study, quantitative analyses were performed to reveal differences in the impressions of politeness between these two types of expressions by using Scheffe's paired comparison method and statistical tests.
Results suggest that in regard to difference in politeness from a plain form, ``prefix GO + verb of Chinese word + auxiliary verb,"" is smaller than ``prefix O + verb of Japanese word + auxiliary verb."" It is suggested that these results are due to the difference between these expressions as to the recognition of honorific expressions.","['はじめに', '待遇表現の丁寧さの定量化', 'オ〜型表現とゴ〜型表現の丁寧さに関する実験', '実験結果', '考察', 'おわりに']",,,,,,,,
V12N05-04.tex,構文的曖昧性を持つ英語固有表現とその対訳表現の獲得,Acquisition of Translation Knowledge of Syntactically Ambiguous Named Entity,"本稿では，前置詞句や等位構造を持つ英語固有表現とそれに対応す
る日本語表現を対訳コーパスから抽出する方法を提案する．
提案方法では，
(1) 意味的類似性と音韻的類似性の二つの観点から英語固有表現と日本語表現の
対を評価し，二種類の類似度を統合して全体としての類似度を求める処理と，
(2) 前置詞句の係り先や等位構造の範囲が不適格である英語固有表現の抽出を抑
制する処理を行なう．
読売新聞とThe Daily Yomiuriの対訳コーパスを用いて実験を行ない，提案方法
の性能と上記のような処理を行なわないベースラインの性能を比較したところ，
提案方法で得られたF値0.678がベースラインでのF値0.583を上回り，
提案方法の有効性が示された．","This paper proposes a method of extracting a bilingual pair 
of a syntactically ambiguous named entity and its counterpart from a 
sentence-aligned English-Japanese parallel corpus.
This method computes the degree of semantic and phonetic similarities
between an English named entity and its translation candidate,  
and calculates the overall score of the pair as the weighted sum of the 
two kinds of scores.
It avoids extracting English named entities with wrong prepositional 
phrase attachment and/or wrong scope of coordination. 
In an experiment using a parallel corpus of Yomiuri Shimbun and The Daily 
Yomiuri, the proposed method has achieved the F-value of 0.678, which 
surpasses 0.583 marked by a baseline method.","['はじめに', '{\\CPNP', '{\\CPNP', '{\\JNP', '{\\MTNP', '評価実験', 'おわりに']",,,,,,,,
V12N05-05.tex,連想メカニズムを用いた時間判断手法の提案,The Method of The Time Judgment Based on An Association Mechanism,我々は，人間と自然な会話を行うことができる知的ロボットの開発を目標に研究を行っている．人間は会話をする際に意識的または無意識のうちに，様々な常識的な概念をもって会話を展開している．このように会話文章から常識的な判断を行い，適切に応答するためには，ある語から概念を想起し，さらに，その概念に関係のある様々な概念を連想できる能力が重要な役割を果たす．本稿では，ある概念から様々な概念を連想できるメカニズムを基に，人間が行う常識的な判断の一つである時間に関する判断を実現する方法について提案している．日常的な時間表現に着目し，基本的な常識知識を事前に与え，知識として持っていない多くの未知の表現にも対応できる柔軟なメカニズムの構築を実現している．結果としては，時間判断システムの正答率が約69.4\%，精度が約81.6\%の割合で人が行う判断結果と一致しており，二段階未知語処理手法を用いた時間判断システムは有効なシステムであるといえる．,"A human-like common sense and judgment is necessary to materialize a computer that can take communication with human. Because, when people talk to each other, we have the concept of time in our mind consciously or unconsciously. In the case, the ability to call concept in mind and to associate with many referred concepts will be an important matter. This paper will propose the method to systemize judgment concerning time, based on the mechanism to associate concept with many other referred concepts. In this research, the aim is rather for daily time-expression and an adaptable mechanism that can even deal with unknown expression. The feature of this paper is, using the knowledge in various ways by the viewpoint of time, from a small amount of given knowledge.  As a result, the percentage of correct answers of the time judgment system is approximately 69.4\%, and the precision is approximately 81.6\%. Therefore, the time judgment system using the technique proposed in this paper is an effective system.","['はじめに', '時間判断システム', '時間判断知識ベース', '概念ベースと関連度計算法', '時間判断手法', '未知語処理手法', '時間判断システムの評価', 'おわりに']",,,,,,,,
V12N05-07.tex,Webと携帯端末向けの新聞記事の \\対応コーパスからの文末言い換え抽出,Extraction of Paraphrasing Pattern by Aligned Corpora of Web and Mobile Terminal News Articles,"本研究では、数十文字程度の長さで携帯端末向けに配信されている
新聞記事と数百文字程度の長さのWeb新聞記事の両者を約3年に渡って収集した。
こうして収集したコーパスから文末表現の縮約などの言い換え表現の抽出
を機械的に行った。
まず、Webから収集した携帯向け新聞記事とWeb新聞記事からなるコーパスに対
して記事単位の対応付けを行い、次に文単位の対応付けを行った。
次に携帯向け記事文の文末の表現を形態素解析を用いて抽出し、その文に対応す
るWeb新聞記事の文を集める。
そしてWeb新聞記事の文の文末から形態素ごとに言い換え先表現を抽出し、
それに対して頻度等を用いた得点付け、および必要な名詞を欠落させてしまう不
適切な言い換えの除去を行うことにより言い換え表現の抽出精度向上を図った。","We have collected both Web news-paper articles of several hundreds of
characters, for three years  and their counter parts distributed for
mobile terminals,
which consist of fifty to a hundred characters.
Then, we extracted a number of candidates of paraphrases of the final
part of sentences from them automatically.
At first we have aligned these two types of corpus first at article
level, then at sentence level.
Next, we extract the final part of mobile article sentences using
morphological analyzer, and collect their counterpart expressions of
Web article sentences.
Finally, we extracted the candidates of morpheme sequence from the
final part of Web article sentence, then we propose the combination of
two methods for
them in order to improve the extraction accuracy of the sets: 1)
ranking based on frequency, branching factor and length of string, and
2) filtering to remove inappropriate expressions which eliminate
semantically indispensable nouns.","['はじめに', '対象とする新聞記事データとその対応付け', '言い換え表現の抽出', '抽出された言い換え元表現の評価', 'おわりに']",,,,,,,,
V12N05-08.tex,音韻論的・形態論的制約を用いた\\モンゴル語句生成・形態素解析,Mongolian Phrase Generation \\and Morphological Analysis based on \\Phonological and Morphological Constraints,"本論文では，現時点で利用可能なモンゴル語の言語資源，特に，名詞・動詞の語
 幹のリスト，および，名詞・動詞に接続する語尾のリストから，モンゴル語の名
 詞句・動詞句を生成する手法を提案する．具体的には，名詞・動詞の語幹に語
 尾が接続する際の音韻論的・形態論的制約を整備し，語幹・語尾の語形変化の
 規則を作成する．評価実験の結果において，100\%近くの場合について，生
 成された名詞句・動詞句の中に正しい句候補が含まれるという性能を達成した．
 さらに，本論文では，この句生成に基づいて，モンゴル語の名詞句・動詞句の
 形態素解析を行なう手法を提案する．具体的には，まず，既存のモンゴル語辞書
 から名詞語幹および動詞語幹を人手で抽出する．
 
 
 次に，これらの語幹に対して，
 モンゴル語名詞句・動詞句生成規則を適用することにより，語幹・語尾の組から
 句を生成するための語形変化テーブルを作成する．そして，この語形変化テー
 ブルを参照することにより，与えられた名詞句・動詞句を形態素解析して語幹・
 語尾に分離する．評価実験の結果においては，語形変化テーブルに登録されて
 いる句については，形態素解析の結果得られる語幹・語尾の組合せの候補の中
 に，正しい解析結果が必ず含まれることが確認できた．","Using currently available Mongolian linguistic resources
 such as the lists of stems of nouns and verbs
 as well as the list of suffixes,
 this paper proposes a method for morphologically analyzing noun/verb
 phrases  of the Mongolian language.  More specifically, 
 we first examine phonological and morphological constraints
 on connecting stems of nouns/verbs and suffixes,
 and invent inflection/conjugation rules for nouns/verbs.
 We experimentally show that, almost 100\% cases,
 correct noun/verb phrases can be found among the candidates
 of noun/verb phrases generated by the proposed method.
 Then, we compile a table for mapping a stem-suffix pair and
 a phrase to be generated from the stem-suffix pair.
 Morphological analysis of noun/verb phrases is performed 
 by simply consulting this mapping table.
 We experimentally show that, by the proposed method,
 correct candidate of stem-suffix pair
 can be obtained from the given noun/verb phrases.","['はじめに', 'モンゴル語の文法', '\\label{scn:vowelagreement', '\\label{scn:suffixagreement', 'モンゴル語句生成', '\\label{sec:morph-analysis', '関連研究', 'おわりに']",,,,,,,,
V12N06-02.tex,メーリングリストを利用した 質問応答システムのための知識獲得,Knowledge Extraction for a Question Answer System Using Emails Posted to a Mailing List,"本研究では最初に，
方法や対処法を問う質問(how型の質問)に質問応答システムが答えるための知識を，
メーリングリストに投稿されたメールから獲得する方法について述べる．
方法や対処法を問う質問に答えるための知識
(「こんな場合にはこうする」など)は，
メーリングリストに投稿されたメールから
質問や説明の中心になる文(重要文)を取り出すことによって獲得する．
次に，メーリングリストに投稿されたメールから獲得した知識を用いる
質問応答システムについて報告する．
作成したシステムは自然な文で表現されたユーザの質問を受けつけ，
その構文的な構造と単語の重要度を手がかりに質問文と
メールから取り出した重要文とを照合してユーザの質問に答える．
作成したシステムの回答と全文検索システムの検索結果を比較し，
メーリングリストに投稿されたメールから
方法や対処法を問う質問に答えるための知識を獲得できることを示す．","The most serious difficulty in developing a QA system is knowledge.
In this paper, 
we first discuss three problems of developing a knowledge base
by which a QA system answers how type questions.
Then, we propose a method of developing a knowledge base 
by using mails posted to a mailing list.
Next, we describe a QA system which can answer how type questions 
based on the knowledge base.
Our system finds question mails which are similar
to user's question and shows the answers to the user.
The similarity between user's question and a question mail is 
calculated by matching of user's question and a significant sentence
in the question mail. 
Finally, we show that mails posted to a mailing list 
can be used as a knowledge base by which a QA system answers how type questions.","['はじめに', '方法や対処法を問う質問に答える\n 質問応答システムで用いる知識', 'メーリングリストに投稿されたメールからの重要文の抽出', 'メーリングリストに投稿されたメールを利用した質問応答システム', 'おわりに']",,,,,,,,
V13N01-01.tex,言語資源を活用した実用的な対訳表現抽出,Practical Translation Pattern Acquisition with Utilizing Language Resources,"高精度の機械翻訳システムや言語横断検索システムを構築するためには，大規模な
  対訳辞書が必要である．文対応済みの対訳文書に出現する原言語と目的言語の単語
  列の共起頻度に基づいて対訳表現を自動抽出する試みは，対訳辞書を自動的に作成
  する方法として精度が高く有効な手法の一つである．本稿はこの手法をベースにし，
  文節区切り情報や対訳辞書などの言語知識を利用したり，抽出結果を人間が確認す
  る工程を設けたりすることにより，高精度で，かつ，カバレッジの高い対訳表現抽
  出方法を提案する．また，抽出にかかる時間を削減するために，対訳文書を分割し，
  抽出対象とする文書量を徐々に増やしながら確からしい対訳表現から段階的に抽出
  していくという手法についても検討する．
  8,000文の対訳文書による実験では，従来手法は精度40\%，カバレッジ79\%であった
  のに対し，言語知識を利用した提案手法では，精度89\%，カバレッジ85\%と向上した．
  さらに人手による確認工程を設けることにより，精度が96\%，カバレッジが85\%と向
  上した．また，16,000文の対訳文書による実験では，対訳文書を分割しない方法では
  抽出時間が約16時間であったのに対し，文書を4分割する方法では，約9時間に短縮さ
  れたことを確認した．","High-quality MT systems and cross-lingual information
  retrieval systems need large-sized translation dictionaries.
  Automatic extraction of translation patterns from parallel corpora
  is an efficient and accurate way to automatically develop
  translation dictionaries, and various approaches have been proposed
  to achieve this. This paper presents a practical translation pattern
  extraction method where translation patterns based on co-occurrence
  frequency of word sequences between English and Japanese can be
  greedily extracted, and manual confirmation or extra linguistic
  resources, such as chunking information and translation
  dictionaries, can be also effectively combined with.  　This paper
  examines the method of extracting probable translation patterns in
  incremental steps by gradually enlarging a unit of segmentalized
  corpus, in order to reduce the time spent on pattern extraction.
  　Our experiments using 8,000 sentences showed that the proposed
  method achieved an accuracy of 89\% for coverage of 85\% while the
  existing method achieved only an accuracy of 40\% for coverage of
  79\%, and this was further improved to an accuracy of 96\% for
  coverage of 85\% when combined with manual confirmation. Our
  experiments using 16,000 sentences showed that the method of
  dividing a corpus in quarters could reduce the extraction time to 9
  hours while the nondividing method required 16 hours.","['はじめに', '従来手法', '提案手法', '実験および考察', '関連研究', 'おわりに']",,,,,,,,
V13N01-03.tex,概念の意味属性と共起情報を用いた関連度計算方式,The Method of Measuring the Degree of Association between Concepts using Attributes of the Concepts and Coincidence Information,我々人間は曖昧な情報を受け取り適宜に解釈することで，会話を進めたり適切な行動を取ることができる．これは，長年の経験により蓄積された知識から築き上げられた言葉に関する「常識」を持っているからである．人間と自然に会話できる知的なコンピュータの実現には，単語の意味を理解するシステムの構築が必要であると考える．この実現には，ある概念から他の類似の概念ばかりでなく常識的に関連の強い概念を連想する連想メカニズムが不可欠である．そこで本稿では，単語の意味を定義している概念ベースを利用し，概念間の関連の強さをより一般的に評価する関連度計算方式について述べる．これまでの概念ベースの属性集合の一致度合いから概念間の関連性（類似度）を評価する手法を拡張し，概念空間における概念の共起情報を用いる関連度計算で補正する方式を提案する．,"Though we receive a word with ambiguous information, we humans can interpret it properly, so we can hold a conversation, and take proper actions. This is possible because we have ""common sense"" concerning the word, which comes from knowledge accumulated from long time experiences. In order to realize an intelligent computer which can talk with human beings, we think that a construction of the system which understands word meaning is necessary. An association mechanism, which associates one word (concept) with other similar concepts, is indispensable to this construction. 
This paper describes a method of measuring the degree of association, which evaluates the relevance between concepts based on the Concept-Base, which defines the meaning of words. There is a problem in the conventional method, which evaluates the relevance between concepts using the degree of overlapping of the attribute sets in the Concept-Base. This paper aims to solve the problem of this conventional method and proposes a method of measuring the degree of association using the coincidence information between concepts.","['はじめに', '概念ベース', '概念間の関連性評価法', '意味関連度計算方式', '共起関連度計算方式', '意味共起関連度計算方式', 'おわりに']",,,,,,,,
V13N01-05.tex,二言語コーパスからの語彙知識獲得のための 対訳辞書登録候補の選別,Selection of Dictionary Entries from Translation Equivalents for Acquisition of Lexical Knowledge from Bilingual Corpora,"機械翻訳システムの翻訳品質を改善するためなどに必要な語彙知識
を獲得するためには，
対訳コーパスにおいて二言語の表現を正しく対応付ける処理と，対応付けられた
表現対を辞書に登録するか否かを判定する選別処理の二つが必要である．
従来，対応付けに関する研究は数多く行なわれてきたが，辞書登録候補の選別に
関する研究はほとんど行なわれていない．
本稿では，従来あまり扱われてこなかった選別問題を採り上げ，この問題を機械
学習によって解く方法を示す．
学習に用いる素性として，二つの表現の間で異なる部分と両者に共通する部分に
着目し，差分部分や共通部分を表現する手段として，表記(文字，形態素)，品詞，
概念識別子を用いる．
評価実験の結果，最も高い選別性能(F値)を示す表現方法は文字であることが明
らかになった．","This paper points out that constructing a bilingual 
dictionary using translation equivalents obtained from bilingual corpora 
needs not only correct alignment of two expressions but also judgment of 
its appropriateness as an entry, 
and addresses the latter task which has been paid little attention.
We show a method of selecting a suitable entry using Support Vector 
Machines, and propose to define the features by the common and the
different parts between a current translation and a new translation.
We examined how the selection performances are 
affected by the four ways of representing the common and the different 
parts: characters, morphemes, parts of speech, and semantic markers. 
Our experimental result found that representation by characters marked 
the best performance, F-measure of 0.837.","['はじめに', '着目した素性', '訓練事例集', '実験と考察', 'おわりに']",,,,,,,,
V13N01-06.tex,連想知識メカニズムを用いた挨拶文の自動拡張方法,An Automatic Enhancing Method of Greeting Sentences Using Association Knowledge Mechanism,会話において，まず行われるのが挨拶である．コンピュータやロボットにおいても挨拶を行うことから次へと会話が広がり人間とのコミュニケーションが円滑に行われる．本研究では会話処理の中でも特に挨拶処理についてのしくみを提案する．挨拶処理は従来テンプレートを適用するのみであり，あまり研究は行われてない．しかし，単に用意されたテンプレートだけを用いると応答が画一化され，設計者の作成した文章のみが出現するという問題点がある．会話文の中でも特に挨拶文は設計者の作成した文章がそのまま使われることが多い．そこで本稿で提案する挨拶処理システムにおける挨拶文は設計者が用意した挨拶知識ベースに存在しない新たな文章も作り出す．人間は言葉に関する汎用的な知識を覚え，その言葉に関する常識を持った上で会話を行っている．これと同じように，挨拶処理において，汎用知識と常識判断にあたる連想知識メカニズムを用いる．挨拶知識ベースにこの連想知識メカニズムを組み合わせて検討することにより，文章を大規模に拡張し，精錬する手法を提案する．,"When we humans converse with a person, we greet at first. Also in computers or robots, the next subject comes after greeting, and so they communicate with a person smoothly. So far the greeting process was only to apply the template, and its research has not been done so much. However, there is a problem that the responses are uniform by only using a prepared template, and only the sentences made by designer appear. Especially it is often that only the greeting sentences made by designer appear in the conversation. Then, the greeting processing system of this paper produces new greeting sentences. These sentences do not exist in the greeting knowledge base that the designer prepared either. For talking, we humans have general knowledge and common sense of words. In the same way, we use in the greeting process, the association knowledge mechanism that has general knowledge and common sense for computer. The method we proposed in this paper is to expand and refine sentences by combining the association knowledge mechanism with the greeting knowledge base.","['はじめに', '挨拶処理システムの構成', '連想知識メカニズム', '挨拶処理システムのための知識ベース', '挨拶文の生成', 'おわりに']",,,,,,,,
V13N02-05.tex,知覚的群化に基づく参照表現の生成,Generating Referring Expressions based-on \\Perceptual Grouping,,,"['はじめに', 'データ収集', '参照表現の生成', 'SOGに対するスコア付け', 'おわりに', '実験に用いた布置']",,,,,,,,
V13N03-01.tex,用例ベース翻訳の確率的モデル化,Probabilistic Formalization for Example-based \\Machine Translation,"用例ベース翻訳は，これまで，経験則にもとづく指標／基準により用例を選択
してきた．
しかし，経験則に頼った場合，その修正を行うのが困難であり，また，アルゴ
リズムが不透明になる恐れがある．
そこで，本研究では用例ベース翻訳を定式化するための確率モデルを提案する．
提案するモデルは，翻訳確率の最も高い用例の組み合わせを探索することで，
翻訳文を生成する．
さらに，本モデルは用例と入力文のコンテキストの類似度を自然に翻訳確率に取り込
む拡張も可能である．
実験の結果，本モデルを用いたシステムは，従来の経験則によるシステムの精
度を僅かに上回り，用例ベース翻訳の透明性の高いモデル化を実現することに
成功した．","Example-based machine translation (EBMT) systems, so far, rely on
heuristic
measures in retrieving translation examples.
Such a heuristic measure costs time to adjust, and might make its
algorithm unclear.
This paper presents a probabilistic model for EBMT.
Under the proposed model, the system searches the translation example
combination which has the highest probability.
The proposed model clearly formalizes EBMT process.
In addition, the model can naturally incorporate the context similarity of
translation examples.
The experimental results demonstrate that the proposed model has a
slightly better translation quality than state-of-the-art EBMT systems.","['はじめに', '提案手法', '翻訳システムの構成', '実験', '関連研究', 'おわりに']",,,,,,,,
V13N03-02.tex,サポートベクタマシンを使った文書分類における 仮想事例の利用,Using Virtual Examples for Text Classification with Support Vector Machines,"本論文では，サポートベクタマシン (SVMs) を使った文書分類において
仮想事例 (virtual examples) がどのように性能を改善するかを調べる．
ある文書から少量の単語を追加したり削除したりしても，その文書が属するカ
テゴリは変化しないとの仮定を置いて，文書分類のために仮想事例を作る方法
を提案する．
提案手法を Reuters-21758 テストセットコレクションで評価した．
実験により，仮想事例はサポートベクタマシンを使った文書分類の性能
向上に役立つことが確認できた．特に，学習事例が少量の場合にその効果は顕
著であった．","We explore how virtual examples (artificially created examples)
improve performance of
text classification with Support Vector Machines (SVMs).
We propose techniques to create virtual examples for text
classification based on the assumption that the category of a document
is unchanged even if a small number of words are added or deleted.
We evaluate the proposed methods by Reuters-21758 test set collection.
Experimental results show virtual examples improve the performance of
text classification with SVMs, especially for small training sets.","['はじめに', 'サポートベクタマシン', '仮想事例と仮想サポートベクタ', '文書分類のための仮想事例', '評価実験と議論', '関連研究との比較', 'おわりに']",,,,,,,,
V13N03-03.tex,選好依存文法とその圧縮共有データ構造「依存森」について,Preference Dependency Grammar and its Packed\\Shared Data Structure ``Dependency Forest'',"選好依存文法(PDG: Preference Dependency Grammar)は，自然言語の形態素，
構文，意味解析を統合的に行う枠組みであり，各レベルの種々の曖昧性を統合
的に効率良く保持し，各レベルの知識により優先度を設定し，全体解釈として
最適な解を計算する．本稿では，PDGの基本モデルである多レベル圧縮共有デー
タ結合モデルとPDGの概要について述べるとともに，選好依存文法で用いられ
るヘッド付き統語森，依存森といった言語解釈を統合保持するデータ構造とそ
の構築手法について説明する．また，文の句構造を圧縮共有する統語森と依存
構造を圧縮共有する依存森との対応関係において完全性と健全性が成立するこ
とを示す．","Preference Dependency Grammar(PDG) is a framework for the
morphological, syntactic and semantic analysis for natural language
sentences. PDG gives packed shared data structures to hold the various
ambiguities in each level of sentence analysis with preference
scores and a method for calculating the most plausible interpretation
for a sentence. This paper describes the sentence analysis model
named the ``Multi-level Packed Shared Data Connection Model'' adopted
in PDG and shows the outline of the PDG framework. This paper
describes the packed shared data structures, such as the Headed
Parse Forest, the Dependency Forest adopted in PDG, and shows the
completeness and the soundness of the mapping between the Parse Forest
and the Dependency Forest.","['はじめに', '選好依存文法(PDG)の概要と圧縮共有データ構造', 'PDGにおける共有データ構造', '統語森と依存森の生成', '例文解析評価実験', 'おわりに']",,,,,,,,
V13N03-04.tex,談話構造解析に基づくスライドの自動生成,Automatic Slide Generation Based on \\Discourse Structure Analysis,"本稿では，テキストから要約スライドを自動生成する手法を提案する．本稿で生
成するスライドは，入力テキストから抽出したテキストの箇条書きからなる．そ
れらに適切なインデントを与えるには，対比関係や詳細化関係などといった文ま
たは節間の関係を解析する必要がある．本手法では，まず，接続詞などの手がか
り表現，語連鎖の検出，二文間の類似度の三つの観点を用いてテキストの談話構
造を解析する．そして，テキストから主題部・非主題部を抽出・整形し，抽出し
たテキストのインデントを談話構造に基づいて決定することにより，スライドを
生成する．実験を行なったところ，入力テキストよりもかなり見やすいスライド
を自動生成できることが確認された．","In this paper, we describe a method for automatically generating summary
slides from a text. The slide consists of itemizations of extracted
texts, and to determine their indentation, we need to analyze relations
between sentences/clauses, such as contrast and elaboration. We first
analyze the discourse structure of the text by considering three types
of information: cue phrases, identification of word chain and similarity
between two sentences. Then, we extract topic/non-topic parts from the
text and generate the slide by placing the extracted texts, whose
indentations are controlled according to the discourse structure. Our
experiments demonstrate that generated slides are far easier to read in
comparison with original texts.","['はじめに', '談話構造解析', 'スライドに表示するテキストの抽出', 'スライドの生成', '実装と評価', '関連研究', 'おわりに']",,,,,,,,
V13N03-05.tex,動詞項構造辞書への大規模用例付与,Augmenting a Semantic Verb Lexicon with\\ a Large Scale Collection of Example Sentences,"本論文では，述語項構造解析の精度向上のために必要となる大規模な
項構造タグ付き事例を
効率的に作成する方法について議論する．
項構造タグ付き事例の効率的な作成方法にはさまざまな方法が考えられるが，本論文では
大規模平文コーパスから抽出した表層格パターンの用例集合を
クラスタリングし，得られたクラスタに項構造タグを付与することで
タグ付与コストを削減する手法を提案する．
提案手法では，(i)表層格パターン同士の類似性と(ii)動詞間の類似性という2種類の類似性を利用してクラスタリングを行う．
評価実験では，
実際に提案手法を用いて8つの動詞の項構造タグ付き事例を作成し，
それを用いた項構造解析の実験を行うことによって，提案手法の
クラスタリングの性能や，人手でタグ付き事例を作成するコストと項構造解析精度の関係
を調査した．","In this paper, we propose a method of reducing the cost of annotating examples with argument structure
 in order to increase accuracy of argument structure analysis.
First, a large raw corpus is parsed, and a large scale collection of example sentences is constructed
 from predicate-argument examples in the parsing results. Second, the collection of example sentences
 is clustered by using two similarities about verb. Finally, the acquired clusters are annotated with
 argument structure by human. We report preliminary experiments using our proposed method, and show that
 the method is effective in reducing the cost of annotating.","['はじめに', '用例の類似性に基づく項構造タグ付与の効率化', '評価実験', 'おわりに', '用例間の類似度計算', 'その他の動詞の項構造辞書の項目']",,,,,,,,
V13N03-06.tex,クラス指向事例収集手法による言い換えコーパスの構築,"Building a Paraphrase Corpus Based on\\ Class-oriented
  Candidate Generation","語彙・構文的言い換えの中には，形態・構文的パターンに基づいて
  一括りにできるものの，表現を構成する語の統語・意味的な特性に依存して
  言い換えの可否や言い換え方が決まる現象が少なくない．本論文では，その
  ような言い換えを語彙構成的言い換えと呼ぶ．たとえば，複合語を構成語に
  分解するような言い換え，機能動詞構文の言い換え，態や格の交替，種々の
  動詞交替，語彙的派生などは語彙構成的言い換えの範疇に含まれる．我々は
  現在，これら語彙構成的言い換えに関わる語の統語・意味的な特性を明らか
  にするため，および言い換え生成技術の定量的評価のために，個々の言い換
  えクラスごとに言い換え事例集（言い換えコーパス）を構築している．本論
  文では，言い換え前後の表現の形態・構文的パターンと既存の言い換え生成
  システムを用いて言い換え事例を半自動的に収集する手法について述べる．
  また，日本語の機能動詞構文の言い換え，動詞の自他交替を対象とした予備
  試行の結果を報告する．","Several classes of paraphrases have a potential to be
  compositionally explained by referring to syntactic and semantic
  properties of constituent words: e.g., composing/decomposing
  compounds, voice/case alternation, various verb alternation, and
  lexical derivation.  Toward analyzing the compositionality
  underlying these paraphrase classes, we have examined a
  class-oriented framework for collecting paraphrase examples, in
  which sentential paraphrases are collected for each paraphrase class
  separately by means of automatic candidate generation based on
  morpho-syntactic paraphrasing patterns, followed by manual
  judgement.  Our preliminary experiments on building two paraphrase
  sub-corpora have so far been producing promising results with regard
  to cost-efficiency, exhaustiveness, and reliability.","['はじめに', '先行研究', '対象とねらい', '形態・構文パターンを用いた言い換え事例の半自動収集', '言い換えコーパス構築の予備試行', '議論', 'おわりに']",,,,,,,,
V13N03-07.tex,関連用語収集問題とその解法,Related Term Collection,本論文で提案する{\em 関連用語収集問題,This paper proposes the {\em related term collection problem,"['はじめに', '関連用語収集問題', '関連用語収集システム', '実験と検討', '関連研究', 'おわりに']",,,,,,,,
V13N03-08.tex,日本語を援用した日本手話表記法の試み,A Japanese Gloss-based Written Notation for Japanese Sign Language,"日本手話をテキストとして表現するための表記法を提案する．本表
  記法の検討に至った直接の動機は，日本語-日本手話機械翻訳を，音声言語間
  の機械翻訳と同様，日本語テキストから手話テキストへの翻訳（言語的な変
  換）と，翻訳結果の動作への変換（音声言語におけるテキスト音声合成と同
  様に手話動画の合成）とに分割し，翻訳の問題から動作合成の問題を切り離
  すことにある．この翻訳過程のモジュール化により，問題が過度に複雑化す
  るのを防ぐことをねらいとする．同時に，手話を書き取り，保存・伝達する
  手段としての利用も念頭に置いている．本表記法で記述される手話文は，手
  話単語，および，複合語等の単語の合成，句読点，非手指要素による文法標
  識で構成される．手話単語は，単語名とそれに付加する語形変化パラメータ
  （方向や位置，その他の手話動作によって付加される語彙的，文法的情報を
  表す）で表す．我々の表記法は，基本的に手話の動作そのものを詳細に記述
  するのではなく，動作によって表される意味内容を記述することをめざした．
  ただし，機械翻訳を念頭に置いているため，動作への変換のための便宜にも
  若干の考慮を払った．本表記法の記述力を検証するため，手話を第一言語と
  する手話話者による手話映像720文を解析し，この表記法での記述を試みた．
  全体で671文を記述することができた．十分表記できないと判断した49文
  （51表現）を分析し，問題点について考察した．","In this paper we propose a notation system for Japanese
  Sign Language (JSL).  This notation system is aimed to help
  modularize the Japanese-JSL machine translation process and to bring
  the JSL generation problem closer to that of traditional oral
  languages.  Accordingly, the main concern of this notation is not
  detailed motions of signs themselves but linguistic structures
  (i.e., lexical and grammatical information) expressed through such
  motions.  JSL sentences in our notation include signs, compounds of
  signs, punctuation marks, and non-manual syntactic markers.  A sign
  is represented by the sign identifier (a Japanese word or phrase)
  and its inflection parameters.  JSL sentences are transcribed in the
  text format with JIS characters.  This makes existing text tools
  available for reading, writing and processing JSL sentences.  We
  conducted a transcribing experiment to evaluate our notation system
  with 720 JSL sentences performed by native JSL signers, and found
  that 51 JSL expressions in the 49 sentences could not be
  sufficiently transcribed.  We classify and investigate those
  expressions.","['はじめに', '手話について', '手話表記法の提案', '手話映像の書き取り実験', '関連研究', 'おわりに', '表記法の構文と表記例']",,,,,,,,
V13N03-09.tex,テキストを対象とした評価情報の分析に関する研究動向,A Survey of Sentiment Analysis,"インターネットが普及し，一般の個人が手軽に情報発信できる環境が整ってきて
いる．この個人の発信する情報には，ある対象に関するその人の評価等，個人の
意見が多く記述される．これらの評価情報を抽出し，整理し, 提示することは，
対象の提供者である企業や，対象を利用する立場の一般の人々双方にとって利点
となる．このため，自然言語処理の分野では，近年急速に評価情報を扱う研究が
活発化している．本論文では，このような現状の中，テキストから評価情報を発
見，抽出および整理，集約する技術について，その基盤となる研究から最近の研
究までを概説する．","In these days, people can easily disseminate the information including
their personal evaluative opinions for some products and services on the
Internet.  The massive amount of their information is beneficial for
both product companies and users who are planning to purchase and use
them.  Because their information is mainly presented as textual form,
in the research field of natural language processing, many
researchers have devoted themselves to developing techniques for
exploring, extracting, mining, and aggregating the opinions and
sentiments.  This sort of techniques are commonly called
\textit{sentiment analysis","['はじめに', 'テキスト評価分析の題材となるテキストデータ', 'テキスト評価分析の要素技術に関する諸研究', 'テキスト評価分析の応用研究', 'テキスト評価分析に関連するその他の話題', '課題', 'おわりに']",,,,,,,,
V13N03-10.tex,日本語発話文における敬語の誤用を指摘するシステムの開発,System for Pointing Out Honorific Misusages in Japanese Speech Sentences,現代の日本社会において，日本語の敬語に関する様々な誤用が指摘されてきている．日本社会における敬語の誤用は，言語によるコミュニケーションを通じた社会的人間関係の構築を妨げる場合がある．敬語の誤用を避けるには，敬語の規範に関する正しい知識の習得が不可欠である．このような知識習得を効率的に行うため，敬語学習を支援する計算機システムの実現が期待される．このような背景の下，我々は日本語発話文に含まれる語形上の誤用，及び運用上の誤用を指摘するシステムを開発した．本システムは，日本語発話文，及び発話内容に関係する人物間の上下関係を表すラベルを入力とし，入力された日本語発話文における誤用の有無，及び誤用が含まれる場合にはその箇所と種類を出力する．発話に関わる人数は最大４名まで取り扱うことができる．正例，及び負例を用いた実験によってシステムの妥当性を検証したところ，一部のケースを除き，本システムが妥当な出力を行うことが確認された．本システムは，特に敬語の初学者に対する学習支援システムとして有用と考えられるが，その他の人々にとっても，文書作成における敬語の語形のケアレスミスをチェックする等の用途として幅広く活用できると考えられる．,"In Japan, politeness plays an important role in social activities, especially in conversations． However, honorific Japanese expressions are increasingly being misused． This misusage is a failure to use the honorific expressions in a way appropriate to the relative social positions assumed in a conversation． One of the causes of this misusage may be a lack of education on honorific conversations． Because honorific expressions take a long time to learn, computer assisted language learning systems for honorific expressions should be developed. We developed a computational system to check the usages of honorific expressions in Japanese speech sentences． The system can point out misused words and phrases, and can also indicate how they have been misused． The validity of the system was tested using ``correct'' sentences including no misused expressions, and ``incorrect'' sentences including misused expressions． The system was able to point out all the misusages in the incorrect sentences. It also judged most of the correct sentences as ``correct'' except some cases.","['はじめに', '敬語の誤用', '敬語誤用指摘システム', 'システムの妥当性の検証', 'むすび']",,,,,,,,
V13N04-01.tex,,Graph Branch Algorithm: An Optimum Tree Search Method for Scored Dependency Graph with Arc Co-occurrence Constraints,,"Preference Dependency Grammar (PDG) is a framework for the
morphological, syntactic and semantic analysis of natural language
sentences. PDG gives packed shared data structures for encompassing
the various ambiguities in each levels of sentence analysis with
preference scores and a method for calculating the most plausible
interpretation of a sentence. This paper proposes the Graph Branch
Algorithm for computing the optimum dependency tree (the most
plausible interpretation of a sentence) from a scored dependency
forest which is a packed shared data structure encompassing all
possible dependency trees (interpretations) of a sentence. The graph
branch algorithm adopts the branch and bound principle for managing
arbitrary arc co-occurrence constraints including the single valence
occupation constraint which is a basic semantic constraint in
PDG. This paper also reports the experiment using English texts
showing the computational complexity and behavior of the graph branch algorithm.","['Introduction', 'Optimum Tree Search in a Scored Dependency Graph', 'Semantic Dependency Graph and Dependency Forest', 'The Optimum Tree Search in the Dependency Forest Based on the Graph Branch Method', 'Example of Optimum Tree Search', 'Experiment', 'Concluding Remarks']",,,,,,,,
V13N04-02.tex,単語リストと生コーパスによる確率的言語モデルの分野適応,Language Model Adaptation \\ with a Word List and a Raw Corpus,"本論文では，単語リストと生コーパスが利用可能な状況における確率的言語モデルの分野適
  応について述べる．このような状況の下での一般的な対処は，単語リストを語彙に加えた自
  動単語分割システムによる生コーパスの自動単語分割の出力文を可能な限り人手で修正し，
  パラメータ推定に利用することである．しかしながら，文単位での修正では，正確な単語分
  割が容易でない箇所が含まれることになり，作業効率の著しい低下を招く．加えて，文単位
  で順に修正していくことが，限られた作業量を割り当てる最良の方法であるかということも
  疑問である．本論文では，コーパスの修正を単語単位とし，修正箇所を単語リストで与えら
  れる適応分野に特有の単語に集中することを提案する．これにより，上述の困難を回避し，
  適応分野に特有の単語の統計的な振る舞いを捕捉するという，適応分野のコーパスを利用す
  る本来の目的にのみコーパス修正の作業を集中することが可能となる．実験では，自動単語
  分割の結果の人手による修正の程度や方法を複数用意し，その結果得られるコーパスから推
  定された確率的言語モデルの予測力やそれに基づく仮名漢字変換の精度を計算した．この結
  果，適応分野に特有の語彙の出現箇所に修正のコストを集中することにより，少ない作業量
  で効率良く確率的言語モデルを分野適応できることが分かった．","In this paper, we discuss stochastic language model adaptation methods given a word
  list and a raw corpus. In this situation, a general method is to segment the raw
  corpus by a word segmenter equipped with a word list, correct the output sentences
  annotated with word boundary information by hand, and build a model from the
  segmented corpus. In this sentence-by-sentence error correction method, however,
  the annotator encounters difficult points and this results in a decrease of the
  productivity. In addition, it is not sure that sentence-by-sentence error
  correction from the beginning is the best way to dispense a limited work force. In
  this paper, we propose to take a word as a correction unit and concentrically
  correct the positions in which words in the list appear. This method allows us to
  avoid the above difficulty and go straight to capture the statistical behavior of
  specific words in the application field. In the experiments, we compared the
  language models built by several methods from the corpora in predictive power and
  Kana-kanji conversion accuracy. The results showed that concentrating on the error
  correction around the words in the list, we can build a better language model with
  less effort.","['まえがき', '確率的言語モデル', '単語リストと生コーパスによる分野適応', '生コーパスの利用方法', '評価', 'おわりに']",,,,,,,,
V13N04-03.tex,属性語の{\bfseries Web,Automatic Discovery \\ of Attribute Words from Web Documents \\ and Criteria for Human Evaluation,"本論文では，広範な概念クラスの属性語を日本語のWeb文書から獲得する手法を提案する．
提案する手法は，Web検索を用いて得られた候補の単語を言語的パターン・HTMLタグ・単語の出現の統計量から計算されるスコアで順位付けする簡単な教師無しの獲得手法である．
また，本論文では，獲得された属性語を人手で評価するための{\bf 質問解答可能性","We propose a method of acquiring attribute words for a wide range of object classes from Japanese Web documents. 
The method is a simple unsupervised method that ranks candidate words according to the score that uses the statistics of lexico-syntactic patterns, HTML tags, and word occurrences, as clues.
To evaluate the attribute words, we also establish an evaluation procedure based on the idea of {\it question-answerability","['はじめに', '獲得手法', '属性語のための評価基準', '実験', '今後の課題', '結論']",,,,,,,,
V14N01-01.tex,,A Linear-Time Algorithm for Dependency Analysis \\of Japanese,"日本語係り受け解析を行なう新しいアルゴリズムを述べる．
このアルゴリズムによれば，トップレベルの精度を落とすことなく線形時間で係り受け
解析が行なえる．
本論文では，アルゴリズムの形式的な記述を行ない，その時間計算量を理論的
に議論する．
加えて，その効率と精度を京大コーパス Version 2 を使って実験的にも評価する．
改良された係り関係のモデルと提案手法を組み合わせると，
京大コーパス Version 2 に対し
て従来手法よりもよい精度が得られた．","We present a novel algorithm for Japanese dependency analysis.
The algorithm allows us to analyze dependency structures of a sentence
in linear-time while keeping a state-of-the-art accuracy.
In this paper, we show a formal description of the algorithm and
discuss it theoretically with respect to time complexity.
In addition, we evaluate its efficiency and performance
empirically against the Kyoto University Corpus Version 2. 
The proposed algorithm with improved models for dependency yields the best
accuracy in the previously published results on the Kyoto University
Corpus Version~2.","['はじめに', '日本語文の解析', '関連研究', 'アルゴリズム', '係り関係を推定するためのモデル', '実験と考察', 'おわりに']",,,,,,,,
V14N01-02.tex,,Japanese Parser Generating Suitable Syntactic \\Structures for Meaning,"英語に比べて語順が自由で省略の多い日本語は，句構造解析には不向きとされ，
係り受け解析が一般的となっている．また，係り受けが交差する入れ子破りが起こる
表現や二つの品詞性のある語などは，句構造解析による木構造ではうまく扱えない．
さらに，現在主流となっている文節構文論（学校文法）に基づく構文解析では
構文解析結果が意味と整合性が良くなく，
時枝文法風の構文解析の方が解析結果に則って
意味がうまく説明できることが指摘されている．
本論文では，
時枝によって提唱された言語過程説を発展的に継承した三浦の言語モデル
（関係意味論に基づく三浦の入れ子構造）とそれらの基づく日本語文法体系（三浦文法）
による文法記述と文法規則適用条件の制御によって
上記のような日本語構文解析上の問題を解決する方法を提案する．
さらに，このような考えに基づき試作した日本語文パーザによって，
一対多・多対一の係り受け関係，文中の局所的入れ子構造，入れ子破りの表現，
主題の「は」と対照の「は」の扱い，二つの品詞性のある語の扱いにおいて
意味的に適切な統語構造が得られることを示した．","Japanese is a free word order language
 and ellipsis frequently occur compared with English.
Japanese sentences are not suitable
to be parsed by the phrase structure analysis
and is generally parsed by the dependency analysis.
Tree structures by the phrase structure analysis does not describe well
in expressions
with crossed dependencies and in words with duality of parts of speech.
The major syntactic analysis based on Japanese school grammar
does not generate suitable syntactic structures for meaning.
In contrast with this situation, a syntactic analysis based on Tokieda grammar
can explain meaning of Japanese sentences well.
Miura grammar is a Japanese grammar on the Constructive Process Theory
proposed by M. Tokieda, and developed by T. Miura.
This paper proposes
a solution to the above problems in Japanese Syntactic Analysis
by grammar descriptions based on Miura language model and Miura grammar,
and by controls of grammar application.
A trial parser based on the ideas generates
suitable syntactic structures for meaning,
in one-to-N or N-to-one dependency relations,
in local nest in sentences, in distinction between topic marker ``ha'' and contrast marker ``ha'' in Japanese particle ``ha'',
and in words with duality of parts of speech.","['まえがき', '三浦の言語モデルによる文の基本構造', '三浦文法に基づく日本語品詞体系', '意味と整合性のよい構文解析', 'パーザへの実装と有効性の検証', 'むすび']",,,,,,,,
V14N01-03.tex,,Semantic Role Labeling based on Japanese FrameNet,"本稿では，日本語フレームネットを背景に，述語項構造における項の意味役割を
推定する統計モデルの定義，および獲得手法を提案する．
本モデルの目的は，表層格では区別できない意味の区別である．
本モデルは文と述語から述語項構造を同定して意味役割を付与すべき項を抽出し，
それらに適切な意味役割を付与する．
評価実験の結果，尤度が閾値を超える意味役割のみを付与する条件の下，意味役
割を付与すべき項がわかっている文に対して精度77％，再現率68％，また，意味
役割を付与すべき項がわかっていない文に対して精度63％，再現率43％で意味役
割推定を実現し，本手法の有効性を示した．
また，同一の表層格をもつ項に対して，複数の異なる意味役割の付与を実現した．","This paper proposes a stochastic model for semantic role labeling based
on Japanese FrameNet and suggests a method to acquire it by machine
learning.
The model distinguishes semantic roles which cannot be separated by
surface cases.
The model receives a sentence and its predicate, identifies its
predicate argument structure, then identifies the arguments to be
labeled, and finally labels them with adequate semantic roles.
The system based on the model achieved 77\% precision and 68\% recall in
identifying the semantic roles of the pre-segmented arguments under the
condition that the system labels the role whose certainty is more than
the threshold.
For more difficult tasks of identifying the arguments which should be
labeled and their roles, the system attained 63\% precision and 43\%
recall under the same condition.
The system also achieved to label different semantic roles to the
arguments whose surface cases are identical.","['はじめに', 'モデルの定義', 'モデルの獲得', '評価', 'おわりに']",,,,,,,,
V14N01-04.tex,,A Keyword Extraction Method by Document Expansion,"キーワード抽出は情報検索に不可欠な技術の一つである．
例えば，検索速度の短縮や検索精度の改善に利用される．
既存のキーワード抽出法としては，語の統計情報や文書の構文上の特徴に基づくものなどがある．
その中で，辞書を一切用いず，反復度と呼ばれる統計量のみに基づくキーワード抽出法がある．
この方法には，文書数に上限があるとき複合語が一般的な語に分割されて，長いキーワードとして抽出できないという問題がある．
そこで本論文では，質問拡張のアイデアを利用して複数文書への繰り返し出現という考えを導入する．
そして，この考えを元にキーワード抽出法を提案する．
結果として，提案したキーワード抽出法のF値は上がった．
また，これまでに取れなかったキーワードが取れるようになった．
結論として，キーワード抽出における文書拡張の有用性を報告する．","Keyword extraction is one of the technology essential for information retrieval. There are methods based on the statistical information of words and features in syntax of the documents as existing keyword extraction methods. Among these methods, there is a keyword extraction method based on only a statistics which is called {\it adaptation","['はじめに', 'キーワードの定義', 'キーワード抽出', '文書拡張による反復度と出現確率のヒストグラム', 'キーワード抽出実験', '先行研究との比較', '結論']",,,,,,,,
V14N01-05.tex,,Generating Referring Expressions Using \\Perceptual Grouping,"参照表現とは，特定の物体を他の物体と混同することなく識別する言語表現で
ある．参照表現の生成に関する従来の研究では，対象物体固有の属性と異な
る2つの物体間の関係を扱ってきた．しかし外見的特徴の差異が少なく他の物体
との関係が対象物体の特定に用を成さない場合，従来の手法では対象物体を特
定する自然な参照表現を生成することはできない．
この問題に対して我々は知覚的群化を利用した参照表現の生成手法を
提案しているが，この手法が扱える状況は強く限定されている．本論文では，
我々が提案した手法を拡張し，より一般的な状況に対応できる参照
表現の生成手法を提案する．18人の被験者に対する心理実験をおこない，本論文の提
案手法を実装したシステムが適切な参照表現を生成できることを確認した．","Referring expressions are the linguistic representations that
identify the referent among objects. Past work
of generating referring expressions mainly utilized attributes of
objects and binary relations between objects. However, such an
approach does not work well when there is no distinctive attribute
among objects.
To overcome this limitation, we proposed a
generation method using perceptual grouping of objects.
However, this method can deal with very limited situations. This paper
proposes an extended method using perceptual grouping that can deal with 
more general situations.
The psychological experiments with 18 subjects showed that the
extented method could effectively generate proper referring
expressions.","['はじめに', 'SOG', '参照表現の生成', '評価と考察', 'まとめ']",,,,,,,,
V14N01-06.tex,,Chunking Japanese Compound Functional Expressions\\ by Machine Learning,"日本語には，複数の語がひとかたまりとなって，全体として1つの機能的な意
  味を持つ表現が多数存在する．このような表現は機能表現と呼ばれ，日本語文
  の構造を理解するために非常に重要である．
  本論文では，形態素を単位とするチャンク同定問題として機能表現検出タスク
  を定式化し，機械学習手法を適用することにより，機能表現の検出を実現する
  方法を提案する．
  Support Vector Machine (SVM)を用いたチャンカーYamChaを利用して，機能表
  現の検出器を実装し，実際のタグ付きデータを用いて性能評価を行った．機能
  表現を構成している形態素の数の情報，機能表現中における形態素の位置情報
  を素性として参照することにより，F値で約92という高精度の検出器を実現で
  きることを示す．","The Japanese language has many compound functional expressions which
  consist of more than one word including both content words and
  functional words.  They are very important for recognizing the
  syntactic structures of Japanese sentences and for understanding their
  semantic contents.
  We formalize detection of Japanese compound functional expressions as
  a chunking problem against a morpheme sequence, and propose to learn a
  detector of them using a machine learning method.
  The chunker YamCha based on Support Vector Machines (SVMs) is applied
  to this task.  Through experimental evaluation, we achieve the
  cross validation result of the F-measure as 92, when the number of
  morphemes constituting a compound functional expression, and the
  position of each morpheme within a functional expression are
  considered as features of SVM.","['はじめに', '日本語機能表現の検出', 'SVMを用いたチャンキングによる機能表現検出', '人手による規則を用いた検出', '実験と考察', '関連研究', 'おわりに']",,,,,,,,
V14N01-07.tex,,Text Generation for Intermediate Non-native Speakers \\of English,,"This paper describes the microplanner of the SILK system
which can generate texts appropriate for intermediate 
non-native users on discourse level. Four factors 
(i.e. nucleus position, between-text-span punctuation, 
embedded discourse markers and punctuation pattern) 
are regarded to affect the readability at discourse level. 
It is the preferences among these factors that decide the 
readability. Since the number of possible combinations of the
preferences is huge, we use Genetic Algorithm to solve such a problem. 
We adopt two methods to evaluate the system: one is evaluating 
the reliability of the SILK system by analysing how often 
it re-generates corpus texts, another is judging readability 
by human subjects. The evaluation results show that the
system is reliable and the generation results are appropriate 
for intermediate non-native speakers on discourse level.","['Background', 'An introduction to the microplanner', 'Preferences among factors affecting the readability on discourse \\\\\\hspace{18pt', 'Text generation using a GA', 'Evaluation', 'Related work', 'Conclusion']",,,,,,,,
V14N01-08.tex,,"Automatic F-term Classification of Japanese Patent 
	Documents Using the k-Nearest Neighborhood \\Method and the SMART Weighting",,"Patent processing is important in 
various fields such as industry, business, and law. 
We used F-terms \cite{Schellner2002_2","['Introduction', 'Background', 'Variations of the k-nearest neighborhood method', 'Method of calculating similarity', 'Regions used to extract terms', 'Experiment', 'Related studies', 'Discussion', 'Conclusion']",,,,,,,,
V14N02-01.tex,,Construction of Related Terms Thesauri from the Web,"本論文ではWeb上の情報を利用し，自動的に関連語のシソーラスを構築する手法を提案する．
検索エンジンを利用し，$\chi^2$値による語の関連度の指標を用い，
従来のWebを用いた関連度の指標の問題点を解決する．
また，新しいクラスタリング手法であるNewman法を用いて語のネットワークを
クラスタリングすることで，従来手法より適切に関連語を同定する．
コーパスおよび既存のシソーラスから生成した関連語正解セットを用い，
提案手法の効果についての検証を行う．","This paper describes a method to costruct related terms thesauri automatically based on Web information.
We utilize Web search engine to obtain word co-occurrence information and propose 
a new efficient similarity metrics applying $\chi^2$ value to solve problems of the existing methods.
We also introduce a new method to identify related terms using word-clustering.
We do word-clustering on that assocative network to identyfy related terms
using latest clustering methods, ``Newman method''.
We make evaluations and show the effectiveness of our approach using sets of related terms extracted from a corpus
 and a current thesaurus.","['はじめに', '関連研究', '検索エンジンを用いた関連性の測定', '関連度を用いたネットワークに基づくクラスタリング', '評価', '議論', '結論']",,,,,,,,
V14N02-02.tex,,Compositional Translation Estimation of Technical Terms Using a Domain/Topic-Specific Corpus Collected \\from the Web,"本論文では，ウェブを利用した専門用語の訳語推定法について述べる．
これまでに行われてきた訳語推定の方法の1つに，パラレルコーパス・コンパラ
ブルコーパスを用いた訳
語推定法があるが，既存のコーパスが利用できる分野は極めて限られている．
そこで，本論文では，訳を知りたい用語を構成する単語・形態素の訳語を既存の
対訳辞書から求め，これらを結合することにより訳語候補を生成し，単言語コー
パスを用いて訳語候補を検証するという手法を採用する．
しかしながら，単言語コーパスであっても，研究利用可能なコーパスが整備されて
いる分野は限られている．
このため，本論文では，ウェブをコーパスとして用いる．
ウェブを訳語候補の検証に利用する場合，サーチエンジンを通してウェブ全体を
利用する方法と，訳語推定の前にあらかじめ，ウェブから専門分野コーパスを収
集しておく方法が考えられる．
本論文では，評価実験を通して，この2つのアプローチを比較し，その得失を論
じる．
また，訳語候補のスコア関数として多様な関数を定式化し，訳語推定の性能と
の間の相関を評価する．
実験の結果，ウェブから収集した専門分野コーパスを用いた場合，ウェブ全体を
用いるよりカバレージは低くなるが，その分野の文書のみを利用して訳語候補の検証を
行うため，誤った訳語候補の生成を抑える効果が確認され，高い精度を達成できることがわかった．","This paper studies how to compile a bilingual
lexicon for technical terms using the Web.
In the task of estimating bilingual term correspondences of technical
terms, it is usually rather difficult to find an existing corpus for the
domain of such technical terms.
In this paper, we adopt an approach of collecting a corpus for the
domain of such technical terms from the Web.
As a method of translation estimation for technical terms, we employ a
compositional translation estimation technique, where
translation candidates of a term are compositionally generated
by concatenating the translation of the constituents of the term.
Then, the generated translation candidates are validated using the
domain/topic-specific corpus collected from the Web.
This paper further quantitatively compares the proposed approach with
another approach of validating translation candidates directly through a
search engine.
We show that the domain/topic-specific
corpus collected from the Web contributes to achieving higher precision
in translation candidate validation.","['はじめに', 'ウェブを用いた専門用語訳語推定', '要素合成法による専門用語訳語推定の定式化', '実験と評価', '関連研究', 'おわりに']",,,,,,,,
V14N02-03.tex,,A Chinese Syntactic Analysis Using Sentence Structure Grammar,"中国語構文解析では，
これまで，句構造文法(Phrase Structure Grammar)
で文の構造を取り扱ってきた．
しかし，句構造文法規則は規則間の衝突による不整合が避けられず，
曖昧性は大きな問題となっている．そこで，本論文では述語を中心とし，
全ての構文要素を文のレベルで取り扱う文構造文法SSG (\underline{S","Chinese sentence is parsed with PSG (Phrase Structure
Grammar) rules in syntactic analysis. Grammatical rules based on PSG
are not consistent so that ambiguity is a big problem.
In this paper, we propose a new kind of grammar SSG (Sentence Structure Grammar),
which describes all component and centers on predicative verb or adjective.
We build a Chinese grammatical rule system based on SSG and mount it on extended chart
parser Schart. The result of experiment showed that the syntactic analysis 
based on SSG that only uses the information of
part of speech and grammatical rules 
is very consistent and very effective to reduce syntactic ambiguity,
and it can gain a higher precision than syntactic analysis based on 
PCFG (Probabilistic Context Free Grammar).","['はじめに', 'PSG文法規則体系に基づく構文解析の問題点', '文構造文法規則体系SSGの基本的考え方', '中国語におけるSSG文法規則体系の設計', '評価実験', 'おわりに']",,,,,,,,
V14N02-04.tex,,Indonesian-Japanese Transitive Translation \\using English for CLIR,,"We propose a query transitive translation system of a CLIR
(Cross Language Information Retrieval) for a source language with a poor
data resource. Our research aim is to do the transitive translation with
a minimum data resource of the source language (Indonesian) and exploit
the data resource of the target language (Japanese). We did two kinds of
translation, a pure transitive translation and a combination of direct
and transitive translations. In the transitive translation, English is
used as the pivot language. The translation consists of two main
steps. The first is a keyword translation process which attempts to make
a translation based on available resources. The keyword translation
process involves many target language resources such as the Japanese
proper name dictionary and English-Japanese (pivot-target language)
bilingual dictionary. The second step is a process to select some of the
best available translations. We combined the mutual information score
(computed from target language corpus) and TF×IDF score in order to
select the best translation. The result on NTCIR 3 (NII-NACSIS Test
Collection for IR Systems) Web Retrieval Task showed that the
translation method achieved a higher IR score than the machine
translation (using Kataku (Indonesian-English) and Babelfish/Excite
(English-Japanese) engines). The transitive translation achieved about
38\% of the monolingual retrieval, and the combination of direct and
transitive translation achieved about 49\% of the monolingual retrieval
which is comparable to the English-Japanese IR task.","['Introduction', 'Overview of indonesian Query', 'Schema of Indonesian-Japanese Query Translation System', 'Compared Method', 'Experiments', 'Conclusions and Future Works']",,,,,,,,
V14N03-01.tex,,"Do Speakers Express Their Emotion, Evaluation, 
	or Attitude by Their Speech to Achieve Their Purposes?: A View from Everyday Oral Communication","「話し手は，迅速で正確な情報伝達や，円滑な人間関係の構築といった目的を果たすために，言語を使って自分の感情・評価・態度を表す」という考えは，言語の研究においてしばしば自明視され，議論の前提とされる．本稿は，話し手の言語行動に関するこの一見常識的な考え（「表す」構図）が，日常の音声コミュニケーションにおける話し手の実態をうまくとらえられない場合があることを示し，それに代わる新しい構図（「する」構図）を提案するものである．

現代日本語の日常会話の音声の記録と，現代日本語の母語話者の内観を用いた観察の結果，「表す」構図が以下3点の問題点をはらむことを明らかにする：(i)目的論的性格を持ち，目的を伴わない発話を収容できない；(ii)外部からの観察に基づいており，当事者（話し手）のきもちに肉薄し得ない；(iii)モノ的な言語観に立ち，言語を行動と見ることができない．

中心的に扱われるのは，あからさまに儀礼的なフィラー，つっかえ方，りきみである．「話し手は自分のきもちに応じて，フィラー・つっかえ方・声質を使い分けている」という「表す」考えが一見正しく思えるが，実はどのような限界を持つのかを，実際のコミュニケーションから具体的に示す．","It is often assumed as self-evidential that speakers express their emotion, evaluation, or attitude by their speech to achieve their purposes. In this paper I shall show that this common view, apart from its seeming plausibility, does not always capture successfully the very nature of speaker's behavior in everyday communication, and suggest an alternative view for understanding the correlation between speaker's speech and their emotion, evaluation, and attitudes.

Close observation on everyday Japanese conversation data, especially focused on disfluent phenomena such as fillers, stuttering, and pressed voice (a kind of creaky voice), by using native speaker introspection reveals that the so-called common view has three defects. The idea that the speaker uses various fillers, various ways of stuttering, various voice qualities to express his/her emotion, evaluation, and attitudes cannot explain the detail of these disfluent phenomena because (i) it does not accept unintended speech because of its teleological nature, (ii) it does not really touch speaker's psychology because it is based on outward perspective, and (iii) it regards language as a thing to express something rather than an expressing action itself.","['まえがき', '「表す」構図の問題点', 'あからさまに儀礼的なフィラー', 'つっかえ', 'りきみ', 'むすび〜個人と共同体の間']",,,,,,,,
V14N03-02.tex,,Copresence and The Functions of ``-desu/-masu'',"「です・ます」は，丁寧語としての用法のみならず場面に応じてさ
まざまな感情・態度や役割の演出などの表示となる．これは「です・ます」が持
つ「話手と聞手の心的距離の表示」という本質と，伝達場面における話手/聞手
のあり方とその関係の変化によって生じるものと考えられる．本稿では「です・
ます」をはじめ聞手を必須とする言語形式を，コンテクストとは独立して話手/ 
聞手の〈共在〉の場を作り出す「共在マーカー」と位置づけ，コンテクストにお
ける聞手の条件による「共在性」と組み合わせることで伝達場面の構造をモデル
化した．コミュニケーションのプロトタイプとしての〈共在〉の場では，「です・
ます」の本質的な機能が働き心的距離「遠」の表示となる．これに対して〈非共
在〉の場では，典型的には「です・ます」は出現しない．しかし，〈非共在〉の
場合でも共在マーカーが使用されると話手のストラテジーとして疑似的な〈共在〉
の場が作り出される．この場合，共在マーカーとしての役割が前面に出ることに
よって聞手が顕在化し，話手/聞手の関係が生じて「親・近」のニュアンスが生
まれる．「です・ます」が表す「やさしい」「わかりやすい」「仲間意識」など
の「親・近」の感情・態度は〈非共在〉を〈共在〉にする共在マーカーの役割に
よって，「卑下」「皮肉」といった「疎・遠」の感情・態度は〈共在〉での心的
距離の操作による話手/聞手の関係変化によって説明できる．",The ``{\it -desu/-masu,"['はじめに', '「です・ます」の諸用法 --- 従来の指摘と本稿の立場', '「伝達場面の構造」モデル', '「です・ます」諸用法発現のメカニズム', 'まとめと今後の課題']",,,,,,,,
V14N03-03.tex,,"Using Fillers as Mental Makers: Effects of Familiarity, Modality, and Task Difficulty in Describing the Figure",本研究の目的は，これまで言語学的には感動詞，言語心理学的には発話の非流暢性として扱われてきた，フィラーを中心に，情動的感動詞，言い差し（途切れ）といった話し言葉特有の発話要素を，人の内的処理プロセスが音声として外化した「心的マーカ」の一部であると捉え，それらが状況によってどのような影響を受けるかを分析し，対応する内的処理プロセスについて検討することであった．実験的統制のもと，異なる条件（役割や親近性，対面性，課題難易度）が設定され，成人男女56名（18--36歳）に対して，ペアでの協調問題解決である図形説明課題を実施し，対話データが収集された．その結果，1) それぞれの出現率は状況差の影響を受けたこと，2) 出現するフィラーの種類別出現率に差があることが示された．これらの結果が先行研究との対比，内的処理プロセスと心的マーカの対応，そして結果の応用可能性という観点から考察される．,"We examined the effects of familiarity, modality, and task difficulty on the use of fillers when describing a figure. A total of 56 adults (aged 18--38) participated in an experiment designed to elicit examples of disfluency words, such as fillers, affective interjections, and speech discontinuities. They were asked to solve a problem in same sex pairs. One was instructed to describe the figure, and the other to identify the correct figure from a choice of six. This experiment was done in various conditions when there was variation in how familiar the participants were with the figure, i.e., variation in familiarity; when the pairs could and not see each other, i.e., variation in modality; and when the task was both easy and difficult, i.e., variation in difficulty. The results showed two things. First, the average rates of filler, affective interjections, and speech discontinuities differed in relation to situational differences. Second, the filler rates varied with the type of filler. These results are discussed in terms of the relationship between mental processing and mental markers.","['はじめに', '方法', '結果', '考察']",,,,,,,,
V14N03-04.tex,,Know the Trigger Reason of Choice Behaviour with Elimination-By-Aspects,"選択肢の選択プロセスは各選択肢の特徴を認知する段階と，それに基づいて取捨選択を行う意思決定の段階，すなわち，「認知する」「決める」の2段階で表現することができる．既存の評判情報研究の多くは「認知する」の情報抽出に焦点を当てているのに対し，本稿では選択行動を意思決定までを含めて包括的に捉え，既存の方法では捉えることが困難だった要素を捉えることを試みる．
本稿では「決める」段階をElimination-By-Aspects (EBA)の意思決定モデルに則って選択の過程を通しで捉える方法を述べる．EBAでは，意思決定は，着目している特徴（アスペクト）を各選択肢が持っているか否かによって候補を順に排除していくことで行われるが，本稿では取捨選択方略に基づいて選択肢が排除または残存する様子を記述することで実現する．
また，ことばに明示的に表れている情報を単純に扱うだけでは不十分であり，行間を読み取る処理が必要である．
さらに，選択または排除されるきっかけの理由を捉えることで，選択肢の相対的な長所・短所を知る方法を示す．","The choice behaviour of alternatives can be expressed as a two-stage process: a stage to recognize the character of each alternative and a stage of decision making based on the character, i.e., `recognize' and `choose' stages.
Many of the existing studies on reputation and opinion analysis focus on the information extraction of the `recognize' stage, whereas this paper describes the entire choice behaviour including the `choose' stage and tries to capture unknown elements.
This paper reports the selection process throughout the `choose' stage, which can be described as conforming to Elimination-By-Aspects (EBA).
EBA performs the selection by eliminating according to whether the alternative has the feature (aspect) in question or not.
This paper achieves this process with choice strategies that eliminate or retain the alternatives.
Moreover, it is insufficient to simply handle the appeared information, hence, it is necessary to read between the lines.
Finally, we can know the merits and demerits of the alternatives by analyzing the reason for the triggering of selection or elimination.","['はじめに', '関連研究', 'データ収集方法', 'EBAに則った取捨選択プロセスの記述', '行間を読み取る', '選択または排除されるきっかけ', 'おわりに']",,,,,,,,
V14N03-05.tex,,Classification of Words Expressing the Emotional Affects Based on the Subjective Similarity Measures,日常生活の様々な体験において，その体験の素晴らしさを表現する言葉として，『感動』という言葉がしばしば用いられる．感動とは，『美しいものや素晴らしいことに接して強い印象を受け，心を奪われること』（大辞林\cite{Book_103,"Some wonderful experiences are expressed in Japanese by the word ``Kandoh'' which could be translated into English as ``emotional-affect''. The ``emotional-affect'' is defined by the dictionary as ``making people have strong feeling in facing beautiful or wonderful things''. According to the public-opinion poll by Mitsubishi Research Institute, Inc. in 2003, the mass media is one of the major subjects evoking emotional-affect. But there have been no studies that tried to define a mental state of emotional-affect.\par
The purpose of our study is to describe the emotional-affect for evaluation of broadcast programs. First, we addressed a questionnaire about emotional-affect and picked up words which expressed the situation of emotional-affect from the answers. Furthermore, we calculated a distance of the word by similarity measures based on the subjective evaluation. The obtained results are 1) the emotional affect could be classified into a few main groups, and 2) individual groups were classified by some factors including the object and kind of emotion, not the emotion itself. These results suggest that emotional-affect is the general term of the conditions of mind such as affirmative impressions and uncontrollable minds due to a very strong passive and compulsory stimulus.","['はじめに', '感動に関する従来の心理学研究', '日常生活における感動の抽出', '感動の分類', '考察', 'まとめ']",,,,,,,,
V14N03-06.tex,,An Analytical Frame of Affective Meanings Using Affect Control Theory,"Affect Control Theory とは，ある概念が意味する内容を計量的にとらえるSD法\footnote{
	C. E. Osgood \citep{Osgood","We measured affective meaning of words by SD method on three
 dimensions (Evaluation, Potency, Activity) as EPA scores.  Affect Control
 Theory is a mathematical and social-psychological theoretical frame in
 order to use these EPA scores how combinations of words generate
 affective meaning of sentences.  First we demonstrate the validity of
 our frame by using EPA score to distinguish homonyms in the
 process of Kana-Kanji translation system.  Second our frame has an
 ability to distinguish cross-cultural affective difference of meanings
 in the process of the translation from Japanese to English and reverse
 as well.","['Affect Control Theory の分析枠組', 'かな漢字変換における同音異義語問題の解決', '情緒的な意味を考慮した翻訳']",,,,,,,,
V14N03-07.tex,,"The Learner's Feeling Evaluation and Learning Attitude Analysis 
	Using the Comment Mail by Mobile Phone","本研究の目的は，携帯メールによって収集した大学生の書いた授業感想文の感情評価を行い，学習態度を分析することによって，授業改善のための方略を得ることである．感情評価の基準として，興味，意欲，知識，考察の4つのカテゴリを使い感想文を分類した．その結果形態素レベルでは有意な差を見出せなかったが，文脈から分類した結果，成績の良い学生の感想文には意欲と考察が多く，成績の悪い学生の感想文には興味と知識の多いことが示された．さらに成績が良かった学生と悪かった学生から無作為に1名ずつを選び，その感想文を比較した結果，成績の良い学生は授業内容を再構成しているが，成績の悪い学生は教師が教えたままを示していることが明らかになった．
以上により授業改善を行うには，学習者の意欲と考察が増すように，学習者が授業内容を再構成するように改善する必要性が示された．","The purpose of this research is to look for the lesson improvement method by 
learner's feeling evaluation and learning attitude analysis using the 
comment mail on a lesson wrote by the university student in a mobile phone. 
The sentences of these comment mails were classified into four categories, 
interest, motivation, knowledge and consideration as the standard of the 
feeling evaluation. Significant differences were not found in the morpheme 
level. However as a result of classification of the meanings of the 
contents, there are many motivation and consideration in high score 
students, and many interest and knowledge in low score students. Each one 
student was selected from high and low score students, and their sentences 
were compared, and it was shown that high score students remake the contents 
of the lesson by their own terms, but low score student makes a copy of 
content the teacher taught. In conclusion, for the lesson 
improvement，lesson should be remade to learner's motivation and 
consideration increase and learner's remaking the contents of the lesson 
increase.","['はじめに', '感情評価の基準', '授業感想文の収集と分析', '感想文を形態素から分類した結果', '感想文を文脈から分類した結果', '成績上位者と下位者の感想文の比較', '授業改善への考察', '成績上位者の授業感想文の例', '成績下位者の授業感想文の例']",,,,,,,,
V14N03-08.tex,,Relationship between Commands of $F_0$ Contour Control and Linguistic Information for Emotional Utterances,表現豊かな合成音声への応用を目的として，「喜び」と「悲しみ」の2種類の感情について複数の程度の感情情報を含む音声に対し，基本周波数パターン生成過程モデルに基づく韻律的特徴の分析を行った．分析により得られた各モデルパラメータの生起タイミングおよびその大きさの変化に関して，発話の言語的情報に基づき検討を行った．その結果，基底周波数に関しては，発話内容に依存する傾向の差は特にみられず，「喜び」「悲しみ」とも感情の程度が強くなるに従って，基底周波数は高くなる傾向にあった．文中でのフレーズ指令の生起に関しては，文節境界の枝分かれ種別と，直前のフレーズ指令以降のモーラ数に影響を受けることを確認した．フレーズ指令の大きさ関しては，その生起位置が文頭の場合と文中の場合とで，感情の程度に対する大きさの変化に違いがみられた．また，文中で生起したフレーズ指令は，その境界の枝分かれ種別により，異なる変化の傾向がみられた．アクセント指令の生起タイミングは，感情の有無やその程度の影響はほとんど見られず，アクセント型にのみ依存することが確認された．アクセント指令の大きさに関しては，文頭からの韻律語数に大きな影響を受け，またアクセント型による違いがみられた．,"In order to make synthetic speech rich in its expression, fundamental frequency contours were analyzed for the utterances of several emotional degrees with joy and sadness based on a model for the process of generation.  Changes in controlling parameters of the model with regard to degrees of emotion were examined in terms of linguistic factors of the utterances.  As a result, the baseline frequency increases as emotional degree increases, especially for sadness utterances.  About the phrase commands, the rate of occurrence increases as emotional degree increases at the right branch boundary in the grammatical structure for both joy and sadness, while the rate of occurrence at the left branch boundary for sadness is almost constant for emotional degrees.  The change of the amplitude of phrase commands is depended on the kind of position of grammatical structure.  About the accent commands, timings of their onsets and offsets are almost constant for emotional degrees.  They are depended on the accent types of prosodic words.  The magnitude of the accent commands changes as emotional degree increases depending on the positions of prosodic words from the beginning of the utterance.","['まえがき', '音声資料', '韻律的特徴の分析手法', '$F_0$パターン制御指令と言語的要因との関係についての検討', 'あとがき']",,,,,,,,
V14N03-09.tex,,"An Estimation Method of Degree of Speaker's Anger Emotion 
	with Acoustic and Linguistic Features","音声認識の精度の向上にともなって，コールセンターなどへの自動音声応答システムの導入の要求が高まり，人間がコンピュータと対話する機会も増加する傾向にある．これまでの対話システムは言語情報のみを扱い，そのパラ言語情報を扱うことは少ないため，人間同士の対話と比較すると，コンピュータとの対話ではコンピュータが得る人間の情報は小さい．
本研究では音声の音響的特徴と言語表現の特徴から推定可能な「怒り」の感情を検出するために，感情の程度による音響的・言語的変化を分析し，コンピュータと人間とのインタラクションにおける人間の感情を捉えることを目指す．非対面の擬似対話により，認識性能に対する不満からくる「苛立ち」や，クレーム対応時におけるユーザの「腹立ち」の内的感情を表現した怒りの音声を収録し，主観評価により感情の程度を付与した音声データを作成した．
本論では，怒りの感情を含むと判定された発話について，つぎの3種の特性，声の高さや強さ等の音響的特徴，
言語形態上の語彙使用の特徴，語用論的な特徴である文末表現の特徴に着目し，発話者の感情表現とその言語表現・音響的特徴との定量的な関係を分析し，怒り表現の音声言語の特徴付けを試みた．
とくに，接続助詞「けど」，「ので」の主節が現れずに発話が中止する接続助詞中止型に
おいて，怒りの程度が高いことを明らかにした．","This report describes a study on an estimation method of degree of speaker's 
emotion by using acoustic and linguistic features expressed in their anger utterances 
during a natural dialogue. We set two types of pseudo dialogues, 
the human-computer and the human-human, to induce anger utterances from 10 speakers. 
To make an emotional speech corpus with degree of emotion, 
a 5-scale subjective evaluation was conducted to grade each utterance 
on its emotional degree. The emotional speech corpus was examined to find acoustic and linguistic features 
which estimate the emotional degree of each utterance. 
Decision trees were adopted as classifiers for our estimation examination to find optimal sets of the acoustic and linguistic features for an anger degree estimation. 
As a result, we find specific tendencies of the tree acousitc features in strong anger utterances, the linguistic parameters' potential to estimate degree of anger emotion, and the capability of decision tree to estimate utterances with two kinds of acoustic features as the strong anger.","['はじめに', '関連研究', '音声資料', '「怒り」を含む発話の分析', '推定実験', 'おわりに']",,,,,,,,
V14N03-10.tex,,Reason-focused Analysis of Emotion Expression-related Components,"映画や書籍などの作品検索への応用を目的として，作品レビューテキスト中の感情表現の構成要素を分析した． まずWeb上の作品レビュー82件1,528文中の653組の主観表現を人手で分析し，「態度」「主体」「対象」「理由」という4つの構成要素と，その各々の下位要素を定義した．653組の主観表現中，「態度」が感情を表している感情は257組あった．次に感情表現の各構成要素の内容や働きを分析し，構成要素間の結びつきや，「主体」や「対象」が省略されるパタン，省略されない特殊なパタンなどを明らかにした．「理由」は，感情が生起した根拠や理由を述べている部分をさし，257組中66件（25.7{\kern0pt","As a preparatory study for using emotion in advanced retrieval systems of 
books and movies, we analyzed expressions of emotion that appeared in film 
and book reviews, and defined the related components. First, through the 
manual analysis of 653 subjective expressions in 82 book reviews selected 
randomly from the web, we defined four major components related to 
expressions of emotion and their subcomponents: ``Attitude'', ``Subject'', 
``Object'', and ``Reason''; we analyzed the existence of each component and 
their combinations in the reviews. Second, we found that different writers 
stated different ``Reasons'' for the same ``Emotion'' about the same 
``Object''; that tendency was confirmed in our additional analysis on 
emotion expressions, in which we focused on ``Reasons'' using different 
corpora consisting of film and book reviews and newspaper articles. Finally, 
we conducted experiments using 16 human subjects in two sessions of focused 
group interviews. The results showed that ``Reasons'' associated with 
emotion expressions appearing in film reviews are important and useful for 
users to select films relevant for them.","['はじめに', '関連研究', '感情表現のモデル作成', 'タグ付けの結果', '「理由」の特性の分析', '被験者実験', '被験者実験の結果と考察', 'おわりに']",,,,,,,,
V14N03-11.tex,,"Construction of Text-dialog Corpus with Emotion Tags Focusing 
	on Facial Expression in Comics","信頼性の高い情緒タグ付きテキスト対話コーパスを実現することを狙い，漫画の対話文を対象に，登場人物の表情を参照する方法によって情緒タグを付与した．また，得られた対話コーパスの信頼性を評価した．通常，言語表現と話者の情緒とは，必ずしも直接的な対応関係を持つとは限らず，多義の存在する場合が多いため，対話文に内包された情緒を言語表現のみによって正しく判定することは難しい．この問題を解決するため，既に，音声の持つ言語外情報を活用する方法が試みられているが，大量の音声データを収集することは容易ではない．そこで，本稿では，漫画に登場する人物の表情が持つ情報に着目し，タグ付与の信頼性向上を図った．具体的には，漫画「ちびまる子ちゃん」10冊の対話文（29,538文）を対象に，1話につき2人のタグ付与作業者が一時的な「表情タグ」と「情緒タグ」を付与した後に，正解とする表情タグと情緒タグを両者が協議して決定するという手順で，コーパスを構築した．決定された正解の情緒タグは16,635個となった．評価結果によれば，付与された一時的な情緒タグの作業者間での「一致率」は78\%で，音声情報を使用した場合（81.75\%）と比べて遜色のない値を示していること，また，最終的に決定した情緒タグに対する作業者以外の者による「同意率」は97\%であることから，タグ付与の安定性が確認された．また，得られたコーパスを「情緒表現性のある文末表現の抽出」に使用したところ，3,164件の文末表現が情緒の共起割合とともに抽出され，自然で情緒的な文末表現が得られたことから，本コーパスに対しての「言語表現と情緒の関係を分析する上での1つの有効性」が示された．以上から，情緒判定において，漫画に登場する人物の表情は，音声に匹敵する言語外情報を持つことが分かり，それを利用したタグ付与方法の信頼性が確認された．","We annotated emotion tags to text-dialogs in comics with focusing facial expressions in order to construct reliable dialog corpus with emotion tags, and evaluated the reliability of the constructed corpus. Generally, the relationship between language expression and emotion of the speaker is ambiguous, so it is difficult to distinguish correct emotion existing inside of the speaker with referring only the language expression in dialog. To solve this problem, there exist many investigations using non-lingual information of acoustic data though it costs to collect much speech data. Therefore, in this paper, we focuse on the facial expression appearing in comics to gain the reliability of the emotional annotating. For instance, we used 10 comic books on ``Chibimaruko-chan'' containing 29,538 sentences and constructed emotional corpus annotated by the scheme, where two annotators annotate facial tags and emotion tags temporally and then decide correct tags by their discussion together. The correct emotion tags were 16,635. The evaluation results proved that the agreement ratio of the temporary emotion tags between the two annotators was 78\% which was as good as the related work in speech (83\%) and the correctness of the decided emotion tags was 97\%. Next, since in our trial experiment to extract emotional suffix expression from the corpus we successed to extract 3,164 ones, the usability for dialog analysis on emotion was clarified. Thus, we confirmed that the facial expression in comics is as effective for distinction of emotions as speech and the schema of corpus construction using facial expression in comics is reliable.","['はじめに', '情緒の位置づけ', 'コーパスの構築', '安定性の評価', '有効性の評価', '考察', 'おわりに']",,,,,,,,
V14N03-12.tex,,The Method of the Emotion Judgment Based on \\an Association Mechanism,我々は，人とのコミュニケーションの仕組みを機器とのインタフェースとして実現することを目標に研究を行っている．人間は会話をする際に意識的または無意識のうちに，様々な常識的な概念をもって会話を展開している．このように会話文章から常識的な判断を行い，適切に応答するためには，ある語から概念を想起し，さらに，その概念に関係のある様々な概念を連想できる能力が重要な役割を果たす．本稿では，ある概念から様々な概念を連想できるメカニズムを基に，人間が行う常識的な判断の一つである感情に関する判断を実現する方法について提案している．「主体語」，「修飾語」，「目的語」，「変化語」の4要素から成るユーザの発話文章から，そのユーザの感情を基本感情10種類，補足感情24種類で判断する手法を提案している．また，本手法を用いた感情判断システムを構築し，その性能を評価した結果，常識的な解の正答率は76.5{\kern0pt,"A human-like common sense and judgment is necessary to materialize a computer that can take communication with human. Because, when people talk to each other, they have the concept of emotion in our mind consciously or unconsciously. In the case, the ability to call concept in mind and to associate with many referred concepts will be an important matter. This paper proposes the method to systemize judgment concerning emotion, based on the mechanism to associate concept with many other referred concepts. As a result, the percentage of correct answers of the emotion judgment system is approximately 88.0\%. Therefore, the emotion judgment system using the technique proposed in this paper is an effective system.","['はじめに', '感情判断システム', '連想メカニズム', '対象語の処理', '変化語の処理', '感情判断の処理', '感情判断システムの性能評価', 'おわりに']",,,,,,,,
V14N03-13.tex,,Emotion Estimation Algorithm based on Emotion Occurrence Sentence Pattern,"近年の情報処理技術の発達に伴い，従来の情報処理の分野ではほとんど
取り扱われなかった人間の感性をコンピュータで処理しようとする試みが盛んになってきた．
擬人化エージェントや感性ロボットが人のように振舞うためには，
人間が表出する感情を認識し，自ら感情を表出することが必要である．
我々は，感性ロボットに応用するための感情認識技術について研究している．
自然言語会話文からの感情推定を行う試みは，多くの場合，
表面的な感情表現のみに絞って行われてきた．しかし，人間の発話時には常に何らかの
感情が含まれていると考えられる．
そこで，本稿では，感情語と感情生起事象文型パターンに基づいた感情推定手法を
提案し，実験システムを構築する．
そして，本手法の有効性を調べるため，シナリオ文を対象にその評価実験を行った．","In recent years, approach which tries to process human's sensibility with computer has
become active as information processing technology develops. 
It is necessary to recognize human emotions so that
the anthropomorphic agent and the sensibility robot
may behave like the person, and to express own emotions.
We are researching the emotion recognition technology to apply it to the sensibility robot.
The approach of most emotion estimation targeted only superficial emotion expression.
In this paper, we propose an emotion estimation algorithm based on
the emotion word (or emotion idiom) and the emotion occurrence event sentence pattern.
A  prototype system based on the proposed method has been constructed and
 an evaluation experiment has been carried out. The result shows that the proposed method is effective.","['はじめに', '従来研究', '提案手法', '評価実験1', '評価実験2', 'まとめ']",,,,,,,,
V14N03-14.tex,,Assigning Polarity Scores to Reviews Using Machine Learning Techniques,本論文では，ある対象を評価している文章（レビュー）が与えられた時，対象物に対する評価が「良い」か「悪い」かでレビューを二値分類するのではなく，どの程度「良い」か「悪い」かの指標（sentiment polarity score （SP score））をレビューに与える新しいタスクを提案する．SP scoreはレビューの簡潔な要約であり，単純な「良い」か「悪い」かの二値分類より詳細な情報を与える．このタスクの難しさは連続した量であるSP scoreをどのようにしてレビューから得られるかにある．本稿ではsupport vector regressionを用いてSP scoreを求める方法を提案する．5段階評価がついた本に対するレビューを用いた実験で，我々の手法がsupport vector machinesを用いた多値分類より高い精度であり，人による指標の予測結果に近いことを示す．また，Naive Bayes Classifierを用いた文単位での主観性分析を用いることにより我々の手法の頑健性が増すことを示す．,We propose a novel type of document classification task that quantifies how much a given document (review) appreciates the target object by using a continuous measure called \textit{sentiment polarity score,"['Introduction', 'Related Work', 'Analyzing Reviews with Polarity Scores', 'Assigning SP scores to Reviews', 'Experiments', 'Conclusion']",,,,,,,,
V14N03-15.tex,,"Classification of Feedback Documents Considering a Level of Reputations 
	and Reliability Evaluation of Reputations","本論文では，Web上の評判情報を有益に活用するために，レビューなどの評価
文書をポジティブ（おすすめ）とネガティブ（おすすめしない）という極性値に分類
する手法を
提案する．本手法では，全体評判情報と部分評判情報という2つの
レベルで評判情報を捉える．全体評判情報とは評価文書の対象全般に関わる評
価表現のことを指し，部分評判情報とは対象の一属性に関する評価表現のこと
を指す．全体評判情報の極性値は評価文書の極性値と一致すると考えられるため，まず
全体評判情報を用いて評価文書を分類し，全体評判情報がない場合は部分評判情報を用
いて分類する．これら2つのレベルの評判情報を考慮することで分類精度の向
上が期待できる．
さらに，これら2つのレベルの評判情報を用いることで，評
判情報の信頼性評価の一手法を提案する．ここでは，評価文書の極性値とその中
の部分評判情報の極性値が異なる場合にその部分評判情報は信頼性が高いと評価する．
映画のレビューを用いた評価実験の結果，ナイーブベイズを用いた分類手法よ
りも本手法の方が良い結果が得られた．また，提案した評価指標が評価文書の信頼性評価の1つと
なりうることを示唆した．","This paper describes a method for classifying feedback documents into
two polarities: positive and negative.
In this method, we classify reputations into ``Object Level
Reputations'' and ``Attribute Level Reputations''.
Object level reputations are the reputations concerning the object of the
feedback document. Attribute level reputations are the reputations
concerning one attribute of the object. Since we asuume that the polarity of the object
level reputation corresponds to the polarity of the feedback document,
feedback documents are first classified using the object level reputation.
The feedback documents that do not contain object level reputations
are classified using the attribute level reputations. In addition,
this paper proposes a method for reliability evaluation of
reputations considering two levels of reputations. In this method,
we regard the attribute level reputations that has the opposite polarity of the
feedback document as reliable reputations.
The experimental results using movie reviews showed that the proposed method could 
classify feedback documents more correctly than the previous method,
and that the proposed measure can be one of the reliability measures for reputations.","['はじめに', '評判情報', '評価文書の分類', '評価情報の信頼性評価', '評価実験', 'おわりに']",,,,,,,,
V14N04-01.tex,,Retrieving Syntactically Annotated Corpora using a Relational Database,本論文では，構文木をクエリとして与え，構文木付きコーパスからクエリと同じ構文木を部分木として含む文を検索する手法を提案する．構文木付きコーパスは，関係データベースに格納する．このような構造検索の過去の研究では，クエリの節点数が増加すると，検索時間が大幅に増加する問題があった．本論文で提案する手法は，節点数が多いクエリを部分木に分割し，漸進的に検索することで検索を効率化する．クエリの分割の単位やその検索順序は，検索対象となるコーパス中の規則の出現頻度をもとに自動的に決定する．本手法の有効性を確認するために7種類のコーパスを用いて評価実験を行ったところ，4種類のコーパスで分割の有効性が確認できた．,"This paper presents a method to retrieve sentences including  the same subtree as a given query from a treebank. Our system stores the treebank in a relational database. One of the problems of the previous work in structure retrieval is efficiency for large queries. 
The proposed method divides a large query into several subtrees, and incrementally narrows down the result by using these subtrees as queries. The number of subtrees and the order are determined automaticaly based on the treebank statistics. We conducted experiments to evaluate the proposed method with seven treebanks and found that the proposed method significantly improved the retrieval efficiency  in four out of seven treebanks.","['はじめに', '構文木付きコーパスのデータベース化', '構文木付きコーパスの検索手法', '評価実験', '考察', 'まとめ']",,,,,,,,
V14N04-02.tex,,Retrieving Translation Candidates from Patent Corpora,本論文では，日英特許コーパスを用いて専門用語の対訳辞書を作成する方法について述べる．提案手法は，言語単位としての妥当性と分野による出現の偏りを数値化することで，コーパス中の単語（列）を専門用語として抽出し，和英辞書などの既知の対訳用語セット（seed word リスト）を介して，コーパスにおける各専門用語の共起パターンを計測し，その類似性が高い用語ペアを対訳として対応付ける．この時，対象となるコーパス間で文脈が類似している対訳のみをseed wordに利用する点が特徴である．本手法を日本語特許抄録とその英訳に適用したところ，専門用語の抽出精度は日本語で90\%，英語で93\%となった．また，訳語対応付けでは，各専門用語の対訳として1位に対応付けられた対訳候補の正解率が53\%（日英）と66\%（英日），10位以内に対応付けられた対訳候補の正解率が83\%（日英）と90\%（英日）と，従来研究と比べて高い精度を得ることができた．本論文ではさらに，PAJの日本語抄録と米国特許抄録を用いた実験を行い，コーパスの違いによる実験結果の違いについても考察する．,"This paper describes a method for retrieving technical terms and finding their translations from bilingual patent corpora. The method extracts terms from each monolingual corpus and finds their translations by using a list of bilingual word pairs called ``seed words''. In the term extraction process, we quantify the unithood and termhood of word sequences to determine if they are technical terms. In the translation alignment process, we select seed words whose contexts are similar in the target corpora. We conducted experiments in term extraction and translation alignment with patent abstracts of Japan and the United States. In the term extraction, the proposed method has achieved a precision of 90\% for Japanese term extraction and 93\% for English term extraction. In the translation alignment, the accuracy was 53\% (Japanese to English) and 66\% (English to Japanese) for the top candidates and 83\% (J to E) and 90\% (E to J) for the top 10 candidates. Comparison of the results between parallel corpora and comparable corpora is also described.","['はじめに', 'コーパスを利用した対訳辞書構築の要素技術', 'アルゴリズム', '実験と評価', 'まとめ']",,,,,,,,
V14N04-03.tex,,Decision Method for Acceptability of Defection of Adnominal Clause or Phrase for Elaboration,"文章推敲に関する従来研究では，主に，タイプミス，構文構造の複雑さ，表記の揺
れを指摘する手法など，表記レベルと統語レベルの手法に重点がおかれていた．
それに対して，本研究では，読みやすさを向上させるために，
説明が不足していて論理展開が読み取りにくいと感じられる箇所を検出する技術
を扱う．文章としては情報を正確に伝達するための仕事文（仕事用の文）を対象とし
て，文単位での情報不足を推敲対象とする．この課題は意味処理に踏み込むため，
これまで十分研究が行われてこなかった．なお，語用論の「協調の原理」によれ
ば，量の格率と呼ばれる情報不足と情報過多に関する遵守すべき原則がある．
このうち情報過多を扱わない理由は，情報過多が，冗長な情報を無視するのに基づく
読者の負担を増やすだけであるのに対し，情報不足は理解困難という深刻
な事態を招き，重要性が高いためである．
実験準備から解析に至る流れは，次の通りである．
まず，原文から連体修飾部を欠落させた課題文を
生成し，次に，被験者にその箇所に情報不足を感じるかどうかを判定させ
正解判定データを作成した．その後，正解判定データの一部から機械学習を行い，
残りのデータを機械判定させる．機械判定に用いる主な素性として，
修飾部の欠落箇所におけるつながりの滑らかさに関係した
語の連鎖に関する統計量を取り上げた．約1,000箇所の判定課題に対し，
SVMによる機械学習アルゴリズムを用いた自動判定により正解率を測定した結果，
機械判定の正解率として，ベースライン50{\kern0pt","Previous work on elaboration mainly focuses on
expression-level and/or structure-level technologies
such as correction of typing errors, detection and indications of
the complexity of syntactic structures, fluctuations of expressions and so on.
In contrast, this paper deals with technologies to detect portions
in each sentence, 
where readers feel difficult in reasoning contexts because of information
defection.
We constrain sentences in business writings used as communication media to
transfer information correctly.
This problem is placed in a semantic-level elaboration that has not
been studied sufficiently.
According to ``cooperative principle'' in pragmatics,
there are principles for information defection or information overload
that are called ``maxims of quantity''.
This paper only deals with information defection.
The reason why this paper does not deal with information overload is
that information overload only imposes burden on readers 
not to take account of redundant information. On the other hand, information defection
leads to serious problems that make readers difficult to understand.
The process from preparation of experiments to analyses is as follows.
Firstly, we generate sentences 
where adnominal regions are eliminated. Secondly, we prepare correct
data sets by subjective judgements whether examinees feel explanations
are insufficient or not. Finally, we apply machine learning and automatic
decision on this data. We used $n$-gram statistics and others to
evaluate smoothness of connections between regions crossing missing
portion of adnominal clause of a phrase.
We obtain correct decision rate 67\% in the result of about 1,000 decision tasks
used with SVMs,
against base-line rate 50\% and upper limit of correct decision rate
76\% (determined by dispersion of decisions by human subjects).","['はじめに', '全体概要', '課題文と正解データの作成', '機械判定で用いる素性の定義', '機械判定の正解率測定結果', '機械判定のパラメータ検討', '考察', '関連研究', 'まとめ']",,,,,,,,
V14N04-04.tex,,A Fully-Lexicalized Probabilistic Model for Japanese Syntactic and Case Structure Analysis,"本稿では，格フレームに基づき構文・格解析を統合的に行う確率モデルを提案す
 る．格フレームは，ウェブテキスト約5億文から自動的に構築した大規模なもの
 を用いる．確率モデルは，述語項構造を基本単位とし，それを生成する確率であ
 り，格フレームによる語彙的な選好を利用するものである．ウェブのテキストを
 用いて実験を行い，特に述語項構造に関連する係り受けの精度が向上することを
 確認した．また，語彙的選好がどの程度用いられているかを調査したところ，
 60.7\%という高い割合で使われていることがわかり，カバレージの高さを確認す
 ることができた．","This paper presents an integrated probabilistic model for Japanese
 syntactic and case structure analysis. Syntactic and case structure are
 simultaneously analyzed based on wide-coverage case frames that are
 constructed from a huge raw corpus in an unsupervised manner. This
 model selects the syntactic and case structure that has the highest
 generative probability. We evaluate both syntactic structure and case
 structure. In particular, the experimental results for syntactic
 analysis on web sentences show that the proposed model significantly
 outperforms known syntactic analyzers.","['はじめに', 'ウェブから獲得した大規模格フレーム', '構文・格解析の統合的確率モデル', '実験', '関連研究', 'おわりに']",,,,,,,,
V14N04-05.tex,,Emotion Recognition using Mel-Frequency \\Cepstral Coefficients,,"In this paper, we propose a new approach to emotion recognition. Prosodic features are currently used in most emotion recognition algorithms. However, emotion recognition algorithms using prosodic features are not sufficiently accurate. Therefore, we focused on the phonetic features of speech for emotion recognition. In particular, we describe the effectiveness of Mel-frequency Cepstral Coefficients (MFCCs) as the feature for emotion recognition. We focus on the precise classification of MFCC feature vectors, rather than their dynamic nature over an utterance. To realize such an approach, the proposed algorithm employs multi-template emotion classification of the analysis frames. Experimental evaluations show that the proposed algorithm produces 66.4\% recognition accuracy in speaker-independent emotion recognition experiments for four specific emotions. This recognition accuracy is higher than the accuracy obtained by the conventional prosody-based and MFCC-based emotion recognition algorithms, which confirms the potential of the proposed algorithm.","['Introduction', 'Emotion Recognition Algorithm', 'Evaluation Experiments', 'Conclusion']",,,,,,,,
V14N05-01.tex,,A Study of the Position of Discourse Markers: Focusing on the Texts Whose Target Audience Was Intermediate Non-native Speakers of English,,"As an international language, English has become more and more important for
non-native speakers. Therefore, writers ought to consider the
needs of non-native speakers, i.e. write English in a way that can be
understood quite well by non-native audience. In this paper, we investigate
the position of six discourse markers within the texts whose target audience 
was intermediate non-native speakers of English. The six discourse markers 
are: \textit{because","['Background', 'Investigating the position of discourse markers', 'Experiment results of C4.5', 'Verifying the experiment results of C4.5 by SVM', 'Discussion', 'Related work', 'Importance of the study', 'Conclusion']",,,,,,,,
V14N05-02.tex,,A Construction of Large-scale Concept-base for Calculation of Degree of Association between Concepts,"人間は日常会話において，様々な連想を行っている．例えば，「車」という語から「タイヤ」，
「エンジン」，「事故」，…，といった語を自然に思い浮かべ，連想によって会話の内容を柔軟に
拡大させている．コンピュータ上での連想機能の実現には，概念ベースが重要な役割を果たす．
概念ベースでは，言葉の意味（概念）を属性とその重みで定義している．概念ベースの構築方式として，
概念（約4万語）とその属性を，電子化国語辞書の語義説明文から抽出する方法が提案されている．
しかしながら，定義的な国語辞書から取得される概念や属性の数が少数であり，連想の精度に
問題がある．

本論文では，電子化国語辞書の語義説明文から構築した概念ベースを核に，電子化新聞等の
一般的な記事文から共起情報を基に概念ベースを拡大し，約12万語規模の概念ベースを構築する
手法を提案している．概念ベースの拡張においては，まず，国語辞書の各見出し語に対する語義
説明文から基本的な概念に対し，信頼性の高い属性を取得する．それらを基に，新聞記事等から
抽出した各概念に対する共起語を属性候補として追加する．その後，属性関連度（概念と属性の
関連の強さ）により不適切な属性（雑音属性）を除去し，属性の質を向上させている．また，各属性に
付与する重み（属性重み）は，概念を属性集合により構成される仮想文書と捉え，文書処理における
キーワードの重み付与方法（$\mathit{tf","We human beings associate various words in daily conversation. For example, we naturally associate `Tire', `Engine', `Accident', and so on with `Automobile', and expand contents of conversation by association. Concept-base is the key role for achievement of association mechanism on computers. The meanings of words (concepts) are defined by attributes and weights in Concept-base. As construction method of Concept-base, it is suggested that concepts (about 40000 words) and attributes are picked up from descriptive texts on electronic dictionaries. However, the number of concepts and attributes picked up from dictionaries are small, and Concept-base has some problems about accuracy of association.

In this paper, Concept-base is expanded by coincidence information of general texts such as electronic newspapers based on Concept-base which is constructed from descriptive texts on electronic dictionaries, and it is suggested that a construction method of 120,000 words scale Concept-base. In extension of Concept-base, first, basic concepts are gotten from descriptive texts on electronic dictionaries about each words which are mentioned in dictionaries and get attributes which have high reliability. Co-occurring words are gotten based on Concept-base which is made from electronic dictionaries as nomination of attributes from electronic newspapers. After this manipulation, improper attributes (noise attributes) are cut off using Degree of Association of attributes, and attributes' quality is made higher. In addition, weights (attributes' weights) of each attributes are given as weights often used in information retrieval and text mining by ascribing Concept-base to virtual documents. At the last, it is shown that accuracy of Concept-base made by suggested method is higher than accuracy of Concept-base made by only dictionaries using experiment of Degree of Association.","['はじめに', '概念ベース', '関連度計算方式', '関連度計算方式を用いた概念ベース評価法', '国語辞書を用いた基本概念ベース構築法', '新聞記事を用いた概念ベース構築法', '$X$-$\\mathit{ABC', 'おわりに']",,,,,,,,
V14N05-03.tex,,A solution for the problem of Existential Expressions in Japanese-Chinese Machine Translation,存在文はいかなる言語にも存在し，人間のもっとも原始的な思考の言語表現の一つであって，それぞれの言語で特徴があり言語により異なりが現れてくる．存在表現の意味上と構文上の多様さのために，更に中国語との対応関係の複雑さのために，日中機械翻訳において，曖昧さを引き起こしやすい．現在の日中市販翻訳ソフトでは，存在表現に起因する誤訳（訳語選択，語順）が多く見られる．本論文では，日中両言語の存在表現における異同について考察し，日中機械翻訳のために，日本語文の構文特徴，対応名詞の属性，中国語文の構文構造などを利用して存在動詞の翻訳規則をまとめ，存在表現の翻訳方法について提案した．これらの翻訳規則を我々の研究室で開発している日中機械翻訳システムJaw/Chineseに組み込んで，翻訳実験を行った．更に手作業による翻訳実験も加えて，この規則を検証し，良好な評価を得た．,"Existential sentence as one of primitive sentence patterns is very important for each language, and has characteristics of itself for different language. However the variety of syntactic and semantic use of existential expression and complicated correspondence to Chinese leads to ambiguities in Japanese-Chinese machine translation. Therefore there are numerous mistranslations by the currently commercially available translation software in existential expression, such as vocabulary selection and word order determination.
In this paper, we propose a method for handling the existential verbs based on the constraint of Japanese syntactic and semantic features, Chinese syntactic features, the attributes of the related nouns and so on. Furthermore we implement the translation rules in Jaw/Chinese which is the Japanese to Chinese translation system developed by our lab and evaluate our rules. And we also made manual experiment over 700 existential sentences and get an accuracy of about 90\%, which is rather high compared to the currently commercially available translation software. Both of the evaluations indicate that our method provides a high accuracy and is available.","['はじめに', '日本語と中国語における存在表現', '存在表現の中国語への機械翻訳', '翻訳実験と評価', '存在文の日中機械翻訳に関連する今後の課題', 'おわりに']",,,,,,,,
V14N05-04.tex,,Ensemble Document Clustering Using Weighted Hypergraph Generated by NMF,"本論文では Non-negative Matrix Factorization (NMF) を利用したアンサン
ブル文書クラスタリングを提案する．

NMF は次元縮約を利用したクラスタリング手法であり，文書クラスタリングの
ようにデータが高次元かつスパースとなる場合に効果を発揮する．ただし NMF
は初期値によって得られるクラスタリング結果が異なるという問題がある．
そのために通常は初期値を様々に変えて，複数個得られたクラスタリング結果から，NMF の分解の精度
の最もよい結果を選択する．しかし NMF の分解の精度はクラスタリング結果の精度を直
接表しているわけではないので，最適な選択が行える保証はない．

ここでは NMF によるクラスタリングの精度を高めるために，
複数個得られたクラスタリング結果をアンサンブルすることを試みる．
アンサンブルは，複数個のクラスタリング結果からハイパーグラフを作成し，そのハイパーグラフ
で表現されたデータをクラスタリングすることで行える．従来，そのハイパーグラフ
は 0 か 1 のバイナリ値が用いられていたが，ここでは NMF の結果を用いて，適
切な実数値の重みを与えることで改良する．実験では k-means，NMF，通常のハイ
パーグラフを用いたアンサンブル手法および重み付きハイパーグラフを用いたア
ンサンブル手法（本手法）のクラスタリング結果を比較し，本手法の有効性を
示す．","In this paper, we propose a new ensemble clustering method using
Non-negative Matrix Factorization (NMF).

NMF is a kind of the dimensional reduction method which is effective
for high dimensional and sparse data like document data.  
NMF has the problem that the result depends on the initial value of
the iteration.  The standard countermeasure for this problem is that
we generate multiple clustering results by changing the initial value,
and then select the best clustering result estimated by 
the NMF decomposition error.  However, this selection does not work well because 
the NMF decomposition error does not always measure the accuracy of the clustering.

To improve the clustering result of NMF, we propose a new ensemble clustering
method.  Our method generates multiple clustering results by using the
random initialization of NMF.  And they are integrated through the
weighted hypergraph, which can directly be constructed through the result
of NMF, instead of the traditional binary hypergraph.

In the experiment, we compared the k-means, NMF, the ensemble method
using the standard hypergraph and the ensemble method using the weighted
hypergraph (our method).  Our method achieved best.","['はじめに', 'NMF と初期値の問題', 'アンサンブルクラスタリング', '実験', '考察と関連研究', 'おわりに']",,,,,,,,
V14N05-05.tex,,A Dictionary of Japanese Functional Expressions with Hierarchical Organization,"日本語には，
「にたいして」や「なければならない」に代表されるような，
複数の形態素からなっているが，
全体として1つの機能語のように働く複合辞が多く存在する．
われわれは，機能語と複合辞を合わせて機能表現と呼ぶ．
本論文では，
自然言語処理のための日本語機能表現辞書について提案する．
日本語の機能表現が持つ主な特徴の1つは，
個々の機能表現に対して，多くの異形が存在することである．
計算機が利用することを想定した辞書を編纂する場合，
これらの異形を適切に扱う必要がある．
われわれが提案する辞書は，
機能表現の異形を体系的に整理するために，
見出し体系として，9つの階層からなる階層構造を用いる．
現在，この辞書には，
341の見出し語と16,771の出現形が収録されており，
既存の機能表現リストと比較した結果，
各々の見出し語に対して，
ほぼすべての異形が網羅されていることが確かめられた．","The Japanese language has a lot of functional expressions,
each of which consists of more than one word and 
behaves like a single function word.
A remarkable characteristic of Japanese functional expressions 
is that 
each functional expression has many different surface forms.
This paper proposes a dictionary of Japanese functional expressions 
with hierarchical organization.
We use a hierarchy with nine abstraction levels:
the root node is a dummy node that governs all entries;
a node in the first level is a headword in the dictionary;
a leaf node corresponds to a surface form of a functional expression.
We have compiled the dictionary with 341 headwords and 
16,771 surface forms,
which covers almost all of surface forms for each headword.","['はじめに', '日本語機能表現', '機能表現辞書の設計', '機能表現の階層構造', '機能表現辞書の編纂', '関連研究', 'おわりに']",,,,,,,,
V14N05-06.tex,,An Efficient and User-friendly Sinhala Input Method Based on Phonetic Transcription,,We propose an application-independent  Sinhala character input method called {\it Sri Shell,"['Introduction', 'Sinhala Language and Characters', 'Sinhala Input Systems', 'Proposed system', 'Evaluation', 'Conclusions and Future Work']",,,,,,,,
V14N05-07.tex,,Automatic Detection of Japanese Compound Functional Expressions and its Application to Statistical Dependency Analysis,"日本語には，「にあたって」や「をめぐって」のように，2つ以上の語から構成
 され，全体として1つの機能的な意味をもつ機能表現という表現が存在する．一
 方，この機能表現に対して，それと同一表記をとり，内容的な意味をもつ表現
 が存在することがある．そして，この表現が存在することによって，機能表現
 の検出は困難であり，機能表現を正しく検出できる機能表現検出器が必要とさ
 れている． 
 そこで，本論文では，日本語機能表現を機械学習を用いて検出する手法を提案
 する．
 提案手法では，Support Vector Machine (SVM) を用いたチャンカーYamChaを利用
 して，形態素解析結果を入力とする機能表現検出器を構築する．
 具体的には，形態素解析によって得られる形態素の情報と，機能表現を構成し
 ている形態素の数の情報，機能表現中における形態素の位置情報，機能表現の
 前後の文脈の情報を学習・解析に使用することにより，F値で約93\%という高精度
 の検出器を実現した．
 さらに，本論文では，機能表現検出器の解析結果を入力として，
 機能表現を考慮した係り受け解析器を提案する．
 提案手法では，Support Vector Machine (SVM)に基づく統計的係り受け解析手法
 を利用して，機能表現を考慮した係り受け解析器を構築する．
 具体的には，京都テキストコーパスに対して，機能表現の情報を人手で付与し，機能表
 現の情報を基に文節の区切りや係り先の情報を機能表現を考慮したものに変換
 した．そして，SVMに基づく統計的係り受け解析の学習・解析ツールCaboChaを
 用いて，変換したデータを学習し，機能表現を考慮した係り受け解析を実現し
 た．評価実験では，従来の係り受け解析手法よりもよい性能を示すことができた．","The Japanese language has many compound functional expressions
 which consist of more than one words including both content words
 and functional words, e.g., \mbox{``にあたって''","['はじめに', '機能表現およびその用法', '機能表現検出', '機能表現を考慮した係り受け解析器', '関連研究', '結論']",,,,,,,,
V14N05-08.tex,,A Game-Theoretic Model of Referential Coherence and Its Statistical Verification Based on Large Japanese and English Corpora,"参照結束性(referential coherence) は，主題の連続性や代名詞化によってもたらされる，文章の滑らかさを表す．
では，なぜ参照結束性が高い表現／解釈が選択されるのだろうか．
参照結束性の標準的理論であるセンタリング理論は，従来，この行動選択のメカニズムをモデル化していなかった．
本研究の目的は以下の2つである．
(1)この行動選択原理をゲーム理論でモデル化した仮説 \shortcite{hasida1996,siramatu2005nlp","Referential coherence represents smoothness of discourse 
resulting from topic continuity and pronominalization. 
By what principle do we select coherent expressions and interpretations? 
Centering theory, the standard theory of referential coherence, 
has not modeled the mechanism for selection of coherent expressions and interpretations. 
Our goals are as follows: 
(1) We aim to verify the hypothesis that models 
the principle of selecting expressions and interpretations 
on the basis of game theory (Hasida et al. 1995; Shiramatsu et al. 2005), 
using corpora of multiple languages. 
(2) We aim to investigate 
whether we can use expected utility as selection criterion, 
and to develop the mechanism of selecting expressions and interpretations 
for discourse processing systems in various languages. For these purposes, we improved the meaning-game-based centering model (MGCM). 
Our improvement, the statistical design of the language-dependent parameters, 
enabled to acquire the parameters from a corpus of the target language. 
It also enabled verification of MGCM using corpora of various languages. 
We verified MGCM using Japanese and English corpora. 
We found out statistical evidences which supported the hypothesis that 
referential coherence was caused by selection with higher expected utility. 
This result indicates language universality of MGCM and the hypothesis.","['はじめに', '従来研究の概要と問題点', '改良MGCM: 知覚効用の統計的な定義', '日本語・英語の大規模コーパスによる検証', '考察', 'まとめ']",,,,,,,,
V15N01-01.tex,,Using Semi-supervised Learning for Question Classification,,"Question classification, an important phase in question
answering systems, is the task of identifying the type of a given
question among a set of predefined types. This study uses
unlabeled questions in combination with labeled questions for
semi-supervised learning, to improve the precision of question
classification task. For semi-supervised algorithm, we selected
Tri-training because it is a simple but efficient co-training
style algorithm. However, Tri-training is not well suitable for
question data, so we give two proposals to modify Tri-training, to
make it more suitable. In order to enable its three classifiers to
have different initial hypotheses, Tri-training bootstrap-samples
the originally labeled set to get different sets for training the
three classifiers. The precisions of three classifiers are
decreased because of the bootstrap-sampling. With the purpose to
avoid this drawback by allowing each classifier to be initially
trained on the originally labeled set while still ensuring the
diversity of three classifiers, our first proposal is to use
multiple algorithms for classifiers in Tri-training; the second
proposal is to use multiple algorithms for classifiers in
combination with multiple views, and our experiments show
promising results.","['Introduction', 'Related work', 'Tri-training semi-supervised learning and its modifications', 'Question data sets and feature selection', 'Experiments', 'Conclusion']",,,,,,,,
V15N01-02.tex,,An Approach to Machine Translation from Japanese Text to JSL Text,"手話は言語でありろう者の母語である．手話と音声言語の間のコミュニケー
  ションには手話通訳が必要となるが，手話通訳士の数は圧倒的に不足してい
  る．両言語間のコミュニケーションを支援する技術が期待される．本論文は
  日本語と手話との間の機械翻訳を目指して，その一つのステップとして，日
  本語テキストから手話テキストへの機械翻訳を試みたものである．
  機械翻訳をはじめとする自然言語処理技術はテキストを対象としているが，
  手話には文字による表現がないため，それらを手話にそのまま適用すること
  ができない．我々は言語処理に適した日本手話の表記法を導入することで，
  音声言語間の翻訳と同様に，日本語テキストから手話テキストへの機械翻訳
  を試みた．日本語から種々の言語への機械翻訳を目的として開発中のパター
  ン変換型機械翻訳エンジンjawをシステムのベースに用いている．目的言語で
  ある手話の内部表現構造を設定し，日本語テキストを手話の表現構造へ変換
  する翻訳規則と，表現構造から手話テキストを生成する線状化規則を与える
  ことで実験的な翻訳システムを作成した．日本手話のビデオ教材等から例文
  を抽出し，その翻訳に必要な規則を与えることで，日本語から手話に特徴的
  な表現を含んだ手話テキストへの翻訳が可能であることを確認するとともに，
  現状の問題点を分析した．","In this paper we present an approach to machine translation from
  Japanese to JSL (Japanese Sign Language).  There is no standard way
  of writing JSL, and that brings complexity and difficulty on natural
  language processing for JSL.  We defined a Japanese gloss-based
  notation system for JSL.  This notation system enables us to divide
  the MT process into two stages, namely, the text-to-text translation
  stage (Japanese to JSL text) and text-to-motion synthesis stage (JSL
  text to JSL motion).  Our current focus is on the former stage.
  This notation also allows us to apply the existing MT techniques to
  JSL.  We implemented a pilot MT system using \textit{jaw","['はじめに', '手話とそのテキスト表現', '日本語から多言語への機械翻訳エンジンjaw', '日本手話テキストへの機械翻訳システムjaw/SL', '翻訳実験', 'おわりに']",,,,,,,,
V15N01-03.tex,,"Collection of Moods in Japanese Web Pages, and a Proposal of an Expanded Mood System","日本語文のムードについて，いくつかの体系が提示されている．しかしながら，既知のムード体系がどのような方法によって構成されたかは明確に示されてはいない．また，多種多様な日本語ウェブページに含まれるような文を対象にして，ムード体系を構成しているとは思われない．したがって，日本語ウェブページを対象にした言語情報処理において，既知のムード体系は網羅性という点で不十分である可能性が高い．本論文では，NTCIRプロジェクトによって収集された11,034,409件の日本語ウェブページに含まれる文を分析して既知のムードとともに新しいムードを収集するための系統的方法について詳述する．その方法の基本的手順は，(1) 
日本語文をChaSenによって単語に分割し，(2) 
様々な種類のムードを表出すると予想される文末語に着目し，(3) 
文末語に手作業でムードを割り当てる，というものである．そして，収集した新しいムードを示し，収集したムードとその他の既知ムードとの比較を行い，収集できなかったムードは何か，新しく収集したムードのうちすでに提示されているものは何か，を明らかにする．比較によって得た知見をもとに，より網羅性を高めるように，拡充したムード体系の構成を提案する．","Some systems of moods of Japanese sentences are presented. However, it is 
not definitely shown what kind of method those known mood systems are 
constituted with. In addition, it does not seem that they are constituted 
through analyzing sentences included in various Japanese web pages. 
Therefore, in linguistic information processing for Japanese web pages, it 
is very likely that those known mood systems are insufficient at a point of 
exhaustiveness of moods. In this article, we describes a systematic method, 
in detail, with which we analyze sentences included in 11,034,409 Japanese 
web pages collected by NTCIR project and we collect new moods with known 
moods. A basic procedure of the method is as follows: (1) Divide a Japanese 
sentence into words with ChaSen; (2) Focus on a sentence-end-word that 
probably has various kinds of moods; (3) Assign a proper mood to the 
sentence-end-word manually. We show the collected new moods, and compare the 
collected moods and the other known moods in order to clarify what moods can 
not be collected and what moods have been already presented among the 
collected new moods. Based on findings from the comparison, we propose an 
expanded mood system such that more exhaustiveness of moods is provided.","['まえがき', 'ムード収集の基本的方針', 'ムードの収集方法', '文末語の網羅性', 'ムードの収集結果', '既知ムードとの比較', 'ムード体系の拡充', 'むすび']",,,,,,,,
V15N01-04.tex,,A Detection of Adjective Phrases Feeling Something Wrong for Natural Computer Conversation,コンピュータとの人間らしい会話のために，代表的な応答事例を知識として与え，文章の可変部を連想によって変化させることができれば，より柔軟で多種多様な会話ができると考えられる．しかし，機械的な語の組み合わせに起因する一般的に見て不自然な語の組み合わせの応答を生成する恐れがある．本論文では，機械的に作成した応答文の内，名詞と形容語の関係に注目し，違和感の有無の観点からその関係を整理することで，形容語の使い方の知識構造をモデル化する．更に，その知識構造を用いて，合成した会話応答文中の違和感のある組み合わせの語を検出する手法を提案する．本稿の手法を用いることで，形容語の違和感のある使い方の判定に関し，87\%の高い精度を得，有効な手法であることを示した．,"For natural computer conversation, if a computer has typical responses, and the changeable parts of sentences can be changed by association, more flexible and more various conversations can be done. However, there is a risk that the generation of response sentences by a computer results in a combinations of feeling of wrongness caused by the mechanical combination of words. This paper focused on a relation of nouns and adjective phrases. Then the knowledge structure of how to use nouns and adjective phrases is modeled by arranging the relation in a point of feeling of wrongness. Also, this paper proposes a technique for detection relation of nouns and adjective phrases by creating a knowledge model from generation of response sentences. Using the method described in this report, we showed that this technique was able to very accurately judge usages of nouns and adjective phrases with 87\% accuracy, thus demonstrating the effectiveness of the technique.","['はじめに', '名詞と形容語の関係', '違和感表現検出', '実験と評価', 'まとめ']",,,,,,,,
V15N02-01.tex,,Estimation of Class Membership Probabilities by Using Multiple Classification Scores,"文書分類の多くのアプリケーションにおいて，分類器が出力するクラスに確信度すなわちクラス所属確率を付与することは有用で，正確な推定値が必要とされる．これまでに提案された推定方法はいずれも 2 値分類を想定し，推定したいクラスの分類スコア（分類器が出力するスコア）のみを用いている. しかし，文書分類では多値分類が適用されることが多く，その場合は，予測されるクラスはクラスごとに出力された分類スコアの絶対的な大きさではなく相対的な大きさにより決定される. 
したがって，クラス所属確率は，推定したいクラスの分類スコアだけでなく他のクラスの分類スコアにも依存すると考えられるため，推定したいクラス以外の分類スコアも用いて推定する必要があると思われる．本稿は，多値分類における任意のクラスについてのクラス所属確率を，複数の分類スコア，特に推定したいクラスと第 1 位のクラスの分類スコアを用いて，ロジスティック回帰により高精度に推定する方法を提案する．提案手法を多値分類に拡張したサポートベクターマシンに適用し，性質の異なる 2 つのデータセットを用いて実験した結果，有効性が示された. また，本稿では，クラス所属確率を推定する別の方法として，各分類スコアを軸として等間隔に区切ってセルを作成する「正解率表」を利用する方法も提案したが，この方法においても複数の分類スコアを用いることは有効であった．提案手法は，分類スコアの組み合わせや分類器の変更に対しても容易に対応できる.","We propose a method for estimating class membership probabilities of a predicted class in multiclass classification, using scores outputted by a classifier (classification scores), not only for the predicted class but also for other classes in a document classification. Class membership probabilities are important in many applications of document classification, in which multiclass classification is often applied. As a method for estimating class membership probabilities by using multiple scores, we propose two kinds of methods. One is generating an accuracy table with smoothing methods such as the moving average or a moving average with coverage, which indirectly estimates class membership probabilities by referring the accuracy table. The other is applying a logistic regression estimated parameters beforehand, which directly estimate these probabilities. Through experiments on two different datasets with both Support Vector Machines and Naive Bayes classifiers, we show that the use of multiple classification scores is much effective in both methods. We also show that the proposed smoothing method for the accuracy table works quite well, and that the method applying a logistic regression is more stable. Moreover, the estimated class membership probabilities by the proposed method are useful in the detection of the misclassified samples.","['序論', '関連研究', '第 1 位のクラスについてのクラス所属確率推定', '第 2 位以下の任意のクラスについてのクラス所属確率推定', '結論']",,,,,,,,
V15N02-02.tex,,A System for Constructing a Synonym Dictionary,"同義語の同定は，情報検索，テキストマイニングなどのテキスト処理を行う上で必要な作業である．
同義語辞書を作成することにより，テキスト処理の効率や精度の向上を期待できる．
特定分野における文書には，専門の表現が多く用いられており，
その中には，分野独特の同義語が多量に含まれている．
例えば，日本語の航空分野では，漢字・ひらがなだけでなく，カタカナ，アルファベット，およびそれら
の略語が同義語として用いられている．
この分野の同義語は，汎用の辞書に登録されていないものが多く，
既存の辞書を使用できないので，辞書を新たに作成する必要がある．
また，辞書作成後も常に新しい語が発生するので，辞書の定期的な更新が必要となるが，
それを人手で行うのは大変な作業である．

本論文では，同義語辞書作成を半自動化するシステムを提案する．システムは，クエリが与
えられると意味的に同じ候補語を提示する．辞書作成者は，その中から同義語を選択して，
辞書登録を行うことができる．
候補語のクエリに対する類似度は，同義語の周辺に出現する語の頻度情報を文脈情報とし，
その余弦から計算する．文脈情報のみでは十分な精度が得られない場合，既知の同義語を
知識としてシステムに与えることにより，文脈語の正規化を行い，精度を向上できることを確認した．
実験は，航空分野の日本語のレポートを対象とし，
システムの評価には平均精度を用いて行い，満足できる結果が得られた．","To identify a synonym is a necessary procedure for text processing such as information retrieval and text mining.
We can expect to improve the proficiency and performance in text processing by 
constructing a synonym dictionary.
Same words might possibly be used as a different meaning if the target field differs, so a synonym dictionary has to be 
constructed for each field.
In some fields in Japanese, such as in aviation, synonym nouns include kanji/hiragana, katakana, 
alphabet and their abbreviations.
Many of these words are not registerd in a general dictionary. 
In addition, as new words always come to be used, the dictionary update is a big issue.

In this paper, we propose a system for constructing a synonym dictionary. The system will 
return synonym candidates on the descending order of similarity against a query. 
A synonym can be easily registered in a dictionary by 
looking the synonym candidates generated by the proposed system.
We define a context information as words frequency appearing around a target word.
Then a similarity is calculated by cosine measure using context information.
We confirmed that the system performance was remarkably improved by providing 
the system with known synonym set to make context word nominalization, especially when the performance was low.
We experimentally evaluated the system performance by aviation safety reports in Japanese and evaluated it
by average precision, and got promising results.","['はじめに', '関連研究', '類似度と平均精度', '提案方式の詳細と実験', '議論', '複合名詞の処理', '同義語辞書作成', '結論および今後の課題']",,,,,,,,
V15N02-03.tex,,Tree-based Phrase Alignment Based on Consistency Criteria,"本論文では対訳文アラインメントの全体的な整合性を評価する新た
な基準を提案する．この手法は係り受けタイプによる木構造上での距離や，距離
スコア関数などの統計的な素性に基づいている．また依存構造木を元にしたアラ
インメント手法であるため，両言語間の言語構造の違いを適切に吸収することが
可能である．さらに本手法により，複数見つかる対応候補の中から適切なものを
選択することも可能である．日英新聞記事コーパスでのアラインメント実験によ
り，本手法によるアラインメント精度は他の言語構造の近い言語対での精度と遜
色ないことが示された．","In this paper, we propose a novel method to measure the
 consistency of alignment as a whole. It is based on probabilistic
 features, using dependency type distance and distance-score
 function. Since this method is based on tree structure, the linguistic
 difference between source and target language is successfully
 grasped. Moreover, with this method, appropriate correspondences can be
 selected among corresponding candidates. We conduct experiments on
 Japanese-English newspaper corpus, and achieve reasonably high accuracy
 compared with other language pairs which have less linguistic
 differences.","['はじめに', '構造的句アラインメント', '整合性尺度に基づく構造的句アラインメント', '実験と考察', '結論と今後の課題']",,,,,,,,
V15N02-04.tex,,Automatic Paraphrasing of Japanese Functional Expressions under Style and Readability Specifications,"日本語には，
「にたいして」や「なければならない」に代表されるような，
複数の形態素からなっているが，
全体として1つの機能語のように働く複合辞が多く存在する．
われわれは，機能語と複合辞を合わせて機能表現と呼ぶ．
本論文では，
形態階層構造と意味階層構造を持つ機能表現辞書を用いることにより，
文体と難易度を制御しつつ，
日本語機能表現を言い換える手法を提案する．
ほとんどの機能表現は，多くの形態的異形を持ち，
それぞれの異形は，その文体として，
常体，敬体，口語体，堅い文体のいずれかをとる．
1つの文章においては，
原則として，一貫して1つの文体を使い続けなければならないため，
機能表現を言い換える際には，文体を制御する必要がある．
また，
文章読解支援などの応用においては，難易度の制御は必須である．
実装した言い換えシステムは，
オープンテストにおいて，入力文節の79\% (496/628)に対して，
適切な代替表現を生成した．","Automatic paraphrasing is a transformation of expressions 
into semantically equivalent expressions within one language.
For generating a wider variety of 
phrasal paraphrases in Japanese,
it is necessary to paraphrase functional expressions
as well as content expressions.
We propose a method of paraphrasing of
Japanese functional expressions
under style and readability specifications
using a dictionary with two hierarchies:
a morphological hierarchy and a semantic hierarchy.
A remarkable characteristic of Japanese functional expressions 
is that each functional expression has many different variants.
Each variant has one of four styles.
In paraphrasing of Japanese functional expressions,
a paraphrasing system should accept style specification,
because consistent use in style is required.
At the same time,
control of readability of generated text is important
in several applications, such as a reading aid,
because functional expressions are critical units
that determine sentence structures and meanings.
Our system generates appropriate alternative expressions 
for 79\% of source phrases in Japanese in an open test.","['はじめに', '2つの階層構造を持つ機能表現辞書', '本論文で提案する機能表現の言い換え手法', '機能表現言い換えシステム', '評価', '関連研究', 'おわりに']",,,,,,,,
V15N02-05.tex,,Estimating Level of Public Interest for Documents,"ある入力文書が多くの人にとってどの程度興味や関心を持つかを算出する指標
を提案する．各個人の興味や関心は多種多様であり，これを把握することで情
報のフィルタリング等を行う研究は知られているが，本研究では不特定多数す
なわち大衆が全体でどの程度の興味を持つかについて検討を行った．このよう
な技術は，不特定多数に対して閲覧されることを想定しているWebサイトにお
ける提示文書の選択や表示順の変更など，非常に重要な応用分野を持っている．
我々は大衆の興味が反映されている情報源として順位付き文書を使用した．本
手法ではこれを学習データとして利用して，文書に含まれる語句及び文書自体
に興味の強弱を値として付与する手法を構築した．興味を値として扱うことで，
興味の強弱を興味がある・ないの2値ではなく興味の程度を知ることや興味発
生の要因分析を行うことが可能である．提案手法は，文書に含まれる語句を興
味判別する素性として扱い，内容語，複合名詞，内容語及び複合名詞の組み合
わせの3種類について比較，議論した．評価は，ニュース記事のランキングを
対象にして，実際の順位とシステムの順位を比較した．その結果，順位相関に
基づいた評価値は0.867であり，手法の有効性を確認した．さらに，ほぼ興味
を持たれない記事に対して抽出精度0.90を超える精度で弁別できることを実験
で確認した．","We propose a new measure to estimate level of public interest given a
document.  Although personal interests is of great variety, public
interest, that is collection of personal interests, has consistency to
some extent regardless of time difference.  The task here is not to
know whether a given document has interest or not, but to know how
much interest a given document has, that expects enabling deep
interest analysis by use of our measure.  This problem has many
applications such as display control of documents on the Web, that is
assumed to be seen by public.  We use in this paper document
collection with ranking information in terms of public interest.  We
estimate level of interest for each word, and then for each document
by utilizing the ranking information.  As feature set we use three
kinds: content words, compound words, and the combination of them.  In
the evaluation we use newspaper ranking as a source, and evaluate the
performance by comparing our output to the real ranking.  The results
illustrates that the extended rank coefficient of these two rankings
is 0.867.  We also show that more than 0.90 accuracy is attained for
rejecting little interest documents.","['はじめに', '興味について', '関連研究', '順位情報付き文書', '提案手法', '評価実験', '処理単位の拡張', '拡張した素性の評価', 'おわりに']",,,,,,,,
V15N02-06.tex,,A Web Corpus and Word Sketches for Japanese,,"Of all the major world languages, Japanese is lagging behind in terms of publicly accessible and searchable corpora. In this paper we describe the development of JpWaC (Japanese Web as Corpus), a large corpus of 400 million words of Japanese web text, and its encoding for the Sketch Engine. The Sketch Engine is a web-based corpus query tool that supports fast concordancing, grammatical processing, `word sketching' (one-page summaries of a word's grammatical and collocational behaviour), a distributional thesaurus, and robot use. We describe the steps taken to gather and process the corpus and to establish its validity, in terms of the kinds of language it contains. We then describe the development of a shallow grammar for Japanese to enable word sketching. We believe that the Japanese web corpus as loaded into the Sketch Engine will be a useful resource for a wide number of Japanese researchers, learners, and NLP developers.","['The Sketch Engine', 'The JpWaC corpus', 'The Sketch Engine for Japanese', 'Evaluation', 'Conclusion and further work']",,,,,,,,
V15N03-01.tex,,Research on Technical Term Extraction in the Nursing Domain,今日，大学は産学連携の一層の活性化が求められており，これを可能にするためには大学側のシーズを簡単に検索できるシステムが望まれる．そこで著者らは，産学連携の専門家が研究のシーズを専門用語によって簡単に検索することができるシステムの構築を狙いとし，その第一段階として専門用語抽出の研究を行っている．本研究ではこれまで研究されていない看護学分野を対象分野とした．予備研究によって，病気の症状や治療法を表す専門用語が情報検索分野における代表的な専門用語の抽出方法では抽出が難しいことが判明した．そこで，専門用語になりうる品詞の組合せの拡張と一般的な語を除去することで専門用語抽出の性能改善を図った．その結果，品詞の組合せを拡張することで再現率は83{\%,"This paper presents a research for extracting technical terms from documents 
in the nursing domain. An exploratory study showed that a well-known term 
extraction method, which has proven to be effective in extracting technical 
terms specified to the computing domain, can not effectively extract 
technical words representing symptoms or treatments of diseases. We propose 
a new technical term extraction method to improve extraction performance. 
Its main characteristics are enhancing permissible combinations of 
word-class; and excluding fundamental vocabulary. Experimental results 
showed that our extraction method attained 99{\%","['はじめに', '従来研究とアプローチ', '提案手法', '実験及び評価', '考察と今後の課題', 'まとめ']",,,,,,,,
V15N03-02.tex,,Identifying Discourse Relation using Example-based Approach,文間の接続関係を同定することは談話解析や複数文書要約，質問応答など多くの分野において重要である．本論文では連続する2文に対して文間の接続関係を同定する手法を提案する．提案手法は，入力文から抽出した構文情報や単語情報を用いて，大量のテキストデータの中から入力の連続2文に最も近い2文を検索し，この接続関係によって入力文の文間接続関係を推定する用例利用型(example-based)の手法によって行う．手法は，クラスタリングによって同じ接続関係を持ちやすい単語のクラスタを生成する．この結果生成された単語クラスタを用いて単語の汎化を行い，必ずしも同じ単語が使われていなくとも接続関係の観点から類似した用例をテキスト中から探す．最後に，この用例の接続関係をもって入力文の接続関係とする．以上の手法によって入力文の文体や語の難易度によらない汎用的な同定手法を実現することが可能となった．評価実験では人手による評価で75{\%,"Identifying discourse relations is important for many applications, such as 
text/con\-versation understanding, single/multi-document summarization and 
question 
\linebreak 
answering. This paper focuses on discourse relations between two 
successive Japanese sentences, and classified the relations into six classes 
in terms of their relation types. We propose an example-based method in 
which the system determines the discourse relation between sentences similar 
to two input sentences. The method utilizes phrasal pattern made from input 
sentences and core words in input sentences. As an evaluation result, the 
accuracy attains over 75{\%","['はじめに', '接続関係の分類', '人手による接続関係の推定', '類似用例による接続関係の推定', 'パタンによる候補文の抽出', '単語のクラスタリング', '候補文のスコア付け', '評価実験及び考察', '結論']",,,,,,,,
V15N03-03.tex,,Evaluation Task of Question Answering for Information Access Dialogues,あるトピックに関して対話的に行われる一連の情報アクセスを質問応答システムが支援する能力，情報アクセス対話の対話相手として情報を提供するために質問応答システムが持つべき能力を定量的に評価するためのタスクを提案する．このタスクでは，対話の実現の基本となる対話文脈を考慮した質問の解釈，つまり照応解消や省略処理等のいわゆる文脈処理の能力を評価する．本稿では，タスクの設計を示し，その根拠となる調査結果を報告する．提案するタスクは以下の点で新規かつ有益である．対話的情報アクセスを対象として，そこで必要な質問応答技術が効果的に評価できるという課題設定と構成の独自性を持つ．評価尺度については応答の自然性において問題となる回答の質や回答列挙の体系の違いに配慮し，複数の体系を許す多段階評価手法を備えている．システムの文脈処理能力をある程度まで切り離して評価することを可能とする参照用テストセットと呼ぶ枠組みを有している．,"A novel task for evaluating question answering technologies
is proposed.  This task assumes interactive use of question answering
systems and evaluates among other things, the abilities needed under
such circumstances, i.e. proper interpretation of questions under a
given dialogue context; in other words, context processing abilities
such as anaphora resolution and ellipses handling. This paper shows the
design of the task and its empirical background. The task proposed is
not only novel as an evaluation of the handling of information access
dialogues, but also includes several valuable ideas such as a measuring
metric in order to obtain intuitive evaluation of the answers to
list-type questions and reference test sets for obtaining information on
context processing ability in isolation.","['はじめに', 'タスクの枠組み', '評価手法', '参照用テストセット', '関連研究', 'おわりに']",,,,,,,,
V15N03-04.tex,,Acquiring Polar Sentences from HTML Documents,"本論文では大規模なHTML文書集合から評価文を自動収集する手法を提案する．
 基本的なアイデアは「定型文」「箇条書き」「表」といった記述形式を利用す
 るというものである．本手法に必要なのは少数の規則だけであるため，人手を
 ほとんどかけずに評価文を収集することが可能である．また，任意のHTML文書
 に適用できる手法であるため，様々なドメインの評価文を収集できることが期
 待される．実験では，提案手法を約10億件のHTML文書に適用したところ，約65
 万の評価文を獲得することができた．","This paper represents a method of acquiring polar sentences from HTML
 documents. The basic idea is to exploit three lexico-syntactic patterns
 and two layout structures of HTML documents. The method requires only a
 small amount of hand-crafted rules and can be implemented in low
 cost. In our experiment, the method was applied to one billion
 documents and 650 thouthands polar sentences were aquired.","['はじめに', 'アイデア', '評価文の自動収集', '実験', '議論', '関連研究', 'おわりに']",,,,,,,,
V15N03-05.tex,,Allocation Method of an Unknown Search Keyword to a Thesaurus Node by Using Web,"日常的な会話の中では，新語や固有名詞などシソーラスに定義されていない単語（未知語）が使用される．未知語についての知識がなければ，適切に会話を行うことができない．Webを利用することで，未知語について調べることができる．
しかし，Webには膨大な情報が存在するため，必要な情報を効率的に得ることは困難である．未知語に対する適切なシソーラスのノードを提示することによって，未知語の意味を獲得することができる．未知語理解はコーパスなど言語データに依存する研究が多く，対応できない未知語が存在するという問題点がある．本論文では，連想メカニズムを構成する概念ベースと関連度計算，さらにWebを用いて，未知語を概念化することで各ノードとの関連性を評価し，固有名詞を含んだ未知語をシソーラス上の最適なノードへ分類する手法を提案する．","The words which are not defined in Thesaurus appear in the daily conversation, including new words and proper nouns. It is unable to carry conversation with no knowledge about these words. We can search about an unknown word by using Web. 
However, it is difficult to obtain necessary information efficiently because large amounts of information exists in Web.
The meaning of the unknown word is acquired by presenting an appropriate node of Thesaurus. As for the research on understanding of the unknown word, there are a lot of researches which relies on language data including corpus. 
These researches have a problem that there are unknown words which can not be responded. 
This paper proposes the technique for finding the best node for the unknown word included proper nouns by conceptualizing the unknown word and evaluating relationship to each node.","['はじめに', '未知語分類システム', '構成技術', '未知語分類手法', '評価', '既存手法との比較', 'おわりに']",,,,,,,,
V15N03-06.tex,,Example-based News Article Summarization by Imitating Summary Instances,"現在，文書の要約をユーザへ提示することで支援を行う自動要約の研究が盛ん
に行われている．既存研究の多くは語や文に対して重要度を計算し，その重要
度に基づいて要約を行うものである．しかし我々人間が要約を行うときには文
法などの知識やどのように要約を行ったら良いのかという様々な経験を用いて
いるため，我々は人間が要約に必要だと考える語や文と相関のあるような重要
度の設定は難しいと考える．さらに人間が要約を行う際は様々な文の語や文節
など織り交ぜて要約を作成するため，文圧縮や文抽出の既存研究ではこのよう
な人間が作成する要約文は作ることができない．そこで本論文ではこれらの問
題点を解決し，人間が作成するような要約を得るため用例利用型の要約手法を
提案した．この要約手法の基本的なアイデアは人間が作成した要約文（用例）
を模倣して文書を要約することである．提案手法は類似用例文の獲得，
文節の対応付け，そして文節の組合せの3つの過程から構成される．評価実験
では従来法の一つを比較手法として挙げ，自動評価と人手による評価を行った．
人手の評価では要約文が読みやすいかという可読性の評価と要約の内容として
適切であるかという内容適切性の評価を行った．実験結果では自動評価及び人
手による評価共に従来法に比べ，本手法の方が有効であることが確認できた．
また本研究で目的としていた複数文の情報を含んだ要約文が作成されたことも
確認できた．","Recently, there are a lot of automatic summarization systems.  Almost
all previous works figure an importance for each word or each
sentence, and compress or extract a sentence by using the importance
of each word or each sentence.  However, when we generate a summary,
we use much knowledge and experience in our mind.  Therefore, it is
difficult to compute the importance which correlates with human sense.
This paper proposes a new summarization method which is based on
example-based approach.  The method has three steps.  First, system
retrieves a similar instance in a instance collection to an input. The
instance collection indicates summaries which are generated by human.
In the second step, the system links the similar phrases in the input
to a phrase in the similar instance.  As third step, the system
combines the corresponding phrases, and outputs summary candidates.
Experimental results have proven that the summarization system attains
approximately 1.81 accuracy on a scale 1 to 4 by human judgments. And
the system has obtained better accuracy than previous work.  From the
examinations, the system has confirmed that the summaries were
generated by combining the phrases in many position of the input,
while those summaries are not given just by common methods such as
sentence extraction methods and sentence compression methods.","['はじめに', '用例利用型のアプローチ', '提案法のシステム概要', '類似用例文の選択', '文節の対応付け', '文節の組合せ', '実験', '結果及び考察', '結論']",,,,,,,,
V15N04-01.tex,,Extraction of Person Information from Historical Materials for Building Historical Ontology,"本研究の目的は，歴史資料（史料）を対象に歴史知識の構造化の基盤となる
「歴史オントロジー」を構築するシステムを開発し，広く提供することによっ
て歴史学の発展に寄与することにある．この目標を具体的に検証するために，
昭和15年に時の帝国学士院において始められた明治前日本科学史の編纂成果で
ある『明治前日本科学史』（刊本全28巻）の全文を日本学士院の許諾の下に電
子化し，明治前の日本の科学技術を創成してきた科学技術者に関する属性およ
び業績の情報を抽出することにより，前近代日本の人物情報データベースの構
築を試みる．人物の属性として人名とそれに対する役職名と地名を，人物の業
績として人名とそれに対する書名を，いずれもパターンマッチングなどのルー
ルベースの手法によって抽出する．『明治前日本科学史総説・年表』を対象と
した性能評価を行った結果，人名，人名とその役職名，および人名とその地名
について，F値で0.8を超える結果が得られた．","Our goal of this study is to contribute to the progress in historical
science by developing a system for building a historical ontology from
historical materials and making it available to the public. We
digitize all the books of ``Meiji-mae Nippon Kagaku-shi'' (Pre-modern
Japanese History of Science and Technology) published by Nippon
Gakushiin (The Japan Academy), and extract the attribution and the
works of scientists and engineers from the books to build a database
of person information in pre-modern Japanese history. We extract the
names of persons, positions, places, and books as the attribution and
the works of persons by pattern matching. The experimental results
show that the F-measures for the names of persons, positions, and
places are over 0.8.","['はじめに', '歴史オントロジー構築プロジェクト', '人物情報の抽出', '評価実験', 'おわりに']",,,,,,,,
V15N04-02.tex,,Effective Use of Indirect Dependency for Distributional Similarity,,"Distributional similarity is a widely adopted concept to compute
  lexical semantic relatedness of words. Whereas the calculation is
  based on the {\it distributional hypothesis","['Introduction', 'Previous Studies', 'Indirect Dependency', 'Synonym Acquisition Method', 'Evaluation', 'Experiments', 'Conclusion']",,,,,,,,
V15N04-03.tex,,Detecting Semantic Relations between \\Named Entities Using Contextual Features,"本論文では，テキストに出現する固有表現の組が意味的な関係を有するか否かを判定する手法，特に異なる文に出現する固有表現の組に有効な手法を提案する．
提案手法では，Salient Referent Listに基づく文脈的素性を新たに導入し，単語や品詞，係り受けなどの伝統的に利用されている素性と組合わせた．
これらの素性はひとつの木構造として表現され，ブースティングに基づく分類アルゴリズムに渡される．
実験結果では，提案手法は従来手法より精度11.3\% ，再現率14.2\% 向上することが確認できた．","This paper proposes a supervised learning method for detecting 
a semantic relation between a given pair of named entities, which may be located in different sentences. 
The method employs newly introduced contextual features based 
on Salient Referent List as well as conventional syntactic and word-based features. 
These features are organized as a tree structure and are fed into a boosting-based 
classification algorithm. Experimental results show the proposed method outperformed 
prior methods, and increased precision and recall by 11.3\% and 14.2\%.","['はじめに', '関係性判定における文脈的素性の利用', '評価実験', 'おわりに']",,,,,,,,
V15N04-04.tex,,"Acquiring Concept Hierarchies of Adjectives from Corpora: 
	Towards Construction of Ontology of Adjectives from a real data","本研究は，実データに基づいた形容詞の観点からみた概念体系の自動構築をめざし，その一環として，
形容詞概念の階層関係構築に焦点を当てたものである．包含関係の尺度によって上位下位関係を求め，
その単語間の上位下位関係に基づき概念階層を自動構築した．
結果については，カバー率などの表層的な面と，階層の作られ方についての質的な面での評価を行った．
階層の質的な面での評価にあたって，概念の継承関係と事例（形容詞）の
各概念の成員としての連続性という観点から心理実験を行い，既存の人手によって作られたEDR辞書の階層と比較した．
実験手法はScheffeの一対比較法を用いた．その結果，自動構築がよい，あるいは既存の辞書と有意差がないと判断された階層は，
全体の43\%となった．抽出した概念数の不足や階層構築の際の問題点など課題も抱えているが，自動生成の階層が既存辞書の
階層に対して，その結果の半分弱の階層で問題を提起するという意味で，ベースラインとなる数値と考える．","The method of organizing word meanings is a crucial issue with lexical databases. We are aiming to extract the semantic structure of concepts of adjectives 
from corpora automatically. The first step to achieving this is to obtain the concepts of adjectives from corpora, for which we used abstract nouns. 
We constructed linguistic data by extracting semantic relations between abstract nouns and adjectives from corpus data. 
This paper describes how to hierarchically organize abstract concepts of adjectives mainly using the Complementary Similarity Measure (CSM) 
which calculates inclusion relations (hypernym/hyponym relations) between words. 
To estimate hypernym/hyponym relations between words, we compared three hierarchical structures of abstract concepts of adjectives: according to CSM, 
CSM with frequency (Freq), and an alternative similarity measure based on coefficient overlap. We evaluated automatically generated concept hierarchies of 
adjectives with those in EDR, and found that 43\% of those automatically generated were better than EDR.","['はじめに', 'オントロジーのタイプ', '言語表現に現れる「概念とその具体事例」という関係', '概念の階層関係の構築—階層構築の手法と閾値の選定基準—', '自動構築の階層とEDR辞書の概念階層との比較評価', '今後の展望—類義関係と階層関係をとらえるために—', 'まとめ', '5.4.1. 30形容詞に対するCOMMON-EDR階層ペア', '5.4.2. 10形容詞に対するCSM-EDR階層比較', '5.4.3.   10形容詞に対するOvlp-EDR階層比較']",,,,,,,,
V15N05-01.tex,,A Study on Cross Transformation of Mongolian Language,,"This paper discusses a segmentation approach of Mongolian for Cyrillic text 
for machine translation. Using this method, the processing of one-to-one 
word permutation between the variations of Mongolian and other languages, 
especially Altaic family languages like Japanese, becomes easier. 
Furthermore, it can be used for two-way conversion between texts of 
Mongolian used in different regions and counties, such as Mongolia and 
China. Our system has been implemented based on DP (dynamic programming) 
matching supported by knowledge-based sequence matching, referred to as a 
multilingual dictionary and linguistic rule bank (LRB), and a data-driven 
approach of the target language corpus (TLC). For convenience, NM (New 
Mongolian) is treated as the source language, and TM (Traditional Mongolian) 
and Todo as the target language in this test. Our application was tested 
using manually transcribed texts with sizes of 5,000 sentences paralleled 
from NM to TM and Todo. We found that our method could achieve 91.9{\%","['Introduction', 'Language structure and features', 'Approach', 'Experiments and results', 'Evaluation', 'Conclusion']",,,,,,,,
V15N05-02.tex,,Expanding Indonesian-Japanese Translation Dictionary Using Pivot Language,"2つの言語に関わる言語横断の言語処理を実現するには，その言語対を対象とす
  る豊富な言語資源が必要である．対訳辞書は，そのような言語資源の中でも特
  に重要であるが，あらゆる言語対に対して大規模な対訳辞書が利用できるわけ
  ではなく，小規模な対訳辞書しか利用できないような言語対も多い．
  そこで本論文では，ある言語対についての既存の小規模な対訳辞書を，その言
  語対と中間言語の言語資源を利用して大規模な対訳辞書に拡充する方法を提案
  する．
  提案法では，対象となる2つの言語のコーパスから得られた言語の異なる共起ベ
  クトルを，種辞書に基づいて比較して，対象となる2つの言語と中間言語の2種
  類の対訳辞書を用いて得られた訳語候補を選択する情報として用いる．
  実際に，小規模なインドネシア語-日本語辞書を，大規模なインドネシア語-英
  語辞書と英語—日本語辞書に基づいて拡充する実験を行い，拡充された辞書が言
  語横断情報検索の精度を向上させるのに役立つことを示した．","Cross-lingual language resources are necessary to realize
  cross-lingual natural language processing.  A large translation
  dictionary is especially important as such a resource, however, large
  dictionaries are available for few language pairs and small ones are
  only available for most language pairs.
  We propose a novel method to expand a small existing translation
  dictionary to a large translation dictionary using a pivot language.
  Cooccurrence vectors in the source language and ones in the
  destination language are compared based on the small existing
  translation dictionary, and provide information to select appropriate
  translations among translation candidates gotten from transitive
  translation using two translation dictionaries.
  Experiments that expand the Indonesian-Japanese dictionary using the
  English language as a pivot language show that the proposed method
  can improve performance of a real CLIR system.","['はじめに', '中間言語を用いた対訳辞書の拡充', '評価実験', '関連研究', 'むすび']",,,,,,,,
V15N05-03.tex,,Construction of story summarization system toward producing coherent summary consistent with story line,"物語は複数の話題で構成された文書である．内容の理解にはこの展開していく
 話題を正しく把握しなければならず，
 そのために原文書の代わりに用いられる要約は特に整合性を重視する必要が
 ある．
 本稿では整合性として話題の繋がりに着目した物語要約手法を提案する．
 提案手法では，まず
 物語を主題に着目した話題単位に分割し，登場人物に着目した重要度によって
 要約として抽出する話題を決定する．その後，話題間の整合性を保つために，
 話題間の状況の変化を示す文を補完する．
 提案手法の有効性を確認するため実際の物語を対象とした被験者の主観的評価
 による比較実験を行った．整合性を考慮しな
 いtf$\cdot$idfを用いた重要文抽出に比べて，提案手法の方が内容の理解にお
 いて良好な結果を得ることができた．","Since a story consists of several scenes and topics, for making a
 summary of a story, it is essential to get hold of relations between
 topics. This means that to make a coherent summary is a key issue for
 informative summary of a story. On the basis of this background, in
 this paper, the author proposes a method to produce a coherent summary
 of story focusing on extracting (1) topic block that consists of
 sentences that may be written on the same topic, and (2) complement
 sentences that may express change of scenes. They are extracted on the
 basis of automatic topic recognition and identification of
 characters. The experimental results of summarization for 9 stories
 show the proposed method produces easier-to-follow summaries than those
 of a tf$\cdot$idf based model.","['はじめに', '関連研究', '整合性を考慮した要約文の抽出', '評価', 'おわりに', '実験に使用した小説', '登場人物抽出の例', '人物抽出の予備実験結果']",,,,,,,,
V15N05-04.tex,,"Construction of Domain Dictionary for Fundamental Vocabulary and 
	its Application to Automatic Blog Categorization with the Dynamic Estimation of Unknown Words' Domains","言葉の意味処理にとってシソーラスは不可欠の資源である．
シソーラスは，単語間の上位下位関係という，いわば縦の関連を
表現するものである．
我々は意味処理技術の深化を目指し，縦の関連に加えて，単語が使用され
るドメインという，いわば横の関連を提案する．
本研究では基本語を対象に，ドメイン辞書を半自動で構築した．
本手法に必要なのは検索エンジンへのアクセスのみで，文書集合や高度
に構造化された語彙資源等は必要ない．
さらに，基本語ドメイン辞書の応用としてブログ自動分類を行った．
各ブログ記事は，記事中の語にドメインとIDF値が付与され，最もIDF値の高い
ドメインに分類される．
基本語ドメイン辞書に無い未知語のドメインは，基本語ドメイン辞書，
Wikipedia，検索エンジンを利用して，リアルタイムで推定する．
結果として，ブログ分類正解率94.0\%(564/600)と，未知語ドメイン推定正解率
76.6\% (383/500)が得られた．","For natural language understanding, it is essential to reveal semantic
relations between words.
To date, only the IS-A relation has been publicly available as a
thesaurus.
Toward deeper natural language understanding, we semi-automatically
 constructed the domain dictionary that represents the domain relation
 between Japanese fundamental words.
Our method does not require a document collection.
As a task-based evaluation of the domain dictionary,
we performed blog categorization, where we assigned a domain for each
word in a blog article and categorize it as the most dominant domain.
In so doing, we dynamically estimated the domains of unknown words,
i.e., those not listed in the domain dictionary.
As a result, our blog categorization achieved the accuracy of 94.0\%
(564/600).
Also, the domain estimation technique for unknown words achieved the
accuracy of 76.6\% (383/500).","['はじめに', '2つの問題 \\label{2issues', '基本語ドメイン辞書構築手法 \\label{domain-construction-method', '基本語ドメイン辞書の詳細 \\label{dic-spec', 'ブログ自動分類への応用 \\label{bunrui-method', '未知語ドメイン推定 \\label{unknown_domest', 'ブログ分類と未知語ドメイン推定の評価実験 \\label{eval', '関連研究 \\label{related-work', 'まとめ \\label{conclusion']",,,,,,,,
V15N05-05.tex,,"Improving Coreference Resolution Using Automatically Acquired
Knowledge of Nominal Relations","本稿では，自動獲得した知識を用いた日本語共参照
解析システムを提案する．日本語における共参照の多くを占める名詞句間の共参
照の解析では，語彙的知識が重要となり，中でも同義表現知識が非常に有効とな
る．そこでまず，大規模なコーパスおよび国語辞典の定義文から同義表現の自動
獲得を行い，自動獲得した同義表現を用いた共参照解析システムを構築する．さ
らに，より精度の高い共参照解析システムの構築のため，自動構築した名詞格フ
レームを用いた名詞句の関係解析を行い，その結果を共参照解析の手掛りとして
使用する．新聞記事およびウェブテキストを用いた実験の結果，同義表現，およ
び，名詞句の関係解析結果を用いることにより，共参照解析の精度は向上し，手
法の有効性が確認できた．","We present a knowledge-rich approach to Japanese coreference
resolution. In Japanese, noun phrase coreference occupies a central
position in coreference relations. To improve coreference resolution for
such language, wide-coverage knowledge of synonyms is required. We first
acquire knowledge of synonyms from large raw corpus and dictionary
definition sentences, and then resolve coreference relations based on
the knowledge. Furthermore, to boost the performance of coreference
resolution, we integrate bridging reference resolution system that uses
automatically constructed nominal case frames into coreference
resolver. We evaluated our approach on news paper article and WEB corpus
and confirmed that the performance of coreference resolution is improved
by using automatically acquired synonyms and bridging reference
resolution.","['はじめに', '同義表現の自動獲得', '名詞句の関係解析', '共参照解析', '実験と考察', '関連研究', 'おわりに']",,,,,,,,
V15N05-06.tex,,A Comparative Study on Effective Context Selection \\ for Distributional Similarity,,"Distributional similarity is a widely adopted concept to capture the
  semantic relatedness of words based on their context in various NLP
  tasks. While accurate similarity calculation requires a huge number
  of context types and co-occurrences, the contribution to the
  similarity calcualtion depends on individual context types, and some
  of them even act as noise. To select well-performing context and
  alleviate the high computational cost, we propose and investigate
  the effectiveness of three context selection schemes:
  category-based, type-based, and co-occurrence based selection. {\em
  Category-based selection","['Introduction', 'Previous studies', 'Contextual information', 'Approach to synonym acquisition', 'Evaluation of synonym acquisition', 'Category-based selection', 'Type-based selection', 'Co-occurrence based selection', 'Comparison of three selection schemes', 'Conclusion']",,,,,,,,
V15N05-07.tex,,TypeAny: Multilingual Text Entry System based on Language Identification,"近年，国際化に伴い，多くの言語を頻繁に切り替えて入力する機会が増えている．
既存のテキスト入力システムにおいては，
言語が切り替わるたびに，ユーザーが手動で，テキスト入力ソフトウェア (IME)を切り替えなければならない点が，ユーザーにとって負担になっていた．
この問題を解決するために，本論文では，多言語を入力する際にユーザーの負担を軽減するシステム，{\name","Computer users increasingly need to produce text written in multiple languages. 
However, typical computer systems require the user to change the text entry software 
each time a different language is used. This is cumbersome, especially when the languages 
change frequently.
To solve this problem, we propose {\name","['はじめに', '関連研究', '準備と設計方針', '言語判別', 'ユーザーインターフェース', '評価', '結論']",,,,,,,,
V15N05-08.tex,,Japanese Dependency Parsing Using a Tournament Model,"日本語係り受け解析においては，工藤らの相対的な係りやすさを考慮した日本語係り受け解析モデルが，
決定的解析アルゴリズムや文脈自由文法のパージングアルゴリズムに基づく手法を上回る精度を示している．
決定的解析手法では係り先候補文節を同時に一つしか考慮しないが，
工藤らの相対モデルではすべての係り先候補文節間の選択選好の強さをlog-linearモデルで推定している．
これに対し本稿では，同時に対象とする係り先候補文節を二候補に限定し，選択選好を二つの候補同士の対戦からなるトーナメントで
直接表現したモデルを提案する．
京大コーパス Version 4.0 を使用した実験において，提案手法は従来手法を上回る精度を示した．","In Japanese dependency parsing, Kudo's relative preference-based method outperforms both deterministic and probabilistic CFG-based parsing methods.
In the relative preference-based method, a log-linear model estimates selectional preferences
for all candidate heads, which cannot be considered in the deterministic parsing methods.
We propose an algorithm based on a tournament model, in which the selectional preferences are directly modeled by one-on-one games in a step-ladder tournament.
In evaluation experiment with Kyoto Text Corpus Version 4.0, the proposed method outperforms the previous research, including the relative preference-based method.","['はじめに', 'トーナメントモデル', '議論', '評価実験', '議論と今後の課題', 'まとめ']",,,,,,,,
V16N01-01.tex,,Detection of Quotations and Inserted Clauses and its Application to Dependency Structure Analysis in Spontaneous Japanese,"話し言葉の係り受け解析を行なう際の最大の問題は，
文境界や引用節・挿入節などの境界が明示されていないことである．
本論文では，話し言葉に対して，引用節・挿入節を自動認定するための手法，
および自動認定した引用節・挿入節の情報を用いて係り受け解析を改善するための手法を提案する．
形態素やポーズの情報などをもとに，
SVMを用いたテキストチャンキングによって，
引用節・挿入節の始端と終端を決定する．始端を決定する際には，
自動推定した係り受けの情報をあわせて利用する．
日本語話し言葉コーパス(CSJ)を用いた評価実験により，
自動認定した引用節・挿入節の情報を利用することで係り受け解析精度が
77.7\% から78.7\% に改善されることを確認し，本手法の有効性を示した．","Japanese dependency structure is usually represented by relationships
 between phrasal units called {\it bunsetsu","['はじめに', '話し言葉に特有の現象と係り受け構造', '係り受け解析と引用節・挿入節の自動認定のアプローチ', '評価実験', 'おわりに']",,,,,,,,
V16N01-02.tex,,"Discernment of Nativeness of English Documents Based on 
	Statistical Hypothesis Testing","本論文では，
ベイズ識別と仮説検定に基づいて，
英文書の作成者の母語話者／非母語
話者の判別を高精度で行う手法を提案する．
品詞 $n$-gram モデルを言語モデルとし，
判別対象の文書の品詞列の生起確率を，母語話者言語モデルにより
求めた場合と非母語話者言語モデルにより求めた場合とで比較し，判別を行う．
$n$ を大きくすると，母語話者／非母語話者固有の特徴をより良く扱うこ
とが可能となり，判別精度の向上が期待できる反面，
ゼロ頻度問題およびスパースネスの問題が顕在化し，
品詞 $n$-gram モデルのパラメタの最尤推定値を信頼すること
はできくなる．そこで，提案手法では，
仮説検定に基づいた方法で両言語モデルにおける生起確率の比を推定する．
実験の結果，従来手法を上回る 92.5\% の精度で判別できることを確認している．","This paper proposes a method to discern the nativeness of English
documents with high precision
based on Bayes decision and a statistical hypothesis testing.
Regarding a document as a sequence of part-of-speeches, 
the proposed method makes a
comparison between probabilities of a document by the statistical
language model of native English and by that of non-native English to
discern the nativeness of the document. The statistical language model
used here is a $n$-gram model.  The $n$-gram model with a large $n$ can
be expected to treat well the difference between the native English and
the non-native one and has the potential to discern the nativeness
with high precision.
However,
when we use the $n$-gram model with a large $n$,
the zero frequency problem and the sparseness problem become acute
and we cannot rely on
the maximum likelihood estimates of $n$-gram probabilities. 
The proposed method estimates the
ratio of the probability of the document by the native English language
model to that by the non-native English language model
using a statistical hypothesis testing.
The experimental result shows that the proposed
method discerns the nativeness with the precision 92.5\%, 
which is significantly higher than by traditional methods.","['はじめに', '文書クラス識別の枠組み', '関連研究', '提案手法', '実験', 'おわりに', '2つの二項母集団の母比率の比の検定', '仮説検定を利用した母比率の比の推定']",,,,,,,,
V16N01-03.tex,,Clause Splitting with Conditional Random Fields,,"In this paper, we present a Conditional Random Fields
(CRFs) framework for the Clause Splitting problem. We adapt the CRFs
model to this problem in order to use very large sets of arbitrary,
overlapping and non-independent features. We also extend N-best list
by using the Joint-CRFs \cite{Shi2007","['Introduction', 'Related Work', 'Clause Splitting Problem', 'Applying CRFs to Clause Splitting', 'Experiments', 'Conclusion']",,,,,,,,
V16N01-04.tex,,"A Method of List-type Question-answering Based on the Distribution 
	of Anwer Score Generated by Ranking-type Q/A System","本論文では，リスト型質問応答に対する回答群の選択手法を提案する．
リスト型質問応答とは，与えられた質問に対し決められた知識源の中から過不足なく解を見つけ列挙するタスクである．
提案手法では，既存の質問応答システムが解候補に付与するスコア分布を利用する．
解候補を，そのスコアを基にいくつかのクラスタに分離することを考える．
すなわち，それぞれのクラスタを一つの確率分布とし，各確率分布のパラメタをEMアルゴリズムにより推定する．
そして，それぞれの分布を正解集合を形成するスコア分布と不正解集合を形成するスコア分布のどちらであるかを推定し，正解集合のスコア分布に由来すると推定された解候補群を最終的な回答とする．
質問応答システムには一般に不得意な質問が存在するが，提案手法では，
複数の分布のパラメタを比較することにより，質問応答システムが正解を適切に
見つけられているか否かを判定することも可能である．
評価実験によれば，スコア分布を求め，それを利用することがリスト型質問応答に対して有効に働くことがわかった．","In this paper, we propose a method of the list-type question-answering.
The list-type question-answering is the task in which a system is requested to enumerate all correct answers to given question.
In the method, we utilize the distribution of the score that an existing question answering system gives to answer candidates.
Answer candidates are separated into some clusters according to their scores.
Here, we assume that each cluster results from a probabilistic model.
Under the assumption, the parameters of these probabilistic distribution models are estimated by using the EM algorithm.
Then, the method judges whether each distribution model is a source of correct answers or a source of incorrect answers.
Answer candidates that originate from the distribution models corresponding to correct answers are regarded as final answers.
Moreover, by comparing model parameters, we can also judge whether or not the question-answering system appropriately found correct answers.
The experimental results show that the use of the score distribution is effective in the list-type question-answering.","['はじめに', '関連研究', '優先順位型質問応答システム', '解スコア分布に基づくリスト型質問応答', '実験及び評価', '考察', 'おわりに']",,,,,,,,
V16N01-05.tex,,Information Extraction of Hypernyms and Ontology from Dictionaries,"辞書の定義文を基にした上位語情報の抽出手法を提案し，その結果に基づく単語オン
トロジーの自動生成を試みた．
提案するのは再帰的語義展開による情報抽出手法である．
本手法では定義文を再帰的に展開し，巨大な単語集合として定義文を再定義する．
このとき，定義文中に上位語が含まれるという仮定を利用すれば，
非常に多くの単語を上位語候補とすることができる．
この手法では上位語となる尤もらしさの指標を得ることができるため，
これを利用して多数の候補の中から上位語を効率よく選択できるようになる．
本手法を適用した上位語抽出実験では，構文解析を用いた既存手法
を上回る精度を示した．
更に本論文では，取り出された上位語情報を用いて単語オントロジーの自動生成を試みた．
自動生成の手法はまだ完全なものではないが，実験結果は上位語情報の有用性を示すものであり，今後のオントロジー自動生成の可能性を示している．","This paper proposes a method for information extraction of hypernyms
from dictionaries, and presents a result of automatic construction of
word ontology based on the extracted information.
The method recursively expands word definitions
to get much larger word sets, 
which will be candidates of hypernyms of the headwords.
At the same time, this method gives likelyhood of the candidates for hypernyms,
which is useful for selecting hypernyms from the candidates.
Computational experiments showed that the proposed method gives better results
than an existing method, which regards HEAD as hypernyms by parsing the explanatory notes.
Additionally, we tryed to create a word ontology with the resulting hypernyms.
This method is still underconstruction, but the results showed effectiveness
of the resulting hypernyms, and showed possibility of 
entirely automatic construction of word ontology.","['まえがき', '辞書からの上位語情報抽出', 'オントロジーの生成', 'むすび']",,,,,,,,
V16N02-01.tex,,Establishment of Corpus-based Cancer Specific Term Set and its Characteristics,"がん患者に対する情報提供の適正化のため，
がん情報処理を可能にする言語基盤であるがん用語辞書を，
医師による人手で作成した．
権威あるコーパスとして国立がんセンターのウェブ文書を用い，
延べ約2万6千語を収集し，
用語候補の集合Cc（Cancer Terms Candidate：語彙数10199語）を得た．
10種のがん説明用コンテンツを対象としたCcの用語の再現率は
それぞれ約95\%以上であった．
次に一般語やがん医学用語との関係と用語集としての
整合性から用語選択基準
（T1：がんそのものを指す，
T2：がんを想起させる用語，
T3：T2の関連語，
T4：がんに関連しない語のうち，
T3までを採用する）を作成し，
Ccに対して適用，
93.7\%が基準に合致し690語を削除，
9509語をがん用語Cとして選択した．
選択基準に従って作成した試験用ワードセットを医師に示すことで，
用語選択基準を評価した．
その結果，T1と(T2, T3, T4)の2つに分割した場合と
(T1, T2), (T3, T4)分割した場合で
一致係数$\kappa$が約0.6，
T1, T2, (T3, T4)の3つに分割した場合は約0.5であり，
選択基準を明示せずに単に用語選択を行った場合の $\kappa$ 値0.4に比べて
高値であったことから，
本研究で提案するがんとの関連性に基づいた用語選択法の妥当性が示された．
さらに，既存の専門用語選択アルゴリズムにより得られた用語集合 (HN) と本研
究で得られた用語集合 (C) を比較したところ，
HNでの再現性は80\%以上と高値だが，精度は約60\%であり，
本研究のような人手による用語選択の必要性が示された．
以上のことから，専門性の高い，がんに関するような用語集合を作成する場合，
本研究で行った，信頼性の高いコーパスを用い，
専門家の語感を信用して，
中心的概念からの距離感を考慮した用語選択を行うことにより，
少人数でも妥当性の高い専門用語集合の作成が可能であることが示された．","For providing the appropriate cancer information to patients, 
we made the Corpus-based Cancer Term Set as the basic linguistic 
infrastructure for analyzing cancer contents. 
The specific terms of cancer was carried out 
by the qualified medical doctors 
by cutting out each word using the whole web contents of 
the National Cancer Center as the authorized corpus. 
Out of over 26,000 words that were carried out, 
10,199 terms were finally collected as the Cancer Terms Candidate (Cc.) 
This term set covers 96.5--99.5\% of 10 different kinds 
of cancer content, which is enough for analysis. 
Considering the contrast between this cancer word set 
and other word set, such as general words, 
general medical words and proper nouns, 
the Cc was investigated based on selection standards. As a result, 
93.7\% terms of Cc was selected into the new word set ``C.''
Secondly, based on the relationship between general terms and 
cancer/medical terminology, as well as on the consistency of the glossary, 
the selection criteria (T1: Cancer itself, T2: Terms directly related to
cancer, T3: Terms related to both T1 and T2, and T4: Terms of unclear
relations to cancer) were proposed. As they were adapted to Cc, 93.7\%
met the criteria, 690 words were removed, and 9,509 were selected as the
C word in terms of cancer. These terms were selected according to the
criteria to create the word set for doctors to test, which indicates
that the criteria for selection were indirectly evaluated. As a result,
in two cases where the word set was split into T1 and (T2, T3, T4,) and
where it was split into (T1, T2) and (T3, T4), coefficient of
contingency, ``$\kappa$,'' was 0.6. And in case where into the word set was
split into T1, T2, (T3, T4) was 0.5. And in case where into the word set
was split into T1, T2, (T3, T4) was 0.5. These ``$\kappa$'' values were higher than
in the different test; making the simple question ``Cancer word or
not.'' Thus, the selection and classification of T1 and T2 terminology
is plausible. Furthermore, the comparison analysis of detected words
were performed for original several cancer corpus using HN
: (auto-specific-word-selecting algorithm (Gen-Sen-Web)) and C. As the
result, the recall rate of HN for C was around 80\%, however the
precision rate of HN for C was around 60\%. Thus, these automatic word
selecting methods are useful for evaluation of consistency for
C. However, the reducing the ignore words selection must be required for
those systems.  Therefore, it was suggested that this method enabled us
to create a low-cost, feasible cancer-specific term set. Thus, the
selection and classification of T1 and T2 terminology is
plausible. Therefore, it was suggested that this method enabled us to
create a low-cost, feasible cancer-specific term set.","['はじめに', '従来研究', 'がん用語候補集合 (Cc) の作成', 'がん用語集合の特性評価', '考察', 'まとめと今後の方針']",,,,,,,,
V16N02-02.tex,,Language Processing Technology and Educational Material Development---Generating English Educational Material using a Database Software---,本稿ではデータベース・ソフトウェアの１つであるFileMaker Proによる，英語学習教材の自動作成における言語処理技術と教材作成の連携可能性を提案する．著者は，実際の英語の授業でも利用しやすいプリント教材や簡易E-learning教材を出力できるツールを開発し，無料公開している．これらのツールではGUI環境での操作が可能であるため，パソコン利用スキルが限られる一般の英語教員にも利用しやすく，任意の英文素材からPhrase Readingを軸とした精読教材およびClozeテストを利用した学習教材を短時間で作成することができる.,"This article provides an example of developing educational material using a database software, linking it with language processing technology.  Teachers can download our software for free and create worksheets for studying phrase reading and e-learning materials based on cloze exercises.  This software makes creating such learning materials very efficient, and provides integrated functions which are almost impossible to do manually.  Since the operations can be done on graphical user interface, or GUI, even computer novices can use the software easily.","['はじめに', 'データベース・ソフトウェアについて', '連携事例 I: Phrase Reading 教材の自動作成', '連携事例 II: Clozeテストの自動作成', 'おわりに']",,,,,,,,
V16N02-03.tex,,Supervised Synonym Acquisition Using Distributional Features and Syntactic Patterns,,"Distributional similarity has been widely used to capture the semantic
relatedness of words in many NLP tasks. However, parameters such as
similarity measures must be manually tuned to make distributional
similarity work effectively. To address this problem, we propose a
novel approach to synonym identification based on supervised learning
and \textit{distributional features","['Introduction', 'Related Work', 'Distributional Similarity', 'Pairwise Classification', 'Pattern-based Features', 'Experiments', 'Conclusion']",,,,,,,,
V16N03-01.tex,,Hyponymy Relation Acquisition from Hierarchical Layouts in Wikipedia,"本稿では，Wikipediaの記事構造を知識源として，高精度で大量の上位
  下位関係を自動獲得する手法について述べる．上位下位関係は情報検索
  や Webディレクトリなど，膨大な Web 文書へのアクセスを容易にする様々な
  技術への応用が期待されており，これまでにも様々な上位下位関係の抽出手
  法が開発されてきた．本稿では，Wikipedia の記事構造に含まれる節や箇条
  書きの見出しから，大量の上位下位関係候補を抽出し，機械学習を用いてフィ
  ルタリングすることで高精度の上位下位関係を
  獲得する手法を開発した．実験では，2007年3月の日本語版Wikipedia 2.2~GB
  から，約77万語を含む約135万対の上位下位関係を精度90\%で獲
  得することができた．","This paper describes a method of extracting a large set of hyponymy
relations with a high precision from hierarchical layouts in Wikipedia articles. Hyponymy relation has been studied as one of the principal
knowledge for information retrieval and web directory, which helps
users to access the growing web. Various methods have been proposed to
automatically acquire hyponymy relations. In this article, we first
extract hyponymy relation candidates from sections and itemizations in
hierarchical layouts of Wikipedia articles, and then filter out
irrelevant candidates by using a machine learning technique. In
experiments, we successfully extracted more than 1.35 million relations
from the hierarchical layouts in the Japanese version of Wikipedia,
with a precision of 90\%.","['まえがき', '関連研究', 'Wikipediaの記事構造', '提案手法', '実験', 'まとめ']",,,,,,,,
V16N03-02.tex,,Associative Document Retrieval Using Concept Base and Earth Mover's Distance,近年，コンピュータとネットワークの発達に伴って，個人が扱える情報は膨大なものとなり，その膨大な情報の中から必要な情報を探し出すのは非常に困難となっている．既存の検索システムは基本的には表記のみを活用するため，意味的には同じ内容の検索でもユーザが入力する語によって検索結果が異なってしまう．そのためユーザが適切なキーワードを考えなければならない．そこで本稿では文書の意味を捉えた検索を実現するために単語の関連性にもとづいた文書間の類似性の定量化手法を提案する．具体的には概念ベースを用い単語間の関連性を求め，Earth Mover's Distanceにより文書間の類似度を計算する方法を提案する．また概念ベースに存在しない固有名詞や新語に対して，Web情報をもとに新概念として意味を定義し，概念ベースを自動的に拡張する方法を提案する．これら提案手法をNTCIR3-WEBによって他の手法と比較実験したところ，本手法が他手法に比べ良好な結果が得られた．,"Recently the development of computers and networks makes amount of information huge. It is very difficult to find necessary information in the huge information. The existing retrieval system uses not the meaning of input words but the notation of them. Therefore, different words bring a defferent result of retreieval even if they have the same meaning. A user of the system has to consider the input words to search the necessary information. This paper proposes the quantification technique of the semantic distance between documents based on relevance of the word to realize the search that captured the meaning of the document. Concretely the related degree between words is calculated by concept-base and the resemblance degree between documents is calculated by Earth Mover's Distance. Besides this paper proposes method that no existence word on concept-base is defined as a concept based on Web information to expand concept-base automatically. Retrieval experiments using the NTCIR3-WEB in comparison with the other method have shown that our method is effective than other method.","['はじめに', '先行研究と本研究の位置付け', '基本事項', '概念ベース', '概念ベースを用いた単語間の関連性の定量化', '概念ベースの自動拡張手法', 'EMDを用いた文書検索', '実験と評価', '概念ベースの自動拡張手法の評価', 'おわりに']",,,,,,,,
V16N03-03.tex,,Treatment of Numerical Classifier Based on Lexical-Functional Grammar,"事物の数量的側面を表現するとき，数詞の後に連接する語を一般に助数詞と呼
ぶ．英語などでは名詞に直接数詞が係って名詞の数が表現されるが，日本語で
は数詞だけでなく助数詞も併せて用いなければならない．名詞と助数詞の関
係を正しく解析するためには，助数詞が本来持つ語彙としての性質と構文中に
現れる際の文法的な性質について考慮する必要がある．本稿では，数詞と助数
詞の構文を解析するためのLexical-Functional Grammar (LFG) の語彙規則と
文法規則を提案し，その規則の妥当性と解析能力について検証した．提案した
規則によって導出される解析結果 (f-structure) と英語，中国語の
f-structureをそれぞれ比較することによって，日本語内での整合性と多言語
間との整合性を有していることが確認できた．また，精度評価実験の結果，従
来のLFG規則に比べて通貨・単位に関する表現では25\%，数量に関する表現で
は5\%，順序に関する表現では21\%のF値の向上が認められた．","Japanese numeral classifier (NC) expresses quantificational
expressions by following number nouns. The parser based on
phrase structure grammar like Lexical-Functional Grammar (LFG) or
Head-driven Phrase Structure Grammar (HPSG) has to define the
relationships between noun and NC or number and NC by the grammatical
rules. NC has various relationships syntactically because of its'
various characteristics.  The treatment of NCs in LFG formalism should
take account of their lexical and syntactical characteristics. This
report proposed LFG rules for NCs and examined their validity and
accuracy.  The comparison between Japanese f-structure inducted by the
rules proposed in this report and English f-structure and Chinese
f-structure which correspond to the Japanese f-structures show that
the rules make f-structures parallel between same phrases in Japanese
and in other languages. The experiment of evaluations shows the rules
improve F-score of NCs for currency and unit from 52.10\% to 77.90\%
and F-score of NCs for quantity from 81.92\% to 87.32\% and F-score of
NCs for Order from 59.62\% to 81.26\%.","['はじめに', '\\label{senkou', '\\label{rule', '\\label{fstr', '\\label{hyouka', 'おわりに']",,,,,,,,
V16N03-04.tex,,Globalization of Incorporating Adjacent Symbol Connection Constraints into an LR Parsing Table,"LR構文解析表（LR表）を作成する際，CFG規則による制約だけでなく
  品詞（終端記号）間の接続制約も同時に組み込むことによって，
  LR表中の不要な動作（アクション）を削除することができる．
  それにより，接続制約に違反する解析結果を受理しないLR表を作成できるだけでなく，
  LR表のサイズを縮小することも可能であり，構文解析の効率の向上が期待できる．
  これまでにも接続制約の組み込み手法はいくつか提案されているが，
  従来手法では，
  注目する動作の前後に実行され得る動作を局所的に考慮するため，
  削除しきれない動作が存在する．
  そこで，本論文では新しい組み込み手法を提案する．
  提案手法では，初期状態から最終状態までの全体の
  実行すべき動作列（アクションチェイン）を考慮し，接続制約を組み込む．
  評価実験の結果，従来手法と比較して，
  不要な動作をさらに約1.2\%削減でき，
  構文解析所要時間は約2.4\%短縮できることが分かった．
  最後に，提案手法の完全性について考察する．","Adjacent symbol connection constraints (ASCCs) are very useful
  for not only morphological analysis of non-segmenting language
  such as Japanese language,
  but also for continuous speech recognition of any language.
  By incorporating ASCCs into an LR parsing table,
  it is possible to reduce the size of the table,
  as well as reject any locally implausible parsing results.
  Although several algorithms have been proposed,
  they cannot remove all of the unnecessary actions
  because they consider only local context.
  This paper proposes a new algorithm and show some evaluation results.
  The proposed algorithm incorporates ASCCs
  by searching for global action chains
  from the initial state to the final state.
  According to the results, the proposed algorithm can remove
  about 1.2\% more actions than a conventional algorithm,
  and the parsing time can be reduced by about 2.4\%.
  Lastly, we show the completeness of our algorithm.","['はじめに', 'MSLRパーザと従来の組み込み手法', '提案アルゴリズム', '実験と評価', '提案アルゴリズムの完全性の証明', '結論と今後の課題']",,,,,,,,
V16N04-01.tex,,A Corpus-based E-learning System \\for Japanese Vocabulary,,"This study presents an initial version of an e-learning system that assists learners of Japanese with their study of vocabulary. The system uses sentences from a corpus to generate context-based exercises. The sentences used in the context-based exercises are selected using a readability formula developed for this system. We used the system with two different types of corpora, a web corpus that we constructed for this system and a sample of the recently released Balanced Corpus of Contemporary Written Japanese (BCCWJ). We compared the two corpora and while the BCCWJ has better word coverage, our web corpus still covered a majority (96.1\%) of the target vocabulary words even though it's relatively small. Evaluation of this system showed that the readability formula performs well, especially when sentences contain the system's target set of vocabulary words. A group of learners of Japanese were also asked to use the system and then fill out a survey. Results of the survey indicate that the learners thought the system was easy to use. Most of the learners also expressed a desire to use this type of system when studying vocabulary.","['Introduction', 'Related work', 'System overview', 'System Components', 'System use scenarios', 'Corpus analysis', 'Experiments', 'Future work', 'Conclusion', 'Sample exercises using the web corpus', 'Sample exercises using the BCCWJ']",,,,,,,,
V16N04-02.tex,,Extraction of Suppositional Adverb and Clause-Final Modality Form Distant Collocations Using a Web Corpus and Corpus Query System and its Application to Japanese Language Learning,"日本語におけるモダリティ形式および推量副詞と文末モダ
リティ形式との共起についての体系的な研究は自然言語処理の分野
において不十分である．さらに，このような情報は日本語教育の分
野においても十分カバーされていない．本稿では，コーパス検索ツ
ールSketch Engine (SkE)を利用した日本語の推量副詞とモダリティ
形式の遠隔共起の抽出を可能にすることとその日本語教育，特に日
本語学習辞典への応用の可能性を示すことを目的とする．そのため
にまず，複数のコーパスを分析した結果として，モダリティ形式と
そのバリエーションの網羅的なリストを作成した．このモダリティ
形式はChaSenでどのように形態素解析されているかを調査し，各モ
ダリティ形式の様々な形態素を新しいモダリティのタグとしてまと
めることによって，ChaSenで形態素解析されているJpWaCという大規
模ウェブコーパスから抽出した2千万語のサンプルへタグの再付与を
行った．最後に，新しくタグ付けされたコーパスをコーパス検索ツ
ールSkEに載せ，「文法関係ファイル」の内容を変更することで，推
量副詞と文末モダリティの共起の抽出を可能にした．抽出された共起
の結果は93\%以上の精度で高く評価された．得られた結果は言語資源
を利用しての日本語教育への応用の一例として，日本語教育における
辞書編集をはじめ様々な教育資源の作成のために，あるいは教室にお
ける直接的に利用可能となることを示した．","A systematic account of Japanese language modality forms as well
 as distant collocations between modal adverbs and clause-final 
modality forms is lacking in the field of natural language processing. 
The same stands for coverage of this kind of linguistic information 
in Japanese language education. In order to remedy this deficiency, 
in this paper we make extraction of Japanese adverbs and clause-final 
modality forms collocations possible using the corpus query system 
Sketch Engine and examine possibilities for its application in 
Japanese language learning, focusing on learner's dictionaries. 
First, as a result of analyzing various Japanese language corpora,
 we create a long list of modality forms and their variations. 
Then, we examine how ChaSen morphologically analyzes the forms 
and retag a sample of the large-scale Japanese language web corpus,
 JpWaC, by grouping all morphemes that correspond to individual 
modality forms together under a new modality tag. Finally, we load
 the newly tagged corpus into the Sketch Engine (SkE), modify the 
gramrel file and as a result obtain Word Sketch results for 
collocations between suppositional adverbs and modality forms. 
The evaluation of the collocation results shows that the proposed
 method reaches accuracy of above 93\%. The results can be utilized
 in the creation of Japanese learners' dictionaries or other language
 material or directly in language teaching or learning.","['はじめに', '先行研究', 'SkEでの推量副詞とモダリティ遠隔共起の抽出', '日本語教育への応用', 'まとめと今後の課題']",,,,,,,,
V16N04-03.tex,,Development and Applications of an English Learner Corpus with Multiple Information Tags,本論文では，まず， eラーニングシステムの研究開発のために構築された英語学習者コーパスについて解説し，次に，このコーパスの分析と，これを用いた英語能力自動測定実験について述べている．本コーパスは，496名の被験者が各々300文の日本語文を英語に翻訳したテキストから構成されており，各被験者の英語の習熟度がTOEICにより測定されている．また，これらに加え，日英バイリンガルによる正解訳も整備されていることから，訳質自動評価の研究に利用することが可能である．このコーパスを用いた応用実験として，BLEU，NIST，WER，PER，METEOR，GTMの6つの翻訳自動評価スコアを用いた実験を行なっている．実験において，各自動評価スコアとTOEICスコアとの相関係数を求めたところ，GTMの相関係数が最も高く，0.74となった．次に，GTMや，英訳結果の文長や単語長などからなる5つのパラメータを説明変数とし，TOEICを目的変数とした重回帰分析を行なった結果，重相関係数は0.76となり，0.02の相関係数の改善が得られた．,"Introduced in this paper is an English learner corpus built for the R \& D of an e-Learning system. Analysis and application experiments of the corpus are also shown. The corpus consists of English sentences that were translated from Japanese by Japanese English learners. Each of them translated 300 Japanese sentences into English. Their English proficiencies were measured through TOEIC. Reference sentences, translated by bilinguals, were also collected for automatic evaluation of the translation quality. In the experiments, automatic scores such as BLEU, NIST, WER, PER, METEOR and GTM were used. According to the experimental results, GTM gives the highest correlation, 0.74 for an automatic score and TOEIC. By  adding 4 parameters (sentence length, word length of the translation of the English learners, etc.) for the multiple linear regression analysis, the correlation improves to 0.76.","['はじめに', '学習者コーパスの収集方法', '学習者コーパスの分析', '訳質自動評価への応用', 'まとめと今後の課題']",,,,,,,,
V16N04-04.tex,,"An Implementation of a Writing Aid System for Students Based on
a Mutual Teaching Model","本論文では，学習者向けの作文支援手法として，学習者，教師，システム間で互
いに作文に関する知識を教えあう相互教授モデルを提案し，{\moda Webベースの
作文支援システムを実現した．","This paper proposed a mutual teaching model for assisting students in
writing compositions, and implemented a writing aid system as a Web
application based on the model where students, teachers and our system
teach each other their knowledge of writing. We designed the system to
use in first language writing courses in the university. The existing
systems have two problems: (a) the limitation of assistance for
structure or contents of composition, (b) few mechanisms that allow
teachers to incorporate their educational objectives into systems. In
our proposed model, a student annotates on his/her own composition and
makes comments on other's compositions. And teachers define
``Composition Rules'' for incorporating their educational objectives
into systems. Using the rules and results of the annotation, our system
provides various assistance for also structure or contents of
composition. {\mod By the proposed model and a coventional model, we made
two composition experiments whose results showed the effectiveness of
the proposed model and ``Composition Rules''.","['はじめに', 'システム構成', '相互教授モデル', '作文規則', '実験', '評価', 'おわりに']",,,,,,,,
V16N04-05.tex,,Design and Implementation of an Issue-oriented Automatic Syllabus Categorization System,近年の科学技術の進展にともない，工学知は幾何級数的に増大したが，その一方で，工学教育の現場においては，学生が自分の興味に合わせて講義・演習を選ぶことが非常に困難な状況になっている．また，教員も同様に，講義全体の効率化のために，講義内容の重複や講義の抜けを知る必要があり，総じて，各講義間の関連性を明確にし，カリキュラムの全体像を明らかにすることが求められている．しかし，講義間の関連から全体の構造を明らかにするためには，通常，人手によりあらかじめ講義内容（シラバス）を分析・分類する必要があり，これは大きな人的コストと時間を必要とする．したがって，この作業を可能な限り自動化し，効率的な手法を開発することが非常に重要な課題となる．本稿では，こうした問題に対して，我々のグループで取り組んでいる課題志向別シラバス分類システムについて，評価実験を交えて解説する．また，東京大学工学部の850以上のシラバスを使った評価実験によって，本システムが実用的な課題志向別シラバス分類の自動化に有効であることを示す．,"The purpose of this study is to develop an issue-oriented automatic syllabus categorization system, in which natural language processing and machine learning based automatic categorization are combined.
  Recent explosion of scientific knowledge due to the rapid advancement of academia and society makes it difficult for learners and educators to recognize overall picture of syllabus. In addition, the growing number of interdisciplinary researches makes it harder for learners to find their proper subjects from the syllabi. In an attempt to present clear directions to their subjects, issue-oriented syllabus structure is expected to be more efficient in learning and education. However, it normally requires categorizing all the syllabi manually in advance, and it is generally a time consuming task. Thus, this emphasizes the importance of developing efficient methods for (semi-) automatic syllabus structuring in order to accelerate syllabus retrieval. In this paper, we introduce design and implementation of an issue-oriented automatic syllabus categorization system. And preliminary experiments using more than 850 engineering syllabi of University of Tokyo show that our proposed syllabus categorization system obtains sufficient accuracy.","['はじめに', '関連研究', 'システム構成', '評価実験', '考察', '結論']",,,,,,,,
V16N05-01.tex,,Language Model Improvement by a Pseudo-Stochastically Segmented Corpus,"言語モデルの分野適応において，適応対象の分野の単語境界情報のない生コーパスの有効な
  利用方法として，確率的単語分割コーパスとしての利用が提案されている．この枠組では，
  生コーパス中の各文字間に単語境界が存在する確率を付与し，それを用いて単語$n$-gram確
  率などが計算される．本論文では，この単語境界確率を最大エントロピー法に基づくモデルに
  よって推定することを提案する．さらに，確率的単語分割コーパスを従来の決定的に単語に
  分割されたコーパスで模擬する方法を提案し，言語モデルの能力を下げることなく計算コス
  トが大幅に削減できることを示す．","Language model (LM) building needs a corpus whose sentences are segmented into
  words.  For languages in which the words are not delimited by whitespace, an
  automatic word segmenter built from a general domain corpus is used. Automatically
  segmented sentences, however, contain many segmentation errors especially around
  words and expressions belonging to the target domain. To cope with segmentation
  errors, the concept of stochastic segmentation has been proposed. In this
  framework, a corpus is annotated with word boundary probabilities that a word
  boundary exists between two characters. In this paper, first we propose a method
  to estimate word boundary probabilities based on an maximum entropy model. Next we
  propose a method for simulating a stochastically segmented corpus by a segmented
  corpus and show that the computational cost is drastically reduced without a
  performance degradation.","['はじめに', '確率的単語分割コーパスからの言語モデルの推定', '最大エントロピー法による単語境界確率の推定', '疑似確率的単語分割コーパス', '評価', 'おわりに']",,,,,,,,
V16N05-02.tex,,Personal Name Disambiguation in Web Search Results Using a Semi-Supervised Clustering Approach,"人名は検索語として，しばしば検索エンジンに入力される．しかし，
この入力された人名に対して，検索エンジンは，いくつかの同姓同名人物に
ついての Web ページを含む長い検索結果のリストを返すだけである．
この問題を解決するために，Web検索結果における人名の曖昧性解消を
目的とした従来研究の多くは，凝集型クラスタリングを適用している．
一方，本研究では，ある種文書に
類似した文書をマージする半教師有りクラスタリングを用いる．我々の
提案する半教師有りクラスタリングは，種文書を含むクラスタの重心の変動を
抑えるという点において，新規性がある．","Personal names are often submitted to search engines as query keywords.
 However, in response to a personal name query, search engines return
 a long list of search results that contains Web pages about several
 namesakes. In order to address this problem,
 most of the previous works that disambiguate personal names
 in Web search results often employ agglomerative clustering approaches.
 In contrast, we have adopted a semi-supervised clustering approach
 to integrate similar documents into a seed document.
 Our proposed semi-supervised clustering approach is
 novel in that it controls the fluctuation of the centroid of a cluster.","['はじめに', '提案手法', '実験', 'むすび']",,,,,,,,
V16N05-03.tex,,On Contribution of Sense Dependencies to Word Sense Disambiguation,,"Traditionally, many researchers have addressed word sense disambiguation (WSD) as an independent classification problem for each word in a sentence.
However, the problem with their approaches is that they disregard the interdependencies of word senses.
Additionally, since they construct an individual sense classifier for each word, their method is limited in its applicability to the word senses for which training instances are served.
In this paper, we propose a supervised WSD model based on the syntactic dependencies of word senses.
In particular, we assume that strong dependencies between the sense of a syntactic head and those of its dependents exist.
We describe these dependencies on the tree-structured conditional random fields (T-CRFs), and obtain the most appropriate assignment of senses optimized over the sentence.
Furthermore, we incorporate these sense dependencies in combination with various coarse-grained sense tag sets, which are expected to relieve the data sparseness problem, and enable our model to work even for words that do not appear in the training data.
In experiments, we display the appropriateness of considering the syntactic dependencies of senses, as well as the improvements by the use of coarse-grained tag sets.
The performance of our model is shown to be comparable to those of state-of-the-art WSD systems.
We also present an in-depth analysis of the effectiveness of the sense dependency features by showing intuitive examples.","['Introduction', 'Problems and related works', 'WordNet', 'WSD model with tree-structured CRFs', 'Experimental setup', 'Result', 'Discussion and analysis', 'Conclusion']",,,,,,,,
V16N05-04.tex,,Acquiring Event Relation Knowledge by Learning Cooccurrence Patterns and Fertilizing Cooccurrence Samples,"大規模コーパスから事態表現間の意味的関係の知識の獲得を目的として，実
  体間関係獲得手法として提案されたEspressoを事態間関係に適用できるよう
  に拡張した．この拡張は主に2つの点からなり，(1) 知識獲得のために事態表
  現を定義し，(2) 事態間関係に適合するように共起パターンのテンプレート
  を拡張した．日本語Webコーパスを用いて実験したところ，(a) 事態間関係獲
  得に有用な共起パターンが多数存在し，パターンの学習が有効であることが
  わかった．また行為—効果関係については5億文 Web コーパスから少なくと
  も5,000種類の事態対を約 66\% の精度で獲得することができた．","Aiming at acquiring semantic relations between events from a large
  corpus, this paper proposes several extensions to a state-of-the-art
  method originally designed for entity relation extraction. First,
  expressions of events are defined to specify the class of the
  acquisition task.  Second, the templates of co-occurrence patterns
  are extended so that they can capture semantic relations between
  event mentions. Experiments on a Japanese Web corpus show that (a)
  there are indeed specific co-occurrence patterns useful for event
  relation acquisition, and (b) For \textit{action-effect","['はじめに', '事態間関係知識獲得の関連研究', 'Espresso', '事態間関係獲得', '実験', '結論と今後の課題']",,,,,,,,
V17N01-01.tex,,Automatic Evaluation of the Fluency of English-to-Japanese Machine Translation Using Word Alignment,本稿では，人間による翻訳({\HUM,"This paper presents a method of automatically evaluating the fluency of 
machine-translated sentences. 
We constructed a classifier that would distinguish machine translations 
from human translations, using Support Vector Machines as machine 
learning algorithms. 
In order to obtain a clue to the distinction, we focused on literal 
translations (word-for-word translations). 
The classifier was constructed based on features derived from word 
alignment distributions between source sentences and human/machine 
translations. 
Our method employed parallel corpora to construct the classifier but 
required neither manually labeled training examples nor multiple 
reference translations to evaluate new sentences. 
We confirmed that our method could assist evaluation on system level. 
We also found that this method could identify the qualitative 
characteristics of machine translations, which would greatly help 
improve the translation quality.","['はじめに', '先行研究', '着目した素性', '実験と考察', 'おわりに']",,,,,,,,
V17N01-02.tex,,Toponym Resolution Based on \\ Surface Difference among Candidates,"地名等の固有名詞は自然言語処理における未知語処理問題の要因の一つであり，これを自動的に認識する手法が盛んに研究されている．
本稿では，地名の所属国を自動的に推定することで，未知語としてノイズの原因となる可能性のある地名語句に情報を与えることを目的とする．
固有名詞である地名の認識では地名辞書が用いられることが多いが，辞書ベースの手法では，辞書未登録語の問題が避けられない．
不特定多数の外国の地名も含めた所属国の推定の実現のため，本稿では，地名辞書や文脈情報を全く使用せず，地名の表層情報のみを利用して，地名の所属国を自動的に判別する手法を提案する．
地名については，言語的な類似性や地理的要因によって所属国の判別が困難な場合がある．
本稿ではこの点に着目し，所属可能性の低い国の除去による候補の絞込み処理と，所属可能性の高い候補の選択処理との組合せによって，再現率を高く保ったまま適合率の向上を実現した．","There have been many researches on toponym resolution as an approach to solve the unknown word problem.
In this paper we propose an area candidate estimation method for toponyms, to assign area information to unknown toponyms.
Our aim is to expand the target toponyms to non-restricted domains.
Thus we aim for a simple system avoiding the use of gazeteers and context information.
Our method is based only on surface information to estimate area candidates to where the toponyms may belong.
Toponym resolution can be difficult because of linguistic or geographic reasons.
Focusing on the surface difference among probable countries, we constructed a system containing a reduction phase for a rough examination and a selection phase for a detailed examination among them.
By our effective combination of these two phases, we succeeded in gaining high precision rate maintaing high recall rate.","['はじめに', '地名の持つ表層情報を活用した所属国推定', '所属国推定のための表層情報', '候補絞込みと候補選択の二段階処理から成る地名の所属国の推定手法', '関連研究', 'おわりに']",,,,,,,,
V17N01-03.tex,,Online Acquisition of Japanese Unknown Morphemes using Morphological Constraints,"日本語の形態素解析における未知語問題を解決するために，オンライン未知語獲
得という枠組みと，その具体的な実現手法を提案する．
オンライン未知語獲得では，形態素解析器と協調して動作する未知語獲得器が，
文が解析されるたびに未知語を検出し，その可能な解釈の候補を列挙し，最適な
候補を選択する．
このうち，列挙は日本語の持つ形態論的制約を利用し，選択は蓄積した複数用例
の比較により行う．
十分な用例の比較により曖昧性が解消されると，解析器の辞書を直接更新し，獲
得された未知語が以降の解析に反映される．
実験により，比較的少数の用例から高精度に未知語が獲得され，その結果形態素
解析の精度が改善することが示された．","To solve the unknown morpheme problem in Japanese morphological
analysis, we propose a novel framework of online unknown morpheme
acquisition and its implementation.
In online unknown morpheme acquisition, an unknown morpheme acquirer,
which works in concert with the morphological analyzer, detects unknown
morphemes from each segmented and POS-tagged sentence, enumerates its
possible interpretations, and selects the best candidate.
In enumeration, morphological constraints of the Japanese language are
utilized, and selection is done by comparing multiple examples kept in
storage.
When the number of examples being compared is large enough for
disambiguation, the acquirer directly updates the dictionary of the
analyzer, and the acquired morpheme will be used in subsequent analysis.
Experiments show that unknown morphemes are acquired from relatively
small numbers of examples with high accuracy and improve the quality of
morphological analysis.","['はじめに', '未知語獲得タスク', 'オンライン未知語獲得', '辞書項目の列挙と選択', '実験', '関連研究', '結論']",,,,,,,,
V17N01-04.tex,,Extracting String Features with Adaptation for Text Classification,"テキスト分類における特徴抽出とは，分類結果を改善するためにテキストの特徴たる単語または文字列を取捨選択する手続きである．ドキュメントセットのすべての部分文字列の数は，通常は非常に膨大であるため，部分文字列を特徴として使用するとき，この操作は重要な役割を果たす．

本研究では，部分文字列の特徴抽出の方法に焦点を当て，反復度と呼ばれる統計量を使って特徴抽出する方法を提案する．反復度は，高確率でドキュメントに二度以上出現する文字列は文書のキーワードであるはずだという仮定に基づく統計量であり，この反復度の性質は，テキスト分類にも有効であると考える．実験では，Zhangら(Zhang 
et al. 2006)によって提案された，条件付確率を用いることで分布が類似した文字列をまとめるという手法（以下，条件付確率の方法と記す）と我々の提案する手法の比較を行う．結果の評価には適合率と再現率に基づくF値を用いることとした．ニュース記事とスパムメールの分類実験の結果，我々の提案する反復度を用いた特徴抽出法を用いると，条件付確率の方法を用いるのに比べて，ニュース記事の分類では分類結果を平均79.65{\%","Feature selection for text classification is a procedure that categorizes 
words or strings to improve the classification performance. This operation 
is especially important when we use substrings as a feature because the 
number of substrings in a given data set is usually quite large.

In this paper, we focus on the substring feature selection technique and 
describe a method that uses a statistic score called ``adaptation'' as a 
measure for the selection. Adaptation works on the assumption that strings 
appearing more than twice in a document have a high probability of being 
keywords; we expect this feature to be an effective tool for text 
classification. We compared our method with a state-of-the-art method 
proposed by Zhang et al. that identifies a substring feature set by removing 
redundant substrings that are similar in terms of statistical distribution. 
We evaluate the classification results by F-measure that is a harmonic mean 
of precision and recall. An experiment on news classification demonstrated 
that our method outperformed Zhang's by 3.74{\%","['はじめに', 'Zhangらの特徴選択方法', '提案手法', '実験', '考察', 'まとめ']",,,,,,,,
V17N01-05.tex,,Statistical Phrase Alignment Model Using Dependency Relation Probability,"語順や言語構造の大きく異なる言語対間の対訳文をアライメントす
る際に最も重要なことは，言語の構造情報を利用することと，一対多もしくは多
対多の対応が生成できることである．本論文では両言語文の依存構造木上での単
語や句の依存関係をモデル化した新しい句アライメント手法を提案する．依存関
係モデルは木構造上でのreorderingモデルということができ，非局所的な語順変
化を正確に扱うことができる．これは文を単語列として扱う既存の単語アライメ
ント手法にはない利点である．また提案モデルはヒューリスティックなルールを
一切用いずに，句となるべき単位の推定を自動的に行うことができる．アライメ
ント実験では，既存の単語アライメント手法と比較して，提案手法にではアライ
メントの精度をF値で8.5ポイント向上させることができた．","When aligning very different language pairs, the most
 important needs are the use of structural information and the
 capability of generating one-to-many or many-to-many
 correspondences. In this paper, we propose a novel phrase alignment
 method which models word or phrase dependency relations in dependency
 tree structures of source and target languages. The dependency relation
 model is a kind of tree-based reordering model, and can handle
 non-local reorderings which sequential word-based models often cannot
 handle properly. The model is also capable of estimating phrase
 correspondences automatically without any heuristic rules. Experimental
 results of alignment show that our model could achieve F-measure 8.5
 points higher than the conventional word alignment model with
 symmetrization algorithms.","['はじめに', '提案モデル', 'トレーニング', 'アライメント実験', '考察', '結論']",,,,,,,,
V17N01-06.tex,,Event Relation Acquisition with Syntactic Patterns and Shared Arguments,"行為—効果関係，行為—手段関係のような事態間の関係を大規模コーパスか
  ら自動的に獲得する．共起パターンを利用する手法では，事態を表現する述
  語間で共有される項を認識することが難しいため，述語間で共有される名詞
  （アンカー）を用いて共有項を獲得し，共起パターンを用いて獲得した所与
  の関係を満たす述語対と共有項を組み合わせることで，共有項と共に事態間
  関係を獲得する．このとき2種類の異なるアンカーを用いることで，精度を保っ
  たまま再現率を向上できることを確認した．","Addressing the task of acquiring semantic relations between events
  from a large corpus, we first argue the complementarity between the
  pattern-based relation-oriented approach and the anchor-based
  argument-oriented approach. We then proposes a two-phased approach,
  which first uses lexico-syntactic patterns to acquire predicate
  pairs and then uses two types of anchors to identify shared
  arguments. The present results of our empirical evaluation on a
  large-scale Japanese Web corpus have shown that (a) the anchor-based
  filtering extensively improves the precision of predicate pair
  acquisition, (b) the two types anchors are almost equally
  contributive and combining them improves recall without losing
  precision, and (c) the anchor-based method achieves high precision
  also in shared argument identification.","['はじめに', '関連研究', '2段階述語間関係獲得手法', '実験設定', '実験結果', 'まとめと今後の研究', 'パターン方式の手法']",,,,,,,,
V17N01-07.tex,,Argument Structure Analysis of Event-nouns Using Lexico-syntactic Patterns of Noun Phrases,"形態素解析や構文解析など自然言語処理の要素技術は成熟しつつあり，
意味解析・談話解析といった，より高次な言語処理の研究が盛んに
なってきた．特に文の意味理解のためには「誰が」「何を」「誰に」といった
要素（項）を同定することが重要である．
動詞や形容詞を対象にした項構造解析のことを述語項構造解析と呼ぶが，
文中の事態を表す表現は動詞や形容詞の他にも名詞も存在することが知られている．
そこで，我々は日本語の名詞を対象とした項構造解析タスクを取り上げ，
機械学習を用いた自動的な解析手法を提案する．
日本語の事態性名詞には事態を指すか否か曖昧性のある名詞があるため，
まず事態性の有無を判定する事態性判別タスクと項同定タスクの2つに分解し，
それぞれ大規模なコーパスから抽出した語彙統語パターンを用いた手法と
述語・事態性名詞間の項の共有現象に着目した手法を提案する．","As fundamental natural language processing techniques like
morphological analysis and parsing have become widely used,
semantics and discourse analysis has gained increasing attention.
Especially, it is essential to identify fundamental elements,
or arguments, such as ``who'' did ``what'' to ``whom.''
Predicate argument structure analysis deals with argument structure
of verbs and adjectives. However, not only verbs and adjectives 
but also nouns are known to have event-hood.
We thus propose a machine-learning based method for automatic argument
structure analysis of Japanese event-nouns.
Since there are ambiguous event-nouns in terms of event-hood,
we cast the task of argument structure analysis of event-nouns into
two parts: event-hood determination and argument identification.
We propose to use lexico-syntactic patterns mined from large
corpora for the first sub-task and to exploit argument-sharing phenomenon
between predicates and event-nouns for the second sub-task.","['はじめに', '関連研究', '事態性名詞の項構造解析へのアプローチ', '事態性判別', '事態性名詞の項同定に有効な素性の検討', 'おわりに']",,,,,,,,
V17N01-08.tex,,Local Coherence Model Based on Entity Grid Augmented with Text Cohesion,"本論文ではentity gridを用いたテキストの局所的な一貫性モデルに対する改善
について述べる．entity gridベースの既存モデルに対して，テキスト結束性に
寄与する要素である接続関係，参照表現，語彙的結束性，また，より詳細な構文
役割の分類を組み込んだモデルを提案し，その性能を検証する．語彙的結束性に
関しては，語彙的連鎖を用いたクラスタリングを行う．テキスト中の文の並びに
対して，より一貫性のある文の順番の判定と，人手による評価に基づいた要約テ
キストの比較の2種類の実験を行い，その結果，本論文で提案する要素がentity
gridモデルの性能の改善に寄与することが明らかになった．","This paper describes improvements made to the entity grid local
coherence model for Japanese text. We investigate the effectiveness of
taking into account cohesive devices, such as conjunction, explicit
reference relation, lexical cohesion, and refining syntactic roles for a
topic marker in Japanese. To take into account lexical cohesion, we
consider lexical chaining. Through the experiments on discrimination
where the system has to select the more coherent sentence ordering, and
comparison of the system's ranking of automatically created summaries
against human judgment based on quality questions, we show that these
factors contribute to improve the performance of the entity grid model.","['はじめに', '関連研究', 'Entity Gridに基づく局所的な一貫性モデル', 'テキスト結束性に関わる要素と構文役割の拡張', '実験と考察', 'おわりに', 'スコアの計算に使用したQuality question']",,,,,,,,
V17N01-09.tex,,Measuring the Appropriateness of Automatically Generated Phrasal Paraphrases,,"The most critical issue in generating and recognizing paraphrases is
developing a wide-coverage paraphrase knowledge base.
To attain the coverage of paraphrases that should not necessarily be
represented at surface level, researchers have attempted to represent
them with general transformation patterns.  However, this approach
does not prevent spurious paraphrases because there is no practical
method to assess whether or not each instance of those patterns
properly represents a pair of paraphrases.
This paper argues on the measurement of the appropriateness of such
automatically generated paraphrases, particularly targeting at
morpho-syntactic paraphrases of predicate phrases.
We first specify the criteria that a pair of expressions must satisfy
to be regarded as paraphrases.  On the basis of the criteria, we then
examine several measures for quantifying the appropriateness of a
given pair of expressions as paraphrases of each other.
In addition to existing measures, a probabilistic model consisting of
two distinct components is examined.  The first component of the
probabilistic model is a structured $N$-gram language model that
quantifies the grammaticality of automatically generated expressions.
The second component approximates the semantic equivalence and
substitutability of the given pair of expressions on the basis of the
distributional hypothesis.
Through an empirical experiment, we found (i)~the effectiveness of
contextual similarity in combination with the constituent similarity
of morpho-syntactic paraphrases and (ii)~the versatility of the Web
for representing the characteristics of predicate phrases.","['Introduction', 'Related work', 'A probabilistic model for measuring appropriateness', 'Experimental settings', 'Input-wise evaluation', 'Score-based evaluation', 'Error analysis', 'Conclusion']",,,,,,,,
V17N01-10.tex,,Resolving Direct and Indirect Anaphora for Japanese Definite Noun Phrases,,"An anaphoric relation can be either direct or indirect. In some
  cases, the antecedent being referred to lies outside of the
  discourse its anaphor belongs to. Therefore, an anaphora resolution
  model needs to consider the following two decisions in parallel:
  \emph{antecedent selection","['Introduction', 'Motivation for our approach', 'Related work', 'Model', 'Dataset', 'Evaluation', 'Conclusion']",,,,,,,,
V17N01-11.tex,,A Thesaurus which Classifies Terms by Facets for Natural Language Processing of Japanese,"従来の情報検索に特化されたシソーラスではなく，構文解析や用語標準化などの自然
言語処理を目的とする420,000語規模のシソーラスを開発した．各用語の持つ関係語の数
が膨大なため，観点（ファセット）を導入して分類し，探しやすくしたシソーラスで
ある．さらに，差別語，表記の揺れなども区別できる．シソーラスを作成する際の留
意点・課題もまとめた．パッケージソフトのカスタマイズ機能およびインターネット
や他の辞書との連動機能，用語の標準化などについても紹介した．","Instead of a thesaurus specialized in conventional information 
retrieval, we developed a thesaurus of 420,000 terms for the purpose of 
natural language processing such as parsing or the term standardization. 
Because each entry term has a large number of terms with various semantic 
relations, we introduce a facet and classify them for finding relative terms
 easily. Furthermore, we distinguish discriminatory terms, and fluctuating 
Japanese spellings. We described points to keep in mind and future tasks in 
making a thesaurus. Our package has the connecting function with the Internet 
and the other dictionaries.","['はじめに', '用語の収集とシソーラスの構造', '用語同士の意味関係', 'パッケージソフトの機能', 'おわりに', '付録\u3000品詞一覧']",,,,,,,,
V17N02-01.tex,,An Application of Related Term Extraction to Transliteration into Chinese,"外国語を翻字するときに，日本語や韓国語ではカタカナやハングルなどの表音文字を用いるのに対して，中国語では漢字を用いる．
しかし，漢字は表意文字であるため，発音が同じでも漢字によって意味や印象は異なる可能性がある．
この問題を解消するために，ユーザが与えた関連語に基づいて翻字に使用する漢字を選択する手法がある．
しかしユーザの負担が大きいため，本研究は，翻字対象の関連語をWorld Wide Webから自動的に抽出し，中国語への翻字に使用する手法を提案する．
評価実験によって提案手法の有効性を示す．","To transliterate foreign words, in Japanese and Korean, phonograms such as Katakana and Hangul are used. In Chinese, the pronunciation of a source word is spelled out with Kanji characters. However, because Kanji comprises ideograms, different characters are associated with the same pronunciation but can potentially convey different meanings and impressions. 
To select appropriate Kanji characters, an existing method requests a user to provide one or more related terms, but it is expensive.
In this paper, we propose a method to select characters in transliteration into Chinese using related terms automatically extracted from the World Wide Web.
We show the effectiveness of our method experimentally.","['はじめに', '提案する翻字手法', '評価実験', 'おわりに']",,,,,,,,
V17N02-02.tex,,"Annotating Predicate-Argument Relations and Anaphoric Relations: 
	Findings from the Building of the NAIST Text Corpus","本論文では，日本語書き言葉を対象とした述語項構造と照応関係のタグ付与に
ついて議論する．
述語項構造解析や照応解析は形態素・構文解析などの基盤技術と自然言語処理
の応用分野とを繋ぐ重要な技術であり，これらの解析のための主要な手法はタ
グ付与コーパスを用いた学習に基づく手法である．
この手法を実現するためには大規模な訓練データが必要となるが，これまでに
日本語を対象にした大規模なタグ付きコーパスは存在しなかった．また，既存
のコーパス作成に関する研究で導入されているタグ付与の基準は，言語の違い
や最終的に出力したい解析結果の粒度が異なるため，そのまま利用することが
できない．
そこで，我々は既存のいくつかのタグ付与の仕様を吟味し，述語項構造と共参
照関係のアノテーションを行うためにタグ付与の基準がどうあるべきかについ
て検討した．本論文ではその結果について報告する．また，京都コーパス
第3.0版の記事を対象にタグ付与作業を行った結果とその際に問題となった点に
ついて報告する．さらにタグ付与の仕様の改善案を示し，その案にしたがい作
業をやり直した結果についても報告する．","This paper addresses how to annotate predicate-argument and
  anaphoric relations in Japanese written text.  Predicate-argument
  structure analysis and anaphora resolution are important problems
  because they bridge the gap between basic techniques in NLP such as
  morpho-syntactic analysis and NLP applications.  To solve these
  problems, researchers have generally made use of annotated corpora
  for machine learning-based approaches.  Although we need large
  corpora where predicate-argument relations and anaphoric relations
  are annotated to examine their occurrence in Japanese text,
  there have been no such resources so far.  In addition, existing
  specifications for annotating predicate-argument and anaphoric
  relations are not directly applicable due to the difference of
  languages and different problem settings.  For these reasons, we
  explore how to annotate these two kinds of relations and then define
  an adequate specification of each annotation task.  In this article, we
  report the results of annotation, taking the Kyoto Corpus 3.0
  as a starting point.  Furthermore, we refine our annotating
  specification to adapt actual phenomena existing in our corpus and
  then report the results of the annotation work according to the new
  specification.","['はじめに', '照応と共参照', '先行研究', 'NAISTテキストコーパスで採用するタグ付与の仕様', 'タグ付与の問題点と今後の展望', 'タグの仕様の改善', 'おわりに']",,,,,,,,
V17N02-03.tex,,A Written Child Corpus with Editing History Tags,"自然言語処理や言語学においてコーパスは重要な役割を果たすが，従来のコーパスは大人の文章を集めたコーパスが中心であり，子供の文章を集めたコーパスは非常に少ない．その理由として，子供のコーパスに特有の様々な難しさが挙げられる．そこで，本論文では，子供のコーパスを構築する際に生じる難しさを整理，分類し，効率良く子供のコーパスを構築する方法を提案する．また，提案方法で実際に構築した「こどもコーパス」についても述べる．提案方法により，81人分（39,269形態素）のコーパスを構築することができ，提案方法の有効性を確認した．この規模は，公開されている日本語書き言葉子供コーパスとしては最大規模である．また，規模に加えて，「こどもコーパス」は作文履歴がトレース可能であるという特徴も有する．","Corpora have played a crucial role in natural language processing and linguistics. However, there have been very few corpora consisting of the writing of children because of difficulties peculiar to child corpus creation. In this paper, we propose a method for avoiding the difficulties and efficiently creating a child corpus. We have used the proposed method to create a child corpus to show its effectiveness. As a result, we have obtained a child corpus called \textit{Kodomo Corpus","['はじめに', '子供コーパス構築の難しさ', '提案する構築方法', 'こどもコーパス', 'おわりに']",,,,,,,,
V17N03-01.tex,,Estimation of Connectivity between Paragraphs in a Mail Text,,"In order to improve the readability, 
we often segment a mail text into smaller paragraphs than necessary.
However,
this oversegmentation is a problem of mail text processing.
It would negatively affect discourse analysis,
information extraction,
information retrieval,
and so on.
To solve this problem,
we propose methods of estimating the connectivity between paragraphs in a mail.
In this paper, 
we compare paragraph connectivity estimation based on machine learning methods (SVM and ME)
with a rule-based method and 
show that the machine learning methods outperform the rule-based method.","['Introduction', 'Clues to paragraph connectivity estimation in a mail text', 'Paragraph connectivity estimation using a rule-based method', 'Paragraph connectivity estimation using machine learning methods', 'Conclusions']",,,,,,,,
V17N03-02.tex,,"Collecting Object-attribute Noun Pairs and Constructing 
	Concept Graphs for the Argument of Adjectives from Japanese N1-Adj-N2 
	Constructions",,"In this paper, we propose a method for exploring the Japanese
construction N1-Adj-N2, which often establishes a relationship between
an object (N2), an attribute (N1), and an evaluation of that attribute
(Adj). As this construction connects two nouns, our method involves
constructing a graph of the noun relations, which can be considered 
as representing selectional restrictions for the argument of a target 
adjective. The exploration of N1-Adj-N2 constructions is useful for
opinion mining, lexicographical analysis of adjectives, and writing
aid, among others.","['Introduction', 'The nature of the N1-Adj-N2 construction', 'Collecting object-attribute pairs', 'Construction of concept graphs for the argument of adjectives', 'Related work', 'Conclusions']",,,,,,,,
V17N03-03.tex,,Improving Vietnamese Word Segmentation and POS Tagging using MEM with Various Kinds of Resources,,"Word segmentation and POS tagging are two important problems included in many NLP tasks.
They, however, have not drawn much attention of Vietnamese researchers all over the world.
In this paper, we focus on the integration of advantages from several resourses to
improve the accuracy of Vietnamese word segmentation as well as POS tagging task. For word segmentation,
we propose a solution in which we try to utilize multiple knowledge resources including dictionary-based model,
N-gram model, and named entity recognition model and then integrate them into a Maximum Entropy model.
The result of experiments on a public corpus
has shown its effectiveness in comparison with the best current models. We got \textbf{95.30\% F1 measure","['Introduction', 'Overview of related models', 'The proposed model for vietnamese word segmentation', 'Proposed model for vietnamese POS tagging', 'Discussions and related works', 'Conclusion and future works']",,,,,,,,
V17N03-04.tex,,Comparison of Chinese Treebanks for Corpus-oriented HPSG Grammar Development,,"Comparing with the traditional way of manually developing grammar based on 
linguistic theory, corpus-oriented grammar development is more promising. To 
develop HPSG grammar through the corpus-oriented way, a treebank is an 
indispensable part. This paper first compares existing Chinese treebanks and 
chooses one of them as the basic resource for HPSG grammar development. Then 
it proposes a new design of part-of-speech tags based on the assumption that 
it is not only simple enough to reduce ambiguity of morphological analysis 
as much as possible, but also rich enough for HPSG grammar development. 
Finally, it introduces some on-going work about utilizing a Chinese 
scientific paper treebank in HPSG grammar development.","['Introduction', 'Comparison of Chinese Treebanks', 'A Chinese Part-of-speech Tag Design for HPSG Grammar Development', 'Applying A Chinese Scientific Paper Treebank in HPSG Grammar Development', 'Conclusion and Future Work']",,,,,,,,
V17N03-05.tex,,"Treatment of Legal Sentences Including Itemization Written in Japanese, English and Vietnamese ---Towards Translation into Logical Forms---",,"This paper reports how to treat legal sentences including itemized
 expressions in three languages.
 Thus far, we have developed a system for
 translating legal sentences into logical formulae. Although our system
 basically converts words and phrases in a target sentence into
 predicates in a logical formula, it generates some useless predicates
 for itemized and referential expressions. 
 In the previous study, focusing on Japanese Law, we
 have made a front end system which substitutes corresponding
 referent phrases for these expressions.
 In this paper, we examine our approach to the Vietnamese Law and the
 United States Code.
 Our linguistic analysis shows the difference in notation among
 languages or nations, and we extracted conventional expressions
 denoting itemization for each language.
 The experimental result shows high accuracy in terms of generating
 independent, plain sentences from the law articles including itemization. 
 The proposed system generates a meaningful
 text with high readability, which can be input into our translation
 system.","['Introduction', 'The current system and problems', 'Analysis of law sentences with itemization', 'Method for removing itemization', 'Experiments and results', 'Conclusion']",,,,,,,,
V17N03-06.tex,,Paraphrasing Training Data for Statistical Machine Translation,,"Large amounts of data are essential for training statistical machine
  translation systems. In this paper we show how training data can be
  expanded by paraphrasing one side of a parallel corpus. The new data
  is made by parsing then generating using an open-source, precise
  HPSG-based grammar. This gives sentences with the same meaning, but
  with minor variations in lexical choice and word order.  In
  experiments paraphrasing the English in the Tanaka Corpus, a
  freely-available Japanese-English parallel corpus, we show
  consistent, statistically-significant gains on training data sets
  ranging from 10,000 to 147,000 sentence pairs in size as evaluated
  by the BLEU and METEOR automatic evaluation metrics.","['Introduction', 'Related Work', 'Resources', 'Method', 'Evaluation', 'Discussion', 'Further Work', 'Conclusion']",,,,,,,,
V17N04-01.tex,,"Tag Confidence Measure for Semi-Automatically \\ Updating Named
Entity Recognition","本稿では条件付確率場に基づく固有表現抽出において，新たなドメインにモデ
ルを適応するためのモデル学習コスト—正解データ作成コスト—を低減する2
つの学習手法を提案する．本手法では，タグ単位の事後確率をタグ信頼度とみ
なし，信頼度の低いタグをシステムの解析誤りとして自動的に検出する．そし
て検出された解析誤りタグのみを修正の対象とするため，文全体の事後確率を
利用する場合と比較して，修正が必要である箇所に効率よくコストを注力させ
ることが可能となる． 第1の学習手法として，能動学習に本手法を適用すると，システム出力の信頼
度が低いタグのみを検出して人手修正対象とすることにより，従来手法と比較
して修正コストが1/3に低減した． また，第2の学習手法として正解固有表現リストを利用したブートストラップ
型学習に適用すると，解析誤りとして検出されたタグの上位候補から半自動的
に正解タグを発見可能であった．この学習法では，大量のプレーンテキストか
ら，半自動で正解データを作成できるため，更に学習コストを低減させる効果
がある．","We present two techniques to reduce machine learning cost, i.e., cost
of manually annotating unlabeled data, for adapting existing CRF-based
named entity recognition (NER) systems to new texts or domains. We
introduce the tag posterior probability as the tag confidence measure
of an individual NE tag determined by the base model. Dubious tags are
automatically detected as recognition errors, and regarded as targets
of manual correction. Compared to entire sentence posterior
probability, tag posterior probability has the advantage of minimizing
system cost by focusing on those parts of the sentence that require
manual correction. Using the tag confidence measure, the first
technique, known as active learning, asks the editor to assign correct
NE tags only to those parts that the base model could not assign tags
confidently. Active learning reduces the learning cost by 66\%,
compared to the conventional method. As the second technique, we
propose bootstrapping NER, which semi-automatically corrects dubious
tags and updates its model.","['はじめに', '固有表現抽出', 'タグ信頼度に基づく解析誤り検出', '能動学習', 'ブートストラップ型固有表現抽出', '関連研究', 'おわりに']",,,,,,,,
V17N04-02.tex,,Web Document Clustering Based on the Clusters of Topic Words,"サーチエンジンの検索結果などのWebページ集合をクラスタリングする手法として，
抽出された各重要語を含むWebページ集合をひとつのクラスタとする手法が広く
用いられている．
しかし，従来の研究では重要語間の類似度を考慮していないために，
類似した話題を表す語句が重要語として抽出されると，話題が類似するクラスタ
が複数出力されてしまうという欠点がある．
そこで本研究では，この問題点を解消するために，単語間の類似度を
考慮したWeb文書クラスタリング手法を提案する．
本手法は，サーチエンジンが返すタイトルとスニペットの単語分布情報から，
互いに類似していない重要語を抽出する．
次に，どのクラスタにも属さないWebページをできるだけ減らすために，
重要語から直接Webページのクラスタを生成せずに，
各重要語に類似したWebページ集合に含まれる単語集合として単語グループを生成し，
それらの単語グループのそれぞれに対応するWebページクラスタを生成する．
そして，実際に人手で分類した正解データを用いて
従来手法（語句間の類似度を考慮しない方法）との比較評価を行い，
本手法のほうがクラスタリング性能が高く，かつ類似したクラスタを生成
してしまうという従来手法の問題点が解消できることを示す．","Many Web page clustering systems construct clusters in such a way that, 
  for each of the extracted keywords, one cluster is constructed to contain all the pages 
  that contain this keyword. However, these systems suffer from one serious problem 
  that similar clusters (i.e., clusters that share many Web pages) are likely
  to be generated from similar keywords, because their clustering method fails to 
  take into account the topical similarity between keywords.
  
  To overcome this problem, this study proposes a new Web page clustering method that
  uses the topical similarity between words. The proposed method first extracts keywords 
  that are dissimilar to each other using distributional statistics 
  of word occurrence in snippets and titles of search results. 
  After that, in order to reduce the number of unclassified Web pages, 
  the method generates word groups each of which is 
  a set of words similar to each extracted keyword, and then constructs
  Web page clusters using the word groups, 
  rather than directly generating Web page clusters from keywords.
  
  This study also conducts an evaluation experiment in which our method is
  compared with the existing method that ignores the similarity of keywords
  using the handmade test data. 
  The result is that our system achieves better performance
  and can overcome the problem of multiple similar clusters.","['はじめに', '単語グループに基づくWeb検索結果のクラスタリング手法', '評価実験', '評価結果と考察', 'おわりに']",,,,,,,,
V17N04-03.tex,,A Semantic Analysis System of Japanese Assisted by a Thesaurus,CGM（消費者生成メディア）が普及してきたため，そのための言語処理技術が必要になってきた．このような文章データの自然文による検索や翻訳のために，解析精度の向上が求められている．解析誤りの発生原因である，用語の異なり，構文構造の異なりに対処できる処理方式を実現する．この両者への対策として，シソーラスを用いて用語間の意味的な距離を決定する方式を提案する．具体的には，用語の標準化や係り受けの正規化をするシステムを実現し，さらに，付属語を調べて，省略された主語を復元すること，「文節意図」を付与することを試みた．「Yahoo!知恵袋」のデータを用いて解析実験をした結果，シソーラスを用いない場合に比較して約1\%の精度の向上がみられた．システムが用いている辞書の内容について概要を述べる．,"Because Consumer Generated Media have spread, language processing technologies for that purpose are necessary. The improvement of parsing precision is demanded for both retrieval by a natural sentence and translation of such text data. We realize processing methods which can deal with analysis errors caused by fluctuating terms and ambiguous sentence structures. Specifically, we propose using a thesaurus to decide semantic distance between the terms. We have realized a system which standardizes the terms and normalizes the syntactic dependencies. Further, we examine the internal structure of predicates to recover omitted subjects and determine the ``intention of a predicate''. When we analyze texts of ``Yahoo! Chiebukuro'', the precision improves by about 1\% compared with when the thesaurus is not used. We summarize the contents of the dictionaries our system uses.","['はじめに', '構文構造の決定', '係り受けデータの整理', '情報の付与', '辞書', '結果', 'おわりに']",,,,,,,,
V17N04-04.tex,,Generalization of Semantic Roles in Automatic Semantic Role Labeling,"FrameNet，PropBankといった意味タグ付きコーパスの出現とともに，機械学習の
枠組みを利用した自動意味役割付与システムが数多く研究されてきた．しかし，
これらのコーパスは個々のフレームに固有の意味役割を定義するため，コーパス
中に低頻度，或いは未出現の意味役割が数多く存在し，効率的な学習を妨げている．
本論文は，意味役割付与における意味役割の汎化問題を取り上げ，既存の汎
化指標と新たに提案する指標を役割の分類精度を通して比較し，それぞれの特徴
を探求する．また，複数の汎化指標を同時に利用する分類モデルが自動意味役割
付与の精度を向上させることを示す．実験では，
FrameNetにおいて全体の精度で$19.16\%$のエラー削減，
F1マクロ平均で$7.42\%$の向上を，PropBankにおいて全体の精度で$24.07\%$の
エラー削減，未知動詞に対するテストで$26.39\%$のエラー削減を達成した．","A number of studies have applied
machine-learning approaches to semantic
role labeling with availability of corpora
such as FrameNet and PropBank. These
corpora define frame-specific semantic roles 
for each frame. It is crucial for the machine-learning approach
because the corpus contain a number of infrequent roles which hinder
an efficient learning.
This paper focus on a generalization problem of semantic roles in
a semantic role labeling task. We compare existing generalization
criteria and our novel criteria, and clarify characteristics of each
criterion. 
We also show that using multiple generalization criteria in a model
improves the performance of a semantic role classification.
In experiments on FrameNet, we achieved $19.16\%$ error reduction in
terms of total accuracy and $7.42\%$ in macro F1 avarage.
On PropBank,  we reduced $24.07\%$ of errors in total accuracy,
and $26.39\%$ of errors in the evaluation for
unseen verbs.","['はじめに', '関連研究', 'フレーム辞書と意味役割付与コーパス', '意味役割分類問題', '意味役割の汎化と役割分類モデル', 'FrameNetにおける複数の汎化手法', 'FrameNetにおける実験と考察', 'PropBankにおける汎化手法', 'PropBankにおける比較実験', 'まとめ']",,,,,,,,
V17N04-05.tex,,An Improvement of Example-based Emotion Estimation Using Similarity between Sentence and each Corpus,"発話文を感情ごとに分類したコーパスを構築し，入力文と最も類似度が高
い発話文を含むコーパスの感情を推定結果として出力する用例ベースの感
情推定手法が提案されている．従来手法ではコーパスを構築する際，発話テキス
トの収集者が個人個人で発話文の分類先を決定しているため，分類先を決
定する基準が個々によってぶれてしまう．これにより，例えば``希望''のコーパスの中に喜
びの発話文が混じるといったことが起こり，推定成功率を下げてしまう．
本稿ではこの問題を解決するため，コーパスごとにおける入力文の形態素列の出
現回数を用いて，入力文とコーパスの類似度を定義する．そしてこの類似度を従
来手法に導入した新たな類似度計算式を提案する．これにより，誤って分
類されてしまった発話文の影響を緩和することができる．評価実験では従
来手法と比べて成功率が\resp{21.5","Example-based emotion estimators need an emotion corpus in which each
sentences are assigned with emotion tags. It is difficult to determine
emotion tags for the sentence consistently because of ambiguity of emotion.
As a result, there are several wrong tags in a corpus. It causes decrease in
the performance of an emotion estimation. 

In order to solve the problem, a
new similarity between input sentence and emotion corpus is proposed. This
similarity is based on frequencies of morpheme N-gram of the both input
sentence and corpus. Experimental results show that the proposed method
improves emotion precision \resp{from 60.3{\%","['はじめに', 'BLEUを類似度計算に用いた用例に基づく感情推定手法', '提案手法', '評価実験', 'まとめ']",,,,,,,,
V17N04-06.tex,,Study for Prosodic Control Command Generation of Synthetic Speech,音声合成をより使いやすくかつ表現力豊かにするために，我々は階層型音声合成記述言語 MSCL を開発した．MSCLは記述という方法によりニュアンスや心情，感情などを合成音声に付加することが可能である．MSCLはS層，I層，P層の3つの階層を有し，初学者から音声学的知識を有する者まで対応可能にする．一方，MSCLのS層が提供する新たなコマンドの作成手法そしてI層に備わる韻律制御コマンドによって生じる聴感上の効果（印象）の検討は MSCL における課題となっていた．そこで，本研究は MSCL の課題である韻律制御と印象の関係について実験を通じて見出した，8つの制御規則を提案し，それぞれの主な印象について連想法を通じて分析した．また，制御規則を組み合わせて得られる印象の変化についても分析を行った．さらに，韻律制御コマンドを利用する上での留意点について言及する．音声合成での韻律制御を行うための1つのアプローチを提案する．,"The Multi-layered Speech/Sound Synthesis Control Language (MSCL) proposed \linebreak
herein facilitates the synthesizing of several speech modes such as nuance, mental state and emotion, and allows speech to be synchronized to other media easily. MSCL is a multi-layered linguistic system and encompasses three layers: and semantic level layer (The S-layer), interpretation level layer (The I-layer), and parameter level layer (The P-layer). This multi-level description system is convenient for both laymen and professional users. Furthermore, research was conducted into mental state tendencies using a test that examined the perceptions of the subject's sensibility to the control of synthetic speech prosody. The results showed the relationships between prosodic control rules and non-verbal expressions. These relationships are of use for constructing semantic prosody control. This paper describes these functions and the effective prosodic feature controls possible with MSCL.","['はじめに', '韻律制御に基づく印象の抽出', '韻律制御に基づく印象の変化の実験', '言葉の意味による影響', '制御規則の組み合わせによる印象の強調', 'まとめ']",,,,,,,,
V17N04-07.tex,,Kana-Kanji Conversion by Using Unknown Word-Pronunciation Pairs with Contexts,"未知語の問題は，仮名漢字変換における重要な課題の1つである．
本論文では，内容の類似したテキストと音声から
未知語の読み・文脈情報をコーパスとして自動獲得し，
仮名漢字変換の精度向上に利用する手法を提案する．
まず，確率的な単語分割によって未知語の候補となる単語をテキストから抽出する．
次に，各未知語候補の読みを複数推定して列挙する．
その後，テキストに類似した内容の音声を認識させることによって正しい読みを選択する．
最後に，音声認識結果を学習コーパスとみなして仮名漢字変換のモデルを構築する．
自動収集されたニュース記事とニュース音声を用いた実験では，
獲得した未知語の読み・文脈情報を仮名漢字変換のための学習コーパスとして
用いることで，精度が向上することを確認した．","One of the significant problems of kana-kanji conversion (KKC) systems is unknown
words.
In this paper,
for the purpose of improvement in KKC accuracy,
we propose a method for extracting unknown words, their
pronunciations and their contexts from similar sets of Japanese text data and
speech data.
Unknown word candidates are extracted from text data with
a stochastic segmentation model, and their possible pronunciation entries are
hypothesized. These entries are verified by conducting automatic speech recognition
(ASR) on audio data on similar topics.
As a result of ASR, we obtain a corpus for training a stochastic model for KKC.
In the experiment, we use automatically-collected news articles and broadcast TV news
covering similar topics.
We made experimental evaluations with our KKC back-end 
enhanced with these corpora
on other web news articles and
observed an improvement in the accuracy.","['はじめに', '単語$n$-gramモデルとその応用', '未知語とその読み・文脈情報の自動獲得', '評価', '関連研究', '結論']",,,,,,,,
V17N04-08.tex,,SMT with Handmade  Phrase Table,"現在，機械翻訳システムの分野において，対訳データから自動的に翻訳モデル
と言語モデルを獲得し，翻訳を行う統計翻訳が注目されている．翻訳モデルで
は，原言語の単語列から目的言語の単語列への翻訳を，フレーズテーブルで管
理する．しかしフレーズテーブルはプログラムで自動作成するため，カバー率
は高いが信頼性は低いと考えられる．一方，手作業で作成した翻訳対は，信頼
性は高いがカバー率は低いと考えられる．そこで，それぞれの長所を生かすた
めに，プログラムで自動作成したフレーズ対に手作業で作成した翻訳対を追加
することで翻訳精度が向上すると考えた．

実験では，手作業で作成された約13万の翻訳対に翻訳確率を与え，プログラム
で自動作成したフレーズテーブルに追加した．翻訳実験の結果，BLEUスコア
が，日英翻訳の単文では0.9\%，重複文では0.8\%向上した．また人間による対
比較実験を行ったところ，有効性が確認された．

以上の結果から，統計翻訳において手作業で作成した翻訳対を追加する提案手
法は有効であることが示された．","Recently, the statistical machine translation (SMT) method is very
popular for machine translation. This SMT method uses an automatically
calculated translation model and language model for large translation
pair sentences. The translation model provides the probability that
the foreign string is the translation of the native string and is
normally controlled using a phrase table.

However, the phrase table is automatically made; it has high coverage
but low reliability.  On the other side, there are many translation
word pairs made by hand, especially in Japanese English
translation. These translation word pairs have low coverage but high
reliability. Therefore, we added these handmade translation word pairs
into the automatically made phrase table.

In this paper, we used 130,000 translation word
pairs and the phrase table with added word pairs. As a result of
the experiments, we obtained a BLUE score of 13.4\% for simple
sentences and 8.5\% for complex sentences. On the other side,
with the base line system, the score was 12.5\% for simple sentences
and 7.7\% for complex sentences. We also studied an ABX
test. In simple sentences, 5 sentences were good using the base line,
and 23 sentences were good using the proposed method. In complex
sentences, 15 sentences were good using the base line, and 35
sentences were good using the proposed method.

As a result of these experiments, the effectiveness of the
proposed method was shown.","['はじめに', '統計翻訳システム', '自動的に作成したフレーズテーブルへの翻訳対の追加（提案方法）', '翻訳実験', '翻訳対の翻訳確率の重みの最適化', '考察', 'おわりに']",,,,,,,,
V17N05-01.tex,,Creation of Sentiment Corpus by Multiple Annotators with an Annotation Tool that has a Function of Referring Example Annotations,"本稿では評判情報関連タスクにおいて必要不可欠と考えられる，評判情報コーパスを人手により効率良く作成する手法について検討し，作成されたコーパスについて基礎的な分析を行う．まず，注釈付けに用いる評判情報モデルとして，項目—属性—属性値—評価の4つ組からなる2層構造モデルを提案する．
次に，複数注釈者の人手によるコーパス作成について検討する．その際に，注釈者間の注釈揺れが問題となる．
予備実験の結果，注釈者が他の注釈者と相談をせずに独自に注釈付けの判断を行った場合には注釈付けの一致率が十分でないことがわかった．
そこで，複数の注釈者間で判断に関する情報を共有するための方法として，注釈事例参照の利用を提案し，注釈事例参照を組み込んだ注釈付け支援ツールの試作を行った．
これにより，注釈付けの判断に関する情報を複数の注釈者間で緩やかに共有することができる．
評価実験によれば，注釈事例の参照機能が注釈揺れ削減に効果があることがわかった．
さらに，上記の手法を用いた評判情報コーパス作成について報告する．
また，注釈事例参照の有効性を確認した後，1万文のレビュー文書に対して10名の注釈者が注釈付けを行い，評判情報コーパスを作成した．そして，作成したコーパスについて，評判情報の各構成要素の統計的調査を行った結果，提案した2層構造モデルを用いて評判情報を捉えることが有効であることがわかった．","In this paper, we investigated an effective way of creation of a sentiment corpus by using manual annotation.
Sentiment corpora are regarded as indispensable resource in related tasks of sentiment information processing.
First, we proposed a two-layered model of structure of sentiment information, each of which is a tuple of four kinds of elements, i.e.,  `item', `attribute', `value', and `evaluation'.
Second, We investigated corpus annotation by multiple annotators.
In this situation, a corpus to be annotated is divided into multiple parts, and they are assigned to multiple annotators.
Result of preliminary experiment shows that agreement of annotation among multiple annotators is not enough when annotators perform annotation individually.
We proposed utilization of example annotations in the corpus of manual annotation, and a tool for supporting manual annotation based on example annotation.
The proposed tool offers annotators with a way to moderately share the criterion of annotation by referring existing annotations that have been already performed.
Our experimental result shows that referring example annotations is effective to control discrepancy in annotation.
Third, we reported an experiment of creating a sentiment corpus by using the tool.
After confirming the effectiveness, ten annotators annotated a corpus of review text that contains of 10,000 sentences in order to create a corpus of sentiment information.
According to a statistical investigation of each element in the annotated corpus,
the proposed two-layered model is effective to analyze sentiment information in text more precisely.","['はじめに', '関連研究', '評判情報の提案モデル', '予備実験', '注釈事例を用いた評判情報の注釈付け', '注釈揺れに対する注釈事例参照の効果の確認実験', '評判情報コーパスの作成実験', 'おわりに', '予備実験1における注釈者への作業指示', '予備実験2における注釈者への作業指示', '評判情報コーパスの作成実験における注釈者への作業指示']",,,,,,,,
V17N05-02.tex,,JDMWE: A Japanese Dictionary of Multi-Word Expressions,"日常の自然言語文には構成性 (compositionality) に基づいて意味を扱う事が難しいイディオムやイディオム的な複数単語からなる表現，また，語の強い結合によって成り立つ決まり文句や決まり文句的表現が数多く使われているが，現在の自然言語処理 (Natural Language Processing: NLP) ではこれらに十分な対応が出来ていない．近年，この種の特異性を持つ表現を複単語表現 (Multi-Word Expression: MWE) と名付け，NLPの立場から英語のMWE全体を俯瞰・考察した論文 (Sag et al. 2002) が端緒となって，その重要性が広く認識されるようになった．しかし，その後の活発な研究にも拘わらず，包括的で信頼性のある言語資源を構築するには至っていない．筆者らは，現代日本語を対象とした概念語相当 MWE 辞書の構築を古くから進めてきており，本論文ではその初版の概要を報告する．本辞書，JDMWE (Japanese Dictionary of Multi-Word Expressions) は主として人の内省に基づき，以下を目標に編纂されている． 1.~典型的なイディオムや決まり文句に限定せず，いわば準イディオム，準決まり文
\linebreak
\phantom{1.~","Since (Sag et al. 2002) is presented, the NLP society has been aware that one of the most crucial problems in NLP is how to cope with idiosyncratic multiword expressions, which occur in authentic sentences with unexpectedly high frequency. Here, the idiosyncrasy of expression is twofold in principle; one is idiomaticity, i.e. non-compositionality of meaning and the other is the strong probabilistic boundness of word combination. Thus, many trials to extract those expressions from corpora by using mostly statistical method have been made in NLP field. However, presumably because of the difficulty with their correct extraction without human insight, no reliable, extensive resource has yet been available. Authors recognized the crucial importance of such irregular expressions in around 1970 and started to develop a machine dictionary which contains Japanese idioms, idiom-like expressions and other multiword expressions which consist of frequently co-occurring words. In this paper, we give an overview of the first version of the dictionary, namely JDMWE (Japanese Dictionary of Multi-Word Expressions).  It has about 104,000 head entries and is characterized by; 1.~the wide notational, syntactic and semantic variety of contained expressions, 2.~the syntactic function and structure given for each entry expression and 3.~the possibility of internal modification indicated for each component word of the 
\linebreak
\phantom{3.~","['はじめに', '関連研究', '採録表現', '記載情報', '考察', 'おわりに']",,,,,,,,
V17N05-03.tex,,Corpus of Texts Composed by Japanese Elementary School Children and its Application in Linguistics and Sociology,"現在共有されている日本人の子供の書き言葉コーパスは非常に少ないが，子供の書き言葉コーパスは，日本語の使用実態の年齢別推移の分析や，子供の言葉に特徴的に現れる言語形式の分析，国語教育・日本語教育への活用など日本語研究での利用はもちろんのこと，認知発達，社会学など，さまざまな分野での応用の可能性がある．そこで本研究では，全国4,950校の小学校のWebサイトを調査し，公開されている作文について，各テキストが子供の書いたテキストであることや学年などの情報を確認の上，作文データの収集を行った．収集したテキスト総数は10,006，語数は1,234,961である．本研究では，大人よりも子供の言語使用において豊富で多様な使用が観察されると予想されるオノマトペに着目し，その学年別の使用実態の推移について調査した．その結果，オノマトペの出現率は学年が上がるにつれ減少していくことが確認できた．さらに，社会学的応用例として，子供と父母との関係性について調査し，父母とのやりとりとそれに対する子供の反応との関係性が，母親の場合の方が強いことを示し，本コーパスのさまざまな応用の可能性を示した．","One of the problems in corpus-based Japanese linguistics is a shortage of shared linguistic corpus written by Japanese children. Written language corpus of Japanese children shared as language resources would enable us to analyze a change of the Japanese use according to age or examine words and grammatical style characteristically used by children. Such corpus would be expected to contribute to a Japanese study or Japanese education as well as related fields such as cognitive development and sociology. Therefore, in this study we collected essays written by Japanese elementary school children shown in Websites of 265 elementary schools. As a result we collected 10,006 texts, about 1.23 million words.   Using date collected through this process, we investigated how children in each school year used mimetic words. Results showed that the amount of mimetic expressions rose to a third grader, the kinds of onomatopoeic expressions increased to a second grader, and then they were dropping. Furthermore, as a sociologically applied study, we investigated what children wrote about their parents and how they reacted to the exchanges with parents. Result showed that the reaction of a child was rich and strong in the case of mother.","['まえがき', '小学生の作文データの収集', '言語学的利用：子供のオノマトペ学年別使用実態の推移', '社会学的利用の可能性', 'むすび', '付録']",,,,,,,,
V18N01-01.tex,,Relationship between Errors and Corrections in Verb Selection: Basic Research for Composition Support,"日本語学習者が産出する名詞 $n$，格助詞 $c$，動詞 $v$ から成る不自然な共
起表現 $\tupple{n,c,v","Some of the inappropriate co-occurrence expressions consisting of a noun
$n$, a case particle $c$ and a verb $v$ (denoted as $\tupple{n,c,v","['はじめに', '提案手法', '関連研究', '評価実験および考察', 'おわりに', '誤用・正用共起表現対と出力例']",,,,,,,,
V18N01-02.tex,,Discourse Structure Analysis System DIA Based on Centering Theory and Object Knowledge,自動要約，照応解析，質問応答，評判分析などの応用では，文章中の文間の役割的関係や話題の推移の解析が必要になりつつある．本研究ではセンタリング理論と対象知識に基づき談話中の話題の移り変わりを意味的に解析し談話構造を生成する手法を提案し，これに基づく談話構造解析システムDIAを開発した．本研究ではセンタリング理論を拡張し文の依存先の決定に用いる．また，語の語意が表す概念の部分／属性関係，上位—下位関係，類似関係といった対象に関する意味的関係をEDR電子辞書中の共起辞書と概念体系辞書から抽出し，文間の接続関係の決定に用いる．談話の中で話題の推移を表すために談話構造木を定義した．談話構造木では，句点で区切られた文をノードとし，各ノードはただ1つの親ノードを持つ．また，文を接続しているアークに9種類の文間接続関係（詳細化，展開，原因—結果，逆接，遷移，転換，並列，例提示，質問—応答）を付与する．談話構造を決定する手法としては，拡張したセンタリング理論を元に各文に対してそれより前にある文に対して親ノードとなる可能性を得点化し，最高点のものを親ノードとする．次に各文ノード間のアークに対して，その接続関係が9種のそれぞれである可能性を評価する経験的なルールを36個定め，最高得点を得た関係ラベルをアークに付与する．評価実験の結果，接続先の特定では82\%，文間接続関係の判定では81\%の精度を実現した．,"We propose the method of the discourse structure analysis based on the 
centering theory and the domain knowledge. We enhanced the centering theory 
to utilize it for the analysis of dependency relation among sentences. 
Moreover, object knowledge among objects such as entire-part relation, 
parent-child relation, object-attribute relation, resemblance are extracted 
from the co-occurrence dictionary and concept dictionary in EDR electronic 
dictionary, and are used for the decision of the functional relation between 
sentences.
We defined the discourse structure tree to show the transition of the topic 
in the discourse. Here, the sentence punctuated by the period makes a 
node, and each sentence node has only one parent node. Moreover, nine kinds 
of connection relationships between sentences: DETAIL, EXPLICATE, CONTRAST, 
CAUSE-RESULT, TRANSITION, CONVERSION, PARALLEL, EXAMPLE and QUESTION-RESPONSE are given to each arc between sentence nodes. Next, we have 
developed 36 rules to evaluate the possibility of the relation between 
sentences being each one of the nine kinds of relation types. These rules 
are applied to each arc, and the relation label which obtained the highest 
score is given to the arc. The evaluation experiment shows the accuracy of 
82{\%","['はじめに', '提案手法', '対象知識', 'センタリング理論', '談話構造木の構築', '文間接続関係の判定', '評価実験と考察', 'おわりに']",,,,,,,,
V18N02-01.tex,,Language Model Estimation from a Stochastically Tagged Corpus,"確率的言語モデルは，仮名漢字変換や音声認識などに広く用いられている．パラメータは，
  コーパスの既存のツールによる処理結果から推定される．精度の高い読み推定ツールは存在
  しないため，結果として，言語モデルの単位を単語（と品詞の組）とし，仮名漢字モデルを比
  較的小さい読み付与済みコーパスから推定したり，単語の発音の確率を推定せずに一定値と
  している．これは，単語の読みの確率を文脈と独立であると仮定していることになり，この
  仮定に起因する精度低下がある．
  このような問題を解決するために，本論文では，まず，仮名漢字変換において，単語と読み
  の組を単位とする言語モデルを利用することを提案する．単語と読みの組を単位とする言語
  モデルのパラメータは，自動単語分割および自動読み推定の結果から推定される．この処理
  過程で発生する誤りの問題を回避するために，本論文では，確率的タグ付与を提案する．こ
  れらの提案を採用するか否かに応じて複数の仮名漢字変換器を構築し，テストコーパスにお
  ける変換精度を比較した結果，単語と読みの組を言語モデルの単位とし，そのパラメータを
  確率的に単語分割し，さらに確率的読みを付与したコーパスから推定することで最も高い変
  換精度となることが分かった．したがって，本論文で提案する単語と読みの組を単位とする
  言語モデルと，確率的タグ付与コーパスの概念は有用であると結論できる．","In this paper, first we propose a language model based on pairs of word and input
  sequence. Then we propose the notion of a stochastically tagged corpus to cope
  with tag estimation errors. The experimental results we conducted using \textit{kana-kanji","['はじめに', '統計的仮名漢字変換', '仮名漢字変換のための言語資源とその処理', '評価', 'おわりに', '自動読み推定']",,,,,,,,
V18N02-02.tex,,A Simple and Fast Algorithm for Approximate \\ String Matching with Set Similarity,本論文では，コサイン係数，ダイス係数，ジャッカード係数，オーバーラップ係数に対し，簡潔かつ高速な類似文字列検索アルゴリズムを提案する．本論文では，文字列を任意の特徴（tri-gram など）の集合で表現し，類似文字列検索における必要十分条件及び必要条件を導出する．そして，類似文字列検索が転置リストにおける $\tau$ オーバーラップ問題として正確に解けることを示す．次に，$\tau$ オーバーラップ問題の効率的な解法として，CPMerge アルゴリズムを提案する．CPMerge は，検索クエリ文字列中のシグニチャと呼ばれる特徴と，解候補が枝刈りできる条件に着目し，$\tau$ オーバーラップ問題の解候補を絞り込む．さらに，CPMerge アルゴリズムの実装上の工夫について言及する．英語の人名，日本語の単語，生命医学分野の固有表現の 3 つの大規模文字列データセットを用い，類似文字列検索の性能を評価する．実験では，類似文字列検索の最近の手法である Locality Sensitive Hashing や DivideSkip 等と提案手法を比較し，提案手法が全てのデータセットにおいて，最も高速かつ正確に文字列を検索できることを実証する．また，提案手法による類似文字列検索が高速になる要因について，分析を行う．なお，提案手法をライブラリとして実装したものは，SimString としてオープンソースライセンスで公開している．,"This paper presents a simple and fast algorithm for approximate string matching in which string similarity is computed by set similarity measures including cosine, Dice, Jaccard, or overlap coefficient. In this study, strings are represented by unordered sets of arbitrary features (e.g., tri-grams). Deriving necessary and sufficient conditions for approximate string, we show that approximate string matching is exactly solvable by $\tau$-overlap join. We propose CPMerge algorithm that solves $\tau$-overlap join efficiently by making use of signatures in query features and a pruning condition. In addition, we describe implementation considerations of the algorithm. We measure the query performance of approximate string matching by using three large-scaled datasets with English person names, Japanese unigrams, and biomedical entity/concept names. The experimental results demonstrate that the proposed method outperforms state-of-the-art methods including Locality Sensitive Hashing and DivideSkip on all the datasets. We also analyze the behavior of the proposed method on the datasets. We distribute SimString, a library implementation of the proposed method, in an open-source license.","['はじめに', '類似文字列検索の定式化', 'データ構造とアルゴリズム', '実験', '関連研究', 'まとめ']",,,,,,,,
V18N02-03.tex,,A Study on Constants of Natural Language Texts,"本稿では，文書量に不変な定数を考える．このような定数には，言語や文書の
複雑さや冗長性を定量化して捉える計算言語学上の意義がある．
これらの指標は既存研究でさまざまなものが提案されてきたが，
ほとんどの場合英語を中心とする小規模な文書を対象としてきた．
本研究では英語以外のさまざまな言語や，大規模な文書も対象として扱い，
主に先行研究において値が文長に依らないとされる 3 つの指標 $K$, $Z$, 
$\mathit{VM","This paper considers various measures which become constant for any
large lengths of a given natural language text.  Consideration of such
measures gives some hints for studies of complexity of natural language. Previously,
such measures have been studied mainly for relatively small English
texts.  In this work, we consider the measures for texts other than
English and also for large scale texts.  Among the measures, we consider
Yule's $K$, Orlov's $Z$, and Golcher's $\mathit{VM","['はじめに', '関連研究', '指標', '実験', '考察', 'まとめ']",,,,,,,,
V18N02-04.tex,,Automatic Word Segmentation using Three Types of Dictionaries,"本論文では，日本語の文の自動単語分割をある分野に適用する現実的な状況において，精度
  向上を図るための新しい方法を提案する．提案手法の最大の特徴は，複合語を参照すること
  が可能な点である．複合語とは，内部の単語境界情報がなく，その両端も自動分割器の学習
  コーパスの作成に用いられた単語分割基準と必ずしも合致しない文字列である．このような
  複合語は，自然言語処理をある分野に適用する多くの場合に，利用可能な数少ない言語資源
  である．提案する自動単語分割器は，複合語に加えて単語や単語列を参照することも可能で
  ある．これにより，少ない人的コストでさらなる精度向上を図ることが可能である．

  実験では，これらの辞書を参照する自動単語分割システムを最大エントロピー法を用いて構
  築し，それぞれの辞書を参照する場合の自動単語分割の精度を比較した．実験の結果，本論
  文で提案する自動単語分割器は，複合語や単語列を参照することにより，対象分野において
  より高い分割精度を実現することが確認された．","In this paper we propose a new method for automatically segmenting a sentence in
  Japanese into a word sequence. The main advantage of our method is that the
  segmenter is, by using a maximum entropy framework, capable of referring to a list
  of compound words, i.e. word sequences without boundary information. This allows
  for a higher segmentation accuracy in many real situations where only some
  electronic dictionaries, whose entries are not consistent with the word
  segmentation standard, are available. Our method is also capable of exploiting a
  list of word sequences. It allows us to obtain a far greater accuracy gain with
  low manual annotation cost.

  We prepared segmented corpora, a compound word list, and a word sequence
  list. Then we conducted experiments to compare automatic word segmenters referring
  to various types of dictionaries. The results showed that the word segmenter we
  proposed is capable of exploiting a list of compound words and word sequences to
  yield a higher accuracy under realistic situations.","['はじめに', '単語分割のための言語資源', '単語分割法', '評価', '関連研究', 'おわりに']",,,,,,,,
V18N02-05.tex,,Active Learning with Subsequence Sampling Strategy\\for Sequence Labeling Tasks,,"We propose an active learning framework for sequence labeling tasks. In each iteration, a set of subsequences are selected and manually labeled, while the other parts of sequences are left unannotated. The learning will stop automatically when the training data between consecutive iterations does not significantly change. We evaluate the proposed framework on chunking and named entity recognition data provided by CoNLL. Experimental results show that we succeed in obtaining the supervised $F_1$ only with 6.98\%, and 7.01\% of tokens being annotated, respectively.","['Introduction', 'Related work', 'Conditional random fields (CRFs)', 'Proposed framework', 'Experiments and results', 'Conclusion and future work']",,,,,,,,
V18N02-06.tex,,"Construction of a Blog Corpus with Syntactic, Anaphoric, and Sentiment Annotations","近年，ブログを対象とした情報アクセス・情報分析技術が盛んに研究されてい
る．
我々は，この種の研究の基礎データの提供を目的とし，249 記事，4,186 文
からなる，解析済みブログコーパスを構築した．
主な特長は次の 4 点である．
i) 文境界のアノテーション．
ii) 京大コーパス互換の，形態素，係り受け，格・省略・照応，固有表現のアノテーション．
iii) 評価表現のアノテーション．
iv) アノテーションを可視化した HTML ファイルの提供．
記事は，大学生 81 名に「京都観光」「携帯電話」「スポーツ」「グルメ」のい
ずれかのテーマで執筆してもらうことで収集した．
解析済みブログコーパスを構築する際，不明瞭な文境界，括弧表現，誤字，方言，顔文字等，多様な形態素への対応が課題になる．
本稿では，本コーパスの全容とともに，いかに上記の課題に対応しつつコーパ
スを構築したかについて述べる．","There has been a growing interest in the technologies of information
 access and analysis targetting blog articles recently. 
In order to provide the research community with the basic data, 
we constructed a blog corpus that consists of 249 articles (4,186
 sentences) and has the following features: 
i) Annotated with sentence boundaries. 
ii) Annotated with grammatical information about
 morphology, dependency, case, anaphora, and named entities, in a way
 consistent with Kyoto University Text Corpus. 
iii) Annotated with sentiment information. 
iv) Provided with HTML files that visualize all the annotations
 above. 
We asked 81 university students to write blog articles about either
the sightseeing of Kyoto, cellphones, sports, or gourmet. 
In constructing the annotated blog corpus, we faced problems concerning 
sentence boundaries, parentheses, 
errata, dialect, a variety of smiley, and other morphological
 variations. 
In this paper, we describe the specification of the corpus and how we
 dealt with the above problems.","['はじめに', '関連研究 \\label{sec:related-work', 'コーパスの全体像 \\label{sec:spec', '構築から配布まで \\label{sec:construction', 'おわりに']",,,,,,,,
V18N03-01.tex,,Construction of Context Models for Word Sense Disambiguation,,"This paper presents a study on the use of word context features for
Word Sense Disambiguation (WSD). State-of-the-art WSD systems achieve
high accuracy by using resources such as dictionaries, taggers, lexical
analyzers or topic modeling packages. However, these resources are
either too heavy or don't have sufficient coverage for large-scale
tasks such as information retrieval. The use of local context for
WSD is common, but the rationale behind the formulation of features
is often based on trial and error. We therefore investigate the notion
of relatedness of context words to the target word (the word to be
disambiguated), and propose an unsupervised method for finding the
optimal weights for context words based on their distance to the target
word. The key idea behind the method is that the optimal weights should
maximize the similarity of two context models constructed from diff{","['Introduction\\label{sec:Introduction', 'The state of the art of WSD', 'Learning the optimal context model', 'Classifiers for supervised WSD tasks', 'Experiments on Semeval-2007 English Lexical Sample', 'Experiments on Semeval-2010 Japanese WSD', 'Discussion', 'Conclusion']",,,,,,,,
V18N03-02.tex,,Semi-Supervised Japanese Word Sense Disambiguation Based on Two-Stage Classification of Unlabeled Data and Ensemble Learning,"本稿では，パラメータ調整を簡略化したブートストラッピング的手法による
日本語語義曖昧性解消を提案する．
本稿で取り上げるブートストラッピングとは，ラベルなしデータを既存の
教師あり学習手法を用いて分類し，その中で信頼度の高いデータを
ラベル付きデータに加え，この手順を反復することによって分類の性能を向上させる
半教師あり学習手法である．
従来のブートストラッピングによる語義曖昧性解消においては，プールサイズ，
ラベル付きデータに追加するラベルなしデータの事例数，手順の反復回数といった
パラメータをタスクに合わせ調整する必要があった．
本稿にて提案する手法はヒューリスティックと教師あり学習（最大エントロピー法）
によるラベルなしデータの二段階の分類，および学習に用いるラベルなしデータの
条件を変えた複数の分類器のアンサンブルに基づく．
これにより必要なパラメータ数は一つになり，
かつパラメータの変化に対し頑健な語義曖昧性解消を実現する．
SemEval-2 日本語タスクのデータセットを用いたベースラインの教師あり手法との
比較実験の結果，パラメータの変化に対し最高で 1.8 ポイント，
最低でも 1.56 ポイントの向上が見られ，提案手法の有効性を示せた．","In this paper, we propose a bootstrapping-like method 
which eases optimal and empirical parameter selection 
for Japanese word sense disambiguation. 
Bootstrapping means, in this paper, semi-supervised learning methods 
based on the following procedures: 
(1) train a classifier on labeled examples, 
(2) use the classifier to select confident unlabeled examples, 
(3) add them to the labeled examples, 
(4) repeat steps 1--3. 
Traditional bootstrapping methods require empirical selection for 
the parameters including the pool size, the number of 
the most confident examples and the number of iterations. 
Our method uses two-stage unlabeled example classification 
based on heuristics and a supervised method (Maximum Entropy classifier) 
and combines a series of classifiers along a sequence of varying conditions. 
This method requires only one parameter 
and enables parameter robust word sense disambiguation. 
Experiments compared with the baseline supervised method 
on the Japanese WSD task of SemEval-2 shows that our method obtained 
accuracy improvement between 1.8 and 1.56 points.","['はじめに', '関連研究', '提案手法', '実験', 'おわりに']",,,,,,,,
V18N03-03.tex,,Effectiveness of Automatic Expansion of Training Data for Japanese Word Sense Disambiguation,"本稿では，訓練データの自動拡張による語義曖昧性解消の精度向上方法について
述べる．評価対象として，SemEval-2010 日本語語義曖昧性解消タスクを利用した．
本稿では，まず，配布された訓練データのみを利用して学習した場合の結果を紹
介する．更に，辞書の例文，配布データ以外のセンスバンク，ラベルなしコーパ
スなど，
さまざまなコーパスを利用して，訓練データの自動拡張を試
みた結果を紹介する．本稿では，訓練データの自動獲得により 79.5\% 
の精度を得ることができた．
更に，対象語の難易度に基づき，追加する訓練データの上限を制御したところ，
最高 80.0\% 
の精度を得ることができた．","In this paper, 
we propose a method to 
expand training data automatically, 
using example sentences of a dictionary, other sensebank and 
unlabelled corpus. 
We tested on the data of SemEval-2010 Japanese WSD task and achieved 
79.5\% accuracy in our experiments. 
Then, we limited the number of used training data and 
achieved 80.0\% accuracy.","['はじめに', 'データ', '実験', '結果と議論', '自動獲得した訓練データの評価', 'おわりに']",,,,,,,,
V18N03-04.tex,,On SemEval-2010 Japanese WSD Task,,"An overview of the SemEval-2 Japanese WSD task is
  presented. The new
  characteristics of our task are (1) the task will use the first
  balanced Japanese sense-tagged corpus, and (2) the task will take into
  account not only the instances that have a sense in the given set
  but also the instances that have a sense that cannot be found in the
  set. It is a lexical sample task, and word senses are defined
  according to a Japanese dictionary, the Iwanami Kokugo Jiten. This dictionary
  and a training corpus were distributed to participants. The
  number of target words was 50, with 22 nouns, 23 verbs, and 5
  adjectives. Fifty instances of each target word were provided,
  consisting of a total of 2,500 instances for the evaluation. Nine systems
  from four organizations participated in the task.","['Introduction', 'Data', 'Word sense tagging', 'Evaluation methodology', 'Participating systems', 'Results', 'Conclusion']",,,,,,,,
V18N04-01.tex,,Annotation for Writer's Attitude based on Hierarchical Semantics,本論文では，日本語コーパス内の命題に書き手の心的態度をアノテーションする基準として階層意味論を検討する．階層意味論とは「命題」と「モダリティ」からなる普遍的な意味構造を規定する概念である．モダリティは心的態度を指す概念として知られているが，既存研究で取り上げられている文法論のモダリティでは対象が文法形式に限定されてしまう．対して階層意味論で定義される「モダリティ」は意味論上の概念であるため形式上の制約が少なく，心的態度を網羅的にアノテーションするという目的により適した概念といえる．ただし，階層意味論で規定される心的態度を母語話者が一貫性を持ってアノテーションできるのか実証的に確認されているとは言い難い．そこで，母語話者に新聞の社説記事に対するアノテーションを実際に行ってもらい，その一貫性を調査した．その結果，4 名の間での Fleiss の $\kappa$ 係数は，真偽判断系，価値判断，拘束判断でそれぞれ 0.49，0.28，0.70 となった．真偽判断系と価値判断は一致度が高いとは言い難いが，真偽判断系に関しては，述語または後続表現の語彙的機能の影響で真偽を読み取ることが困難な命題を取り除くと 0.58 まで改善した．加えて，語彙，文法形式によって明示的に心的態度が表されていない命題でも 0.50，0.28，0.53 の値を示した．このことから，心的態度を表す語句，文法形式が明示されていなくてもある程度の一貫性が得られることが伺える．,"In this paper, we examine Hierarchical Semantics for an annotation of
a writer's attitude toward a proposition in Japanese text. 
Hierarchical Semantics defines a universal semantic structure 
composed of a ``proposition'' and a ``modality''. 
A modality is known as a linguistic concept of writer's attitude 
and there are some linguistic modality theories which have been adopted in previous works. 
But the theories cover only grammatical forms 
because they are grammatical theories. On the other hand,
Hierarchical Semantics defines modality not as syntax but
semantics. The semantic definition is more useful to cover all of
writer's attitudes than the syntactic definition since there is hardly
any excess formal condition. To confirm whether we can annotate
the semantic information consistently, we examined the degree of
consistency among Japanese native speakers' annotations of truth,
value and deontic judgments on Japanese newspaper editorials. The
result shows that Fleiss's kappa coefficients of truth, value and
deontic judgments between them are 0.49, 0.28 and 0.70. The one
of truth judgment can be increased to 0.58 by removing
propositions which are inappropriate to ask the truth-value. In
addition, these coefficients were 0.50, 0.28 and 0.53 even when we
removed propositions which syntactically depend on a modal form or a subjective
expression. It means that Japanese native speakers understand
a writer's attitude consistently
even when it cannot be explained by lexcical or grammatical rules.","['はじめに', '自然言語処理とモダリティ論との相違点', '階層意味論に基づく心的態度の規定', 'アノテーションの不一致を引き起こす要因と対策', 'おわりに']",,,,,,,,
V18N04-02.tex,,shWiiFit Reduce Dependency Parsing,"本稿では係り受け構造情報のタグ付けの一貫性について考える．係り受け構造には，統語的制約により一意に決まる構造と選択選好性によるタグ付け作業者に委ねる構造がある．多くの場合，統語的制約を優先してタグ付けられるが，選択選好性に影響され誤ってタグ付ける例が多々ある．このような事例について誤り傾向の差分を評価するために，ゲームを用いた新しい心理言語実験手法を提案する．
埋め込み構造によるガーデンパス文を用いて\mmodified{13","This paper argues about consistency of syntactic dependency corpus annotation.
Dependency structure involves unambiguous substructures specified by syntactic constraints and underspecified substructures evaluated by selectional preferences of annotators.
In most cases, the syntactic constraints have a priority over the selectional preferences.  In some cases, however, the selectional preferences are given priority over the syntactic constraints. 
We propose a new psycholinguistic experimental environment to use 
a fitness game application, in which a user stands
on Nintendo Wii Balance Board and plays a parsing game competing on
the accuracy and the speed.
Tha game-based experiments investigate how correctly a human can parse written sentences.
We evaluate syntactically confusing sentences with \mmodified{13","['はじめに', '日本語係り受け解析', '日本語ガーデンパス文', 'ゲーム： shWiiFit Reduce Dependency Parsing', '実験 \\label{sec:5', 'おわりに']",,,,,,,,
V18N04-03.tex,,Morphological Analysis with Pointwise Predictors,"本論文では，形態素解析の問題を単語分割と品詞推定に分解し，それぞれの処理で点予測を
  用いる手法を提案する．点予測とは，分類器の素性として，周囲の単語境界や品詞等の推定
  値を利用せずに，周囲の文字列の情報のみを利用する方法である．点予測を用いることで，
  柔軟に言語資源を利用することができる．特に分野適応において，低い人的コストで，高い
  分野適応性を実現できる．提案手法の評価として，言語資源が豊富な一般分野において，既
  存手法である条件付き確率場と形態素$n$-gramモデルとの解析精度の比較を行い，同程度の
  精度を得た．さらに，提案手法の分野適応性を評価するための評価実験を行い，高い分野適
  応性を示す結果を得た．","This paper proposes a pointwise approach to Japanese morphological analysis that
  decomposes the process into word segmentation and part-of-speech (POS) tagging.
  The pointwise approach refers, as features, only to the surface information of the
  input and not relies on any prediction results such as word boundaries or POS
  tags. This design allows us to use a variety of linguistic resources flexibly.
  This characteristic enables a fast and low-cost domain adaptation with a minimum
  amount of annotation. An evaluation was performed on a well-resourced general
  domain morphological task, and it was found that the proposed method achieved
  results comparable to those of existing methods such as CRFs and morpheme $n$-gram
  models. In addition, a domain adaptation experiment showed that the proposed
  method is able to achieve an effective domain adaptation with a smaller amount of
  annotations.","['はじめに', '点予測を用いた形態素解析', '評価', 'おわりに']",,,,,,,,
V19N01-01.tex,,Generating Information-Rich Taxonomy Using Wikipedia,"単語の上位下位関係を自動獲得する研究はこれまで活発に行われてきたが，
上位概念の詳細さに関する議論はほとんどなされてこなかった．
自動獲得された上位下位関係の中には，
例えば\isa{作品","Hyponymy relation acquisition has been
 extensively studied. However,
the informativeness of acquired hypernyms
has not been sufficiently discussed.
We found that the hypernyms in automatically acquired hyponymy relations
are often too vague for their hyponyms.
For instance, \xmpE{work","['はじめに', '自動獲得された上位概念の問題 \\label{sec:hh-problems', 'Wikipediaを用いた上位下位関係の獲得\n\\label{sec:Base-hh', '詳細な上位下位関係の獲得手法\n\\label{sec:proposed-method', '評価実験 \\label{sec:evaluation', '応用 \\label{sec:discussion', '関連研究 \\label{sec:related-word', 'おわりに \\label{sec:conclusion']",,,,,,,,
V19N01-02.tex,,Study on Supervised Learning of Vietnamese Word Sense Disambiguation Classifiers,,"It is said that Vietnamese is a language with highly ambiguous words.
However, there has been no published Word Sense Disambiguation (WSD hereafter) research on this language.
This current research is the first attempt to study Vietnamese WSD. 
Especially, we would like to explore the effective features for training WSD classifiers and verify the
applicability of the `pseudoword' technique to both investigating effectiveness of features and training WSD classifiers.
Three tasks have been conducted, using two corpora which were built manually based on Vietnamese Treebank
and automatically by applying pseudowords technique.
Experiment results showed that Bag-Of-Word feature performs well for all three categories of words (verbs, nouns, and adjectives).
However,
its combination with POS, 
Collocation or Syntactic features can not significantly improve the performance of WSD classifiers.
Moreover, the experiment results confirmed that pseudoword is a suitable technique to explore the effectiveness of features in disambiguation of Vietnamese verbs and adjectives.
Furthermore, 
we empirically evaluated the applicability of the pseudoword technique as an unsupervised learning method for real Vietnamese WSD.","['Introduction', 'Related work', 'Our method', 'Tasks', 'Evaluation', 'Discussion', 'Conclusion']",,,,,,,,
V19N02-01.tex,,Splitting Katakana Noun Compounds\\ by Paraphrasing and Back-transliteration,"日本語を含めた多くの言語において，複合名詞内部の単語境界は空白で分かち書
きされない．こうした複合名詞を構成語列へと分割する処理は，多くの自然言語
処理の応用において重要な基礎技術となる．日本語の場合，片仮名語は生産性が
高く未知語が多いことから，特に片仮名複合名詞の扱いが技術的な問題となる．
この問題の解決を図るため，本論文は片仮名複合名詞の言い換えと逆翻字を分
割処理に利用する方法を提案する．実験では，言い換えと逆翻字をラベルなしテ
キストから抽出し，その情報を利用することによって，分割精度が統計的に有意
に向上することを確認した．","Word boundaries within noun compounds are not marked by white
	spaces in a number of languages including Japanese, and it is
	beneficial for various NLP applications to split such noun
	compounds. In the case of Japanese, noun compounds made up of
	katakana words are particularly difficult to split, because
	katakana words are highly productive and are often
	out-of-vocabulary. To overcome this difficulty, we propose using
	paraphrases and back-transliteration of katakana noun compounds
	for splitting them. Experiments demonstrated that splitting
	accuracy is improved with a statistical significance by extracting
	both paraphrases and back-transliterations from unlabeled
	textual	data, and then using that information for constructing
	splitting models.","['はじめに', '関連研究', '教師あり学習に基づく手法', '言い換え素性', '逆翻字素性', '実験と議論', 'おわりに']",,,,,,,,
V19N02-02.tex,,Entity Set Expansion based on Bootstrapping Methods using Topic Information,"本論文ではブートストラップ法を用いた語彙獲得を行う際に，トピック情報を用いることでセマンティックドリフトを緩和し，獲得精度を向上できることを示す．
獲得対象とする語を含む文書の大域的情報であるトピック情報を，統計的トピックモデルを用いて推定し，識別モデルを用いたブートストラップ法における3つの過程で利用する．1つ目は識別モデルにおける素性として，2つ目は負例生成の選択基準として，3つ目は学習データの多義性解消のために用いる．
実験において，提案手法を用いることでセマンティックドリフトを軽減し，語彙の獲得精度が6.7から28.7\%向上したことを示す．","This paper proposes three modules based on latent topics of documents for alleviating ``semantic drift'' in bootstrapping entity set expansion. 
These new modules are added to a discriminative bootstrapping algorithm to realize topic feature generation, negative example selection and positive example disambiguation.
In this study, we model latent topics with  LDA (Latent Dirichlet Allocation) in an unsupervised way. 
Experiments show that the accuracy of the extracted entities is improved by 6.7 to 28.2\% depending on the domain.","['はじめに', 'ブートストラップ法を用いた語彙獲得における課題', 'トピック情報を用いたブートストラップ法', '実験', '関連研究', 'まとめと今後の課題']",,,,,,,,
V19N03-01.tex,,Relevance Feedback using Surface and Latent Information in Texts,"適合性フィードバックの手法の多くは，テキストに表層的に出現する単語の情報
だけを用いて検索結果をリランキングしている．これに対し，本稿では，テキス
トに表層的に出現する単語の情報だけでなく，テキストに潜在的に現れうる単語
の情報も利用する適合性フィードバックの手法を提案する．提案手法では，まず
検索結果に対して Latent Dirichlet Allocation (LDA) を実行し，各文書に潜在
する単語の分布を推定する．ユーザからフィードバックが得られたら，これに対
しても LDA を実行し，フィードバックに潜在する単語の分布を推定する．そして，
表層的な単語の分布と潜在的な単語の分布の両方を用いてフィードバックと検索
結果中の各文書との類似度を算出し，これに基づいて検索結果をリランキングす
る．実験の結果，$2$文書（合計$3,589$単語）から成るフィードバックが与えら
れたとき，提案手法が初期検索結果の Precision at $10$ (P@10) を$27.6\%$改
善することが示された．また，提案手法が，フィードバックが少ない状況でも，
初期検索結果のランキング精度を改善する特性を持つことが示された（e.g.,
フィードバックに$57$単語しか含まれていなくても，P@10 で$5.3\%$の改善が見
られた）．","Most of the previous relevance feedback methods re-rank search results
using only the information of surface words in texts. In this paper, we
present a novel method that uses not only the information of surface
words, but also that of latent words that are highly probable from the
texts. In the proposed method, we infer the latent word distribution in
each document in the search results using latent Dirichlet allocation
(LDA). When user feedback is given, we also infer the latent word
distribution in the feedback using LDA. We calculate the similarities
between the user feedback and each document in the search results using
both the surface and latent word distributions, and then, we re-rank the
search results based on the similarities. Evaluation results show that
when user feedback that consists of two documents ($3,589$ words) is
given, our method improves the initial search results by $27.6\%$ in
precision at $10$ (P@10). Additionally, it proves that our method has
the advantage of performing well even when only a small amount of user
feedback is available (e.g., improvement of $5.3\%$ in P@10 was achieved
even when user feedback constituted only $57$ words).","['はじめに', '言語モデルに基づくランキング', 'LDA', '提案手法', '実験', 'おわりに']",,,,,,,,
V19N03-02.tex,,Automatic Selection of Domain Adaptation Method for WSD using Decision Tree Learning,"ソースドメインのデータによって分類器を学習し，ターゲットドメインに適応することを領域適応といい，
近年さまざまな手法が研究されている．しかし，語義曖昧性解消 (WSD: Word
Sense Disambiguation) について領域適応を行った場合，
最も効果的な領域適応手法は，ソースデータとターゲットデータの性質により異なる．
本稿ではそれらの性質から，
WSDの対象単語タイプ，ソースドメインとターゲットドメインの組み合わせに対して，
最も効果的な領域適応手法を決定木学習を用いて自動的に選択する手法について述べるとともに，
どのような性質が効果的な領域適応手法の決定に影響を与えたかについて考察する．","Domain adaptation (DA), which involves adapting a classifier developed from source 
to target data, has been studied intensively in recent years. 
However, when DA for word sense
disambiguation (WSD) was carried out, the optimal DA
method varied according to the properties of the source and 
target data. This paper describes how the optimal
method for DA was determined depending on these properties
using decision tree learning given a triple of the target word type of WSD, the source domain, and the target domain 
and discusses what properties affected the determination of the best method when Japanese WSD was performed.","['はじめに', '関連研究', '領域適応手法の自動選択', 'データ', '決定木学習におけるラベル付きデータの作成方法と学習方法', '結果', '考察', 'まとめ', '生成された決定木']",,,,,,,,
V19N03-03.tex,,A Pointwise Approach to Training Dependency Parsers from Partially Annotated Corpora,,"We introduce a word-based dependency parser for Japanese that
  can be trained from partially annotated corpora, allowing for 
  effective use of available linguistic resources and reduction of the
  costs of preparing new training data. This is especially important
  for domain adaptation in a real-world situation. We use a pointwise
  approach where each edge in the dependency tree for a sentence is
  estimated independently. Experiments on Japanese dependency parsing
  show that this approach allows for rapid training and achieves
  accuracy comparable to state-of-the-art dependency parsers trained
  on fully annotated data.","['Introduction', 'Pointwise estimation for dependency parsing', 'Domain adaptation for dependency parsing', 'Evaluation', 'Comparison with a phrase-based dependency parser', 'Related work', 'Conclusion']",,,,,,,,
V19N03-04.tex,,Automatic Generation of Article Correspondence Tables for the Comparison of Local Government Statutes,"地方自治体が制定する条例（規則も含め，以下例規という）は，章節／条項号という階層を有する，
基本的に構造化された文書である．各自治体はそれぞれ別個に各議会等でこの例規を制定するため，
複数の自治体が同一の事柄に関する規定（例えば「淫行処罰規定」など）を有している事が多い．
この同一の事柄に関する規定の自治体間における異同を明らかにするための比較は，
法学教育や法学研究，地方自治体法務，企業法務において実施されている．
実務における法の比較では，対応する条項を対とし，それらの条文を左右または上下に
並べた条文対応表の作成が主体となっている．
これまで条文対応表は手作業で作成されてきたが，
対象とする例規の条数や文字数が多い場合の表作成には3時間以上も必要としていた．
そのため計算機による条文対応表の作成支援が強く求められているが，本件に関する研究はこれまでに行われていない．
そこで我々の研究は，条文対応表を計算機で自動作成することによる条文対応表の作成支援を目的とする．
この目的を達成するため，我々は条文対応表を，各条をノードとする二部グラフとしてモデル化し，
このモデルに基づき条文対応表を自動作成するために有効な手法の検討を行った．
二文書間の類似度を定義する多くの研究がこれまでに報告されている．
これらの類似度比較手法より
本研究ではベクトル空間モデル，最長共通部分列，及び文字列アライメント（編集コスト可変のレーベンシュタイン距離）
に基づく96個の類似尺度の性能を比較した．
評価には愛媛県の11の条例とそれに対応する香川県の11の条例を用い，
法学者が作成した条文対応表に基づき正解率を求めた．
その結果，名詞，副詞，形容詞，動詞，連体詞を対象としたベクトル空間モデルに基づく類似尺度の
正解率が85\%と最も高かった．また，文字列アライメントに基づく類似尺度の正解率は最高で81\%，最長共通部分列は
最高で75\%であった．
本研究は条文対応表の作成支援であるため，
推定された対応関係の信頼度，あるいは尤もらしさを提示する事が望ましい．
そこで
各比較手法で最も正解率の高かったパラメータを用いた合計3つの類似尺度に対して
受信者操作特性曲線による評価を行ったが，曲線下面積がいずれも狭くて
信頼度の尺度として適さない．
そこで，推定された対応関係の類似度を二番目に高い類似度を持つ対応関係の値で
割る事による正規化を行ったところ，
最長共通部分列の曲線下面積が0.80と最も高く，
ベクトル空間モデルの面積は0.79と良好であった．
以上の評価結果より，条文対応表の作成支援では
条見出しに対して最長共通部分文字列を，
条文に対してベクトル空間モデルをそれぞれ適用した類似尺度を
併用する事が，
そして得られた条文対応関係の信頼度を評価する尺度としては
二番目に高い類似度で割った値を用いるとよい事を明らかにした．","Local governments establish ordinances and regulations (hereinafter collectively referred to as ``statutes''). They are structured documents that
possess a chapter $>$article $>$paragraph $>$item hierarchy. Since each local government establishes statutes in its councils independently, similar statutes
on the same matter are often found in separate local governments (e.g. punishment for obscene habits). In legal education, legal research and legal
works at local government and business enterprise, comparisons are made to clarify the differences between similar statutes. In the comparison of laws
for practical purposes, article correspondence tables are normally created with pairs of corresponding articles aligned horizontally or vertically.
The objective of our research is to use a computer to automatically generate the article correspondence tables that are currently created
manually. In order to accomplish this objective, we have focused on the relationships between articles in article correspondence tables, which were
modeled with directed bipartite graphs that used each article as a node. 96 methods based on the vector space model, longest common subsequence and
sequence alignment were examined in order to clarify effective methods for searching for corresponding articles. In the course of the research, we
automatically generated article correspondence tables  of 22 statutes in total (11 statutes of Ehime and Kagawa Prefectures, respectively). Their
accuracy rates were calculated based on article correspondence tables created by legal scholars. Consequently, the vector space model-based method
proved the highest accuracy rate at 85\%. Its targets were nouns, adverbs, adjectives, verbs and attributives. The sequence alignment-based method
showed up to 81\% of accuracy rate, while the rate with the longest common subsequence method was 75\%.
As the results of the computer-generated article correspondence tables are checked by legal scholars on a practical level,
it is required to posess the degree of reliability for each relationships between articles.
To meet the requirement, we examined two measurements for the three methods by receiver operating charasteristic curve.
The results shows the ratio of the selected relations and the runner-up gives 0.80 AUC for longest common subsequences.
In this research, the problem was defined by focusing on the correspondence relationship between articles in the article correspondence tables.
For practical purposes, there is a need to focus not just on the correspondence relationship between articles, but also on the clarification of
different words used in corresponding articles. Since  vector space model cannot be used to clarify such differences, sequence alignment---with which
it is feasible to clarify differing texts---is necessary. Composite methods that combine those two will therefore be required in the future.","['はじめに', 'モデル化と問題定義', '条間の類似尺度', '評価実験', 'まとめ']",,,,,,,,
V19N04-01.tex,,Constructing Large-Scale General Ontology\\from Wikipedia,"Wikipediaをis-a 関係からなる大規模な汎用オントロジーへ再構成した．Wikipediaの記事にはカテゴリが付与され，そのカテゴリは他のカテゴリとリンクして階層構造を作っている．Wikipediaのカテゴリと記事をis-a関係のオントロジーとして利用するためには以下の課題がある．(1) Wikipedia の上位階層は抽象的なカテゴリで構成されており，これをそのまま利用してオントロジーを構成することは適切でない．(2) Wikipedia のカテゴリ間，及びカテゴリと記事間のリンクの意味関係は厳密に定義されていないため，is-a関係でないリンク関係が多く存在する．これに対して我々は(1)を解決するため，上位のカテゴリ階層を新しく定義し，Wikipediaの上位階層を削除して置き換えた．さらに(2)を解決するため，Wikipediaのカテゴリ間，及びカテゴリ記事間のnot-is-a関係のリンクを3つの手法により自動で判定し切り離すことで，Wikipediaのカテゴリと記事の階層をis-a関係のオントロジーとなるように整形した．本論文ではnot-is-a関係を判定するための3つの手法を適用した．これにより，``人''，``組織''，``施設''，``地名''，``地形''，``具体物''，``創作物''，``動植物''，``イベント''の9種類の意味属性を最上位カテゴリとした，1つに統一されたis-a関係のオントロジーを構築した．
実験の結果，is-a関係の精度は，カテゴリ間で適合率95.3\%， 再現率96.6\%，カテゴリ‐記事間で適合率96.2\%，再現率95.6\%と高精度であった．提案手法により，全カテゴリの84.5\%（約34,000件），全記事の88.6\%（約422,000件）をオントロジー化できた．","We have built a Japanese large-scale general ontology restructured from 
Wikipedia, that represents a {\it is-a","['序論', 'オントロジーとWikipedia', '汎用オントロジー構築手法', '実験条件', '実験結果', '考察', '関連研究', '結論']",,,,,,,,
V19N04-02.tex,,Automated Evaluation of Japanese Compositions based on Features along Japanese Education and Construction of the Individual Evaluation Model,本稿では，文章に対する評点と国語教育上扱われる言語的要素についての特徴量から，個々の評価者の文章評価モデルを学習する手法について述べる．また，学習した文章評価モデルにおける素性毎の配分を明示する手法について述べる．評価モデルの学習にはSVRを用いる．SVRの教師データには，「表層」「語」「文体」「係り受け」「文章のまとまり」「モダリティ」「内容」というカテゴリに分けられる様々な素性を用意する．これらには日本の国語科教育において扱われる作文の良悪基準に関わる素性が多く含まれる．なおかつ，全ての素性が評価対象文章に設定される論題のトピックに依存しない汎用的なものである．本手法により，文章の総合的な自動評価，個々の評価者が着目する言語的要素の明示，さらに評点決定に寄与する各要素の重みの定量化が実現された．,"We propose a method to learn an individual model, which is to evaluate Japanese Compositions via Support Vector Regression, based on features along Japanese education and scores, marked by human in advance. We also propose a method to represent a way of evaluation. Features in training data of SVR are categorized as 7 types according to what each features refer to. The features include some features regarding criterions of Japanese compositions in education. Besides, all the features do not depend on topic of a composition's prompt. Our methods implemented to score an integrated point of a composition automatically, and also to account elements considered by individual evaluator, to quantify weights of the each elements that contributes decision of scores.","['はじめに', '関連研究', '評価基準の共通性に関する調査', '評価モデルの学習', '実験・考察', 'おわりに']",,,,,,,,
V19N04-03.tex,,Detection of New Word Senses by the Outlier \\Detection Method,"本論文では対象単語の用例集合から，その単語の語義が新語義（辞書に未記載の語義）となっている
用例を検出する手法を提案する．
ここでのアプローチの基本は，新語義の用例が用例集合中の外れ値になると考え，
データマイニング分野の外れ値検出の手法を利用することである．
ただし外れ値検出のタスクは教師なしの枠組みになるが，
新語義検出という本タスクの性質を考慮すると，一部のデータ（用例）にラベル（対象単語の語義）
が付与されているという枠組みで考える方が適切である．
そのため本論文では一部のデータにラベルがついているという教師付きの枠組みで外れ値検出を行う．
具体的には 2 つの手法（教師付き LOF と生成モデル）を用い，それら出力の共通部分（積集合）を最終的な出力とする．
この教師付き LOF と生成モデルの積集合を出力する手法を提案手法とする．
実験では SemEval-2 日本語 WSD タスクのデータを用いて，提案手法の有効性を示した．
また WSD のアプローチを単独で利用しただけでは，本タスクの解決が困難であることも示した．","In this paper, we propose a method to detect new word senses of a target word 
from sentences that contain it.
To achieve this, we assume a new word sense sentence as an outlier of a data set
constructed by sentences that contain the target word.
Then using outlier detection methods in the data mining domain, we detect the new word senses.
Generally, outlier detection methods are considered to be unsupervised.
However, our method utilises data sets including some sentences with the labelled target word.
Therefore, our outlier detection method is classified under the supervised framework.
We propose an ensemble method of two methods to detect new word sense sentences:
the supervised LOF (Local Outlier Factor) and the supervised generative model.
The final output is the intersection of outputs of both methods.
We demonstrate the effectiveness of our method using SemEval-2 Japanese WSD task data. 
Moreover we show that  word sense disambiguation systems cannot solve our task by themselves.","['はじめに', '従来の新語義検出手法', '外れ値検出手法', '提案手法', '実験', '考察', 'おわりに']",,,,,,,,
V19N04-04.tex,,Evaluation Framework Design of Spoken Term Detection Study at the NTCIR-9 IR for Spoken Documents Task,,"This paper describes a design of spoken term detection (STD) studies and 
their evaluating framework at the STD sub-task of the NTCIR-9 IR for 
Spoken Documents (SpokenDoc) task. STD is the one of information access 
technologies for spoken documents. The goal of the STD sub-task is to 
rapidly detect presence of a given query term, consisting of word or a 
few word sequences spoken, from the spoken documents included in the 
Corpus of Spontaneous Japanese. To successfully complete the sub-task, 
we considered the design of the sub-task and the evaluation methods, and 
arranged the task schedule. Finally, seven teams participated in the STD 
sub-task and submitted 18 STD results. This paper explains the STD 
sub-task details we conducted, the data used in the sub-task, how to 
make transcriptions by speech recognition for data distribution, the 
evaluation measurement, introduction of the participants' techniques, 
and the evaluation results of the task participants.","['Introduction', 'Outline of STD', 'Related evaluation frameworks', 'STD task design at the NTCIR-9', 'NTCIR-9 SpokenDoc management', 'Conclusions']",,,,,,,,
V19N05-01.tex,,\protect\setlength{\baselineskip,オノマトペとは，擬音語や擬態語の総称である．文章で物事を表現する際に，より印象深く，豊かで臨場感のあるものにするために利用される．このようなオノマトペによる表現は，その言語を\addtext{母語,"Onomatopoeic words are frequently used for expression of rich presence.
These words can be understood easily for native speakers. Therefore
most of onomatopoetic words are not written in a national language
dictionary, or only a part of meaning is described.
On the other hand, it is hard to understand a meaning of onomatopoetic
words for non-native speakers. They can neither feel a meaning of an
onomatopoetic word nor look it up in a dictionary.
In this paper, an estimation method of feeling of an onomatopoeic
word has been proposed. The feeling of the onomatopoeic word is inferred
by using several features, such as morae sequence pattern of a
onomatopoeic word, feeling of each mora, and so on.
From the experimental results, the estimation performance of the
proposed method was 0.345 (F-value). It was approximately
80\% of the estimation performance given by human (F-value was 0.427).
It can be said that the proposed method is useful for supporting
learners of onomatopoeic words.","['はじめに', '関連研究と本研究の位置づけ', 'オノマトペの印象の推定方法', '評価と考察', 'おわりに']",,,,,,,,
V19N05-02.tex,,Particle Error Correction of Japanese Learners \\from Small Error Data,"本稿では，置換，挿入，削除操作を行う識別的系列変換で日本語
学習者作文の助詞誤りを自動訂正する．誤り訂正タスクの場合，難しいのは大
規模な学習者作文コーパスを集めることである．この問題を，識別学習の枠組
み上で2つの方法を用いて解決を図る．一つは日本語としての正しさを測るた
め，少量の学習者作文から獲得したn-gram二値素性と，大規模コーパスから獲
得した言語モデル確率を併用する．もう一つは学習者作文コーパスへの直接的
補強として，自動生成した疑似誤り文を訓練コーパスに追加する．さらに疑似
誤り文をソースドメイン，実際の学習者作文をターゲットドメインとしたドメ
イン適応を行う．実験では，n-gram二値素性と言語モデル確率を併用すること
で再現率の向上ができ，疑似誤り文をドメイン適応することにより安定した精
度向上ができた．","This paper presents grammatical error correction of
Japanese particles written by foreign Japanese learners.  Our method
is based on discriminative sequence conversion, which corrects
particle errors by substitution, insertion, or deletion.  For this
kind of error correction task, it is difficult to collect large
learners' corpora.  We attempt to solve this problem based on a
discriminative learning framework which uses the following two
methods.  First, language model probabilities obtained from large
Japanese corpora are combined with n-gram binary features obtained
from the learners' corpora.  This method is applied in order to
measure the correctness of Japanese sentences.  Second, automatically
generated pseudo-error sentences are added to the learners' corpora in
order to enrich the corpora directly.  Furthermore, we apply domain
adaptation, in which the pseudo-error sentences (the source domain)
are adapted to the real-error sentences (the target domain).
Experimental results show that the recall rate has been improved by
using both the language model probabilities and the n-gram binary
features. Stable improvement has been achieved by using pseudo-error
sentences with the domain adaptation.","['はじめに', '日本語学習者の誤り傾向', '識別的系列変換', '疑似誤り文を用いたペア文の拡張', '誤り訂正実験', '関連研究', 'おわりに']",,,,,,,,
V19N05-03.tex,,"Facemark Recommendation based on Emotion, Communication, and Motion Type Estimation \\in Microblogs",現在，電子メール，チャット，マイクロブログなどのメディアで，顔文字は日常的に使用されている．顔文字は，言語コミュニケーションで表現できない，ユーザの感情やコミュニケーションの意図を表すのに便利であるが，反面，その種類は膨大であり，場面に合った顔文字を選ぶことは難しい．本研究では，ユーザの顔文字選択支援を目的として，ユーザが入力したテキストに現れる感情，コミュニケーション，動作のタイプ推定を行い，顔文字を推薦する方法を提案する．感情，コミュニケーション，動作のタイプは，{\it Twitter,"Many users use facemarks everyday in recent computer mediated communication environments such as e-mail, chatting, and Microblogs. Although facemarks are useful to express the emotion or communication intentions beyond natural language communication, many users feel difficult to choose the right one from lots of candidates according to the situation. We propose a method to recommend facemarks based on the estimation of emotions, communication, or motion types in texts written by users. Emotion, communication, or motion types are defined with {\it Twitter","['はじめに', '関連研究', '顔文字推薦に用いるカテゴリの定義', '顔文字推薦システムの実現', '顔文字推薦システムの評価', 'おわりに']",,,,,,,,
V19N05-04.tex,,Microblog-based Infectious Disease Detection using Document  Classification and Infectious \\Disease Model,"近年，ウェブの情報を用いて，感染症などの疾病状態を監視するシステム
に注目が集まっている．
本研究では，ソーシャルメディアを用いたインフルエンザ・サーベイランスに注目する．  これまでの多くのシステムは，単純な単語の頻度情報をもとに患
  者の状態を調査するというものであった．しかし，この方法では，実際に疾
  患にかかっていない場合の発言を収集してしまう恐れがある．また，そもそ
  も，医療者でない個人の自発的な発言の集計が，必ずしもインフルエンザの
  流行と一致するとは限らない．本研究では，前者の問題に対応するため， 発
  言者が実際にインフルエンザにかかっているもののみを抽出し集計を行う．
  後者の問題に対して，発言と流行の時間的なずれを吸収するための感染症モ
  デルを提案する．実験においては，Twitterの発言を材料にした
  インフルエンザ流行の推定値は，感染症情報センターの患者数と相関係数0.910という高い相関を示し，その有効性を示した．
  本研究により，ソーシャルメディア上の情報をそのまま用いるのではなく，文章分類や疾患モデルと組み合わせて用いることで，さらに精度を向上できることが示された．","With the recent rise in popularity and size of social media, there
  is a growing need for systems that can extract useful information
  from this amount of data. We address an issue of detecting
  influenza epidemics.  Although previous methods rely 
   mainly on the
  frequencies of the influenza related words, such methods had
  suffered from the noisy tweets that do not express influenza symptoms. To
  deal with this problem, this study proposed two methods. First,
  the sentence classifier judges whether a person really catches the
  influenza or not. Next, the infectious model closes a time gap
  between the people web activity and the illness period. In the
  experiments, the combination of two techniques achieved 
  the high performance (correlation coefficient 0.910 to the number of the 
  influenza patients).
  This result suggests that not only natural language processing but
  also disease study contributes to social media based surveillance.","['はじめに', '関連研究', 'インフルエンザ・コーパス', '提案手法', '実験', 'おわりに']",,,,,,,,
V20N01-01.tex,,Structuring Opinions by Relative Characteristics \\ for User-Opinion Aggregation,"本論文では，レビュー集合から多数の評価視点が得られる状
況において，評価対象間の相対的特徴を考慮した重要度（ス
コア）に従って評価視点をランキングする課題について述
べる．また，レビューはその数だけ書き手が存在することか
ら評価視点の異表記が生じやすく，これがランキングに悪影
響を与える．本論文では，評価視点に対してクラスタリング
を適用することで異表記問題へ対応する手法を提案する．
評価実験を通して，提案したスコア関数がランキング性能の
向上に有効であること，およびクラスタリングに基づくラン
キング補正手法によって，平均適合率（MAP指標）が向上する
ことを確認した．","We propose a scoring function for aspect ranking that
facilitates user-opinion aggregation.  The function
uses the log-likelihood ratio to capture relative
characteristics between opinions.  Review documents
contain many variant expressions because they are
written by different web users.  We also propose a
re-ranking method that identifies variant expressions
having the same meaning by using several clustering
techniques.  The experimental results indicate that
the proposed scoring function and the re-ranking
method are more effective than baseline methods.","['はじめに', '関連研究', '提案手法', '異表記問題への対応', '評価実験', 'おわりに']",,,,,,,,
V20N01-02.tex,,Database of Human Evaluations of Machine Translation Systems for Patent Translation,,"This paper discusses a database of human evaluations of patent machine translation, from Chinese to English, Japanese to English, and English to Japanese. 
The evaluations were conducted for the NTCIR-9 Patent Machine Translation Task (PatentMT). 
Different types of systems, such as research systems and commercial systems, and rule-based systems and statistical machine translation systems were evaluated. 
Since human evaluation results are important when investigating automatic evaluation of translation quality, the database of the evaluation results is valuable. 
From the NTCIR project, resources including the human evaluation database, translation results, and test/reference data are available for research purposes.","['Introduction', 'Task design', 'Participants and submissions', 'Human evaluation results', 'Validation of Human Evaluation Results', 'Meta-Evaluation of the Automatic Evaluation Measure of BLEU', 'Method for Obtaining the Database', 'Conclusion', 'Adequacy Criterion', 'Acceptability Criterion', 'Instructions for the Human Evaluation Procedure']",,,,,,,,
V20N02-01.tex,,Interactive Method for Generation of Mediatory Summary to Verify Credibility of Web Information,"我々は，Web上の情報信憑性判断を支援するための技術として，調停要約の自動生成に関する研究を行っている．
調停要約とは，一見すると互いに対立しているようにみえる二つの言明の組が実際にはある条件や状況の下で両立できる場合に，両立可能となる状況を簡潔に説明している文章をWeb文書から見つける要約である．
しかしながら，対立しているようにみえる言明の組は一般に複数存在するため，利用者がどの言明の組を調停要約の対象としているのかを明らかにする必要がある．
本論文では，利用者が調停要約の対象となる言明の組を対話的に明確化した状況下で調停要約を生成できるように改善した手法を提案する．
また，提案手法は，従来の調停要約生成手法に，逆接，限定，結論などの手掛かり表現が含まれる位置と，調停要約に不要な文の数を考慮することで精度の向上を図る．
調停要約コーパスを用いた実験の結果，従来手法と比較して，調停要約として出力されたパッセージの上位10件の適合率が0.050から0.231に向上したことを確認した．","We have been studying automatic generation of a mediatory summary for facilitating users in assessing the credibility of information on the Web.
A mediatory summary is a brief description extracted from relevant Web documents in situations where a pair of statements that appear to contradict each other at first glance can actually coexist under a certain situation.
In general, because there are several such pairs, users should clarify the pair whose credibility they are assessing.
In this paper, we propose an interactive method for generating a mediatory summary in which users specify the pair of statements they are interested in assessing.
Furthermore, we attempt to improve the method in terms of both precision and recall by introducing the position of key expressions such as adversative conjunctions, conditional expressions, and conclusive conjunctions and the number of sentences that are not useful for the mediatory summary. 
Results of the analysis performed using the mediatory summary corpus indicate that the proposed method achieved a precision of 0.231 for the generated summaries ranked in the top 10, while the previous method (Shibuki et al. 2011a) achieved a precision of 0.050.","['はじめに', '関連研究', '調停要約', '提案手法', '調停要約コーパス', '実験', 'おわりに']",,,,,,,,
V20N02-02.tex,,Proposal of a Method to Convert Difficult Words in Newspaper Articles to Plain Expressions,ロボットと人間の双方でより円滑なコミュニケーションを行うためには，ロボットにも人間のような会話能力が求められると考える．人間の会話はあいさつや質問応答，提案，雑談など多岐に渡るが，ロボットがこういった会話，例えば何かしらの情報を持った雑談のように能動的な会話を行うには，新聞記事のようなリソース中の表現を会話テンプレートに埋め込むという方法が考えられる．しかし新聞記事中の語と会話に用いられる語の馴染み深さには違いがある．例えば新聞記事中の「貸与する」という語は，会話に用いる場合には「貸す」という表現の方が自然である．つまり，人間にとって違和感のない会話のためのリソースとして新聞記事を用いるには，難解語を平易な表現へ変換する必要があると考える．そこで本稿ではロボットと人間との自然な会話生成を担う技術の一端として，新聞記事中の難解な語を会話表現に見あった平易な表現へと変換する手法を提案する．提案手法では人間が語の変換を行う際の処理になぞらえ，1つの語を別の1語で変換する1語変換および文章で変換する$N$語変換を組み合わせることでより人間にとって自然に感じる変換を行い，その有効性を示した．結果として変換すべき難解語を75.7\%の精度で平易な表現に，81.1\%の精度で正しい意味を保持した表現に変換することが出来た．,"To smoothen the communication between robots and humans, robots must have human-like conversational abilities. Humans converse in various ways: greetings, question-answers, suggestions, and chatting, among others. Thus, methods that extract expressions from resources such as newspaper articles and embed them into the conversation template are viewed as a way to let individuals participate in active conversations, such as chat with some information by robot. However, there is a difference in the degree of difficulty of the words used in newspaper articles and the ones used in a conversation. Words in newspaper articles are generally more difficult than those used in a conversation. Hence, it is important to convert difficult words in newspaper articles to plain expressions when using newspaper articles as a resource for a robot's conversation, in order to avoid human discomfort. This paper suggests a method of converting difficult words to plain expressions, considering the differences in the degree of difficulty of words used in newspapers and in a conversation. The proposed method aims to convert feels natural for human by combining two approaches: a method that converts one word to another and a method that converts one word to a sentence. The results show that the proposed method converts words in newspaper articles from difficult to plain expressions with an accuracy of 75.7\% and converts words while retaining their meaning with an accuracy of 81.1\%. Therefore, the proposed word conversion method is found to be effective.","['はじめに', '関連研究と本研究の特徴', '難解語の変換手法の概要', '語概念連想', '語の変換処理の流れ', '難解語の判別', '1語変換', '$N$語変換', '提案手法の評価と考察', 'おわりに']",,,,,,,,
V20N02-03.tex,,Extracting Translation Pairs from Comparable Corpora through Graph-based Label Propagation,,"This paper proposes a novel method for bilingual lexicon extraction from comparable corpora using graph-based label propagation. 
A previous study found that performance drastically decreases when the coverage of a seed lexicon is small. 
We address this problem by using indirect relations with bilingual seeds together with direct relations, in which each word is represented by a distribution of lexical seeds. 
The seed distributions are propagated over a graph that represents relations among words. Translation pairs are extracted by identifying word pairs with high similarities in the seed distributions. 
We propose two types of graphs: (1) a co-occurrence graph, representing co-occurrence relations between words; and (2) a similarity graph, representing context similarities between words. 
Evaluations on comparable corpora of English and Japanese patent documents show that our proposed graph propagation method outperforms conventional methods. 
Further, the similarity graph improved performance by clustering synonyms into the same translation.","['Introduction', 'Context-Similarity-Based Extraction', 'Label Propagation Based Extraction', 'Experiment', 'Discussion', 'Related Studies', 'Conclusion']",,,,,,,,
V20N02-04.tex,,Negation Naive Bayes for Text Classification,"本論文は，文書分類のための新手法として，Negation Naive Bayes (NNB)を提案する．
  NNBは，クラスの補集合を用いるという点ではComplement Naive Bayes (CNB) と等しいが，
  Naive Bayes (NB) と同じ事後確率最大化の式から導出されるため， 事前確率を数学的に正しく考慮している点で異なっている．
  NNBの有効性を示すため，オークションの商品分類の実験とニュースグループの文書分類の実験を行った．
  ニュースグループの文書分類では，一文書あたりの単語数（トークン数）を減らした実験と，クラスごとの文書数を不均一にした実験を行い，NNBの性質を考察した．
  NB，CNB，サポートベクターマシン (SVM) と比較したところ，特に一文書当たりの単語数が減り，クラスごとの文書数が偏る場合において，NNBが他のBayesianアプローチより勝る手法であること，
また，時にはSVMを有意に上回り，比較手法中で最も良い分類正解率を示す手法であることが分かった．","In this study, we proposed negation naive Bayes (NNB), a new method for text classification. 
Similar to complement naive Bayes (CNB), NNB uses the complement class. However, unlike CNB, NNB  properly considers the prior in a mathematical way  
because NNB is derivable from the same equation (the maximum a posteriori equation) from which naive Bayes (NB) is derived. 
We carried out classification experiments on products offered on an internet auction site and on the 20 Newsgroups data set. 
For the latter, we carried out experiments in the following two settings and discussed the properties of NNB: 
(1) settings in which the number of words in each document decreases and (2) settings in which the distribution of documents over classes is skewed. 
We compared NNB with NB, CNB, and support vector machine (SVM). Our experiments showed that NNB outperforms other Bayesian approaches 
when the number of words in each document decreases and when texts are distributed non-uniformly over classes. 
Our experiments also showed that NNB sometimes provides the best accuracy and significantly outperforms SVM.","['はじめに', '関連研究', 'Negation Naive Bayesの導出', '実験', '結果', '考察', 'まとめ']",,,,,,,,
V20N02-05.tex,,Use of Sound Symbolism in Sentiment Classification,,"In linguistics, sound symbolism is an idea that the vocal sounds of certain words carry meaning in themselves.
This paper focuses on the sound symbolism of onomatopoeic words and demonstrates the close relationship between sound symbolism and sentiment polarity.
Because onomatopoeic words imitate the sounds they represent, they can help us better understand the sentiment of a sentence when utilizing sound symbolism.
Therefore, we modeled sound symbolism with N-gram-based features and applied the model to a series of sentiment classification tasks.
The experimental results show that this method with sound symbolism significantly outperformed the baseline method without sound symbolism,
which effectively demonstrates that a close relationship exists between sound symbolism and sentiment polarity.","['Introduction', 'Background', 'Proposed Method', 'Experiments', 'Conclusion and Future Work']",,,,,,,,
V20N02-06.tex,,Temporal Information Annotation on `Balanced Corpus of Contemporary Written Japanese',"時間情報表現は，テキスト中に記述される事象の生起時刻を推定するための重要な手がかりである．
時間情報表現を含む数値表現の抽出は，固有表現抽出の部分問題として解かれてきた．
英語においては，評価型国際会議が開かれ，
時間情報表現のテキストからの切り出しだけではなく，曖昧性解消・正規化のための様々な手法
が提案されている．さらに，時間情報と事象とを関連づけるアノテーション（タグづけ）基準 TimeML の定義や
新聞記事にアノテーションを行ったコーパス TimeBank の整備が進んでいる．
一方，日本語においては時間情報処理に必要なアノテーション基準の定義及びコーパスの整備が進んでいない．
本稿では，TimeML の時間情報表現を表す \timexiii タグに基づいた
時間情報のアノテーション基準を日本語向けに再定義し，
『現代日本語書き言葉均衡コーパス』(BCCWJ)コアデータの一部にアノテーションを行った．
問題点を検討し，今後事象の生起時刻を推定するために必要な課題を考察する．","Temporal information is important for grounding event expressions  
on a timeline.
Temporal expression extraction has been performed as numerical representation extraction, which is a subtask of named entity extraction.
For English texts, evaluation workshops were held in which temporal expressions
were extracted and normalized.
An annotation schema, {\it TimeML","['はじめに', '背景 \\label{sec:previous_work', '対象とする時間情報表現', 'TimeML \\timexiii タグに基づいた日本語時間情報アノテーション基準', '\\timexiii 日本語適応上の問題点', '\\modified{作業環境と作業対象', 'タグの分析', 'おわりに']",,,,,,,,
V20N02-07.tex,,Enrichment of a Dictionary with Images from the Internet,"既存のテキストのみからなる辞書に対し，インターネット上にある膨大な画像を
関連付けることができれば，文字列情報からだけでは得られない，視覚的な情報
を利用できるようになり，用途が広がると期待できる．
そのため，本稿では，辞書の出来る限り広い語義に対して画像を付与すること
を考える．
作成・維持コストを考えれば，なるべく自動的に画像を付与することが望ましい
が，大量の辞書エントリに対して，高い精度で画像を付与することは容易ではな
い．
また，そもそもどういった語義には画像を付与できるのか，あるい
はできないのかといった調査が大規模になされた例はなく，画像が付与できる語
義を自動的に判別することも困難である．
そこで本稿では，まず語義別に画像が付与された辞書を人手で構築す
ることを第一の目標とする．その上で，画像が付与できる語義とできない語義に
ついて，品詞や意味クラスとの関連性に着目して分析する．
具体的には，名詞，動詞，形容詞，形容動詞，副詞を含む25,481 語，39,251 語義
を対象に画像付与実験と分析を行ない，その結果，全語義の94.0\%は画像付与
が可能であること，品詞や意味クラスに応じて画像付与の可否が変わることを示
す．
また，幅広い語義に適切な画像を付与するため，インターネットから画像検
索によって画像を獲得する．検索時に重要となるのが検索語である．
本稿の第二の目標は，語義毎に適切な画像を得るための検索語を調査することで
ある．本稿では，複数の検索語の組合せ（以下，検索語セット）の中から最も適切
な画像を得られる検索語セットを作業者に選択してもらい，適切な検索語セット
がない場合には修正してもらう．こうして最終的に利用された検索語セットを分
析し，提案手法の改良点を探る．
さらに，検索語セットの優先順位の決定方法も提案，
その妥当性を示すことを本稿の第三の目標とする．
新しい辞書への適用等を考えると，人手による画像付与が
できない場合でも，優先順位の高い検索語セットによる検索結果が利用できれば，
有用だと考えられるからである．
提案手法では，対象語義がメジャーな語義かどうかで優先順位を変化させる．
実験では，2種類の評価方法を通してその妥当性を示す．","The Internet is an immense resource storehouse for images. 
Establishing a connection
between images and dictionary definitions would enable the creation of
rich dictionary resources with multimedia information. Therefore, this
study aims at providing several suitable images for dictionary
definitions. In this study, we targeted 25,481 words, including nouns,
verbs, adjectives, and adverbs, split into 39,251 senses by querying an
image search engine. 
The results showed that 94\% of word senses could be defined as
suitable images.
Then, we analyzed the relationship
between the vis\-ualization of each word sense and parts of speech or
semantic classes. To obtain images for each word sense, we expanded the
query by appending queries extracted from definitions for each word
sense. Second, we analyzed both manually-selected and fixed queries and
examined query expansion methods more deeply. 
This paper proposes a
method to set queries in priority order depending on the primary word
sense. 
Third, we show the suitability of our method through two types of
evaluations because in the application of new dictionaries or new target
senses, it is valuable to obtain images automatically using
high-priority queries.","['はじめに', '言語資源', '検索語の拡張', '実験方法', '優先順位の妥当性の評価', '検索語に関する分析', '画像表示可能／不可能な語義の分析', 'まとめと今後の課題']",,,,,,,,
V20N02-08.tex,,Jointly Extracting Japanese Predicate-Argument Relation \\with Markov Logic,"本稿ではマルコフロジックを利用した日本語述語項構造解析について述べる．
日本語述語項構造解析に関する従来研究の多くは，格毎に独立した解析器を用意し，
他の述語項関係との依存関係を無視したまま解析を行っていた．
これに対し，本研究では同一文内にある全ての述語項候補を同時に考慮して解析する手法を提案する．
この手法は複数の述語項関係の間にある依存関係を考慮した上で，
文内における全ての述語項関係の候補から，最適な状態を見つけ出すことができる．
さらに，本研究では，述語の項として妥当でないものを削除するための
新たな論理的制約を考案し，ゼロ照応も含めて正しい項を効果的に見つけ出すことができるように工夫した．
NAISTテキストコーパスにおける実験で，本研究の提案手法は，大規模データを利用せずに，
従来手法と同等の結果を達成した．","This paper describes a new Markov Logic approach for Japanese Predicate-Argument (PA) relation extraction. 
Most previous work built separated classifiers corresponding to each case role and independently identified the PA relations, neglecting dependencies (constraints) between two or more PA relations. 
We propose a method which collectively extracts PA relations by optimizing all argument candidates in a sentence. 
Our method can jointly consider dependency between multiple PA relations and find the most probable combination of predicates and their arguments in a sentence. 
In addition, our model involves new constraints to avoid considering inappropriate candidates for arguments and identify correct PA relations effectively. 
Compared to the state-of-the-art, our method achieves competitive results without large-scale data.","['はじめに', '関連研究', 'Markov Logic', '提案手法', '実験と結果', 'おわりに']",,,,,,,,
V20N02-09.tex,,Set Expansion Using Sibling Relationships Between Semantic Categories,"集合拡張手法の多くはシードインスタンスだけを手掛かりに新たなインスタンスを取得するものであり，対象が複数のカテゴリであっても，各カテゴリのインスタンスの収集を独立に行う．
しかし，複数カテゴリを対象にした集合拡張ではカテゴリ間の関係など，シードインスタンスとは別の事前知識も利用できる．
本研究ではこのようなカテゴリ間の関係，特に兄弟関係を事前知識として活用した集合拡張手法を提案する．
さらに，Wikipediaから半自動で抽出したインスタンスと兄弟関係を事前知識として実験を行い，兄弟関係が集合拡張に有用であることを示す．","Most set expansion algorithms are assumed to independently acquire new instances of each of the different semantic categories even when instances of multiple semantic categories are seeded.
However, in the setting of set expansion with multiple semantic categories, we might leverage other types of prior knowledge about semantic categories.
In this paper, we present a method of set expansion in case ontological information related to target semantic categories is available.
Specifically, the proposed method uses sibling relationships between semantic categories as an additional type of prior knowledge.
We demonstrate the effectiveness of using sibling relationships in set expansion on a dataset in which instances and sibling relationships are extracted from Wikipedia in a semi-automatic manner.","['はじめに', '関連研究', '提案手法', '実験', 'まとめ']",,,,,,,,
V20N03-01.tex,,Returning-Home Analysis in Tokyo Metropolitan Area \\ at the Time of the Great East Japan Earthquake \\ using Twitter Data,"本論文では東日本大震災発生時に首都圏で引き起こされた帰宅困難者問題の発生要因や通
勤者の帰宅意思決定行動に対して，Twitterにおける各ユーザーの発言内容をもとにその要
因を明らかにする．まず，発言データから行動データを抽出することを目的として，
Twitterの発言内容から，各ユーザーの帰宅行動をサポートベクターマシンを用いて識別す
る．次に，ジオタグデータを用いて職場・自宅間距離等を作成するとともに，
ツイートデータを用いて外的要因や心理的説明要因を作成する．
当日の帰宅意思決定行動をこれらの要因を用いて離散選択モデルによりモデル化する．
このモデル化によるシナリオシミュレーションを行った結果，
避難所施設・一時滞在場所の有無が待機・宿泊行動を促進すること，
地震発生後の家族間の安否確認の可否が徒歩帰宅行動に影響を与える
可能性が示された．以上より，今後の災害時における帰宅困難者問題への対策を考察する．","This paper clarifies the occurrence factors of commuters unable to return home and the returning-home decision-making at the time of the Great East Japan Earthquake by using Twitter data. First, to extract the behavior data from the tweet data, we identify each user's returning-home behavior using support vector machines. Second, we create non-verbal explanatory factors using geo-tag data and verbal explanatory factors using tweet data. The non-verbal explanatory factors include distance between home and office, time taken in travelling by walking or public transport, etc. On the other hand, the verbal explanatory factors include external and psychological factors. Then, we model users' returning-home decision-making by using a discrete choice model and clarify the factors quantitatively. Finally, by sensitivity analysis, we show the effects of the existence of emergency evacuation facilities and line of communication.","['はじめに', 'Tweet Data $\\rightarrow$ Behavioral Data', '帰宅行動要因の分析', '行動データから意思決定モデルの構築', '推定結果とシミュレーション', 'おわりに']",,,,,,,,
V20N03-02.tex,,"Effect of Singular Value Decomposition and Weighting by Singular Value of Document-Term Matrix, for Large-scale Data Perspective and Targeted Data Extraction",東日本大震災ビッグデータワークショップにおいて提供された，震災当日を含めた1週間分のツイートのうち，震災対応の初動期間にあたる震災後72時間を含む4日分のツイッターを解析した．ツイートのクラスタリングによって得られる全体の俯瞰を行ってから目的に応じた分類項目を設定し，その項目に即したツイートを抜き出す抽出器を作成した．一連の作業をよく行うためには，分類項目を設定するために用いられるクラスタリングの性能向上が重要な要素となっている．本研究では，古典的な類義語処理手法である特異値分解をクラスタリングに適用する際に，良く知られている次元圧縮に留まらず，特異値の大きさを特徴量の重みづけの大きさとして活用する手法を提案する．また，クラスタリング結果を人手で修正する作業の容易度を測るための新たな指標を提案し，人手による実作業の効率と比較する実験を行った．その結果，クラスタリングについては，主に作業効率の観点から，特異値による重みづけの有効性と提案する作業指標の妥当性が確認された．分類問題であるターゲットデータ抽出については，学習過程にそもそも重みづけの機構が備わっているにもかかわらず，検出率の向上に若干の効果が見られた．,"We analyzed tweets broadcasted until four days after the occurrence of the 
Great East Japan Earthquake, which are provided by the Project 311. After 
obtaining a general view from tweets clustering, we created a set of 
targeted extraction categories from them and constructed a tweet extractor 
tailored to the target. In a sequence of such processes, improvement of the 
clustering, which is used to discover the target category for extraction, 
becomes very important. A method is proposed that utilizes the Singular 
Value as weights for features, while the well-known conventional use of 
Singular Value Decomposition is limited to reducing its dimension. In 
addition, we proposed an evaluation criterion for a human-aided clustering 
task, and conducted experiments to compare these criteria, including 
commonly-used ones, with the actual time spent by humans for performing such 
a task. The experiments show the effectiveness of the proposed weighting 
method and the competency of our criterion, mainly from the perspective of 
time efficiency of the task. As for the targeted data-extraction task, which 
is also a classification problem, some improvement in accuracy is observed 
although the training process itself involves a weighting mechanism.","['はじめに', '関連研究', '情報抽出器作成までの手順', '文書‐単語行列の効果的な変換', '情報抽出器作成の過程で得られた知見と残された課題', 'おわりに']",,,,,,,,
V20N03-03.tex,,A Disaster Information Analysis System\\ Based on Question Answering,"本論文では，地震や津波などの災害時に個人からソーシャルメディア上に発信さ
れる大量の書き込みから，救援者や被災者が欲している
情報を自動的に取得する情報分析システムについて報告する．このシステムでは，
質問応答技術により，災害時の被災地の状況や救援状況を俯瞰的に把握し，被災
地からの想定外も含めた情報を取得することを目的としている．システムで利用
している質問応答処理では構文パターンの含意に基づき質問文を拡張し，ソーシャルメディアへの書き込みに対して地名・場所名を補完することにより，
幅広い質問に対応する．さらに，本システムを拡張することにより，被災地からの重要な情報
提供が必ずしも救援者へ届かない問題に対応できることについて述べる．NPOや
自治体などの救援者が状況把握のための質問を予め登録しておけば，
救援を望む被災者がTwitterやBBS等へ書き込んだ時点で，情報を求める
側と提供する側の双方に自動的に通知できる．これにより救援者と被
災者の双方向のコミュニケーションが担保され，救援活動がより効率的になると
期待される．本システムの質問応答性能を我々が用意した300問のテストセット
のうち回答が対象データに含まれる192問を用いて評価したところ1質問あたり平
均605.8個の回答が得られ，再現率は0.519，適合率は0.608であった．","In this paper we introduce an information analysis system that
automatically acquires from social media like Twitter the kind of
vital information that rescue workers or disaster victims need
in case of large-scale disasters like earthquakes or tsunami. This
system uses
question-answering (QA) technology with the aim of helping users get a
comprehensive overview of the state of affairs and the various conditions of
ongoing rescue efforts in the afflicted areas, and detect potentially
unanticipated information from the disaster areas. The system expands
the input question with various entailment expressions, and
augments the original social media input data by analyzing the mentioned
place names
in order to handle a wide variety of questions relevant to disaster
scenarios. Moreover, we extend this system to address rescue workers'
crucial problem of getting access to relevant information from the
disaster areas in times of crisis. To tackle this problem we introduce
a mechanism by which rescue workers from NPOs or municipalities can
register certain questions for situation assessment in advance, so
that when a disaster victim posts some urgent requests for food, 
medicines or other essentials on Twitter or some other BBS, both
information sender and information requester are automatically
notified of this. We expect that such a mechanism can safeguard the
two-way communication between rescue workers and disaster victims, and
ultimately lead to a more effective rescue effort. We evaluate the
system on a test set of 300 questions and their answers. For
192 questions whose answers are actually included in our system's index, 
we obtained on average 605.8 answers per question, with 51.9\% recall
and 60.8\% precision.","['はじめに', '質問応答に基づく対災害情報分析システム', 'システムの評価実験', 'さらに行き届いた被災情報の活用を目指して', '関連研究', 'おわりに']",,,,,,,,
V20N03-04.tex,,Rescue Activity for the Great East Japan Earthquake Based on a Website that Extracts Rescue Requests from the Net,"東日本大震災初期，Twitter に寄せられた膨大なツィートには，
  緊急性の高い救助要請候補が多数含まれていたものの，
  他の震災関連ツィートや「善意のリツィート」によって，
  通報されるべき情報が埋もれてしまった．
  この様な状況を解消するために，
  筆者らは2011年3月16日，Twitter 上の救助要請情報をテキストフィルタリングで抽出し，
  類似文を一つにまとめ一覧表示するWebサイトを開発・公開した．
  本論文では，本サイト技術のみならず，通報支援活動プロジェクト {\tt \#99japan","In the early stages of the Great East Japan Earthquake, 
a vast number of tweets were related to high-urgency rescue requests; 
however, most of these tweets were buried under many other tweets, 
including some well-intentioned retweets of the rescue requests. 
To better handle such a situation, 
the authors have developed and published a website that
automatically lists similar statements to extract rescue requests 
from Twitter on March 16, 2011. 
This paper describes not only the technology of the system but also the start of a rescue project {\tt \#99japan","['はじめに', 'Twitter', '救助要請情報', '救助活動', '考察', 'おわりに']",,,,,,,,
V20N03-05.tex,,Analyzing the Statement Structure on Twitter using Replies to Tweets and Quoted Tweets,"東日本大震災では安否確認や被災者支援のためにTwitterが活躍したが，一方
で多種多様な情報が流通し，混乱を招いた．我々は，情報の信憑性や重要性を
評価するには，ツイート空間の論述的な構造を解析・可視化し，情報の「裏」を
取ることが大切だと考えている．本稿では，ツイートの返信および非公式
\addspan{リツイート","Although Twitter played an important role in supporting victims of the 2011 Tohoku earthquake and tsunami disaster, 
we encountered a number of situations in which the vast flow of unauthorized information was problematics. 
To assess the credibility and importance of a piece of information, 
we find that it is important to analyze the statement structure on Twitter and to understand the background of information. 
In this study, we propose a method for analyzing the statement relation between a tweet and its reply or quoted tweet. 
More specifically, we assume that a reply or quoted tweet expresses a statement relation (e.g., {\it agreement","['はじめに', '関連研究', '返信・非公式リツイートの態度分類', '返信・非公式リツイートの態度分類の評価', '一般的なツイート間関係認識への拡張', '\\addspan{一般的なツイート間の関係認識結果', 'おわりに']",,,,,,,,
V20N03-06.tex,,Extracting False Information on Twitter \\ and Analyzing its Diffusion Processes \\ by using Linguistic Patterns for Correction,"東日本大震災では，「コスモ石油の爆発で有害物質の雨が降る」などの誤情報
 の拡散が問題となった．本研究の目的は，東本日大震災後1週間の全ツイート
 から誤情報を網羅的に抽出し，誤情報の拡散と訂正の過程を分析することであ
 る．本稿では，誤情報を訂正する表現（以下，訂正パターン）に着目し，誤情
 報を認識する手法を提案する．具体的には，訂正パターンを人手で整備し，訂
 正パターンにマッチするツイートを抽出する．次に，収集したツイートを内容
 の類似性に基づいてクラスタリングし，最後に，その中から誤情報を過不足な
 く説明する1文を選択する．実験では，誤情報を人手でまとめたウェブサイトを
 正解データとして，評価を行った．また，誤情報とその訂正情報の拡散状況を，
 時系列で可視化するシステムを構築した．本システムにより，誤情報の出現・
 普及，訂正情報の出現・普及の過程を分析できる．","During the 2011 East Japan Earthquake and Tsunami Disaster, a
considerable amount of false information was disseminated on Twitter;
for example, after the Cosmo Oil fire, it was rumored that harmful
substances will come down with rain. This paper exhaustively extracts
pieces of false information from tweets within one week after the
earthquake, and analyzes the diffusion of false information and its
correction information.  By designing a set of linguistic patterns that
correct false information, this paper proposes a method for detecting
false information.  Specifically, the method extracts text passages that
match the correction patterns, clusters the passages into topics of
false information, and selects, for each topic, a passage explaining the
false information most suitably.  We report the performance of the
proposed method on the data set extracted manually from websites that
specialize in collecting false information.  In addition, we build a
system that visualizes emergences, diffusions, and terminations of a
piece of false information and its correction.  We also propose a method
for discriminating false information from its correction, and discuss
the possibility of alerting against false information.","['はじめに', '関連研究', '提案手法', '実験', '誤情報の拡散状況の分析', 'おわりに']",,,,,,,,
V20N03-07.tex,,Effects of Rumors on Microblogs,"マイクロブログの普及により，ユーザは様々な情報を瞬時に取得することができるようになった．
一方，マイクロブログでは流言も拡散されやすい．
流言は適切な情報共有を阻害し，場合によっては深刻な問題を引き起こす恐れがある．
これまで，マイクロブログ上の流言拡散に関する分析は多かったが，ある流言がどのような影響を引き起こすかについての考察はない．
本論文では，東日本大震災直後のTwitterを材料とし，どのような流言が深刻な影響を与えるかを，
有害性と有用性という観点からの主観評価および修辞ユニット分析により分析した．
その結果，震災時の流言テキストの多くは行動を促す内容や，状況の報告，予測であること，
また，情報受信者の行動に影響を与えうる表現を含む情報は，震災時に高い有用性と有害性を持つ可能性があることを明らかにした．","Microblogs have enabled us to exchange information in real time, which has led to the spread of not only beneficial but also potentially harmful information, such as rumors.
Rumors may block the process of adequate information sharing, which may, in turn, cause serious problems.
Several studies have already analyzed the impact of rumors on microblogging media; however, the way in which these rumors can cause potential problems largely remains unclear.
This paper analyzes people’s perceptions of rumors on Twitter during disasters using subjective evaluation and rhetorical unit analysis.
The results showed that many subjects perceived rumors as containing information that instigates people to take action, reports on a current situation, or predicts future events.
Moreover, information that instigates people to take action has been perceived as beneficial in some contexts, while it is also seen as harmful in other cases.","['はじめに', '関連研究', '分析の概要', '分析結果と考察', '将来への展望', 'おわりに', '各評価値における修辞機能と脱文脈化指数の割合', '1つの流言に対するテキストを限定した場合の分析結果']",,,,,,,,
V20N03-08.tex,,Health Crisis Management and Natural Language Processing,"災害は被災地住民の健康に多大な影響を及ぼし，その対応に際し保健医療
分野に膨大な文書を生じる．そこで，災害時の保健医療活動を支援するため，
自然言語処理による各種文書の効率的処理が期待されている．本稿では，保健
医療の観点から，そうした情報の特性を被災者，被災者集団，支援者のそれぞ
れについて整理したうえで，自然言語処理が有効と考えられる諸課題を列挙す
る．そのうえで，2011年に発生した東日本大震災において筆者らが関わった日
本栄養士会支援活動報告，石巻圏合同救護チーム災害カルテ，医療や公衆衛生
系メーリングリスト情報の3つの事例を紹介し，「健康危機管理」に自然言語
処理が果たしうる貢献について検討する．これらの事例に示されるように，災
害時には保健医療に関わる膨大なテキストが発生するものの，保健医療分野の専門
家は大量の自由記載文を効率的に処理する手段を有していない．今後，東日本
大震災において生じたデータを活用し，保健医療情報における大量の自由記載
文を効率的に処理する備えを行っておくことが望ましい．","Disasters may cause a variety of health problems in the victim
population, and public health authorities are forced to assess such
situations rapidly in order to take appropriate countermeasures. This
process may involve the processing of numerous unstructured texts, and
hence, natural language processing (NLP) has significant application
potential in the field of crisis response. This report classifies the
information related to public health in a crisis situation into three
categories---victims, victim groups, and care providers---and
summarizes the characteristics of these categories to clarify the
tasks suitable for NLP. This analysis is followed by three case
studies of the Great East Japan Earthquake response. These case
studies illustrate the contribution of NLP in an actual health crisis
and suggest that the authorities do not possess appropriate means to
process the texts that may accumulate in such a situation. The archive
of the earthquake would be the best source for the analysis to prepare
for future disasters.","['はじめに', '健康危機管理における情報とその特徴', '東日本大震災における健康危機と自然言語処理', 'おわりに']",,,,,,,,
V20N04-01.tex,,Recognizing Semantically Equivalent Predicate Phrases Based on Several Linguistic Clues,大量のテキストから有益な情報を抽出するテキストマイニング技術では，ユーザの苦情や要望を表す述部表現の多様性が大きな問題となる．本稿では，同じ出来事を表している述部表現をまとめ上げるため，「メモリを消費している」と「メモリを食っている」の「消費している」と「食っている」のような述部表現を対象に，異なる2つの述部が同義か否かを認識する同義判定を行う．述部の言語構造分析をもとに，「辞書定義文」，「用言属性」，「分布類似度」，「機能表現」という複数の言語知識を用い，それらを素性とした識別学習で同義判定を行った．実験の結果，既存手法に比べ，高い精度で述部の同義性を判定することが可能になった．,"This paper proposes the recognition of semantically equivalent predicate 
phrases such as ``consumes'' and ``eats'' in ``it consumes/eats a lot of 
memory.'' Differences in predicate expressions pose a serious problem in 
natural language processing applications such as text mining, which extracts 
text data according to a user's needs and wants. We propose a novel 
technique that uses various linguistic clues ranging from abstract semantic 
features to contextual features in order to detect a semantic similarity in 
different predicates. The results indicate that our proposed method achieved 
the highest f-score compared with baseline methods.","['はじめに', '関連研究', '述部の言語的特徴', '提案手法：複数の言語的特徴を用いた同義判定', '同義述部コーパスの作成', '実験', '考察', '結論']",,,,,,,,
V20N04-02.tex,,How to Translate Dialects: \\ A Segmentation-Centric Pivot Translation Approach,,Recent research on multilingual statistical machine translation (SMT) focuses on the usage of {\em pivot languages,"['Introduction', 'Dialect Translation', 'Experiments', 'Conclusion']",,,,,,,,
V20N04-03.tex,,Multi-Document Summarization Model Based on Redundancy-Constrained Knapsack Problem,"本論文では，複数文書要約を冗長性制約付きナップサック問題として捉える．
この問題に基づく要約モデルは，ナップサック問題に基づく要約モデルに対し，冗長性を削減するための制約を加えることで得られる．
この問題は NP 困難であり，計算量が大きいことから，高速に求解するための近似解法として，ラグランジュヒューリスティックに基づくデコーディングアルゴリズムを提案する．
ROUGE に基づく評価によれば，我々の提案する要約モデルは，モデルの最適解において，最大被覆問題に基づく要約モデルを上回る性能を持つ．
要約の速度に関しても評価を行い，我々の提案するデコーディングアルゴリズムは最大被覆問題に基づく要約モデルの最適解と同水準の近似解を，整数計画ソルバーと比べ100倍以上高速に発見できることがわかった．",\vspace*{-5pt,"['はじめに', '関連研究', '最大被覆モデルとナップサックモデル', '冗長性制約付きナップサックモデル', 'デコーディング', '実験', 'まとめ', '評価情報の抽出方法']",,,,,,,,
V20N05-01.tex,,ILP-based Inference for Cost-based Abduction on First-order Predicate Logic,,"Abduction is desirable for many natural language processing (NLP)
  tasks. While recent advances in large-scale knowledge acquisition
  warrant applying abduction with large knowledge bases to real-life
  NLP problems, as of yet, no existing approach to abduction has
  achieved the efficiency necessary to
  be a practical solution for large-scale reasoning on real-life
  problems. In this paper, we propose an efficient
  solution for large-scale abduction. The contributions of our study
  are as follows: (i) we propose an efficient method of cost-based
  abduction in first-order predicate logic that avoids computationally
  expensive grounding procedures; (ii) we formulate the
  best-explanation search problem as an integer linear programming
  optimization problem, making our approach extensible;
	(iii) we show how cutting plane inference, which is an iterative optimization
	strategy developed in operations research, can be applied to make abduction in
	first-order logic tractable; and (iv) 
  the abductive inference engine presented in this paper is made
  publicly available.","['Introduction', 'Background', 'ILP-based inference for cost-based abduction', 'Runtime Evaluation', 'Related work', 'Conclusion']",,,,,,,,
V20N05-02.tex,,Temporal Ordering Annotation on `the Balanced Corpus of Contemporary Written Japanese',"時間情報抽出は大きく分けて時間情報表現抽出，時間情報正規化，時間的順序関係解析の三つのタスクに分類される．
一つ目の時間情報表現抽出は，固有表現・数値表現抽出の部分問題として解かれてきた．
二つ目の時間情報正規化は書き換え系により解かれることが多い．
三つ目のタスクである時間的順序関係解析は，事象の時間軸上への対応付けと言い換える
ことができる．
\modified{日本語においては時間的順序関係解析のための言語資源が整備されているとは言い難く，
アノテーション基準についても研究者で共有されているものはない．
本論文では国際標準である ISO-TimeML を日本語に適応させた時間的順序関係アノテーション基準を示す．
我々は『現代日本語書き言葉均衡コーパス』(BCCWJ) の新聞記事の部分集合に対して，動詞・形容詞事象表現に TimeML
の \event\ 相当タグを付与し，その事象の性質に基づき分類を行った．
また，この事象表現と先行研究 \cite{小西-2013","Temporal information extraction can be divided into the following tasks:
temporal expression extraction, time normalization and temporal ordering
relation resolution.
The first task is a subtask of a named entity and
numeral expression extraction.
The second task is often performed by rewriting systems.
The third task consists of event anchoring.
This paper proposed a Japanese temporal ordering annotation scheme
and performed annotations by referring to `{\it the 
Balanced Corpus of Contemporary Written Japanese","['はじめに', '関連研究', 'アノテーション基準', 'アノテーション情報の分析', 'おわりに']",,,,,,,,
V20N05-03.tex,,Complaint Sentence Detection via Automatic Training Data Generation using Sentiment Lexicons and Context Coherence,"本論文では，レビュー文書からクレームが記述された文を自
動検出する課題に対して，従来から問題となっていた人手負
荷を極力軽減することを指向した次の手続きおよび拡張手法
を提案する：
(1) 評価表現と文脈一貫性に基づく教師データ自動生成の手続き．
(2) 自動生成された教師データの特性を踏まえたナイーブベイズ・モデルの拡張手法．
提案手法では，大量のレビュー生文書の集合と評価表現辞書
が準備できれば，クレーム検出規則の作成・維持・管理，あ
るいは，検出規則を自動学習するために必要となる教師デー
タの作成にかかる人手負荷は全くかからない利点をもつ．
評価実験を通して，提案手法によって検出対象文の文
脈情報を適切に捉えることで，クレーム文の検出精度を向上
させることができること，および，
人手によって十分な教師データが作成できない状況において
は，提案手法によって大量の教師データを自動生成すること
で，人手を介在させる場合と同等あるいはそれ以上のクレー
ム検出精度が達成できることを示した．","We propose an automatic method for detecting complaint
sentences from review documents. The proposed method
consists of two procedures. One is a data generation
procedure using sentiment lexicons and context
coherence and the other is the expansion of a naive
Bayes classifier based on the characteristics of the
training data.  This method has an advantage of not
requiring human effort for the creation of large-scale
training data and management of rules for complaint
detection. The experimental results indicate that this
method is more effective than the baseline methods.","['はじめに', '教師データ自動生成', 'ナイーブベイズ・モデルの拡張', '評価実験', '関連研究', 'おわりに']",,,,,,,,
V20N05-04.tex,,Domain Adaptation for Word Sense Disambiguation using k-Nearest Neighbor Algorithm and Topic Model,"本論文では語義曖昧性解消(Word Sense Disambiguation, WSD)の領域適応に対する手法を提案する．
WSD の領域適応の問題は，2つの問題に要約できる．1つは領域間で語義の分布が
異なる問題，もう1つは領域の変化によりデータスパースネスが生じる問題であ
る．本論文では上記の点を論じ，前者の問題の対策として学習手法に k~近傍法
を補助的に用いること，後者の問題の対策としてトピックモデルを用いるこ
とを提案する．具体的にはターゲット領域から構築できるトピックモデル
によって，ソース領域の訓練データとターゲット領域のテストデータ
にトピック素性を追加する．拡張された素性ベクトルから SVM を用いて語義識
別を行うが，識別の信頼性が低いものには k~近傍法の識別結果を用い
る．BCCWJ コーパスの2つの領域 PB（書籍）と OC（Yahoo!知恵袋）から共に頻度が 50 以上の
多義語 17 単語を対象にして，WSD の領域適応の実験を行い，
提案手法の有効性を示す．
別種の領域間における本手法の有効性の確認，
領域の一般性を考慮したトピックモデルを WSD に利用する方法，
および WSD の領域適応に有効なアンサンブル手法を考案することを今後の課題とする．","In this paper, we propose the method of domain adaptation for word sense disambiguation (WSD).
This method faces the following problems for WSD. 
(1) The difference between sense distributions on domains.
(2) The sparseness of data caused by changing the domain.
In this paper, we discuss and recommend  the countermeasure for each problem.
We use the k-nearest neighbor algorithm (k-NN) and the topic model 
for the first and second problems, respectively.
In particular, we append 
topic features developed by the topic model for target domain corpus to 
to training data in source domain and test data in target domain.
Using the extended features of support vector machine (SVM) classifier, we solve WSD.
However, when the reliability of decision of the SVM classifier for a test instance is low,
we use the decision of the k-NN.
In the experiment, we select 17 ambiguous words 
in both domains, PB (books) and OC (Yahoo! Chie Bukuro) 
in the balanced corpus of contemporary written Japanese (BCCWJ corpus),
which appear 50 times or more in these domains,
and  conduct the experiment of domain adaptation for WSD using these words
to show the effectiveness of our method.
In the future, we will apply the proposed method to other domains and
examine a way to use the topic model considering 
the universality of a corpus,
and an effective ensemble learning for domain adaptation for WSD.","['はじめに', 'WSD の領域適応の問題', '提案手法', '実験', '考察', 'おわりに']",,,,,,,,
V20N05-05.tex,,Morphological Analysis of Historical Japanese Text,"単語情報がタグ付けされた本格的な通時コーパスを構築するためには，歴史的な日本語資料の形態素解析が必要とされるが，従来はこれを十分な精度で行うことができなかった．そこで，現代語用のUniDicに語彙の追加を行い，明治時代の文語文と平安時代の仮名文学作品のコーパスを整備することで，「近代文語UniDic」と「中古和文UniDic」を作成した．この辞書によりコーパス構築に利用可能な約96〜97\%での解析が可能になった．この辞書の学習曲線をもとに歴史的資料の形態素解析辞書に必要な訓練用のタグ付きコーパスのサイズを調査した結果，約5万語のコーパスで精度95\%を超える実用的な解析が可能になること，5,000語程度の少量であっても対象テキストの訓練コーパスを用意することが有効であることを確認した．","To construct a richly annotated diachronic corpus of Japanese, the morphological analysis of historical Japanese text is required. However, conventional analysis of old Japanese texts with adequate accuracy is impossible. To facilitate such analyses, we extended dictionary entries from UniDic for Contemporary Japanese and prepared training corpora including articles illustrating the literary style of the Meiji Era and literature of the Heian Era, thus creating new dictionaries: ``UniDic-MLJ (Modern Literary Japanese)'' and ``UniDic-EMJ (Early Middle Japanese).'' These dictionaries achieve a high accuracy (96--97\%) as that required for constructing a diachronic corpus of Japanese. Moreover, we investigated the optimal size of the training corpus for the morphological analysis of historical Japanese text on the basis of the learning curves obtained by using these dictionaries. We confirmed that a 50,000-word corpus achieves an adequate accuracy of over 95\%, and even a small-sized corpus (only 5,000 words) is effective as long as the corpus is particularly constructed for the target domain.","['はじめに', '研究の背景', '見出し語の追加と学習用コーパスの作成', '解析精度の評価', 'エラー分析', 'おわりに']",,,,,,,,
V21N01-01.tex,,Japanese Predicate Argument Structure Analysis by Comparing Candidates in Different Positional Relations between Predicate and Arguments,"一般に，項は述語に近いところにあるという特性がある．
そのため，
従来の述語項構造解析の研究では，
候補を述語との位置関係でグループ分けし，
あらかじめ求めておいたグループ間の
優先順序に従って正解項を探索してきた．
しかしながら，その方法には
異なるグループに属する候補同士の比較ができないという問題がある．
そこで我々は，異なるグループごとに最尤候補を選出し，
それらの中から最終的な出力を決めるモデルを提案する．
このモデルは
優先度の高いグループに属する候補以外も参照することによって最終的な決定を行うことができ，
全体的な最適化が可能である．
実験では，提案手法は優先順序に従う解析よりも精度が向上することを確認した．","In general, arguments are located near the predicate.
A previous study has exploited this characteristic to group candidates by positional relations
between a predicate and its candidate arguments and then searched for the final candidate using a predetermined priority list of the groups.
However, in such an analysis, candidates in different groups cannot be compared.
Therefore, we propose a Japanese predicate 
\linebreak
argument structure analysis model that gathers the most likely candidates from all the groups and then selects the final candidate amongst them.
We can account for candidates with less priority before making a final decision to perform global optimization.
Experimental results show that our model outperforms deterministic models.","['はじめに', '関連研究', '述語と項の位置関係ごとの候補比較による日本語述語項構造解析', '評価実験', '議論', '事例分析', 'おわりに']",,,,,,,,
V21N01-02.tex,,Sentence Hedge Detection without Cue Annotation: A Heuristic Cue Selection Approach,,"This paper presents a simple yet effective approach to sentence-level uncertainty detection which does not require cue word annotation. Unlike previous works, the proposed method focuses on cue selection, decoupling it from disambiguation and by optimizing it over sentence hedging error rate. High performance for the task is achieved in experiments, even for settings with poor disambiguation, without cue annotation and with otherwise unreliable corpora from a machine learning point-of-view.","['Introduction', 'Datasets and baselines for the hedging detection task', 'Process overview', 'Proposed method for cue selection', 'Experiments', 'Conclusion']",,,,,,,,
V21N01-03.tex,,Exploiting Inter-label Dependencies in Hierarchical Multi-Label Document Classification,"階層的複数ラベル文書分類においては，あらかじめ定義されたラベル階層の利用
が中心的な課題となる．
本稿では，複数の出力ラベル間の依存関係という，従来研究が用いてこなかった
手がかりを利用する手法を提案する．
これを実現するために，まずはこのタスクを構造推定問題として定式化し，
複数のラベルを同時に出力する大域モデルと，動的計画法による厳密解の探索手
法を提案する．
次に，ラベル間依存を表現する枝分かれ特徴量を導入する．
実験では，ラベル間依存の特徴量の導入により，精度の向上とともに，モデルの
大きさの削減が確認された．","The main challenge in hierarchical multi-label document classification
is the means by which hierarchically organized labels are leveraged.
In this paper, we propose to exploit dependencies among multiple labels
to be output, which has not been considered in previous studies.
To accomplish this, we first formalize the task as a structured
prediction problem and propose (1) a global model that jointly outputs
multiple labels and (2) a decoding algorithm that finds an exact
solution with dynamic programming.
We then introduce features that capture inter-label dependencies.
Experiments show that these features improve performance while reducing
the model size.","['はじめに', '問題設定', '提案手法', '実験', '関連研究', 'おわりに']",,,,,,,,
V21N01-04.tex,,Domain Adaptations for Word Sense Disambiguation under the Problem of Covariate Shift,"本稿では語義曖昧性解消 (Word Sense Disambiguation, WSD) の領域適応が共
変量シフトの問題と見なせることを示し，共変量シフトの解法である確率密
度比を重みにしたパラメータ学習により，WSD の領域適応の解決を図る．共
変量シフトの解法では確率密度比の算出が鍵となるが，ここでは Naive
Bayes で利用されるモデルを利用した簡易な算出法を試みた．そし
て素性空間拡張法により拡張されたデータに対して，共
変量シフトの解法を行う．この手法を本稿の提案手法とする．BCCWJ コーパ
スの3つ領域 OC （Yahoo! 知恵袋），PB（書籍）及び PN（新聞）を選
び，SemEval-2 の日本語 WSD タスクのデータを利用して，多義語 16種類を
対象に，WSD の領域適応の実験を行った．実験の結果，提案手法
は Daum{\'e","In this report, we show that the problem of domain adaptation for
word sense disambiguation (WSD) can be treated as a covariate shift
problem, and we try to solve it by maximizing the log-likelihood by
weighting the probability density ratio, which is the standard
solution of covariate shift.  The key to solving this problem lies
in the estimation of the probability density ratio.  We estimate the
probability density ratio using simple method employing the Naive
Bayes model.  In our proposed method, we apply the covariate shift
method to the training data expanded by the Daum{\'e","['はじめに', '関連研究', '期待損失最小化からみた共変量シフト', '重み付き対数尤度の最大化', '確率密度比の算出', '提案手法', '実験', '考察', 'おわりに']",,,,,,,,
V21N02-01.tex,,"An Environment for the Usage of Spoken Discourse Corpora
that Effectively Utilizes Existing Tools","近年，コーパスアノテーションは多様化し，多層アノテーションを統合利用す
る仕組みが欠かせない．とくに話し言葉コーパスでは，言語・非言語に関する
10種類以上もの単位とそれらの相互関係を統合し，複数の単位を組み合わせ
た複雑な検索を可能にする必要がある．本研究では，このような要請に応える
ため，(1) マルチモーダル・マルチチャネルの話し言葉コーパスを表現でき
る，汎用的なデータベーススキーマを設計し，(2) 既存のアノテーションツ
ールで作成された，種々の書式を持つアノテーションを入力とし，汎用的なデ
ータベーススキーマから具現化されたデータベースを構築するツールを開発す
る．話し言葉の分野では，広く使われている既存のアノテーションツールを有
効に利用することが不可欠であり，本研究は，既存のアノテーションツールや
コーパス検索ツールを用いたコーパス利用環境を構築する手法を提案する．提
案手法は，開発主体の異なる複数の話し言葉コーパスに適用され，運用に供さ
れている．","In this paper, we (i) propose a general-purpose database schema that can
represent multichannel and multimodal spoken discourse corpora, and (ii)
develop tools to construct a database, instantiating this schema with
reference to configuration files, from annotations in various formats
that have been created with existing annotation tools. Spoken discourse
corpora involve more than 10 different annotations including both verbal
and nonverbal information. They require the integration of a large
number of linguistic/nonlinguistic units and relations among them and
the function to search them with complex queries referring to multiple
units. In spoken discourse corpora, it is essential to utilize existing
annotation tools, which are widely used in the community. We propose a
method to construct an environment for the usage of spoken discourse
corpora that effectively utilizes existing annotation and search
tools. The method has been applied to spoken discourse corpora developed
by different organizations, and has been used effectively for
corpus-linguistic research.","['はじめに', '話し言葉を表現できる汎用的なデータベーススキーマの設計', 'データベース構築ツールの開発', '適用事例', '議論', 'おわりに', '隣接語対を抽出するクエリ', '実用性評価で用いたクエリ']",,,,,,,,
V21N02-02.tex,,Construction of Regional Assembly Minutes Corpus and\\ a Proposal for Annotation Scheme for Implementing Political Information System,近年，国会や地方議会などの会議録がWeb上に公開されている．会議録は，首長や議員の議論が書き起こされた話し言葉のデータであり，長い年月の議論が記録された通時的なデータであることから，政治学，経済学，言語学，情報工学等の様々な分野において研究の対象とされている．国会会議録を利用した研究は会議録の整備が進んでいることから，多くの分野で行われている．その一方で，地方議会会議録を利用した研究については，各分野で研究が行われているものの，自治体によりWeb上で公開されている形式が異なることが多いため，収集作業や整形作業に労力がかかっている．また，各研究者が重複するデータの電子化作業を個別に行っているといった非効率な状況も招いている．このような背景から，我々は多くの研究者が利用することを目的として，地方議会会議録を収集し，地方議会会議録コーパスを構築した．本稿では，我々が構築した地方議会会議録コーパスについて論ずる．同コーパスは，Web上で公開されている全国の地方議会会議録を対象として，「いつ」「どの会議で」「どの議員が」「何を発言したのか」などの各種情報を付与し，検索可能な形式で収録した．また，我々は会議録における発言を基に利用者と政治的に近い考えをもつ議員を判断して提示するシステムを最終的な目的としており，その開発に向けて，分析，評価用のデータ作成のために会議録中の議員の政治的課題に対する賛否とその積極性に関する注釈付けをコーパスの一部に対して行った．本稿では，注釈付けを行った結果についても報告する．,"In recent years, minutes of regional assemblies and the National Diet have been published on the web. Those minutes have long recorded transcribed discussions of mayors and members of assemblies. Therefore, they are a target of study in various fields such as politics, economics, linguistics, information engineering. Since the minutes of the National Diet are maintained in electronic form and freely available via a search system, many researchers have utilized the minutes as a target of study. Minutes of regional assembly meetings are also the focus of researchers in various fields. However, researchers have had trouble gathering and preparing minutes for their study, because the way in which minutes are made available to the public varies assembly by assembly. It is very inefficient for each researcher to make the effort to digitize minutes separately. To improve the situation and contribute to research communities, we have collected regional minutes of assemblies and constructed the corpus of regional assembly minutes. In this paper, we discussed the construction of the corpus of regional assembly minutes. The corpus records minutes from regional assemblies all over Japan that are available on the web. We added additional information to the corpus, such as ``date,'' ``name of meeting,'' ``name of speaker,'' ``text of statement,'' so that users may search statements across the corpus using such information. The final goal of our project is to build a political information system that can recommend a suitable person, or members of an assembly, according to the consistency between users' opinions and statements of assembly members. As a preliminary step of development, we annotated a part of the corpus with information about the speaker’s attitude to specific political subjects, including degree of approval/disapproval. In this paper, we also report the result of the annotation.","['はじめに', '関連研究', '地方議会会議録コーパス', '賛否の積極性に関する注釈付け', 'おわりに', '各会議録検索システムのクロールプログラムの仕様']",,,,,,,,
V21N02-03.tex,,Annotation of Web Documents for Automatic \\ Summarization to Verify Information Credibility,"我々は，利用者が信憑性を判断する上で必要となる情報を Web 文書から探し出し，要約・整理して提示する，情報信憑性判断支援のための要約に関する研究を行っている．
この研究を行う上で基礎となる分析・評価用のコーパスを，改良を重ねながら 3 年間で延べ 4 回構築した．
本論文では，人間の要約過程を観察するための情報と，性能を評価するための正解情報の両方を満たすタグセットとタグ付与の方法について述べる．
また，全数調査が困難な Web 文書を要約対象とする研究において，タグ付与の対象文書集合をどのように決定するかといった問題に対して，我々がどのように対応したかを述べ，コーパス構築を通して得られた知見を報告する．","Over a span of three years, we have constructed and improved four corpora that are the basis for generating summaries for the verification of information credibility.
The summary generated to verify the credibility of information is a brief document composed of extracts from Web documents; it provides material to the user for judging the validity of a statement.
In this paper, we describe a set of tags designed for observing annotation and preparing a gold standard for the summary. Further, we describe the method of annotation.
Because examining each web document for its appropriateness in contributing to the summary is difficult, we describe the methodology of obtaining appropriate documents. 
Furthermore, we share our observations and learnings from the process of constructing these corpora.","['はじめに', '情報信憑性判断支援のための要約', 'サーベイレポートコーパスの構築', '調停要約コーパスの構築', '関連研究', 'おわりに', 'サーベイレポートコーパスのタグ一覧と文書型定義', '抜粋要約中の論点の一覧', '調停要約コーパスのタグ一覧と文書型定義']",,,,,,,,
V21N02-04.tex,,Building and Analyzing a Diverse Document Leads Corpus Annotated with Semantic Relations,"現在，自然言語処理では意味解析の本格的な取り組みが始まりつつある．
意味解析の研究には意味関係を付与したコーパスが必要であるが，従来の意味関係のタグ付きコーパスは新聞記事を中心に整備されてきた．
しかし，文書には多様なジャンル，文体が存在し，その中には新聞記事では出現しないような言語現象も出現する．
本研究では，従来のタグ付け基準では扱われてこなかった現象に対して新たなタグ付け基準を設定した．
Webを利用することで多様な文書の書き始めからなる意味関係タグ付きコーパスを構築し，その分析を行った．","Recently, there have been active studies of semantic analysis in the field of natural language processing.
To study semantic analysis, a corpus annotated with semantic relations is required.
Although existing corpora annotated with semantic relations have been restricted to newspaper articles, there are texts of various genres and styles containing linguistic expressions that are missing in newspaper articles.
In this paper, we define annotation criteria for linguistic phenomena which have not been treated using existing criteria.
We have built a diverse  document leads corpus annotated with semantic relations.
We report the statistics of this corpus.","['はじめに', 'タグ付与対象の文書の収集', 'タグ付け', '著者・読者表現', '複数の解釈が可能な表現に対するタグ付け', '作成されたコーパス', '関連研究', 'まとめ', '複数解釈可能な表現の例とタグ付け基準']",,,,,,,,
V21N02-05.tex,,Annotation of Focus for Negation in Japanese Text,"「誰がいつどこで何をする」という文に「ない
」や「ん」，「ず」などの語が付くと，いわゆる否定文
となる．否定文において，否定の働きが及ぶ範囲をスコ
ープと呼び，その中で特に否定される部分を焦点と呼ぶ
．否定の焦点が存在する場合，一般にその焦点の箇所を除
いた文の命題は成立する．それゆえ，自然言語
処理において，否定の焦点が存在するか，および，どの
部分が否定の焦点になっているかを自動的に判定する処
理は，含意認識や情報抽出などの応用処理の高度化のた
めに必要な技術である．本論文では，否定の焦点検出シ
ステムを構築するための基盤として，日本語における否
定の焦点をテキストにアノテーションする枠組みを提案
し，構築した否定の焦点コーパスについて報告する．否
定文において否定の焦点を判断するための基準を提案し
，否定の形態素および焦点の部分にアノテーションすべ
き情報について議論する．否定の焦点の判断には，「は
」や「しか」などのとりたて詞や前後の文脈などが手が
かりとなるため，これらを明確にアノテーションする．
我々は，提案するアノテーション体系に基づいて，楽天
トラベルのレビューデータと『現代日本語書き言葉均衡
コーパス』内の新聞を対象としてアノテーションコーパ
スを構築した．本論文では，コーパス内に存在する1,327
の否定に対するアノテーション結果を報告する．","This paper proposes an annotation scheme for the focus of negation in Japanese text. 
Negation has a scope, and its focus falls within this scope. 
The scope of negation is the part of the sentence that is negated. 
The focus of negation is the part of the scope that is prominently negated. 
In natural language processing, correct interpretation of negated statements requires precise detection of the focus of negation in the statements. 
As a foundation for developing a focus detector, we have annotated a part of ``Rakuten Travel: User Review Data'' and a part of a newspaper subcorpus of the ``Balanced Corpus of Contemporary Written Japanese,'' with our annotation scheme. 
In this scheme, a negation cue in the text data is linked to the focus by annotation with identifying clues. 
These clues include focus particles such as ``wa'' and ``shika,'' and other expressions in the context. 
We report 1,327 negation cues and the foci in the corpora.","['はじめに', '関連研究', '否定の焦点アノテーションの基本指針', '否定の焦点アノテーションの枠組み', '否定の焦点コーパス', 'おわりに']",,,,,,,,
V21N02-06.tex,,Research on Annotation of Sentiment Analysis in Community QA,意見分析の研究が盛んになり，世論調査，評判分析など，多岐にわたる応用が実現されている．意見分析の研究においては，他の言語処理研究と同様に，コーパスの重要性が指摘されている．意見分析研究のコーパスは，応用目的に応じて，対象とする文書ジャンルが変化し，アノテーションすべき意見の情報も変更する．現在，意見分析コーパスは，ニュース，レビュー，ブログなどの文書ジャンルを対象としたものが多い．一方で，対話型の文書ジャンルには焦点が当てられておらず，アノテーションについての明確な方針がない．本稿では，『現代日本語書き言葉均衡コーパス』に含まれるコミュニティQAの文書を対象として，詳細な分類タイプに基づく意見情報ならびに関連した情報のアノテーションを行い，コーパスを作成する．また，複数のアノテーション情報を重ね合わせることにより，コーパス中の質問や回答に現れる意見の特徴を明らかにすることで，ドメインを横断した意見分析や，意見質問の応答技術といった，現在の意見分析研究が直面している難しい課題に対する新たな知見を提供できることを示す．,"Recent sentiment analysis studies have demonstrated that many services such as public opinion surveys and reputation analyses are derived from a variety of documentary resources. The annotated corpus in sentiment analysis is one essential resource, as are other NLP technologies such as POS tagging and named entity extraction. The sentiment annotation policy should be defined according to the task and relevant document genre. Recently, many sentiment corpora have been published in news, review, and blog genres. However, a sentiment corpus in the dialog document genre, which involves questions and answers, has yet to be studied, and a sentiment annotation policy has yet to be clearly defined. 
In this paper, we explain an approach to annotating and creating a sentiment corpus with detailed sentiment types using community QA documents in BCCWJ. We also identify the different sentiment characteristics in a corpus through combinations of annotations to provide novel insights in the challenging topics of opinion question answering and domain adaptation.","['はじめに', '関連研究', 'コミュニティQAデータを対象とした意見情報のアノテーション', 'コミュニティQAを対象とした意見情報のアノテーション作業の特徴', 'Yahoo! 知恵袋コーパスを用いた意見・体験情報の分析', 'おわりに']",,,,,,,,
V21N02-07.tex,,"Design, Implementation,  and Operation of Annotation Support System for Morphological Information of BCCWJ",『現代日本語書き言葉均衡コーパス』は1億語を超える大規模なコーパスであり，17万ファイル以上のXML文書に短単位・長単位の形態論情報アノテーションが施されている．このコーパスの構築を目的としてアノテーションのためのシステムが開発された．このシステムは，辞書見出しデータベースと，タグ付けされたコーパスとを関連付けて，整合性を保ちつつ多くの作業者が編集していくことを可能にするものである．このシステムは，関係データベースで構築されたサーバ「形態論情報データベース」と，辞書を参照しながらコーパスの修正作業を可能にするコーパス修正用のクライアントツール「大納言」，形態素解析辞書UniDicの見出し語の管理ツール「UniDic Explorer」から成る．本稿はこのデータベースシステムの設計・実装・運用について論ずる．,"``Balanced Corpus of Contemporary Written Japanese'' is a large-scale Japanese corpus of 100 million words. It contains 170,000 XML files annotated with two levels of morphological information: short-unit word and long-unit word. We have constructed an annotation system to compile this corpus. The system allows many users to modify corpus annotations and dictionary entries, which are related to each other, while ensuring consistency. The system consists of a relational database server called the ``Morphological Information Database,'' a client tool that maintains the morphological information of the corpus called ``Dynagon,'' and a tool that manages dictionary entries for morphological analysis called ``UniDic Explorer.'' This paper describes the design, implementation, and operation of this ``Morphological Information Database'' for BCCWJ.","['はじめに', '前提となる知識', '関連する先行事例との比較', '形態論情報データベースシステムの概要', '辞書データベースの設計と実装', 'コーパスデータベースの設計・実装・運用', 'クライアントツールの開発', 'おわりに']",,,,,,,,
V21N02-08.tex,,Issues on Annotation Guidelines for Japanese Predicate-Argument Structures,"日本語の述語項構造アノテーションコーパスは，これまでにいくつかの研究によって整備され，
その結果，日本語の述語項構造解析の研究は飛躍的にその成果を伸ばした．
一方で，既存のコーパスのアノテーション作業者間一致率やアノテーション結果の定性的な分析をふまえると，
ラベル付与に用いる作業用のガイドラインには未だ改善の余地が大きいと言える．
本論文では，より洗練された述語項構造アノテーションのガイドラインを作成することを目的とし，
NAISTテキストコーパス(NTC), 京都大学テキストコーパス(KTC)のアノテーションガイドラインと実際のラベル付与例を参考に，
これらのコーパスの仕様策定，仕様準拠のアノテーションに関わった研究者・アノテータ，
仕様の改善に関心のある研究者らの考察をもとにガイドライン策定上の論点をまとめ，
現状の問題点や，それらに対する改善策について議論・整理した結果を報告する．
また，アノテーションガイドラインを継続的に改善可能とするための
方法論についても議論する．","Japanese corpora annotated with predicate-argument structure (PAS) have been constructed as part of several research projects and these annotated corpora have significantly advanced the field of PAS analysis. 
However, according to an inter-annotator agreement study and qualitative analysis of the existing corpora, there is still a strong need for further improvement of the annotation guidelines of the corpora. 
To improve the quality of PAS annotation guidelines, we have collected and summarized the practical knowledge and a list of problematic issues concerning the task of the PAS annotation through discussions with researchers actively engaged in the construction of NAIST Text Corpus (NTC) and Kyoto Text Corpus (KTC), researchers concerned with existing PAS annotation guidelines, and an annotator who is working on the annotation task, using NTC and KTC guidelines. 
This paper reports the problems and suggestions that we collected and possible solutions to those problems on the basis of results of the discussions. 
Finally, we suggest a method for continuously improving annotation guidelines.","['はじめに', '関連研究', 'NAISTテキストコーパス', '論点の収集方法', '個別の論点', '見通しの良いフレームワークの設計', 'まとめ']",,,,,,,,
V21N02-09.tex,,Adaptation of Long-Unit-Word Analysis System to Different Part-Of-Speech Tagset,"言語研究において，新しい品詞体系を用いる場合には，既存の辞書やコーパス，
解析器では対応できないことが多いため，これらを再構築する必要がある．
これらのうち，辞書とコーパスは再利用できることが少なく，新たに構築する場合が多い．
一方，解析器は既存のものを改良することで対応できることが多いものの，
どのような改良が必要かは明らかになっていない．
本論文では，品詞体系の異なるコーパスの解析に必要となる
解析器の改良点を明らかにするためのケーススタディとして，
品詞体系の異なる日本語話し言葉コーパス（以下，CSJ）と現代日本語書き言葉均衡コーパス（以下，BCCWJ）を利用して，
長単位情報を自動付与した場合に生じる誤りを軽減する方策について述べる．
具体的には，CSJを基に構築した長単位解析器をBCCWJへ適用するため，
CSJとBCCWJの形態論情報における相違点に応じて，長単位解析器の学習に用いる素性やラベルを改善した．
評価実験により提案手法の有効性を示す．","Existing dictionaries, corpora, analyzers are not usually applicable to 
research using new part-of-speech tagset in the fields of linguistic research. 
Dictionaries and corpora are often newly constructed. 
On the other hand, existing analyzers can be reused by improving them. 
However, it is not clear how they could be improved. 
This paper describes how an analyzer constructed for analyzing a certain corpus 
can be applied to another corpus with a different part-of-speech tagset. 
In particular, we improved the features and labels used to train a long-unit-word analyzer 
based on Corpus of Spontaneous Japanese (CSJ)
by focusing on the differences between CSJ and Balanced Corpus of Comtemporary Written Japanese (BCCWJ)
and applied the analyzer to BCCWJ. 
The experimental results show the advantage of the proposed method.","['はじめに', 'CSJとBCCWJの形態論情報における相違点', '長単位解析手法', '評価実験', '長単位解析ツールComainu', 'まとめ']",,,,,,,,
V21N03-01.tex,,Evaluating Translation Quality with Word Order Correlations,"効率的に機械翻訳システムを開発していくためには，質の高い自動評価法が必要
となる．
これまでに様々な自動評価法が提案されてきたが，
参照翻訳とシステム翻訳との間で一致するNグラムの割合に基づきスコアを決定
する BLEU や最大共通部分単語列の割合に基づきスコアを決定するROUGE-Lなどがよく用い
られてきた．
しかし，こうした方法にはいつくかの問題がある．ルールベース翻訳(RBMT)の訳
を人間
は高く評価するが，従来の自動評価法は低く評価する．これは，RBMT
が参照翻訳と違う訳語を使うことが多いのが原因である．
これら従来の自動評価法は単語が一致しないと大きくスコアが下がるが，人間は
そうとは限らない．
一方，統計的機械翻訳
(SMT)で英日，日英翻訳を行うと，「AなのでB」と訳すべきところを「BなのでA」
と訳されがちである．この訳には低いスコアが与えられるべきであるが，Nグラム
の一致割合に着目するとあまりスコアは下がらない．
こうした問題を解決するため，本稿では，訳語の違いに寛大で，かつ，大局的な
語順を考慮した自動評価法を提案する．大局的な語順は順位相関係数で測定し，
訳語の違いは，単語適合率で測定するがパラメタでその重みを調整できるよう
にする．
NTCIR-7，NTCIR-9 の特許翻訳タスクにおける英日，日英翻訳のデータを
用いてメタ評価を行ったところ，提案手法が従来の自動評価法よりも優れている
ことを確認した．","Automatic evaluation of Machine Translation (MT) quality is essential to
develop high-quality MT systems.
Various evaluation metrics have proposed, and among them, BLEU is widely
used as the de facto standard metric. BLEU counts N-grams common between
reference and hypothesis translation. 
On the other hand, ROUGE-L counts longest common subsequences.
However, these methods have some problems. 
People give high scores to Rule-based MT (RBMT), but these methods do not,
because RBMT tends to use alternative words. Conventional metrics are
severe against the difference of words, but people accept them if the
translation has the same 
\linebreak
meaning. Statistical MT (SMT) tends to
translate ``A because B'' as ``B because A'' in case of translation
between Japanese and English.
BLEU does not care about global word order, and this severe mistake is
not penalized very much. In order to consider global word order, this
paper proposes a lenient automatic evaluation metric based on rank
correlation of word order. By focusing on only words common between the
two translations, 
this method is lenient with the use of alternative words. The difference of words is
measured by precision of words, and its weight is controlled by a parameter.
By using submissions of NTCIR-7 \& 9's Patent Translation task, the
proposed method outperforms conventional measures in terms of system
level comparison.","['はじめに', 'Nグラム一致率に基づく自動評価法の問題点', 'LCSに基づく自動評価法の問題点', '語順の相関に基づく自動評価法', '実験の設定', '実験結果と考察', 'まとめと今後の課題']",,,,,,,,
V21N03-02.tex,,Automatic Estimation of Speaking Style in Speech Corpora Focusing on Speech Transcriptions,近年，計算機技術の進歩に伴って大規模言語データの蓄積と処理が容易となり，音声言語コーパスの構築と実用化の研究が盛んに行われている．我々は，speaking style に関心を持つ利用者に音声言語コーパスを探しやすくさせるために，音声言語コーパスの speaking style の自動推定を目指している．本研究では，1993 年に Eskenazi が提唱した speaking style の 3 尺度を導入し，従来の文体・ジャンルの判別や著者推定などの自然言語処理の分野で用いられた言語の形態論的特徴を手がかりとし，音声に付随する書き起こしテキスト（本論文では転記テキストと呼ぶ）に着目した speaking style 推定モデルの構築を試みた．具体的な手続きとしては，はじめに様々な音声言語コーパスから音声に付随する転記テキストを無作為に抽出する．次にこれらの転記テキストを刺激として用い，3 尺度の speaking style の評定実験を行う．そして，評定結果を目的変数，転記テキストの品詞・語種率と形態素パタンを説明変数とし，重回帰分析により 3 尺度それぞれの回帰モデルを求める．交差検定を行った結果，本研究の提案手法によって3尺度の内 2 尺度の speaking style 評定値を高い精度で推定できることを確認した．,"Recent developments in computer technology have allowed the construction and wide\-spread application of large-scale speech corpora. To enable users of speech corpora to easier data retrieval, we attempt to characterise the speaking style of speakers recorded in the corpora. We first introduce the three scales for measuring speaking style which were proposed by Eskenazi in 1993. We then use morphological features extracted from speech transcriptions that have proven effective in discriminating between styles and identifying authors in the field of natural language processing to construct an estimation model of speaking style. More specifically, we randomly choose transcriptions from various speech corpora as text stimuli with which to conduct a rating experiment on speaking style perception. Then, using the features extracted from these stimuli and rating results, we construct an estimation model of speaking style, using a multi-regression analysis. After cross-validation (leave-1-out), the results show that among the three scales of speaking style, the ratings of two scales can be estimated with high accuracy, which proves the effectiveness of our method in the estimation of speaking style.","['はじめに', 'Speaking styleの自動推定手法', '評定実験', 'Speaking style 推定モデルの構築', 'おわりに']",,,,,,,,
V21N03-03.tex,,"A Surface-Similarity-Based Solver of Comprehension Questions Referring
to Underlined Passages in Contemporary Japanese of the National Center
Test","大学入試センター試験『国語』の現代文で出題される，いわゆる「傍線部問題」
を解く方法を定式化し，実装した．本方法は，問題の本文の一部と5つの選択肢
を照合し，表層的に最も類似した選択肢を選ぶことにより問題を解く．実装し
た方法は，「評論」の「傍線部問題」の半数以上に対して正解を出力した．","We formalized and implemented a method for solving comprehension
questions referring to underlined passages in Contemporary Japanese of
the National Center Test. A target question consists of a text body
extracted from a critical essay or novel, a question sentence, and
five choices; the question sentence refers to an underlined passage in
the text body and asks a question related to the passage, such as the
interpretation of the passage, the reason for the author's claim (in
the essay), and the emotional state or feeling of a character (in the
novel). Given a question, the method determines which text segment is
the key to the question and selects the choice that is most similar to
the segment. The method correctly solved more than half the ``critical
essay'' questions in previous tests of the National Center Test.","['はじめに', 'センター試験『国語』 と傍線部問題', '傍線部問題の定式化', '実装', '実験と検討', '結論']",,,,,,,,
V21N03-04.tex,,Unlabeled Dependency Parsing Based Pre-reordering for Chinese-to-Japanese SMT,,"In statistical machine translation, Chinese and Japanese is a well-known 
long-distance language pair that causes difficulties to word alignment techniques. 
Pre-reordering methods have been proven efficient and effective; however, they 
need reliable parsers to extract the syntactic structure of the source sentences. 
On one hand, we propose a framework in which only part-of-speech (POS) tags and 
unlabeled dependency parse trees are used to minimize the influence of parse 
errors, and linguistic knowledge on structural difference is encoded in the form 
of reordering rules. We show significant improvements in translation quality of 
sentences in the news domain over state-of-the-art reordering methods. On the 
other hand, we explore the relationship between dependency parsing and our 
pre-reordering method from two aspects: POS tags and dependencies. We observe 
the effects of different parse errors on reordering performance by combining 
empirical and descriptive approaches. In the empirical approach, we quantify the 
distribution of general parse errors along with reordering quality. In the descriptive 
approach, we extract seven influential error patterns and examine their correlations 
with reordering errors.","['Introduction', 'Related Work', 'Unlabeled Dependency Parsing based Pre-reordering (DPC)', 'Experiments', 'Analysis on Causes of Reordering Errors', 'Conclusion', 'Summary of Part-of-Speech Tag Set in Penn Chinese Treebank', 'Head Rules for Penn2Malt to Convert the Penn Chinese Treebank']",,,,,,,,
V21N03-05.tex,,Japanese Morphological Analysis of Picture Books,"これまで，主に新聞などのテキストを対象とした解析では，形態素解析器を始めとし
て高い解析精度が達成されている．しかし分野の異なるテキストに対しては，既存
の解析モデルで，必ずしも高い解析精度を得られるわけではない．そこで本稿では，
既存の言語資源を対象分野の特徴にあわせて自動的に変換する手法を提案する．本
稿では，絵本を解析対象とし，既存の言語資源を絵本の特徴にあわせて自動的に変
換し，学習に用いることで相当な精度向上が可能であることを示す．学習には既存
の形態素解析器の学習機能を用いる．さらに，絵本自体にアノテーションしたデー
タを学習に用いる実験を行い，提案手法で得られる効果は，絵本自体への約~11,000
行，90,000 形態素のアノテーションと同程度であることを示す．また，同じ絵本の
一部を学習データに追加する場合と，それ以外の場合について，学習曲線や誤り内
容の変化を調査し，効果的なアノテーション方法を示す．考察では，絵本の対象年
齢と解析精度の関係や，解析精度が向上しにくい語の分析を行い，更なる改良案を
示す．また，絵本以外への適用可能性についても考察する．","Picture books have a significant influence on children's language
  development. However, the sentences in picture books are difficult
  to analyze automatically. Therefore, to improve the accuracy of the
  morphological analysis of such sentences, we propose an automatic
  method to transform existing resources into applicable training data
  for picture books. In this paper, we first compare picture books
  with common corpora and then analyze the reasons for the difficulty
  in morphological analysis. Based on this analysis, we propose a
  transforming method for existing resources and show its
  effectiveness using the learning function of an existing
  morphological analyzer. Second, we perform further experiments using
  annotated data of picture books themselves. Then we reveal that our
  proposed method provides us with the same effect, with around 11,000
  lines, that is 90,000 morphological annotations of picture books. In
  addition, we demonstrate an effective annotation strategy by
  investigating the learning curves and change in error types. In a
  discussion, we analyze the results focused on a picture book's
  target ages and difficult to learn words and then further refine our
  proposed method. Finally, we also briefly consider the applicability
  of our method to other domains.","['はじめに', '解析対象', '形態素解析器', '絵本を対象とした形態素解析における問題分析', '提案手法', '評価実験(1): 教師なし分野適応', '評価実験(2): 教師あり分野適応', '考察', 'まとめと今後の課題']",,,,,,,,
V21N03-06.tex,,Collective Sentiment Classification Based on User Leniency and Product Popularity\footnotetext{\llap{*~,,"We propose a method of collective sentiment  classification that assumes dependencies among labels of an input set of reviews. 
The key observation behind our method is that the distribution of polarity labels over reviews written by each user or written on each product is often skewed in the real world; intolerant users tend to report complaints while popular products are likely to receive praise.  
We encode these characteristics of users and products (referred to as \textit{user leniency","['Introduction', 'Related Work', 'Method', 'Experiments', 'Conclusion']",,,,,,,,
V21N03-07.tex,,Japanese Zero Reference Resolution Considering Zero Exophora and Author/Reader Mentions,"日本語では用言の項が省略されるゼロ照応と呼ばれる現象が頻出する．ゼロ照応
は照応先が文章中に明示的に出現する文章内ゼロ照応と，明示的に出現しない
外界ゼロ照応に分類でき，従来のゼロ照応解析は主に前者を対象としてきた．
近年，Webが社会基盤となり，Web上でのテキストによる情報伝達がますます重
要性をましている．そこでは，情報の送り手・受け手である著者・読者が重要
な役割をはたすため，Webテキストの言語処理においても著者・読者を正確にと
らえることが必要となる．しかし，文脈中で明確な表現（人称代名詞など）で言及されていない著
者・読者は，従来の文章内ゼロ照応中心のゼロ照応解析では多くの場合対象外
であった．
このような背景から，本論文では，外界ゼロ照応および文章の著者・読者を扱
うゼロ照応解析モデルを提案する．提案手法では外界ゼロ照応を扱うために，
ゼロ代名詞の照応先の候補に外界ゼロ照応に対応する仮想的な談話要素を加え
る．また，語彙統語パターンを利用することで，文章中で著者や読者に言及し
ている表現を自動的に識別する．実験により，我々の提案手法が外界ゼロ照応
解析だけでなく，文章内ゼロ照応解析に対しても有効であることを示す．","In Japanese, zero references often occur and many of them are categorized into zero exophora, in which a referent is not mentioned in the document. 
However, previous studies have focused   only  on zero  endophora,  in  which a  referent  explicitly  appears.   We  present  a zero  reference  resolution  model  considering zero  exophora and  authors/readers  of  a  document. 
To deal with zero exophora, our model adds  pseudo-entities  corresponding  to  zero exophora to candidate referents of zero pronouns.
In addition, the model automatically detects mentions that refer to the author and reader of a document using lexico-syntactic patterns.
We present particular behavior of authors/readers in a discourse as a feature vector of a machine learning model.
The experimental results demonstrate the effectiveness of our model for not only zero exophora but also zero endophora.","['はじめに', '関連研究', 'ランキング学習', 'ベースラインモデル', 'コーパス', '著者・読者表現推定', '外界照応および著者・読者表現を考慮したゼロ照応解析モデル', '実験', 'まとめ']",,,,,,,,
V21N04-01.tex,,Using WFSTs for Efficient EM Learning of Probabilistic CFGs and Their Extensions\footnotetext{\llap{*~,,"Probabilistic context-free grammars (PCFGs) are a widely known class
of probabilistic language models. The Inside-Outside (I-O) algorithm
is well known as an efficient EM algorithm tailored for PCFGs.
Although the algorithm requires inexpensive linguistic resources,
there remains a problem in its efficiency.
This paper presents an efficient method for training PCFG
parameters in which the parser is separated from
the EM algorithm, assuming that the underlying CFG is given.
A new EM algorithm exploits the compactness
of well-formed substring tables (WFSTs) generated by the parser.
Our proposal is general in that the input grammar need not
take Chomsky normal form (CNF) while it is equivalent to
the I-O algorithm in the CNF case.
In addition, we propose a polynomial-time EM algorithm for
CFGs with context-sensitive probabilities, and report
experimental results with the ATR dialogue corpus
and a hand-crafted Japanese grammar.","['Introduction', 'Preliminaries', 'Proposed method', 'Experiments on training time', 'EM learning of the extensions of PCFGs', 'Related work', 'Conclusion', 'Justification of the graphical EM algorithm', ""Relation to Stolcke's probabilistic Earley parser""]",,,,,,,,
V21N04-02.tex,,Construction of Practical Japanese Parsing System \\Based on Lexical Functional Grammar\footnotetext{\llap{*~,,"This paper describes a Japanese parsing system with a linguistically fine-grained grammar based on Lexical-Functional Grammar (LFG). The system is the first 
\linebreak
Japanese LFG parser with over 97\% coverage of real-world text. We evaluated the accuracy of the system by comparing it with standard Japanese dependency parsers. The LFG parser shows roughly equivalent performance in dependency accuracy with standard parsers. It also provides reasonably accurate results of case detection.","['Introduction', 'Parallel Grammar Project', 'Japanese LFG system', 'Japanese LFG grammar', 'Experimental Evaluation', 'Conclusion']",,,,,,,,
V21N04-03.tex,,Gradual Fertilization of Case Frames\footnotetext{\llap{*~,,"This article proposes an automatic method of gradually constructing case
 frames. First, a large raw corpus is parsed, and base case frames
 are constructed from reliable predicate-argument examples in the
 parsing results. Second, case analysis based on the base case frames
 is applied to the large corpus, and the case frames are upgraded by
 incorporating newly acquired information. Case frames are gradually
 fertilized in this way. We constructed case frames from 26 years of newspaper
 articles consisting of approximately 26 million sentences. The case frames are
 evaluated manually as well as through syntactic and case
 analyses. These results presented the effectiveness of the constructed
 case frames.","['Introduction', 'Target Expressions', 'Compilation of Base Case Frames and Case Structure Analysis using Base Case Frames', 'Fertilization of the Base Case Frames', 'Post-processing of Case Frames', 'Evaluation of Acquired Case Frames', 'Related Work', 'Conclusions']",,,,,,,,
V21N04-04.tex,,Improvements of Katz K Mixture Model\footnotetext{\llap{*~,,"A simpler distribution that fits empirical word distribution about as well as a negative binomial is the Katz K mixture. In the K 
mixture model, the basic assumption is that the conditional probabilities of repeats for a given word are determined by a constant decay factor that 
is independent of the number of occurrences which have taken place. However, the probabilities of the repeat occurrences are generally lower than 
the constant decay factor for the content-bearing words with few occurrences that have taken place. To solve this deficiency of the K mixture model, 
in-depth exploration of the characteristics of the conditional probabilities of repetitions, decay factors and their influences on modeling term 
distributions was conducted. Based on the results of this study, it appears that both ends of the distribution can be used to fit models. That is, 
not only can document frequencies be used when the instances of a word are few, but also tail probabilities (the accumulation of document 
frequencies). Both document frequencies for few instances of a word and tail probabilities for large instances are often relatively easy to estimate 
empirically. Therefore, we propose an effective approach for improving the K mixture model, where the decay factor is the combination of two 
possible decay factors interpolated by a function depending on the number of instances of a word in a document. Results show that the proposed model 
can generate a statistically significant better estimation of frequencies, especially the frequency estimation for a word with two instances in a 
document. In addition, it is shown that the advantages of this approach will become more evident in two cases, modeling the term distribution for 
the frequently used content-bearing word and modeling the term distribution for a corpus with a wide range of document length.","['Introduction', 'Experiment Data', 'Notations', 'Katz K Mixture', 'Observations', 'Our Approach', 'Conclusion']",,,,,,,,
V21N04-05.tex,,Preference Dependency Grammar and its Packed Shared Data Structure ``Dependency Forest''\footnotetext{\llap{*~,,"Preference dependency grammar (PDG) is a framework for integrating
morphological, syntactic, and semantic analyses. PDG provides packed
shared data structures that can efficiently encompass all possible
interpretations at each level of sentence analyses with preference
scores. Using the structure, PDG can calculate a globally optimized
interpretation for the target sentence. This paper first gives an
overview of the PDG framework by describing the base model of PDG,
which is a sentence analysis model, called a ``multi-level packed
shared data connection model.'' Then this paper describes packed
shared data structures, e.g., headed parse forests and dependency
forests, adopted in PDG. Finally, the completeness and soundness of
the mapping between the parse forest and the dependency forest are
revealed.","['Introduction', 'Overview of PDG and Packed Shared Data Structure', 'Packed Shared Data Structures in PDG', 'Generation of the Phrase Structure Forest and the Initial \\\\\\hspace{19pt', 'Experiment for Analysis of Example Sentences', 'Concluding Remarks', 'Problem in the Syntactic Graph', 'Proof of the Completeness and Soundness of the Dependency Forest']",,,,,,,,
V21N04-06.tex,,A Fully-Lexicalized Probabilistic Model \\ for Japanese Syntactic and Case Structure Analysis\footnotetext{\llap{*~,,"We present an integrated probabilistic model for Japanese syntactic and
 case 
\linebreak
structure analysis. Syntactic and case structures are
 simultaneously analyzed on the basis of wide-coverage case frames that are
 constructed from a huge raw corpus in an unsupervised manner. This
 model selects the syntactic and case structures that have the highest
 generative probability. We evaluate both syntactic structure and 
\linebreak
case
 structure. In particular, the experimental results for syntactic
 analysis on web sentences show that the proposed model significantly
 outperforms the known 
\linebreak
syntactic analyzers.","['Introduction', 'Automatically Constructed Case Frames', 'Integrated Probabilistic Model for Syntactic and Case Structure Analysis', 'Experiments', 'Related Work', 'Conclusion']",,,,,,,,
V21N04-07.tex,,Construction of a Domain Dictionary for Fundamental Vocabulary and its Application to Automatic Blog Categorization Using Dynamically Estimated \\ Domains of Unknown Words\footnotetext{\llap{*~,,"The semantic relations between words are essential for natural language
understanding.
Toward deeper natural language understanding, we semi-automatically
 constructed a domain dictionary that represents the domain relations
 between fundamental Japanese words.
Our method does not require a document collection.
As a task-based evaluation of the domain dictionary,
we categorized blogs by assigning a domain for each
word in a blog article and categorizing it as the most dominant \mbox{domain.","['Introduction', 'Two Issues', 'Domain Dictionary Construction', 'Resulting Domain Dictionary \\label{sec:resulting-domain-dictionary', 'Blog Categorization', 'Domain Estimation for Unknown Words', 'Evaluation', 'Related Work', 'Conclusion']",,,,,,,,
V21N04-08.tex,,Generalization of Semantic Roles in Automatic \\ Semantic Role Labeling\footnotetext{\llap{*~,,"Numerous studies have applied machine-learning approaches 
to semantic role labeling with the availability of corpora such as FrameNet and PropBank. 
These corpora define frame-specific semantic roles for each frame, 
which are problematic for a machine-learning approach 
because the corpus contains a number of infrequent roles that hinder efficient learning. 
This paper focuses on the generalization problem of semantic roles 
in a semantic role labeling task. 
We compare existing generalization criteria with our novel criteria, 
and clarify the characteristics of each criterion. 
We also show that using multiple generalization criteria in a single model
improves the performance of a semantic role classification.
In experiments on FrameNet, we achieved $19.16\%$ error reduction in
terms of total accuracy, and $7.42\%$ in macro-averaged F1.
On PropBank,  we reduced $24.07\%$ of errors in total accuracy,
and $26.39\%$ of errors in the evaluation for
unseen verbs.","['Introduction', 'Related work', 'Frame dictionaries and annotated corpora with semantic roles', 'Semantic role classification', 'Semantic role generalization and role classification models', 'Role generalization on FrameNet', 'Experiments on FrameNet and discussion', 'Role generalization on PropBank', 'Experimental comparisons on PropBank', 'Conclusion']",,,,,,,,
V21N04-09.tex,,Study on Constants of Natural Language Texts\footnotetext{\llap{*~,,"This paper considers different measures that might become constants for any length of a given natural language text. Such measures indicate a potential for studying the complexity of natural language but have previously only been studied using relatively small English texts. In this study, we consider measures for texts in languages other than English, and for large-scale texts. Among the candidate measures, we consider Yule's $K$, Orlov's $Z$, and Golcher's $\mathit{VM","['Introduction', 'Related Work', 'Measures', 'Experiment', 'Discussion', 'Conclusion']",,,,,,,,
V21N04-10.tex,,Splitting Katakana Noun Compounds by Paraphrasing \\ and Back-transliteration\footnotetext{\llap{*~,,"Word boundaries within noun compounds in a number of languages,
	including 
\linebreak
Japanese, are not marked by white spaces. Thus, it is
	beneficial for various NLP applications to split such noun
	compounds. In the case of Japanese, noun compounds composed of
	katakana words are particularly difficult to split because
	katakana words are highly productive and are often out of
	vocabulary. Therefore, we propose using	paraphrasing and
	back-transliteration of katakana noun compounds	to split
	them. Experiments
	in which paraphrases and back-transliterations from unlabeled
	textual data were extracted and used to construct splitting
	models
	improved splitting 
\linebreak
accuracy with statistical significance.","['Introduction', 'Related Work', 'A Supervised Approach', 'Paraphrase Features', 'Back-transliteration Features', 'Experiments and Discussion', 'Conclusion']",,,,,,,,
V21N04-11.tex,,Relevance Feedback using Surface and Latent Information in Texts\footnotetext{\llap{*~,,"Most relevance feedback methods re-rank search results using only the
information of surface words in texts. We present a method that uses not
only the information of surface words but also that of latent words that
are inferred from texts. We infer latent word distribution in each
document in the search results using latent Dirichlet allocation
(LDA). When feedback is given, we also infer the latent word
distribution in the feedback using LDA. We calculate the similarities
between the user feedback and each document in the search results using
both the surface and latent word distributions and re-rank the search
results on the basis of the similarities. Evaluation results show that
when user feedback consisting of two documents ($3,589$ words) is given,
the proposed method improves the initial search results by $27.6\%$ in
precision at $10$ (P@10). Additionally, it proves that the proposed
method can perform well even when only a small amount of user feedback
is available. For example, an improvement of $5.3\%$ in P@10 was
achieved when user feedback constituted only $57$ words.","['Introduction', 'Language Modeling Approaches to IR', 'LDA', 'Proposed Method', 'Experiments', 'Conclusion']",,,,,,,,
V21N04-12.tex,,Particle Error Correction from Small Error Data \\for Japanese Learners\footnotetext{\llap{*~,,"This paper shows how to correct the grammatical errors of Japanese
particles made by Japanese learners. Our method is based on
discriminative sequence conversion, which converts one sequence of
words into another and corrects particle errors by substitution,
insertion, or deletion. However, it is difficult to collect large
learners' corpora. We solve this problem with a discriminative
learning framework that uses the following two methods. First,
language model probabilities obtained from large, raw text corpora are
combined with $n$-gram binary features obtained from learners'
corpora. This method is applied to measure the accuracy of Japanese
sentences. Second, automatically generated pseudo-error sentences are
added to learners' corpora to enrich the corpora
directly. Furthermore, we apply domain adaptation, in which the
pseudo-error sentences (the source domain) are adapted to the real
error sentences (the target domain). Experiments show that the recall
rate is improved using both language model probabilities and $n$-gram
binary features. Stable improvement is achieved using pseudo-error
sentences with domain adaptation.","['Introduction', 'Errors in Sentences Written by Japanese Learners', 'Error Correction by Discriminative Sequence Conversion', 'Expansion of Parallel Sentences Using Pseudo-error Sentences', 'Experiments', 'Related Studies', 'Conclusion']",,,,,,,,
V21N05-01.tex,,A Generative Dependency N-gram Language Model: Unsupervised Parameter Estimation and Application,,We design a language model based on a generative dependency structure for sentences. The parameter of the model is the probability of a {\em dependency N-gram,"['Introduction', 'Related Work', 'Generative Dependency Model', 'Parameter Estimation', 'Experiments', 'Application to Microblog Data', 'Conclusion and Future Work', 'Calculation of the Estimation Algorithm']",,,,,,,,
V21N05-02.tex,,Unsupervised Domain Adaptations for Word Sense Disambiguation by Learning under Covariate Shift,"本論文では語義曖昧性解消(Word Sense Disambiguation，WSD)の教師なし領域適応の問題
に対して，共変量シフト下の学習を試みる．共変量シフト下の学習では
確率密度比 $w({\bm x","In this paper, we apply the learning under covariate shift
to the problem of unsupervised domain adaptation for word sense disambiguation (WSD).
This learning is a type of weighted learning method, 
in which the probability density ratio $w({\bm x","['はじめに', '関連研究', '期待損失最小化に基づく共変量シフト下の学習', '確率密度比の算出', '実験', '考察', 'おわりに']",,,,,,,,
V21N05-03.tex,,Incremental Word Re-Ordering and Article Generation: Its Application to Japanese-to-English Machine Translation,"本稿では，機械翻訳の単語並べ替え問題に
シフトリデュース構文解析法を応用するための手法を提案する．
提案手法では，単一言語の Inversion Transduction 文法によって
単語並べ替え問題を定式化する．また，日本語文と英語文との
単語対応をとりやすくするため，あらかじめ除去した英冠詞を
翻訳結果へ挿入する問題も単語並べ替えと同時に定式化する．
提案法を日英特許翻訳に適用したところ，
句に基づく統計的機械翻訳の BLEU スコア 29.99 に対して，
$+3.15$の改善が得られた．","This paper introduces a novel word re-ordering model
for statistical machine translation
that employs a shift-reduce parser
for inversion transduction grammars.
The proposed model also solves article generation problems
simultaneously with word re-ordering.
We applied it
to the post-ordering
of phrase-based machine translation (PBMT)
for Japanese-to-English patent translation tasks.
Our experimental results suggest that our
method achieves a significant improvement
of $+3.15$ BLEU scores
against $29.99$ BLEU scores of
the baseline PBMT system.","['はじめに', '構文解析による事後並べ替え', 'シフトリデュース構文解析による単語並べ替えと冠詞生成', '実験', '関連文献', 'まとめと今後の課題']",,,,,,,,
V21N05-04.tex,,Introduction to Combinatorial Optimization: Model Building in Integer Programming,"線形計画問題において変数が整数値を取る制約を持つ整数計画問題は，産業や学術の幅広い分野における現実問題を定式化できる汎用的な最適化問題の1つであり，最近では分枝限定法に様々なアイデアを盛り込んだ高性能な整数計画ソルバーがいくつか公開されている．
しかし，整数計画問題では線形式のみを用いて現実問題を記述する必要があるため，数理最適化の専門家ではない利用者にとって現実問題を整数計画問題に定式化することは決して容易な作業ではない．
本論文では，数理最適化の専門家ではない利用者が現実問題の解決に取り組む際に必要となる整数計画ソルバーの基本的な利用法と定式化の技法を解説する．","The integer programming (IP) model is a general-purpose optimization model that can formulate a surprisingly wide class of real applications using integer variables in linear programming (LP) models.
Recent development in IP software systems has significantly improved our ability to solve large-scale instances.
However, it is still difficult for most non-expert users to formulate real applications into IP models, because all conditions need to be written in linear inequalities.
This paper demonstrates how to use IP software systems and formulate real applications into IP models.","['はじめに\\label{sec:introduction', '線形計画問題と整数計画問題', '整数計画問題の応用事例', '整数計画ソルバーを利用する', '線形計画問題に定式化する', '整数計画問題に定式化する', '最適解が求められない場合の対処法', 'おわりに']",,,,,,,,
V21N06-01.tex,,Noise-aware Character Alignment for Extracting Transliteration Fragments,,"This paper proposes a novel noise-aware character alignment method
for automatically extracting transliteration fragments in phrase pairs that are extracted from parallel corpora.
The proposed method extends a many-to-many Bayesian character alignment method
by distinguishing transliteration (signal) parts from non-transliteration (noise) parts.
The model can be trained efficiently by a state-based blocked Gibbs sampling algorithm with signal and noise states.
The proposed method bootstraps statistical machine transliteration
using the extracted transliteration fragments to train transliteration models.
In experiments using Japanese-English patent data,
the proposed method was able to extract transliteration fragments with much less noise than an IBM-model-based baseline,
and achieved better transliteration performance
than sample-wise extraction in transliteration bootstrapping.","['Introduction', 'Related Work', 'Bayesian Many-to-many Alignment', 'Proposed Method', 'Experiments', 'Conclusion']",,,,,,,,
V21N06-02.tex,,Usage Analysis of Old-Fashioned Words based on the Balanced Corpus of Contemporary Written Japanese,従来の紙版の国語辞典はコンパクトにまとめることが優先され，用例の記述は厳選され，必要最小限にとどめられていた．しかし，電子化編集が容易になり，電子化された国語辞典データや種々のコーパスが活用できるようになった今，豊富な用例を増補した電子化版国語辞典の構築が可能になった．そうした電子化版国語辞典は，人にも計算機にも有用性の高いものと期待される．著者らはその用例記述の際に見出し語のもつ文体的特徴を明記する方法を提案し，より利用価値の高い，電子化版の「コーパスベース国語辞典」の構築を目指している．文体的特徴の記述は，語の理解を助け，文章作成時にはその語を用いる判断の指標になり得るため，作文指導や日本語教育，日本語生成処理といった観点からの期待も高い．本論文では，古さを帯びながらも現代語として用いられる「古風な語」を取り上げる．これに注目する理由は，三点ある．一点目は，現代語の中で用いられる「古風な語」は少なくないにも関わらず，「古語」にまぎれ辞書記述に取り上げ損なってしまう危険性のあるものであること．二点目は，その「古風な語」には，文語の活用形をもつなど，その文法的な扱いに注意の必要なものがあること．三点目は，「古さ」という文体的特徴を的確かつ，効果的に用いることができるよう，十分な用法説明が必要な語であるということ，である．そこで，本論文では，これら三点に留意して「古風な語」の用法をその使用実態に即して分析し，その辞書記述を提案する．はじめに，現行国語辞典5種における「古風な語」の扱いを概観する．次に，「古風な語」の使用実態を『現代日本語書き言葉均衡コーパス』に収録される図書館サブコーパスを用いて分析し，「古風な語」の使用を，(1) 古典の引用，(2) 明治期から戦前まで，(3) 時代・歴史小説，(4) 現代文脈，に4分類する．そして，その 4 分類に基づく「コーパスベース国語辞典」の辞書記述方法を提案する．このような辞書記述は例えば，作文指導や日本語教育，日本語生成処理の際の語選択の参考になるものと期待される．,"\pretolerance=10000
A fundamental issue in compiling a Japanese dictionary is selecting lexical 
entries and describing the meaning(s) and use cases for the selected 
entries. Because some old-fashioned Japanese words continue to be used even 
now, modern Japanese dictionaries usually include certain old-fashioned 
words. However, up to now, no systematic study has investigated the 
selection and usage description of old-fashioned words. Therefore, here we 
first review five already-published modern Japanese dictionaries and clarify 
the characteristics and variations among them. Subsequently, we propose four 
categories of old-fashioned Japanese words in terms of the nature and 
chronological features of the text where those words appear. According to 
the categorization, we analyze the use cases of the old-fashioned words in 
the ``Balanced Corpus of Contemporary Written Japanese.'' Finally, we 
discuss a systematic methodology of lexical description for such entries, 
with a typical example.","['はじめに', '「古風な語」の現行辞典での扱い', '「古風な語」の従来の国語辞典における記述', '「古風な語」のコーパス分析', 'コーパス分析を活かした辞書記述', 'おわりに']",,,,,,,,
V21N06-03.tex,,Language-independent Approach to High Quality Dependency Selection from Automatic Parses,,"Many knowledge acquisition tasks are tightly dependent on fundamental
  analysis technologies, such as part of speech (POS) tagging and parsing. Dependency
  parsing, in particular, has been widely employed for the acquisition of
  knowledge related to predicate-argument structures. For such tasks,
  the dependency parsing performance can determine quality of acquired knowledge, regardless of target
  languages. Therefore, reducing dependency parsing errors and selecting
  high quality dependencies is of primary importance. In this study, we
  present a language-independent approach for automatically selecting
  high quality dependencies from automatic parses. By considering several
  aspects that affect the accuracy of dependency parsing, we created a
  set of features for supervised classification of reliable
  dependencies. Experimental results on seven languages show that our
  approach can effectively select high quality dependencies from
  dependency parses.","['Introduction', 'Related Work', 'High Quality Dependency Selection', 'Main Experiments', 'Additional Experiments', 'Discussion', 'Conclusion and Future Work']",,,,,,,,
V21N06-04.tex,,A Simple Approach to Unknown Word Processing \\ in Japanese Morphological Analysis,"本論文では，形態素解析で使用する辞書に含まれる語から派生した表
記，および，未知オノマトペを対象とした日本語形態素解析における効率的な未
知語処理手法を提案する．提案する手法は既知語からの派生ルールと未知オノマ
トペ認識のためのパターンを利用し対象とする未知語の処理を行う．Webから収集
した10万文を対象とした実験の結果，既存の形態素解析システムに提案手法を導
入することにより新たに約4,500個の未知語を正しく認識できるのに対し，解析が
悪化する箇所は80箇所程度，速度低下は6\%のみであることを確認した．","This paper presents a simple but effective approach to
unknown word processing in Japanese morphological analysis, which
handles 1) unknown words that are derived from words in a pre-defined
lexicon and 2) unknown onomatopoeias. Our approach leverages derivation
rules and onomatopoeia patterns, and correctly recognizes certain types
of unknown words. Experiments revealed that our approach recognized
about 4,500 unknown words in 100,000 Web sentences with only roughly 80
harmful side effects and a 6\% loss in speed.","['はじめに', '日本語形態素解析', '提案手法', '実験と考察', 'まとめ', '濁音化した形態素の生起コスト', '長音記号・小書き文字が置換・挿入された形態素の生起コスト', '反復型オノマトペの生起コスト', '非反復型オノマトペの生成に使用した平仮名，片仮名の一覧']",,,,,,,,
V21N06-05.tex,,Automatic Knowledge Acquisition for Case Alternation between the Passive/Causative and Active Voices,"日本語において受身文や使役文を能動文に変換する際，格交替が起こ
る場合がある．本論文では，対応する受身文・使役文と能動文の格の用例や分布
の類似性に着目し，Webから自動構築した大規模格フレームと，人手で記述した少
数の格の交替パターンを用いることで，受身文・使役文と能動文の表層格の対応
付けに関する知識を自動獲得する手法を提案する．さらに，自動獲得した知識を
受身文・使役文の能動文への変換における格交替の推定に利用することによりそ
の有用性を示す．","We propose a method for automatically acquiring knowledge
 about case alternations between the passive/causative and active
 voices. Our method leverages large lexical case frames obtained from a
 large Web corpus, and several alternation patterns. We then use the
 acquired knowledge to a case alternation task and show its usefulness.","['はじめに', '関連研究', '格の交替パターン', 'Webから自動構築した大規模格フレーム', '格フレームの対応付け', '自動獲得した対応付け知識の評価', 'おわりに']",,,,,,,,
V22N01-01.tex,,Predicate-Argument Structure Analysis and Zero-Anaphora Resolution for Dialogue Analysis,"本稿では，日本語を対象とした対話用述語項構造解析を提案する．従来，述語
項構造解析は，主に新聞記事を対象に研究されてきた．新聞と対話ではさまざ
まな違いが存在するが，本稿ではこれを包括的に扱うため，対話用述語項構造
解析器の構築を，新聞から対話への一種のドメイン適応とみなす．具体的には，
対話では省略や代名詞化が新聞記事に比べて頻繁に現れるため，ゼロ代名詞照
応機能付きの述語項構造解析をベースとし，これを対話に適応させる．パラメー
タ適応と，訓練コーパスがカバーしきれない語彙知識を大規模平文コーパスか
ら自動獲得することにより，新聞記事用のものに比べ，対話に対して高精度な
述語項構造解析を実現した．","This paper presents a predicate-argument structure (PAS) analysis for
dialogue systems in Japanese.  Conventional PAS analyses have been
applied to newspaper articles; however, there are differences between
newspapers and dialogues.  Therefore, to comprehensively deal with
these differences, we constructed a PAS analyzer for dialogues as a
type of domain adaptation from newspapers.  Because pronominalization
and ellipses frequently appear in dialogues, we utilized a strategy
that simultaneously resolves zero-anaphora and adapts our PAS analyzer
to dialogues.  By incorporating parameter adaptation and automatically
acquiring knowledge from large text corpora, we performed a PAS
analysis that is specific to dialogues.  Our PAS analyzer has a higher
accuracy compared with the analyzer for newspaper articles.","['はじめに', '関連研究', '雑談対話の特徴', 'ゼロ代名詞照応付き述語項構造解析', '雑談対話への適応', '実験', 'まとめ']",,,,,,,,
V22N01-02.tex,,Identification of Cross-Document Sentence Relations \\ from Document Pairs,"本論文では，手紙文書とそれに対する応答文書など対となる二つの文書間における文レベルでの対応関係を推定する課題を提案し，解決手法を検討する．これまで，単一の文書内における文同士の関係や対話における発話同士の関係を対象とした研究は盛んに行われて来たのに対し，二文書間における文書を跨いだ文対応関係にはあまり注目されて来なかった．このような関係の例として，質問と応答，依頼と回答などが挙げられる．文対応関係を用いることで文書によるコミュニケーションをより細かい単位で説明できることから，本関係の推定が実現すれば様々な応用が期待できる．一例として，文書対の群から対応を持つ文を抽出すれば，各文書対でどのようなコミュニケーションが行われているかを提示することが可能となる．我々は文対応関係の自動推定を実現するため，本課題を文対応の有無を判定する分類問題とみなして条件付確率場を用いる手法を提案する．具体的には，推定した文の種類を文対応推定に活用する対話文書を対象とした従来手法を，本論文の課題に適用する手法を示す．加えて，文種類の推定と文対応の推定を同時に行う拡張モデルによる手法を提案する．実際の宿泊予約ウェブサイトにおけるレビュー・返答対を対象とした評価実験の結果，拡張モデルは拡張前のモデルよりも高い性能である適合率 46.6\%, 再現率 61.0\%の推定性能を得た．","We propose a novel task that identifies cross-document sentence relations from document pairs. Although there are numerous studies that focus on finding sentence relations from just one document or conversation, only few studies are proposed for cross-documents. Examples of cross-document sentence relations are question--answer relations, request--response relations, and so on. Finding such relations will lead to many applications since the cross-document sentence relations are useful to explain document-based conversations on a more fine-grained level. For instance, we can extract communications from cross-documents by accumulating sentences having relations. To detect such relations, we regard this task as the classification problem and employ the conditional random fields. In particular, we modify a previous method that focuses on finding relations from conversations using sentence types to our task. Furthermore, we propose a combined model that simultaneously estimates sentence types and relations. The experiments are performed on review and reply on an internet service for hotel reservation, and the results show that our proposed model achieves 46.6\% precision and 61.0\% recall, which outperforms previous models.","['はじめに', '関連研究', '提案手法', '評価実験', 'まとめと今後の課題']",,,,,,,,
V22N02-01.tex,,Vocabulary Expansion of Medical Language Resources for Medical Information Extraction,"近年，医療文書の電子化が進み，大規模化する医療データから有用な情報を
  抽出・活用する技術が重要となっている．特に，診療記録中の症状名や診断
  名などの用語を自動抽出する技術は，症例検索などを実現する上で必要不可
  欠である．機械学習に基づく用語抽出では，辞書などの語彙資源の利用が訓
  練データに含まれない用語の認識に有効である．しかし，診療記録では多様
  な構成語彙の組合せからなる複合語が使用されるため，単純なマッチングに
  基づく辞書の利用では検出できない用語が存在し，語彙資源利用の効果は限
  定的となる．そこで，本稿では，語彙資源を有効活用した用語抽出を提案す
  る．資源活用の1点目として，資源中の用語に対して語彙制限を行うことで，
  用語抽出に真に有用な語彙の獲得を行う．2点目として，資源から複合語の構
  成語彙である修飾語を獲得し，元の語彙に加えて獲得した修飾語を活用する
  ことで，テキスト中のより多くの用語を検出する拡張マッチングを行う．検
  出された用語の情報は機械学習の素性として用いる．NTCIR-10 MedNLPテスト
  コレクションを用いた抽出実験の結果，単純な語彙資源の利用時と比較して
  適合率および再現率の向上を実現し，本手法の有効性を確認した．また，肯
  定・否定などのモダリティ属性の分類を含めた抽出では，従来手法に対して，
  本手法が最も高い精度を実現した．","With the increasing number of medical documents written in an electronic 
format, automatic term extraction technologies from unstructured texts have 
become increasingly important. 
 Particularly, the extraction of medical terms such as complaints and 
diagnoses from medical records is crucial because they serve as the basis 
for more application-oriented tasks, including medical case retrieval.
 For machine-learning-based term extraction, language resources such as 
lexica and corpora are effective for recognizing expressions that rarely 
or do not occur in training data.
 However, the use of lexica by simple word-matching approaches has limited 
effects because there are compound words that comprise various combinations 
of constituent terms in medical records.
 Therefore, this study presents term extraction systems that can exploit 
language resources by the acquisition and utilization of beneficial terms 
and constituents from the resources.
 Our experimental results on the NTCIR-10 MedNLP test collection, which 
comprises medical history summaries, show increased precision and recall, 
indicating the effectiveness of the proposed system.
 Moreover, compared to existing systems developed for the NTCIR-10 MedNLP 
task, the proposed system achieved optimum performance for complaint and 
diagnosis recognition, including the classification of extracted terms 
into modality attributes.","['はじめに', '関連研究', 'Linear-chain CRFに基づく症状・診断名抽出システム', '医療用語資源の語彙拡張と症状・診断名抽出への利用', '評価実験', 'おわりに', 'モダリティ素性における正規表現', '修飾語の限定に用いたひらがな表現']",,,,,,,,
V22N02-02.tex,,Definition of Recipe Terms and Corpus Annotation for their Automatic Recognition,"自然言語処理において，単語認識（形態素解析や品詞推定など）の次に実用化可
能な課題は，ある課題において重要な用語の認識であろう．この際の重要な用
語は，一般に単語列であり，多くの応用においてそれらに種別がある．一般的
な例は，新聞記事における情報抽出を主たる目的とした固有表現であり，人名や組
織名，金額などの7つか8つの種別（固有表現クラス）が定義されている．この
重要な用語の定義は，自然言語処理の課題に大きく依存する．
我々は
この課題をレシピ（調理手順の文章）に対する用語抽出として，
レシピ中に出現する
重要な用語を定義し，実際にコーパスに対してアノ
テーションし，実用的な精度の自動認識器を構築する過程について述べる．
その応用として，
単純なキーワード照合を超える知的な検索や，映像と言語表現のマッチ
ングによるシンボルグラウンディングを想定している．
このような背景の下，
本論文では，レシピ用語タグセットの定義と，実際に行ったアノテーショ
ンについて議論する．
また，レシピ用語の自動認識の結果を提示し，必要
となるアノテーション量の見通しを示す．","In natural language processing (NLP), recognizing important terms after word recognition (word segmentation, part-of-speech tagging, etc.) is practical.
In general, terms are word sequences and are classified into different types in many applications.
A famous example is the named entity that aims to extract information from newspaper articles.
This has seven or eight types (named entity classes) such as person name, organization name and amount of money.
The definition of important terms depends heavily on the NLP task.
We chose term extraction from recipes (cooking procedure texts) as our task.
We discuss a process to define terms and types, annotate corpus, and construct a practically accurate automatic recognizer of recipe terms.
The recognizer can potentially be applied to search functions that are more intelligent than simple keyword match and symbol grounding researches, 
wherein we can match videos and language expressions.
Based on these backgrounds, in this study, we discuss the definition of a tag set for recipe terms and real annotation work.
Furthermore, we present the experimental results of automatic recognition of recipe terms and provide an insight into the number of annotations required for realizing a certain degree of accuracy.","['はじめに', '関連研究', 'レシピ用語タグセットの定義', 'レシピ用語の自動認識', '実際のアノテーション作業とその考察', 'おわりに']",,,,,,,,
V22N03-01.tex,,Parallel Sentence Extraction Based on Unsupervised Bilingual Lexicon Extraction from Comparable Corpora,,"Parallel corpora are crucial for statistical machine translation (SMT); however, they are 
quite scarce for most language pairs and domains. As comparable corpora are far more available,
many studies have been conducted to extract parallel sentences from them for SMT. 
Parallel sentence extraction relies highly on bilingual lexicons that are also very scarce. 
We propose an unsupervised bilingual lexicon extraction based parallel sentence extraction system
that first extracts bilingual lexicons from comparable corpora and then extracts parallel sentences
using the lexicons. Our bilingual lexicon extraction method is based on a combination of topic model and context 
based methods in an iterative process. The proposed method does not rely on any prior knowledge,
and the performance can be improved iteratively. 
The parallel sentence extraction method uses a binary classifier for parallel 
sentence identification. The extracted bilingual lexicons are used for the classifier to improve
the performance of parallel sentence extraction.
Experiments conducted with the Wikipedia data indicate
that the proposed bilingual lexicon extraction method greatly outperforms existing 
methods, and the extracted bilingual lexicons significantly improve the performance of parallel 
sentence extraction for SMT.","['Introduction', 'Related Work', 'Bilingual Lexicon Extraction Based Parallel Sentence Extraction System', 'Experiments', 'Conclusion']",,,,,,,,
V22N03-02.tex,,Stacking Approach to Temporal Relation Classification\\ with Temporal Inference,,"Traditional machine-learning-based approaches to temporal relation classification use only local features, i.e., those relating to a specific pair of temporal entities (events and temporal expressions), and thus fail to incorporate useful information that could be inferred from nearby entities.
In this paper, we use timegraphs and stacked learning to perform temporal inference for classification in the temporal relation classification task.
In our model, we predict a temporal relation by considering the consistency of possible relations between nearby entities. 
Performing 10-fold cross-validation on the Timebank corpus, we achieve an F1 score of 60.25\% using a graph-based evaluation, which is 0.90 percentage points higher than that of the local approach, outperforming other proposed systems.","['Introduction', 'Temporal Relation Classification', 'Stacked model for temporal relation classification', 'Experimental evaluation and results', 'Discussion', 'Conclusion']",,,,,,,,
V22N03-03.tex,,Summarizing a Document by Trimming a Nested Tree Structure,"近年の抽出型要約の多くの手法は，原文書の情報を網羅し，かつ与えられる要約長の制約
に柔軟に対応すべく，文抽出と文圧縮を併用した組み合わせ最適化問題として要約を定式
化している．
つまり，文書から文という文法的な単位を維持するよう単語を抽出することで要約を生成
している．
従来の手法は非文の生成を避けるため，構文木における単語間の関係を利用して文を圧縮
しているものの，文書における大域的な文と文の間の関係，つまり談話構造には着目して
こなかった．しかし，談話構造を考慮することは要約の一貫性を保つ上で非常に重要で
あり，文書の重要箇所の同定にも役立つ．
我々は，文書を文間の依存関係，単語間の依存関係をあらわした入れ子依存木とみなし，
単語重要度の和が最大となるように木を刈り込むことで要約を生成する手法を提案する．
実験の結果，提案手法が要約精度を有意に向上させたことが確認できた．","Many methods of text summarization that have recently been proposed combine
sentence selection and sentence compression. Although the dependency between
words has been used in most of these methods, the dependency between sentences,
i.e., the rhetorical structure, has not been exploited in such joint methods. We
use both the dependency between words and the dependency between sentences by
constructing a nested tree, in which nodes in a document tree representing the
dependency between sentences were replaced by a sentence tree representing the
dependency between words. We formulate a summarization task as a combinatorial
optimization problem, in which the nested tree is trimmed without losing
important content in the source document. The results from an empirical
evaluation revealed that our method based on the trimming of the nested tree
significantly improved the performance of text summarization.","['はじめに', '関連研究', '入れ子依存木の刈り込みによる要約文書生成', '評価実験', 'まとめ']",,,,,,,,
V22N04-01.tex,,Retrieval Term Prediction Using Deep Belief Networks,"本稿は機械学習を用いて関連語・周辺語または説明文書から適切な検索用語を予測する手法を提案する．機械学習には深層学習の一種である Deep Belief Network (DBN) を用いる．DBN の有効性を確認するために，用例に基づくベースライン手法，多層パーセプトロン (MLP)，サポートベクトルマシン (SVM) との比較を行った．学習と評価に用いるデータは手動と自動の 2 通りの方法でインターネットから収集した．加えて，自動生成した疑似データも用いた．各種機械学習の最適なパラメータはグリッドサーチと交差検証を行うことにより決定した．
実験の結果，DBN の予測精度はベースライン手法よりはるかに高く MLP と SVM のいずれよりも高かった．また，手動収集データに自動収集のデータと疑似データを加えて学習することにより予測精度は向上した．さらに，よりノイズの多い学習データを加えても DBN の予測精度はさらに向上したのに対し，MLP の精度向上は見られなかった．このことから，DBN のほうが MLP よりもノイズの多い学習データを有効利用できることが分かった．","This paper presents a method to predict retrieval terms from relevant/surrounding words or descriptive texts in Japanese by using deep belief networks (DBN), one of two typical types of deep learning. To determine the effectiveness of using DBN for this task, we tested it along with baseline methods using example-based approaches and conventional machine learning methods, i.e., multi-layer perceptron (MLP) and support vector machines (SVM), for comparison. The data for training and testing were obtained from the Web in manual and automatic manners. Automatically created pseudo data was also used. A grid search was adopted for obtaining the optimal hyperparameters of these machine learning methods by performing cross-validation on training data. Experimental results showed that (1) using DBN has far higher prediction precisions than using baseline methods and higher prediction precisions than using either MLP or SVM; (2) adding automatically gathered data and pseudo data to the manually gathered data as training data is an effective
measure for further improving the prediction precisions; and (3) DBN is able to deal with noisier training data than MLP, i.e., the prediction precision of DBN can be improved by adding noisy training data, but that of MLP cannot be.","['はじめに', '関連語・周辺語コーパス', '深層学習', '実験', '本課題の意義について', '結び']",,,,,,,,
V22N04-02.tex,,Left-corner Parsing for Dependency Grammar,,"In this article, we present an incremental dependency parsing algorithm with an arc-eager variant of the left-corner parsing strategy.
Our algorithm's stack depth captures the center-embeddedness of the recognized dependency structure.
A higher stack depth occurs only when processing deeper center-embedded sentences in which people find difficulty in comprehension.
We examine whether our algorithm can capture the syntactic regularity that universally exists in languages through two kinds of experiments across treebanks of 19 languages.
We first show through oracle parsing experiments that our parsing algorithm consistently requires less stack depth to recognize annotated trees relative to other algorithms across languages.
This result also suggests the existence of a syntactic universal by which deeper center-embedding is a rare construction across languages, a result that has yet to be quantitatively cross-linguistically examined.
We further investigate the above claim through supervised parsing experiments and show that our proposed parser is consistently less sensitive to constraints on stack depth bounds when decoding across languages, while the performance of other parsers such as the arc-eager parser is largely affected by such constraints.
We thus conclude that the stack depth of our parser represents a more meaningful measure for capturing syntactic regularity in languages than those of existing parsers.","['Introduction', 'Background', 'Stack Depth of Existing Transition Systems', 'Left-corner Dependency Parsing', 'Empirical Stack Depth Analysis', 'Parsing Experiment', 'Related Work and Discussion', 'Conclusion']",,,,,,,,
V22N04-03.tex,,Recurrent Neural Networks for Word Alignment,"本論文では，隠れ層の再帰的な構造により，過去のアラインメント履歴全体を活用するリカレントニューラルネットワーク (RNN) による単語アラインメントモデルを提案する．
ニューラルネットワークに基づくモデルでは，従来，教師あり学習が行われてきたが，本論文では，本モデルの学習法として，Dyerらの教師なし単語アラインメント\cite{dyer11","This paper proposes a novel word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers. 
In addition, we perform unsupervised learning inspired by \cite{dyer11","['はじめに', '従来の単語アラインメントモデル', 'RNNに基づく単語アラインメントモデル', 'モデルの学習', '評価実験', '考察', 'まとめ']",,,,,,,,
V22N05-01.tex,,Classification of Word Sense Disambiguation Errors Using a Clustering Method,"語義曖昧性解消の誤り分析を行う場合，まずどのような原因からその誤りが生
じているかを調べ，誤りの原因を分類しておくことが一般的である．この分類
のために，分析対象データに対して分析者 7人が独自に設定した誤り原因のタ
イプを付与したが，各自の分析結果はかなり異なり，それらを議論によって統
合することは負荷の高い作業であった．そこでクラスタリングを利用してある
程度機械的にそれらを統合することを試み，最終的に 9種類の誤り原因として
統合した．この 9種類の中の主要な 3つの誤り原因により，語義曖昧性解消の
誤りの 9割が生じていることが判明した．またタイプ分類間の類似度を定義す
ることで，統合した誤り原因のタイプ分類が，各自の分析結果を代表している
ことを示した．また統合した誤り原因のタイプ分類と各自の誤り原因のタイプ
分類を比較し，ここで得られた誤り原因のタイプ分類が標準的であることも示
した．","As a first step of word sense disambiguation (WSD) errors analysis, 
generally we need investigate the causes of errors and classify them. 
For this purpose, seven analysts classified the error data for analysis from
their unique standpoints.  
Next, we attempted to merge the results from the analyses.  
However, merging these results through discussions was difficult
because the results differed significantly.  
Therefore, we  used a clustering method for a certain level of automatic merger.
Consequently, we classified WSD errors into nine types,
and it turned out that the three main types of errors covers 90\% of the total WSD errors.
Moreover, we showed that 
the merged error types represented seven results
and was standardized by defining the similarity between two classifications 
and comparing it with each analysis result.","['はじめに', '分析対象データ', '各人の分析結果', 'クラスタリングを用いた分析結果の統合', '考察', 'おわりに', '誤り分析対象の 50 用例', '各人の誤り原因の一覧']",,,,,,,,
V22N05-02.tex,,\rule{0pt,\rule{0pt,\rule{0pt,"['はじめに', 'コーパス', '誤り分析', '事実性解析', '主体解析', '事実性解析と主体解析', '関連研究', 'おわりに']",,,,,,,,
V22N05-03.tex,,Understanding the Technical Issues in Japanese Factuality Analysis Through Error Analysis,"事実性は，文中の事象の成否について，著者や登場人物の判断を表す情報である．
事実性解析には，機能表現や，文節境界を越えて事実性に影
響を与える語とそのスコープなどの4種類の問題が含まれており，性能の向上が容易ではない．
本研究では，事実性解析の課題分析を行うために，機能表現のみを用いたルールベース
の事実性解析器を構築し，1,533文に含まれる3,734事象に適用した結果の誤りを分析した．
このとき全ての事象表現について，付随する機能表現に対して人手で意味ラベルを付与した．
その結果，主事象の事実性解析については，機能表現の意味ラベル
が正しく解析できれば，現在の意味ラベルの体系と本研究で用いた
単純な規則だけでも，90\%に近い正解率が得られることがわかった．
従属事象の事実性解析では，後続する述語やスコープといった従属事象特有の誤りが多く見られた．
それらの要素についてさらなる分析を行い，今後の事実性解析の指針を示した．","Event factuality is information pertaining to whether events mentioned in the natural language correspond to either actual events that have occurred in the real world or events that are of uncertain interpretation. In factuality analysis, sufficient performance is yet to be achieved because of the complexity of issues such as functional expression and linguistic scope. This paper discusses the issues involved in factuality analysis by analyzing errors when applying a rule-based system to 3,734 events in 1,533 sentences. We annotate functional expression labels for all events. In the main events, the factuality analyzer, consisting of simple functional expression rules, achieves approximately 90\% accuracy if correct functional expression labels are provided. In subordinate events, we found many errors specific to subordinate events, such as errors caused by predicates and linguistic scopes. We provide guidelines for factuality analysis through additional discussion regarding predicates and linguistic scope.","['はじめに', '関連研究', '誤り分析を通した課題分析の方針', '主事象に対する事実性解析', '従属事象における事実性解析', 'おわりに']",,,,,,,,
V22N05-04.tex,,Error Analysis of Argument Ellipsis Completion in Japanese Predicate-argument Structure Analysis,"本稿では，日本語述語項構造解析における中心的課題である項の省略を伴う事例の精度改善を目指し，
現象の特徴を詳細に分析することを試みた．具体的には，文内に照応先が出現する事例（文内ゼロ照応）に対象を絞り，
人手による手がかりアノテーションと統語的・機能的な構造を元にした機械的分類の二種類の方法により事例を類型化し，
カテゴリ毎の分布と最先端のシステムによる解析精度を示した．
分析から，特に照応先と直接係り関係にある述語Oが対象述語Pと項を共有する事例が全体の58\%存在し，
OとPの間の統語的・意味的関係が重要な手がかりであることを数値的に示したほか，
手がかりの種類や組み合わせが広い分布を持つこと，各手がかりが独立に確信度を上げる事例だけでなく，
局所的な手がかりの連鎖が全体で初めて意味を成す事例が一定数存在することを明らかにした．","This paper provides a deep analysis of linguistic phenomena related to argument ellipsis, one of central issues for improving the accuracy of Japanese predicate-argument structure analysis. 
We specifically focus on cases where a target predicate and its ellipsed argument appear in the same sentence, and we categorize instances based on two criteria: a clue annotation by a human annotator and systematic categorization based on both syntactic and semantic structure. 
We then show both the distribution of instances among the categories and the accuracy for each category achieved by a state-of-the-art system. 
As a result, we show that $58\%$ of the intra-sentencial zero anaphora are the case when an argument of a target predicate $P$ is shared with another predicate $O$ that is in a direct syntactic dependency relation with the argument. 
This fact implies that analyzing syntactic and semantic relations between $O$ and $P$ is important for Japanese predicate-argument structure analysis. 
We also show that the distribution of clue combinations is very broad. 
Finally, we discovered that not only are there cases where each clue independently increases the certainty, but we also discovered cases where clues became relevant when all of them composed a chain.","['はじめに', '関連研究', '分析対象', '構造パターンの自動分類による事例カテゴリ分析', '人間の直感にもとづく手がかりアノテーションによる分析', '結論']",,,,,,,,
V23N01-01.tex,,Error Analysis Framework for Automatic Summarization,"本稿では自動要約システムの誤り分析の枠組みを提案する．
この誤り分析の枠組みは，要約が満たすべき3つの要件と誤った要約が生じる5つの原因からなり，要約の誤りをこれらからなる15種類の組み合わせに分類する．
また，システム要約において15種類の誤りのうちどの誤りが生じているかを調査する方法もあわせて提案する．
提案する誤り分析の枠組みに基づき，本稿ではまず，システム要約を分析した結果を報告する．
さらに，分析の結果に基づいて要約システムを改良し，誤り分析の結果として得られる知見を用いてシステムを改良することでシステム要約の品質が改善されることを示す．","We propose an error analysis framework for automatic summarization.
The framework presented herein incorporates five problems that cause automatic summarization systems to produce errors and three metrics for quality.
We classify errors in automatic summaries into 15 categories comprising a combination of the three quality metrics and five problems.
We also present a method to classify automatic-summary errors into these categories.
Using our error analysis framework, we analyze the errors in an automatic summary produced by our system and present the results.
We use these results to refine our system and then show that the quality of the automatic summary is improved.
The error analysis framework that we propose is demonstrably useful for improving the quality of an automatic summarization system.","['はじめに', '基本的な前提', '誤り分析の枠組み', '分析の実践', '分析に基づく要約システムの改良', '関連研究', 'おわりに']",,,,,,,,
V23N01-02.tex,,Error Analysis on Product Attribute Value Extraction,"本稿では商品の属性値抽出タスクにおけるエラー分析のひとつの事例研究
について報告する．
具体的には，属性値辞書を用いた単純な辞書マッチに基づく属性値抽出システ
ムを構築し，人手により属性値がアノテーションされたコーパスに対してシス
テムを適用することで明らかとなる False-positive, False-negative 事例の
分析を行った．属性値辞書は商品説明文に含まれる表や箇条書きなどの半構造
化データを解析することで得られる自動構築したものを用いた．
エラー分析は実際のオンラインショッピングサイトで用いられている5つの商品
カテゴリから抽出した100商品ページに対して行った．そして分析を通してボトム
アップ的に各事例の分類を行ってエラーのカテゴリ化を試みた．
本稿ではエラーカテゴリおよびその実例を示すだけでなく，誤り事例を無くすため
に必要な処理・データについても検討する．","This paper reports error analysis results on the product attribute
value extraction task. We built the system that extracted attribute
values from product descriptions by simply matching the descriptions
and entries in an attribute value dictionary.
The dictionary is automatically constructed by parsing semi-structured data
such as tables and itemizations in product descriptions.
We run the extraction system on the corpus where product attribute
values were annotated by a single subject, and then investigated
false-positives and false-negatives.
We conducted the error analysis procedure on 100 product pages
extracted from five different product categories of an actual
e-commerce site, and designed error type categories according to the
results of the error analysis on those product pages.
In addition to show the error type categories and their instances, we
also discuss processing and data resources required for reducing the
number of error instances.","['はじめに', '分析対象データ', '商品の属性値抽出システム', 'エラー分析', 'おわりに']",,,,,,,,
V23N01-03.tex,,Text Chat Dialogue Corpus Construction and Analysis of Dialogue Breakdown,対話システムが扱う対話は大きく課題指向対話と非課題指向対話（雑談対話）に分けられるが，近年Webからの自動知識獲得が可能になったことなどから，雑談対話への関心が高まってきている．課題指向対話におけるエラーに関しては一定量の先行研究が存在するが，雑談対話に関するエラーの研究はまだ少ない．対話システムがエラーを起こせば対話の破綻が起こり，ユーザが円滑に対話を継続することができなくなる．しかし複雑かつ多様な内部構造を持つ対話システムの内部で起きているエラーを直接分析することは容易ではない．そこで我々はまず，音声誤認識の影響を受けないテキストチャットにおける雑談対話の表層に注目し，破綻の類型化に取り組んだ．本論文では，雑談対話における破綻の類型化のために必要な人・機械間の雑談対話コーパスの構築について報告し，コーパスに含まれる破綻について分析・議論する．,"In general, there are two types of dialogue systems: the task-oriented dialogue system and the non-task-oriented or chat dialogue system. In recent years, chat dialogue systems have received much attention mainly because of the advances in automatic knowledge acquisition from the web. Nevertheless, few studies are dedicated to the error analysis of chat dialogue systems. This is in contrast with the many error-analysis-related studies on task-oriented dialogue systems. An error in a chat dialogue system can lead to the dialogue breakdown, where users are no longer willing to continue the conversation. Therefore, error analysis is crucial in such systems. However, it is difficult to analyze errors in chat dialogue systems because of the complex internal structures of the systems. In the present study, we analyze and categorize the errors in a text chat dialogue system on the basis of the surface form of the conversations. We construct a chat dialogue corpus between a chat system and users and analyze the dialogue breakdowns included in the corpus.","['はじめに', '雑談対話データの収集', '初期アノテーション', '残りの対話へのアノテーション', '対話破綻の類型化', '関連研究', 'おわりに']",,,,,,,,
V23N01-04.tex,,Error Selection Methods for \\ Machine Translation Error Analysis,"複雑化する機械翻訳システムを比較し，問題点を把握・改善するため，誤り分析が利用される．
その手法として，様々なものが提案されているが，多くは単純にシステムの翻訳結果と正解訳の差異に着目して誤りを分類するものであり，人手による分析への活用を目的とするものではなかった．
本研究では，人手による誤り分析を効率化する手法として，機械学習の枠組みを導入した誤り箇所選択手法を提案する．
学習によって評価の低い訳出と高い訳出を分類するモデルを作成し，評価低下の手がかりを自動的に獲得することで，人手による誤り分析の効率化を図る．
実験の結果，提案法を活用することで，人手による誤り分析の効率が向上した．","Error analysis is used to improve accuracy of machine translation (MT) systems.
Various methods of analyzing MT errors have been proposed; however, most of these methods are based on differences between translations and references that are
translated independently by human translators, and few methods have been proposed for manual error analysis.
This work proposes a method that uses a machine learning framework to identify errors in MT output, and improves efficiency of manual error analysis.
Our method builds models that classify low and high quality translations, then identifies features of low quality translations to improve efficiency of the manual analysis.
Experiments showed that by using our methods, we could improve the efficiency of MT error analysis.","['はじめに', '機械翻訳の自動評価と問題点', 'スコアに基づく誤り候補$\\boldsymbol{n', '誤り候補$\\boldsymbol{n', '実験', 'おわりに']",,,,,,,,
V23N01-05.tex,,The Todai Robot Project: \\ Error Analysis on the Results of the Yozemi Center Test,"「ロボットは東大に入れるか」は，大学入試試験問題を計算機で解くという挑戦を通じ，
言語処理を含む AI 諸技術の再統合と，知的情報処理の新たな課題の発見を目指す
プロジェクトである．知的能力の測定を第一目的として設計された入試問題は，
AI 技術の恰好のベンチマークであるとともに，
人間の受験者と機械のエラー傾向を直接比較することが可能である．
本稿では，大手予備校主催のセンター試験形式模試を主たる評価データとして，
各科目の解答システムのエラーを分析し，高得点へ向けた今後の課題を明らかにする
とともに，分野としての言語処理全体における現在の課題を探る．","The Todai Robot Project aims at integrating various AI technologies including
natural language processing (NLP), as well as uncovering novel AI problems that 
have been missed while the fragmentation of the research field, through the development of 
software systems that solve university entrance exam problems.
Being primarily designed for the measurement of human intellectual abilities,
university entrance exam problems serve as an ideal benchmark for AI technologies.
They also enable a quantitative comparison between the AI systems and human test takers.
This paper analyzes the errors made by the software systems on the mock university entrance exams 
hosted by a popular preparatory school.
Based on the analyses, key problems towards higher system performances and the current issues 
in the field of NLP are discussed.","['はじめに', 'センター試験タスクの概要', '英語問題のエラー分析', '国語 評論問題のエラー分析', '数学問題のエラー分析', '物理問題のエラー分析', '世界史・日本史のエラー分析', 'おわりに']",,,,,,,,
V23N02-01.tex,,Sequence Alignment as a Set Partitioning Problem,"2つの系列が与えられたときに，系列の要素間での対応関係を求めること
を系列アラインメントとよぶ．系列アラインメントは，自然言語処理分野においても
文書対から対訳関係にある文のペアを獲得する対訳文アラインメント等に広く利用さ
れる．既存の系列アラインメント法は，アラインメントの単調性を仮定する方法か，
もしくは連続性を考慮せずに非単調なアラインメントを求める方法かのいずれかであっ
た．しかし，法令文書等の対訳文書に対する対訳文アラインメントにおいては，単調
性を仮定せず，かつ対応付けの連続性を考慮できる手法が望ましい．本論文では，あ
る大きさの要素のまとまりを単位として系列の順序が大きく変動する場合にアライン
メントを求めるための系列アラインメント法を示す．手法のポイントは，系列アライ
ンメントを求める問題を組合せ最適化問題の一種である集合分割問題として定式化し
て解くことで，要素のまとまりの発見と対応付けとを同時に行えるようにした点にあ
る．さらに，大規模な整数線形計画問題を解く際に用いられる技法である列生成法を
用いることで，高速な求解が可能であることも同時に示す．","Sequence alignment, which involves aligning elements of two
given sequences, occurs in many natural language processing (NLP) tasks such
as sentence alignment.  Previous approaches for solving sequence alignment
problems in NLP can be categorized into two groups. The first group assumes
monotonicity of alignments; the second group does not assume monotonicity or
consider the continuity of alignments. However, for example, in aligning
sentences of parallel legal documents, it is desirable to use a sentence
alignment method that does not assume monotonicity but can consider
continuity.  Herein, we present a method to align sequences where block-wise
changes in the order of sequence elements exist.  Our method formalizes a
sequence alignment problem as a set partitioning problem, a type of
combinatorial optimization problem, and solves the problem to obtain an
alignment.  We also propose an efficient algorithm to solve the optimization
problem by applying column generation.","['はじめに', '関連研究', '単調性を仮定した対訳文アラインメント法', '集合分割問題に基づく対訳文アラインメントのモデル化', '列生成法', '検証', 'おわりに']",,,,,,,,
V23N02-02.tex,,Hierarchical Annotation and Automatic Error-Type Classification of Japanese Language Learners' Writing,"近年，様々な種類の言語学習者コーパスが収集され，言語教育の調査研究に利用されている．
ウェブを利用した言語学習アプリケーションも登場し，膨大な量のコーパスを収集することも可能になってきている．
学習者が生み出した文には正用だけでなく誤用も含まれており，それらの大規模な誤用文を言語学や教育などの研究に生かしたいと考えている．
日本語教育の現場では，学習者の書いた作文を誤用タイプ別にし，フィードバックに生かしたい需要があるが，大規模な言語学習者コーパスを人手で分類するのは困難であると考えられる．
そのような理由から，本研究は機械学習を用いて日本語学習者の誤用文を誤用タイプ別に分類するというタスクに取り組む．
本研究は，以下の手順で実験を行った．
まず，誤用タイプが付与されていない既存の日本語学習者コーパスに対し，誤用タイプ分類表を設計し，誤用タイプのタグのアノテーションを行った．
次に，誤用タイプ分類表の階層構造を利用して自動分類を行う階層的分類モデルを実装した．
その結果，誤用タイプの階層構造を利用せず直接多クラス分類を行うベースライン実験より13ポイント高い分類性能を得た．
また，誤用タイプ分類のための素性を検討した．
機械学習のための素性は，単語の周辺情報，依存構造を利用した場合をベースライン素性として利用した．
言語学習者コーパスの特徴として，誤用だけではなく正用も用いることができるため，拡張素性として正用文と誤用文の編集距離，ウェブ上の大規模コーパスから算出した正用箇所と誤用箇所の置換確率を用いた．
分類精度が向上した誤用タイプは素性によって異なるが，全ての素性を使用した場合は分類精度がベースラインより6ポイント向上した．","Recently, various types of learner corpora have been compiled and utilized for linguistic and educational research. 
As web-based application programs have been developed for language learners, we can now collect a large amount of language learners' output on the web. 
These learner corpora include not only correct sentences but also incorrect ones, and we aim to take advantage of the latter for linguistic and educational research. 
To this end, this study aims to automatically classify incorrect sentences written by learners of Japanese according to error types (or classes) by a machine-learning method.
First, we annotate a corpus of the learners' writing with error types defined in a tree-structured class set. 
Second, we implement a hierarchical error-type classification model using the tree-structured class set. 
As a result, the proposed method performs better in the error-classification task than in the flat-structured multiclass classification baseline model by 13 points. 
Third, we explore features for error-type classification tasks. 
We use contextual information and syntactic information, such as dependency relations, as the baseline features. 
In addition, because a corpus of language learners contains not only correct sentences but also incorrect ones, we propose two extended features: the edit distance between correct usages and incorrect ones and the substitution probability at which characters in a sequence change to other characters.
Although the performance varies according to error types, the proposed model with all features outperforms the model with the baseline features by six points.","['はじめに', '関連研究', '機械学習による誤用タイプ分類実験', '実験結果', '分類実験に関する考察', 'おわりに', '誤用タイプ項目', '誤用タイプ76項目']",,,,,,,,
V23N03-01.tex,,Chinese Word Segmentation and Unknown Word Extraction by Mining Maximized Substring,,"Chinese word segmentation is an initial and important step in Chinese language processing. Recent advances in machine learning techniques have boosted the performance of Chinese word segmentation systems, yet the identification of out-of-vocabulary words is still a major problem in this field of study. Recent research has attempted to address this problem by exploiting characteristics of frequent substrings in unlabeled data. We propose a simple yet effective approach for extracting a specific type of frequent substrings, called maximized substrings, which provide good estimations of unknown word boundaries. In the task of Chinese word segmentation, we use these substrings which are extracted from large scale unlabeled data to improve the segmentation accuracy. The effectiveness of this approach is demonstrated through experiments using various data sets from different domains. In the task of unknown word extraction, we apply post-processing techniques that effectively reduce the noise in the extracted substrings. We demonstrate the effectiveness and efficiency of our approach by comparing the results with a widely applied Chinese word recognition method in a previous study.","['Introduction', 'Maximized Substring Extraction', 'Chinese Word Segmentation', 'Unknown Word Extraction', 'Evaluation', 'Comparison to Related Work', 'Conclusion']",,,,,,,,
V23N03-02.tex,,Boosting Abductive Reasoning with Functional Literals,"仮説推論は，与えられた観測に対する最良の説明を見つける推論の枠組みである．
仮説推論は80年代頃から主に人工知能の分野で長らく研究されてきたが，近年，
知識獲得技術の成熟に伴い，大規模知識を用いた仮説推論を実世界の問題へ適用
するための土壌が徐々に整いつつある．しかしその一方で，大規模な背景知識を用いる際に生じる
仮説推論の計算負荷の増大は，重大な問題である．特に言語の意味表示上の依
存関係を表すリテラル（本論文では機能リテラルと呼ぶ）が含まれる場合に生じる
探索空間の爆発的増大は，実問題への仮説推論の適用において大きな障害となっ
ている．
これに対し本論文では，機能リテラルの性質を利用して探索空間の枝刈りを行う
ことで，効率的に仮説推論の最適解を導く手法を提案する．具体的には，意味的
な整合性を欠いた仮説を解空間から除外することで，推論全体の計算効率を向上
させる．また，このような枝刈りが，ある条件が満たされる限り本来の最適解を
損なわないことを示す．
評価実験では，実在の言語処理の問題に対して，大規模背景知識を用いた仮説
推論を適用し，その際の既存手法との計算効率の比較を行った．その結果として，
提案手法が既存のシステムと比べ，数十〜数百倍ほど効率的に最適解が得られて
いることが確かめられた．","Abduction is also known as Inference to the Best Explanation. It has long been
considered as a promising framework for natural language processing (NLP). While
recent advances in the techniques of automatic world knowledge
acquisition warrant developing large-scale knowledge bases,
the computational complexity of abduction hinders its application to
real-life problems. In particular, when a knowledge base contains
functional literals, which express the dependency relation between
words, the size of the search space will substantially increase.
In this study, we propose a method to enhance the efficiency of first-order
abductive reasoning. By exploiting the property of functional literals, the proposed
method prunes inferences that do not lead to reasonable
explanations. Furthermore, we prove that the proposed method is sound under a
particular condition.
In our experiment, we apply abduction having a large-scale knowledge
base to a real-life NLP task. We show that our method significantly
improves the computational efficiency of first-order abductive reasoning
when compared with a state-of-the-art system.","['はじめに', '背景', '関係を表すリテラルに起因する計算非効率性', '等価仮説への制約による効率化', 'A*-based Abduction の効率化', '実験', 'まとめ', '提案手法が解の最適性を保持することの証明']",,,,,,,,
V23N03-03.tex,,A Generalized Dependency Tree Language Model for SMT,,"In this paper we describe a generalized dependency tree language model for machine
translation. We consider in detail the question of how to define tree-based $n$-grams,
or `$t$-treelets',
and thoroughly explore the strengths and weaknesses of our approach by evaluating
the effect on translation quality
for nine major languages. In addition, we show that it is possible to attain a significant
improvement in translation quality for even non-structured machine translation
by reranking filtered parses of $k$-best string output.","['Introduction', 'Related Work', 'Model Details', 'Experimental Setup', 'Optimization of Model Parameters', 'Final Evaluation and Error Analysis', 'Conclusion and Future Work', 'Example Sentences (Improved)', 'Example Sentences (Worsened)']",,,,,,,,
V23N04-01.tex,,Unsupervised Word Alignment Using Frequency Constraint in Posterior Regularized EM,,"Generative word alignment models, such as IBM Models, are restricted to one-to-many alignment, and cannot explicitly represent many-to-many relationships in bilingual texts.
The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other.
However, this constraint cannot take into account the grammatical difference of language pairs.
In particular, function words are not trivial to align for grammatically different language pairs, such as Japanese and English.
In this paper, we focus on the posterior regularization framework \cite{ganchev2010posterior","['Introduction', 'Related Work', 'Statistical Word Alignment with Posterior Regularization Framework', 'Posterior Regularization with Frequency Constraint', 'Experiment', 'Analysis', 'Conclusion']",,,,,,,,
V23N04-02.tex,,Parser Self-Training for Syntax-Based Machine Translation,"構文情報を考慮する機械翻訳手法である統語ベース翻訳では，構文解析器の精度が翻訳精度に大きな影響を与えることが知られている．
また，構文解析の精度向上を図る手法の一つとして，構文解析器の出力を学習データとして用いる構文解析器の自己学習が提案されている．
しかし，構文解析器が生成する構文木には誤りが存在することから，自動生成された構文木が常に精度向上に寄与するわけではない．
そこで本論文では，機械翻訳における自動評価尺度を用いて，このような誤った構文木を学習データから取り除き，自己学習の効果を向上させる手法を提案する．
具体的には，解析された$n$-best構文木それぞれを用いて統語ベース翻訳を行い，それぞれの翻訳結果に対し，自動評価尺度でリスコアリングする．
この中で，良いスコアを持つ構文木を自己学習に使用することで，構文構造はアノテーションされていないが，対訳が存在するデータを用いて，構文解析・機械翻訳の精度を向上させることができる．
実験により，本手法で自己学習したモデルを用いることで，統語ベース翻訳システムの翻訳精度が2つの言語対で有意に向上し，また構文解析自体の精度も有意に向上することが確認できた．","In syntax-based machine translation, it is known that the accuracy of parsing greatly affects the translation accuracy.
Self-training, which uses parser output as training data, is one method to improve the parser accuracy.
However, because automatically generated parse trees often include errors, these parse trees do not always contribute to improving accuracy.
In this paper, we propose a method for removing noisy incorrect parse trees from the training data to improve the effect of self-training by using automatic evaluation metrics of translations.
Specifically, we perform syntax-based machine translation using $n$-best parse trees, then we re-scoring parse trees based on the automatic evaluation score of translations.
By using the parse trees that have higher score among the candidates for self-training, we can improve parsing and machine translation accuracy by using parallel corpora that are not annotated syntax structure.
In experiments, using higher score parse trees for self-training, we found that our self-trained parsers significantly improve a state-of-the-art syntax-based machine translation system in two language pairs, 
and self-trained parsers significantly improve the accuracy of the parsing itself.","['はじめに', 'Tree-to-String翻訳', '構文解析の自己学習', '統語ベース翻訳のための構文解析器の標的自己学習', '評価', 'おわりに']",,,,,,,,
V23N05-01.tex,,Recognition of Sarcasm in Microblogging Based on Sentiment Analysis and Coherence Identification,,"Recognition of sarcasm in microblogging is important in a range of NLP applications, such as opinion mining.
However, this is a challenging task, as the real meaning of a sarcastic sentence is the opposite of the literal meaning.
Furthermore, microblogging messages are short and usually written in a free style that may include misspellings, grammatical errors, and complex sentence structures.
This paper proposes a novel method for identifying sarcasm in tweets.
It combines two supervised classifiers, a Support Vector Machine (SVM) using N-gram features and an SVM using our proposed features.
Our features represent the intensity and contradictions of sentiment in a tweet, derived by sentiment analysis.
The sentiment contradiction feature also considers coherence among multiple sentences in the tweet, and this is automatically identified by our proposed method using unsupervised clustering and an adaptive genetic algorithm.
Furthermore, a method for identifying the concepts of unknown sentiment words is used to compensate for gaps in the sentiment lexicon.
Our method also considers punctuation and the special symbols that are frequently used in Twitter messaging.
Experiments using two datasets demonstrated that our proposed system outperformed baseline systems on one dataset, while producing comparable results on the other.
Accuracy of 82\% and 76\% was achieved in sarcasm identification on the two datasets.","['Introduction', 'Related Work', 'Proposed Method', 'Evaluation', 'Conclusions']",,,,,,,,
V23N05-02.tex,,Patent Claim Translation based on Sublanguage-specific Sentence Structure,"近年の統計的機械翻訳の進展によって特許文翻訳の精度は大きく向上したが，特許文中で特に重要性の高い特許請求項文に対する翻訳精度は依然として低い．特許請求項文は，(1) 
極めて長い1文から構成される，(2) 
特殊な文構造を持っている，という2つの特徴を持つサブ言語であるとみなせる．そしてこれらが翻訳精度の低さの原因となっている．本論文では，サブ言語に特有の特徴を処理する枠組みの導入によって，特許請求項の翻訳精度を向上させる手法について述べる．提案手法では，同期文脈自由文法を用いて原言語文が持つサブ言語に特有の文構造を目的言語側の文構造に変換することにより，適切な文構造を持った訳文を生成する．さらに本手法では，文全体ではなく，文を構成する構造部品を翻訳の処理単位とすることにより長文の問題に対処する．英日・日英・中日・日中の4翻訳方向で評価実験を行ったところ，全翻訳方向においてRIBES値が25ポイント以上向上し，本手法によって訳文品質が大幅に改善したことがわかった．英日・日英翻訳ではさらにBLEU値が5ポイント程度，中日・日中では1.5ポイント程度向上した．","Patent claim sentences, despite their legal importance in patent documents, 
still pose difficulties for state-of-the-art statistical machine translation 
(SMT) systems owing to their extreme lengths and their special sentence 
structure. This paper describes a method for improving the translation 
quality of claim sentences, by taking into account the features specific to 
the claim sublanguage. Our method overcomes the issue of special sentence 
structure, by transferring the sublanguage-specific sentence structure from 
the source language to the target language, using a set of synchronous 
context-free grammar rules. Our method also overcomes the issue of extreme 
lengths by taking the sentence components to be the processing unit for SMT. 
An experiment demonstrates that our proposed method significantly improves 
the translation quality in terms of RIBES scores by over 25 points, in all 
of the four translation directions i.e., 
English-to-Japanese, Japanese-to-English, Chinese-to-Japanese and 
Japanese-to-Chinese directions. Alongside the improvement in RIBES 
scores, improvements of approximately five points in BLEU scores are 
observed for English-to-Japanese and Japanese-to-English directions, and that of 
1.5 points are observed for Chinese-to-Japanese and Japanese-to-Chinese 
directions.","['はじめに', '関連研究', 'サブ言語に特有の文構造の変換', '請求項翻訳のための処理パイプライン', '評価実験', 'おわりに', '実験に用いたSCFG規則']",,,,,,,,
V23N05-03.tex,,An Investigation of Machine Translation Evaluation Metrics in Cross-lingual Question Answering,"質問応答システムが高い精度で幅広い質問に解答するためには，大規模な知識ベースが必要である．
しかし，整備されている知識ベースの規模は言語により異なり，小規模の知識ベースしか持たない言語で高精度な質問応答を行うためには，
機械翻訳を用いて異なる言語の大規模知識ベースを利用して言語横断質問応答を行う必要がある．
ところが，このようなシステムでは機械翻訳システムの翻訳精度が質問応答の精度に影響を与える．
一般的に，機械翻訳システムは人間が与える評価と相関を持つ評価尺度により精度が評価されている．
そのため，この評価尺度による評価値が高くなるように機械翻訳システムは最適化されている．
しかし，質問応答に適した翻訳結果は，人間にとって良い翻訳結果と同一とは限らない．
つまり，質問応答システムに適した翻訳システムの評価尺度は，人間の直感に相関する評価尺度とは必ずしも合致しないと考えた．
そこで本論文では，複数の翻訳手法を用いて言語横断質問応答データセットを作成し，
複数の評価尺度を用いてそれぞれの翻訳結果の精度を評価する．
そして，作成したデータセットを用いて言語横断質問応答を行い，質問応答精度と翻訳精度との相関を調査する．
これにより，質問応答精度に影響を与える翻訳の要因や，質問応答精度と相関が高い評価尺度を明らかにする．
さらに，自動評価尺度を用いて翻訳結果のリランキングを行うことによって，言語横断質問応答の精度を改善できることを示す．","Through using knowledge bases, question answering (QA) systems have come to be able to answer questions accurately over a variety of topics.
However, knowledge bases are limited to only a few major languages, 
and thus it is often necessary to build QA systems that answer questions in one language based on an information source in another language (cross-lingual QA: CLQA).
Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT system improves QA accuracy.
However, it is not clear whether an MT system that is better for human consumption is also better for CLQA.
In this paper, we investigate the relationship between manual and automatic translation evaluation metrics and CLQA accuracy
by creating a data set using both manual and machine translation, and performing CLQA using this created data set.
As a result, we find that QA accuracy is closely related with a metric that considers frequency of words,
and as a result of manual analysis, we identify two factors of translation results that affect CLQA accuracy.
One is mistranslation of content words and another is lack of question type words.
In addition, we show that using a metric which has high correlation with CLQA accuracy 
 can improve CLQA accuracy by choosing an appropriate translation result from translation candidates.","['はじめに', '本調査の概観', 'データセット作成', '質問応答システム', '実験', 'まとめ']",,,,,,,,
V23N05-04.tex,,On Document Similarity Measures,"文書間類似度は，内容の類似度と表現の類似度の二つの側面を持っている．
自動要約や機械翻訳ではシステム出力の内容評価を行うために参照要約（翻訳）との類似
度を評価する尺度が提案されている．
一方，表現を対照比較するための手段として，形態素（列）を特徴量とする空間上の計量が用いられる．
本稿では，さまざまな文書間類似度について，距離・類似度・カーネル・順序尺度・相関
係数の観点から，計量間の関係や同値性を論じた．
さらに内容の同一性保持を目標として構築したコーパスを用いて，内容の差異と表現の差
異それぞれに対する各計量のふるまいを調査し，文書間類似度に基づく
自動評価の不安定さを明らかにした．","Document similarity measuring techniques are used to evaluate both content and writing style.
Evaluation measures for comparing the summary or translation of a
system-generated source text with that of human-generated text have been proposed in text summarization and machine translation fields.
The distance metrics are measures in terms of morphemes or morpheme sequences 
to evaluate or register different writing styles.
In this study, we discuss the relations among the equivalence properties of mathematical metrics, similarities, kernels, ordinal scales, and correlations.
In addition, we investigate the behavior of techniques for measuring content and
style similarities for several corpora having similar content.
The analysis results obtained using different document similarity measurement
techniques indicate the instability of the evaluate system.","['はじめに', '評価指標と距離・類似度・カーネル・順序尺度・相関係数', '評価に用いる言語資源', '尺度の定性的な分析', 'おわりに', '\\ref{sec:sim', '指標・スコア・距離・カーネル・相関係数の関係まとめ', '言語生成過程と尺度']",,,,,,,,
V23N05-05.tex,,Improving Pivot Translation by Remembering the Pivot,"統計的機械翻訳において，特定の言語対で十分な文量の対訳コーパスが得られない場合，中間言語を用いたピボット翻訳が有効な手法の一つである．
複数のピボット翻訳手法が考案されている中でも，特に中間言語を介して2つの翻訳モデルを合成するテーブル合成手法で高い翻訳精度を達成可能と報告されている．
ところが，従来のテーブル合成手法では，フレーズ対応推定時に用いた中間言語の情報は消失し，翻訳時には利用できない問題が発生する．
本論文では，合成時に用いた中間言語の情報も記憶し，中間言語モデルを追加の情報源として翻訳に利用する新たなテーブル合成手法を提案する．
また，国連文書による多言語コーパスを用いた実験により，本手法で評価を行ったすべての言語の組み合わせで従来手法よりも有意に高い翻訳精度が得られた．","In statistical machine translation, the pivot translation approach allows for translation of language pairs with little or no parallel data by introducing a third language for which data exists.
In particular, the triangulation method, which translates by combining source-pivot and pivot-target translation models into a source-target model is known for its high translation accuracy.
However, in the conventional triangulation method, information of pivot phrases is forgotten, and not used in the translation process.
In this research, we propose a novel approach to remember the pivot phrases in the triangulation stage, and use a pivot language model as an additional information source at translation phase.
Experimental results on the united nations parallel corpus showed significant improvements in all tested combinations of languages.","['はじめに', '統計的機械翻訳', 'ピボット翻訳手法', '同期文脈自由文法におけるテーブル合成手法の応用', '中間言語情報を記憶するピボット翻訳手法の提案', '実験的評価', 'まとめ']",,,,,,,,
V24N01-01.tex,,Active Listening System for a Conversation Robot,高齢者の認知症や孤独感の軽減に貢献できる対話ロボット開発のため，回想法に基づく傾聴を行う音声対話システムの開発を行った．本システムは，ユーザ発話の音声認識結果に基づき，相槌をうったり，ユーザ発話を繰り返したり，ユーザ発話中の述語の不足格を尋ねたりする応答を生成する．さらに，感情推定結果に基づき，ユーザ発話に対して共感する応答を生成する．本システムの特徴は，音声認識結果に誤りが含まれることを前提とし，音声認識信頼度を考慮して応答を生成する点である．110 名の一般被験者に対する評価実験の結果，「印象深い旅行」を話題とした場合で，45.5\% の被験者が 2 分以上対話を継続できた．また，システムの応答を主観的に評価した結果，約 77\% のユーザ発話に対して対話を破綻させることなく応答生成ができた．さらに，被験者へのアンケートの結果，特に高齢の被験者から肯定的な主観評価結果が得られた．,"We have developed an active listening system for a conversation robot, specifically for reminiscing. The aim of the system is to contribute to the prevention of dementia in elderly persons and to reduce loneliness in seniors living alone. Based on the speech recognition results from a user's utterance, the proposed system produces back-channel feedback, repeats the user's utterance and asks information about predicates that were not included in the original utterance. Moreover, the system produces an appropriate empathic response by estimating the user's emotion from their utterances. One of the features of our system is that it can determine an appropriate response even if the speech recognition results contain some errors. Our results show that the conversations of 45.5\% of the subjects (n = 110) with this robot continued for more than two minutes on the topic ``memorable trip''. The system response was deemed correct for about 77\% of user utterances. Based on the results of a questionnaire, positive evaluations of the system were given by the elderly subjects.","['はじめに', '傾聴システムの概要', '音声認識，および，認識信頼度アルゴリズム', '問い返し応答の生成アルゴリズム', '共感応答の生成アルゴリズム', '評価実験', 'まとめ']",,,,,,,,
V24N01-02.tex,,Picture-Book Search System ``Pitarie''\\---Finding Appropriate Books for Each Child---,"本稿では，子どもに「内容」と「読みやすさ」がぴったりな絵本を見つける
  ためのシステム「ぴたりえ」を提案する．本システムは，親や保育士，司書
  など，子どもに絵本を選ぶ大人が利用することを想定している．絵本を読む
  ことは，子どもの言語発達と情操教育の両面で効果が期待できる．しかし，
  難しさも内容も様々な絵本が数多くある中で，子ども1人1人にとってぴった
  りな絵本を選ぶのは容易なことではない．そこで，ぴたりえでは，ひらがな
  の多い絵本のテキストを高精度に解析できる形態素解析や，文字の少ない絵
  本に対しても精度の高いテキストの難易度推定技術などの言語処理技術によ
  り，子どもにぴったりな絵本を探す絵本検索システムを実現する．本稿では，
  こうした言語処理技術を中心にぴたりえの要素技術を紹介し，各技術の精度が高いことを示す．また，システム全体としても，
  アンケート評価の結果，ぴたりえで選んだ絵本は「読みやすさ」も「内容」
  も，5 段階評価で平均値が 4.44〜4.54 と高い評価が得られたことを示
  す．","In this paper, we present a novel picture-book search system
  Pitarie, which can find a picture book that matches a child's
  interests and language developmental stage.  By reading the
  appropriate picture book to children, positive effects such as
  faster language development and enhanced emotional education are
  expected.  Pitarie searches are based on two new natural language
  processing technologies particularly designed for picture books:
  morphological analysis and text readability estimation for sentences
  written mainly in Hiragana script. In this paper, we introduce
  Pitarie with a focus on such novel technologies and their level of
  quality.  Finally, we report the results of the questionnaire for
  the entire system.  Books that were selected based on
  recommendations by Pitarie had an average rating of 4.44--4.54 on a
  5-point evaluation scale from both children's interest and language
  developmental stage viewpoints.","['はじめに', '関連システム', 'システム概要と言語処理', '絵本データベース', '要素技術', 'システム評価', 'まとめ']",,,,,,,,
V24N01-03.tex,,Bilingual KWIC---GUI Translation Support Tool \\Based on Bilingual Expression Extraction,計算機による対訳表現抽出を可視化することにより，対訳辞書の構築や翻訳を支援するツール Bilingual KWIC を開発した．本ツールは，入力されたキーワードに対する対訳表現を自動的に推定し，それらを含む原言語文と対象言語文をそれぞれ KWIC 形式で表示することにより，ユーザの翻訳作業などを支援する．技術的には，形態素解析などを利用せずに文字列情報だけから対訳を抽出するため，どのような言語対にも適用可能であり，さらには単語以外の表現に対しても対訳を表示することが可能である．また対訳表現を KWIC 形式で表示することにより，システムの抽出誤りに対する修正を容易にするだけでなく，派生表現の獲得や複数の対訳表現の比較も可能としている．本稿では，Bilingual KWIC の特徴と開発経緯について述べる．,"This paper proposes a GUI support tool for bilingual dictionary compilation and translation, called ``Bilingual KWIC.'' Bilingual KWIC acquires bilingual expressions from a parallel corpus and displays the result in KWIC format. Displaying in KWIC format enables users to easily correct errors of word alignment and compare two or more types of equivalents. Since Bilingual KWIC does not use morphological information and uses only character-level information, it can deal with any language pairs and can acquire translation equivalents for any input other than words. In this paper, we introduce Bilingual KWIC and describe its features and development process.","['はじめに', 'Bilingual KWICの概要', 'Bilingual KWICの特徴', 'Bilingual KWICの技術的詳細', 'Bilingual KWICの開発', 'ユーザによる評価', '関連研究', 'まとめ']",,,,,,,,
V24N01-04.tex,,Improving User Experience for Recommender System using Diversity,"推薦システムのユーザ体験を高めるために重要な指標の1つが多様性(Diversity)である．
多様性は推薦システムが提示するリスト内には様々なコンテンツが含まれるべきという考え方であり，
過去の研究では多様性が含まれるリストの方がユーザに好まれるとされている．
しかし実際のサービス上で推薦システムを検証したという報告は少なく，
サービス上で多様性がユーザにどのような影響を与えるのかは明らかになっていない．
本研究では実際にサービスとして提供されているウェブページ推薦システムを分析し，
その推薦システムに多様性を導入して比較を行った事例について報告する．
まず多様性が導入されていない推薦システムのユーザ行動を分析し，結果としてリストの中位以降に表示するウェブページに課題があることを明らかにした．
その上で多様性を導入し，多様性のない既存システムとサービス上でのユーザ行動を比較した．
結果として継続率やサービス利用日数が有意に改善していることを示し，
従来研究で示されていた多様性を含む推薦リストの方がユーザに好まれるということを実サービス上で示した．
そして利用日数が増えるに従ってリスト全体のクリック数が改善していくこと，特にリスト下部のクリック率が多様性のない手法では下がっていくのに対して，多様性のある手法では向上していくことを示した．","Diversity is an important indicator for improving user experience in recommender systems. Previous research indicate that people prefer diverse recommended item lists. However, few studies have experimented with online user experience of recommender systems owing to lack of clarity regarding the effects of diversity of recommender systems on user experience. This paper reports the online experience of diversity of web service recommender systems. We analyzed the recommender system without diversity for user activity in web services. As a result, the second half of the recommended list is underwhelming. We have constructed a diverse recommender system by decreasing user features, and have compared our system to the existing system for user activity in web services. Consequently, our system has succeeded in improving the weekly retention and active rates. Therefore, the number of clicks on the recommended list have increased.","['はじめに', '関連研究', 'グノシーの推薦システム', '推薦システムへの多様性の導入', '多様性の導入によるユーザ行動の変化', 'まとめ']",,,,,,,,
V24N01-05.tex,,An FAQ Search Method Using a Document Classifier Trained With Automatically Generated Training Data,"本論文では，ユーザからの自然文による問い合わせを対応するFrequently Asked Question (FAQ) に分類する文書分類器を用いたFAQ検索手法を提案する．
本文書分類器は，問い合わせ中の単語を手掛かりに，対応するFAQを判別する．しかし，FAQの多くは冗長性がないため，FAQを学習データとして文書分類器を作成する方法では，ユーザからの多様な問い合わせに対応するのが難しい．そこで，この問題に対処するために，蓄積されたユーザからの問い合わせ履歴から学習データを自動生成し，文書分類器を作成する．さらに，FAQおよび文書分類用に自動生成した学習データを用いて，通常使われる表層的な手がかりに加えて，
本文書分類器の出力を考慮するランキングモデルを学習する．ある企業のコールセンターの4,738件のFAQおよび問い合わせ履歴54万件を用いて本手法を評価した．
その結果，提案手法が，pseudo-relevance feedbackおよび，統計的機械翻訳のアライメント手法を用いて得られる
語彙知識によるクエリ拡張手法と比較し，高いランキング性能を示した．","We propose an Frequently Asked Question (FAQ) search method that uses a document classifier for classifying a natural language query to a corresponding FAQ. The document classifier classifies a query with words that occur in the query. However, since FAQs have little redundancy, using FAQs as training data for the document classifier is not sufficient for classifying queries that have the similar meaning but different surface expressions. To tackle this problem, our method generates training data automatically from FAQs and corresponding histories and trains the document classifier with them. Furthermore, with the automatically generated training data, our method learns a ranking model that uses classification results of the document classifier. Experimental results on a company FAQs and corresponding histories showed that our method outperformed pseudo-relevance feedback and query expansion model that uses word alignment model in statistical machine translation.","['はじめに', '関連研究', '提案手法', '実験', 'おわりに', 'Elasticsearchで利用したtfidfスコアの計算式']",,,,,,,,
V24N01-06.tex,,An Automatic Occupation and Industry Coding System in Sociology,"社会学では，職業や産業は性別や年齢などと同様に重要な変数であるとの認識から，正確を期するために，自由回答で収集したデータを研究者自身によりコードに変換することが多い．
これは職業・産業コーディングとよばれるが，大規模調査の場合，膨大な労力と時間がかかる上に，結果における一貫性の問題も存在する．
そこで，ルールベース手法と機械学習 (SVM) を適用したコーディング自動化システムを開発した．
本システムは，国内・国際標準の職業・産業コードを第3位まで予測し，第1位の予測コードには，自動コーディング後に人手によるチェックが必要か否かの目安となる3段階の確信度も付与する．
現在，本システムは，東京大学社会科学研究所附属社会調査・データアーカイブ研究センター (CSRDA) からWebによる利用サービスが試行提供されており，研究目的であれば，だれもが指定された形式の入力ファイルをアップロードして，希望するコードに変換された結果ファイルをダウンロードすることができるようになっている．","In sociology, occupation and industry variables are as important as sexual and age variables. For the purpose of statistical processing, answers collected from open-ended questions in social surveys need to be converted into code, which requires considerable time and effort and often results in inconsistencies in large scale surveys. This work deals with occupation and industry coding. In this work, we develop an automatic system using hand-crafted rules and Support Vector Machines. Our system can assign three candidate codes to an answer and estimates the confidence level of the primary predicted code for each national/international standard code sets. The system has now been released through the website of the Center for Social Research and Data Archives. 
The user can get the required coding result by uploading the data file in a specific format.","['はじめに', '自動化システムの変遷', '職業・産業コーディング自動化システム', 'システムの評価', 'システムの利用方法', '関連研究', 'おわりに']",,,,,,,,
V24N02-01.tex,,Automatic Synonym Acquisition Using a Context-Restricted Skip-gram Model,本論文では，分布仮説に基づく同義語獲得を行う際に，周辺単語の様々な属性情報を活用するために，文脈限定 Skip-gram モデルを提案する．既存の Skip-gram モデルでは，学習対象となる単語の周辺単語（文脈）を利用して，単語ベクトルを学習する．一方，提案する文脈限定 Skip-gram モデルでは，周辺単語を，特定の品詞を持つものや特定の位置に存在するものに限定し，各限定条件に対して単語ベクトルを学習する．したがって，各単語は，様々な限定条件を反映した複数の単語ベクトルを所持する．提案手法では，これら複数種類の単語ベクトル間のコサイン類似度をそれぞれ計算し，それらを，線形サポートベクトルマシンと同義対データを用いた教師あり学習により合成することで，同義語判別器を構成する．提案手法は単純なモデルの線形和として構成されるため，解釈可能性が高い．そのため，周辺単語の様々な単語属性が同義語獲得に与える影響の分析が可能である．また，限定条件の変更も容易であり，拡張可能性も高い．実際のコーパスを用いた実験の結果，多数の文脈限定 Skip-gram モデルの組合せを利用することで，単純な Skip-gram モデルに比べて同義語獲得の精度を上げられることがわかった．また，様々な単語属性に関する重みを調査した結果，日本語の言語特性を適切に抽出できていることもわかった．,"This research proposes a context-restricted Skip-gram model for acquiring synonyms by employing various properties of the context words. The original Skip-gram model learned the word vector of each target word by utilizing all the context words around it. In contrast, the proposed context-restricted Skip-gram model learns multiple word vector types of each target word by limiting the context words to those pertaining to specific parts of speech or those present at specific relative positions. The proposed method calculates the cosine similarities on multiple word vector types and combines these similarities using linear support vector machines. The proposed method has high interpretability because it is a weighted linear summation of simple models. The interpretability of the proposed method enables us to investigate the degree of influence for acquiring synonyms from various properties of the context words. Moreover, the proposed method has high extendability because the conditions of context restriction can be easily changed and added. Experimental results using actual Japanese corpora showed that the proposed method aggregating multiple context-restricted models achieved a higher performance than the previous single Skip-gram model. In addition, the estimated weights of various properties of the context words could appropriately elucidate some grammatical characteristics of the Japanese language.","['序論', '関連研究', '提案手法', '使用データと予備実験', '実験結果', '結論']",,,,,,,,
V24N02-02.tex,,Construction of a Multilingual Annotated Corpus for Deeper Sentiment Understanding in Social Media\\[6pt],,"The surge of social media use, such as Twitter, introduces new opportunities for understanding and gauging public mood across different cultures. However, the diversity of expression in social media presents a considerable challenge to this task of opinion mining, given the limited accuracy of sentiment classification and a lack of intercultural comparisons. Previous Twitter sentiment corpora have only global polarities attached to them, which prevents deeper investigation of the mechanism underlying the expression of feelings in social media, especially the role and influence of rhetorical phenomena. To this end, we construct an annotated corpus for multilingual Twitter sentiment understanding that encompasses three languages (English, Japanese, and Chinese) and four international topics (iPhone 6, Windows 8, Vladimir Putin, and Scottish Independence); our corpus incorporates 5,422 tweets. Further, we propose a novel annotation scheme that embodies the idea of separating emotional signals and  rhetorical context, which, in addition to global polarity, identifies rhetoric devices, emotional signals, degree modifiers, and subtopics. Next, to address low inter-annotator agreement in previous corpora, we propose a pivot dataset comparison method to effectively improve the agreement rate. With manually annotated rich information, our corpus can serve as a valuable resource for the development and evaluation of automated sentiment classification, intercultural comparison, rhetoric detection, etc. Finally, based on observations and our analysis of our corpus, we present three key conclusions. First, languages differ in terms of emotional signals and rhetoric devices, and the idea that cultures have different opinions regarding the same objects is reconfirmed. Second, each rhetoric device maintains its own characteristics, influences global polarity in its own way, and has an inherent structure that helps to model the sentiment that it represents. Third, the models of the expression of feelings in different languages are rather similar, suggesting the possibility of unifying multilingual opinion mining at the sentiment level.\\[6pt]","['Introduction', 'Related Work', 'Dataset Creation', 'Annotation Scheme', 'Annotation Process', 'Basic Analysis of the Corpus', 'Rhetoric and Global Polarity', 'Individual Differences and Annotation Deficiencies', 'Conclusion and Future Work', 'Detailed Statistics of Data Selection', 'Example of Annotation Result in XML', 'Detail of Coding Manual', 'Lexicons Used for WPN Computation']",,,,,,,,
V24N02-03.tex,,Constrained Partial Parsing Based Dependency Tree Projection for Tree-to-Tree Machine Translation,,"Ideally, tree-to-tree machine translation (MT) that utilizes syntactic parse trees on both source and target sides could preserve non-local structure, and thus generate fluent and accurate translations. In practice, however, firstly, high quality parsers for both source and target languages are difficult to obtain; secondly, even if we have high quality parsers on both sides, they still can be non-isomorphic because of the annotation criterion difference between the two languages. The lack of isomorphism between the parse trees makes it difficult to extract translation rules. This extremely limits the performance of tree-to-tree MT. In this article, we present an approach that projects dependency parse trees from the language side that has a high quality parser, to the side that has a low quality parser, to improve the isomorphism of the parse trees. We first project a part of the dependencies with high confidence to make a partial parse tree, and then complement the remaining dependencies with partial parsing constrained by the already projected dependencies. Experiments conducted on the Japanese-Chinese and English-Chinese language pairs show that our proposed method significantly improves the performance on both the two language pairs.","['Introduction', 'The Difficulties of Tree-to-Tree MT', 'Projection of Dependency Trees with Constrained Partial Parsing', 'Experiments', 'Related Work', 'Conclusion']",,,,,,,,
V24N02-04.tex,,Morphological Analysis for Japanese Noisy Text based on Extraction of Character Transformation Patterns and Lexical Normalization,ソーシャルメディア等の崩れた日本語の解析においては，形態素解析辞書に存在しない語が多く出現するため解析誤りが新聞等のテキストに比べ増加する．辞書に存在しない未知語の中でも，既知の辞書語からの派生に関しては，正規形を考慮しながら解析するという表記正規化との同時解析の有効性が確認されている．本研究では，これまで焦点があてられていなかった，文字列の正規化パタン獲得に着目し，アノテーションデータから文字列の正規化パタンを統計的に抽出する．統計的に抽出した文字列正規化パタンと文字種正規化を用いて辞書語の候補を拡張し形態素解析を行った結果，従来法よりも再現率，精度ともに高い解析結果を得ることができた．,"Social media texts are often written in a non-standard style and include many lexical variants such as insertions, phonetic substitutions, and abbreviations that mimic spoken language. The normalization of such a variety of non-standard tokens is one promising solution for handling noisy text. A normalization task is very difficult for the morphological analysis of Japanese text  because there are no explicit boundaries between words. To address this issue, we propose a novel method herein for normalizing and morphologically analyzing Japanese noisy text. First, we extract character-level transformation patterns based on a character alignment model using annotated data. Next, we generate both character-level and word-level normalization candidates using character transformation patterns and search for the optimal path based on a discriminative model. Experimental results show that the proposed method exceeds conventional rule-based system in both accuracy and recall for word segmentation and POS (Part of Speech)  tagging.","['はじめに', '関連研究', '提案手法', '実験', 'まとめと今後の課題']",,,,,,,,
V24N03-01.tex,,Extending Various Thesauri by Finding Synonym Sets \\from a Formal Concept Lattice,,"In this paper, we solve the problem of extending various thesauri using a single method. Thesauri should be extended when unregistered terms are identified. Various thesauri are available, each of which is constructed according to a unique design principle. We formalise the extension of one thesaurus as a single classification problem in machine learning, with the goal of solving different classification problems. Applying existing classification methods to each thesaurus is time consuming, particularly if many thesauri must be extended. Thus, we propose a method to reduce the time required to extend multiple thesauri. In the proposed method, we first generate clusters of terms without the thesauri that are candidates for synonym sets based on formal concept analysis using the syntactic information of terms in a corpus. Reliable syntactic parsers are easy to use; thus, syntactic information is more available for many terms than semantic information. With syntactic information, for each thesaurus and for all unregistered terms, we can search candidate clusters quickly for a correct synonym set for fast classification. Experimental results demonstrate that the proposed method is faster than existing methods and classification accuracy is comparable.","['Introduction', 'Problem Formalisation', 'Related Work', 'Extending Various Thesauri with a Concept Lattice', 'Experiment with Well-known Materials in NLP', 'Conclusion']",,,,,,,,
V24N03-02.tex,,Meaning Estimation Scheme of Alphabetical Abbreviations using Conceptualization of Words and Wikipedia,元来から日本は外来語を受け入れやすい環境にあるといわれており，外来語が益々増加する中，特に，英語の場合，外国語の表記を利用するシーンも増えている．また，英単語など頭文字をつなげて表記する略語も利用されている．しかし，英字略語は別のことを表現しても，表記が同じになる多義性の問題を持っている．そこで，本稿では，英字略語の意味を推定する方法を提案する．提案手法では，英字略語の意味推定を未知語の意味推定とみなし，ある概念から様々な概念を連想する語彙の概念化処理を可能とする概念ベースと，概念化した語彙の意味的な近さを判断できる関連度計算または Earth Mover's Distance を用いる．さらに，英字略語ゆえの情報の欠如を，世界で最も収録語数が多いとされている Wikipedia を使用することで補完する．これらを用いることで，英字略語の多義性を解消し，英字略語の本来の意味を推定する．提案手法は，129 件の新聞記事に対して，最高で 80\% 近い正答率を示したことに加え，比較方法より良好な結果を得ることができた．,"It is believed that Japan is open to loanwords, and they are often used in daily activities. Particularly, in English, scenes using foreign language notation are increasing. In addition, alphabetical abbreviations, which comprise initials of each English word, are used. However, polysemy is a major concern for the alphabetical abbreviations. In this paper, we propose a scheme to estimate the meaning of an alphabetical abbreviation. The proposed scheme considers the meaning estimation of an alphabetical abbreviation as the meaning estimation of an unknown word. This scheme uses the concept base, and, Calculation of Degree of Association or Earth Mover's Distance. The scheme allows for the conceptualization of a word and the evaluation of semasiological association between conceptualized words. In addition, Wikipedia is used to complement the lack of information due to alphabetical abbreviations. This paper makes use of 129 articles to evaluate the proposed scheme. The experiments showed that the accuracy of the proposed scheme was nearly 80\% and that the scheme was more effective than other schemes.","['研究背景', '関連研究', '提案方法の概要', '使用要素技術', '英字略語の意味推定方法', '評価実験', 'まとめ']",,,,,,,,
V24N03-03.tex,,Left-Corner Parsing for Identifying PTB-Style Nonlocal Dependencies,,"Nonlocal dependencies represent syntactic phenomenon such as wh-movement, A-movement in passives, topicalization, raising, control, and right node raising. Nonlocal dependencies play an important role in semantic interpretation. This paper proposes a left-corner parser that identifies nonlocal dependencies. Our parser integrates nonlocal dependency identification into a transition-based system. We adopt a left-corner strategy in order to use the syntactic relation c-command, which plays an important role in nonlocal dependency identification. To utilize the global features captured by nonlocal dependencies, our parser uses a structured perceptron. In experimental evaluations, our parser achieved a good balance between constituent parsing and nonlocal dependency identification.","['Introduction', 'Nonlocal Dependency', 'Transition-Based Left-Corner Parsing', 'Nonlocal Dependency Identification', 'Parsing with a Structured Perceptron', 'Experiment', 'Conclusion']",,,,,,,,
V24N03-04.tex,,Generalized Hierarchical Word Sequence Framework for Language Modeling,,"Language modeling is a fundamental research problem that has wide application for many NLP tasks. For estimating probabilities of natural language sentences, most research on language modeling use n-gram based approaches to factor sentence probabilities. However, the assumption under n-gram models is not robust enough to cope with the data sparseness problem, which affects the final performance of language models. In this paper, we propose a generalized hierarchical word sequence framework, where different word association scores can be adopted to rearrange word sequences in a totally unsupervised fashion. Unlike the n-gram which factors sentence probability from left-to-right, our model factors using a more flexible strategy. For evaluation, we compare our rearranged word sequences to normal n-gram word sequences. Both intrinsic and extrinsic experiments verify that our language model can achieve better performance, proving that our method can be considered as a better alternative for n-gram language models.","['Introduction', 'Cognitive Grammar Structure and Its Advantage', 'A Generalized Framework for HWS', 'Intrinsic Evaluation', 'Extrinsic Evaluation', 'Conclusion']",,,,,,,,
V24N03-05.tex,,Improving Sublanguage Translation via Global Pre-ordering,"法律文書や技術文書等の専門文書に対する機械翻訳では，翻訳対象のサブ言語に特有の大域的な文構造を適切に捉えて翻訳することが高品質な訳文を得る上で必要不可欠である．本論文では，文内の長距離な並べ替えに焦点を当てることによって，大域的な並べ替えを行うための手法を提案する．提案する大域的並べ替え手法では，アノテートされていない平文学習データを対象として，構文解析を行うことなく大域的な並べ替えモデルを学習する．そして，大域的な並べ替えを従来型の構文解析による並べ替えと併用することによって，高精度な並べ替えを実現する．公開特許公報英文抄録 (Patent Abstracts of Japan, PAJ) のサブ言語を対象とした日英翻訳および英日翻訳の評価実験を行ったところ，両言語方向において，大域的な並べ替えと構文に基づく並べ替えを組み合わせることによって翻訳品質向上が達成できることがわかった．","When translating formal documents, capturing the sentence structure specific to the sublanguage is extremely necessary to obtain high-quality translations. This paper proposes a novel global reordering method that focuses on long-distance reordering to capture the global sentence structure of a sublanguage. The proposed method learns global reordering models without syntactic parsing from a non-annotated parallel corpus and works in conjunction with conventional syntactic reordering. The experimental results regarding patent abstract sublanguage show concrete improvements in translation quality, both for Japanese-to-English and English-to-Japanese translations.","['はじめに', '関連研究', '大域的並べ替え手法', '評価実験', '実験結果', '考察', 'おわりに']",,,,,,,,
V24N03-06.tex,,Improvement in Domain Specific Word Segmentation by Symbol Grounding,本稿は，自動単語分割における精度向上を実現するために，非テキスト情報とその説明文に対するシンボルグラウンディングを用いた新しい単語分割法を提案する．本手法は，説明文が付与された非テキスト情報の存在を仮定しており，説明文を擬似確率的単語分割コーパスとすることで，非テキスト情報と分野固有の単語との関係をニューラルネットワークにより学習する．学習されたニューラルネットワークから分野固有の辞書を獲得し，得られた辞書を単語分割のための素性として用いることでより精度の高い自動単語分割を実現する．将棋局面が対応付けされた将棋解説文から成る将棋解説コーパスを用いて実験を行い，シンボルグラウンディングにより得られた辞書を用いることで単語分割の精度が向上することが確認できた．,"We propose a novel framework for improving a word segmenter using information acquired from symbol grounding. The framework uses a dataset consisting of pairs of non-textual information and a commentary. We generate a pseudo-stochastically segmented corpus from the commentaries, and then build a neural network to predict relationships between non-textual information and the words. We generate a domain specific term dictionary by using the neural network for word segmenter. We applied our method to game records of Japanese chess with commentaries. The experimental results show that the accuracy of a word segmenter can be improved by incorporating the generated dictionary.","['はじめに', '確率的単語分割コーパス', 'シンボルグラウンディング', 'シンボルグラウンディングの結果を用いた単語分割', '評価', '関連研究', '終わりに']",,,,,,,,
V24N03-07.tex,,"Selecting Syntactic, Non-redundant Segments \\ in Active Learning for Machine Translation",能動学習は機械学習において，逐次的に選択されたデータに対してのみ正解ラベルを付与してモデルの更新を繰り返すことで，少量のコストで効率的に学習を行う枠組みである．この枠組みを機械翻訳に適用することで，人手翻訳のコストを抑えつつ高精度な翻訳モデルを学習可能である．機械翻訳のための能動学習では，人手翻訳の対象となる文またはフレーズをどのように選択するかが学習効率に大きな影響を与える要因となる．既存研究による代表的な手法として，原言語コーパスの単語$n$-gram頻度に基づき $n$-gram カバレッジを向上させる手法の有効性が知られている．この手法は一方で，フレーズの最大長が制限されることにより，句範疇の断片のみが提示されて，人手翻訳が困難になる場合がある．また，能動学習の過程で選択されるフレーズには，共通の部分単語列が繰り返し出現するため，単語数あたりの精度向上率を損なう問題も考えられる．本研究では原言語コーパスの句構造解析結果を用いて句範疇を保存しつつ，包含関係にある極大長のフレーズのみを人手翻訳の候補とするフレーズ選択手法を提案する．本研究の提案手法の有効性を調査するため，機械翻訳による擬似対訳を用いたシミュレーション実験および専門の翻訳者による人手翻訳と主観評価を用いた実験を実施した．その結果，提案手法によって従来よりも少ない単語数の翻訳で高い翻訳精度を達成できることや，人手翻訳時の対訳の品質向上に有効であることが示された．,"Active learning is a framework that makes it possible to efficiently train statistical models by selecting informative examples from a pool of unlabeled data. Previous work has found this framework effective for machine translation (MT), making it possible to train better translation models with less effort, particularly when annotators translate short phrases instead of full sentences. However, previous methods for phrase-based active learning in MT fail to consider whether the selected units are coherent and easy for human translators to translate, and also have problems with selecting redundant phrases with similar content. In this paper, we tackle these problems by proposing two new methods for selecting more syntactically coherent and less redundant segments in active learning for MT. Experiments using both simulation and extensive manual translation by professional translators find the proposed method effective, achieving both greater gain of BLEU score for the same number of translated words, and allowing translators to be more confident in their translations.","['はじめに', '機械翻訳のための能動学習', '単語 $n$-gram 頻度に基づく文・フレーズ選択手法', '極大フレーズ選択手法と部分構文木選択手法', 'シミュレーション実験', '人手翻訳実験', 'まとめ']",,,,,,,,
V24N03-08.tex,,Phrase Structure Annotation and Parsing for Learner English,,"Learner English often contains grammatical errors with structural characteristics such as omissions, insertions, substitutions, and word order errors. These errors are not covered by the existing context-free grammar (CFG) rules. Therefore, it is not at all straightforward how to annotate learner English with phrase structures. Because of this limitation, there has been almost no work on phrase structure annotation for learner corpora despite its importance and usefulness. To address this issue, we propose a phrase structure annotation scheme for learner English, that consists of five principles. We apply the annotation scheme to two different learner corpora and show (i) its effectiveness at consistently annotating learner English with phrase structure (i.e., high inter-annotator agreement); (ii) the structural characteristics (CFG rules) of learner English obtained from the annotated corpora; and (iii) phrase structure parsing performance on learner English for the first time. We also release the annotation guidelines, the annotated data, and the parser model to the public.","['Introduction', 'Phrase Structure Annotation Scheme', 'Corpus Annotation', 'Parsing Performance Evaluation', 'Conclusions']",,,,,,,,
V24N04-01.tex,,Dialog Act Classification Using Features Intrinsic to Dialog Acts in an Open-Domain Conversation,対話行為の自動推定は自由対話システムにおける重要な要素技術のひとつである．機械学習を用いた既存の対話行為の推定手法では，機械学習に用いる特徴のセットを1つ設定するが，この際に個々の対話行為の特質は十分に考慮されていなかった．機械学習の特徴の中にはある特定の対話行為の分類にしか有効に働かないものもあり，そのような特徴は他の対話行為の分類精度を低下させる要因になりうる．これに対し，本論文では対話行為毎に適切な特徴のセットを設定する．まず，28個の初期の特徴を提案する．次に，対話行為毎に初期特徴セットから有効でない特徴を削除することで最適な特徴セットを獲得する．これを基に，入力発話が対話行為に該当するかを判定する分類器を対話行為毎に学習する．最後に，個々の分類器の判定結果ならびに判定の信頼度から，適切な対話行為をひとつ選択する．評価実験の結果，提案手法は唯一の特徴セットを用いるベースラインと比べてF値が有意に向上したことを確認した．,"The classification of dialog acts of user's utterance is one of the important fundamental techniques in open-domain conversational systems. Most previous studies on the classification of dialog acts were based on supervised machine learning; however, the characteristics of individual dialog acts were not considered. Some features for machine learning may increase the accuracy of classification for a particular dialog act, whereas decrease the accuracy for other dialog acts. In this study, an appropriate feature set is defined for each dialog act to improve the performance of the classification of the dialog acts. First, 28 features are proposed as an initial set. Second, for each dialog act, an optimal set of the features is identified by removing ineffective features from the initial set. Third, binary classifiers that judge whether a dialog act is suitable for a given utterance are trained using the optimized feature set. Finally, one dialog act is chosen based on the results provided by the binary classifiers.
The reliability of the judgment of the binary classifiers is also considered. Results of experiments showed that our proposed method significantly outperformed a baseline that was trained using a single feature set.","['はじめに', '関連研究', '提案手法', '評価実験', 'おわりに', '対応表']",,,,,,,,
V24N04-02.tex,,Building a Sentiment Dictionary for News Analytics using Stock Prices,本研究では，金融分野に特化した極性辞書の作成を目的とし，ニュースデータと株式価格データから極性辞書の作成を行う．株式価格情報から単語のポジティブ／ネガティブの情報を獲得するため，ニュース記事が言及している銘柄の株式価格変動を算出する．株式価格変動をニュース記事の教師情報として，教師あり学習を行ったのち，学習器から単語の極性情報を抽出することで，単語に対して重み付き極性値の付与を試みた．そして，作成した極性辞書を用いて，学習に用いたメディアのニュース記事分類と他メディアのニュース記事分類を行うことにより，本研究手法の有効性を検証した．検証の結果，ニュース記事配信日の株式リターンに関して，将来のニュース記事を分類できること，また，異なるメディアのニュース記事も分類できることを示した．一方で，ニュース記事配信日から2営業日以上離れると，ニュース記事分類が困難であることが示された．,"This paper proposes a method of building a sentiment dictionary using only news and stock price data for textual analysis in finance. To obtain word polarity from stock price fluctuations, we calculate stock price returns following announcements of news articles. We constructed learners with support vector regression, using stock price returns as supervised labels of news articles, and built a sentiment dictionary by extracting word polarity from learners. Furthermore, we examined whether our sentiment dictionary is effective in classifying news articles as negative or positive. We found that our sentiment dictionary is also effective in classifying news articles provided by other news media other than news media we employed in constructing the algorithm. In addition, we found that it is difficult to classify news articles on a date that is two trading days away from the news announcement date.","['はじめに', 'データ', 'キーワードリストの作成方法', 'キーワードリストを用いた分類検証', '極性辞書の自動生成に関する先行研究', 'おわりに']",,,,,,,,
V24N04-03.tex,,Cross-lingual Product Recommendation System Using Collaborative Filtering,,"We developed a cross-lingual recommender system using collaborative filtering with English-Japanese translation pairs of product names to help non-Japanese buyers who speak English when they are visiting Japanese shopping websites. Customer purchase histories at an English shopping site and those at another Japanese shopping site were used for the experiments. Two experiments were conducted to evaluate the system. They were (1) two-fold cross validation where half of the translation pairs were masked and (2) experiments where all of the translation pairs were used. The precisions, recalls, and mean reciprocal ranks (MRRs) of the system were evaluated to assess the general performance of the recommender system in the first set of experiments. We investigated the effect formatting the translation pairs and the performance according to the type of feature value of the vectors (binary versus rating values). In contrast, the kind of items that were recommended in a more realistic scenario were shown in the second experiment. The results reveal that masked items were found more efficiently than when the bestseller recommender system was used and, further, that items only on the Japanese site that seemed to be related to the buyers' interests could be found by the system in the more realistic scenario.","['Introduction', 'Related Work', 'Recommendation System to Help Non-Japanese Buyers', 'Data', 'Item Identification and Recommendation', 'Experiments', 'Results', 'Discussion', 'Conclusion']",,,,,,,,
V24N04-04.tex,,Multi-domain Adaptation for Statistical Machine Translation Based on Feature Augmentation,ドメイン適応は，機械翻訳を実用に使用するときの大きな課題の一つである．本稿では，複数ドメインを前提とした，統計翻訳の適応方式を提案する．本稿の方式は，カバレッジが広い（未知語が少ない）コーパス結合モデルと，素性関数の精度がよい単独ドメインモデルを併用する．これらを，機械学習のドメイン適応に用いられている素性空間拡張法の考え方で結合する．従来の機械翻訳における素性空間拡張法は，単一のモデルを用いていたが，本稿の提案方式は，複数のモデルを用いることにより，両者の利点を活かすことが特徴である．実験では，単独ドメインモデルに比べ，翻訳品質が向上または同等を保持した．提案法は，当該ドメインの訓練コーパスが小規模である場合に高い効果を持ち，100万文規模の大規模コーパスを持つドメインへの適応に使用しても，翻訳品質を下げることなく，ドメインによっては品質向上の効果がある．基本的な対数線形モデルでも，モデルの選択と設定を適切に行うことで，最先端品質の適応方式が実現できることを示す．,"Domain adaptation is a major challenge when machine translation is applied to practical tasks. In this study, we present domain adaptation methods for machine translation that assume multiple domains. The proposed methods combine two typesof models: a corpus-concatenated model covering multiple domains and single-domain models that are accurate but sparse in specific domains. We combine the advantages of both the models using feature augmentation for domain adaptation in machine learning; however, a conventional method of feature augmentation for machine translation uses a single model. Our experimental results show that the translation qualities of the proposed method improved or were at the same level as those of the single-domain models. The proposed method is extremely effective in low-resource domains. Even in domains having a million bilingual sentences, the translation quality was at least preserved and even improved in some domains. These results demonstrate that state-of-the-art domain adaptations can be realized with appropriate model selection and appropriate settings, even when standard log-linear models are used.","['はじめに', '統計翻訳のドメイン適応', 'マルチドメイン適応方式', '実験', 'まとめ']",,,,,,,,
V24N04-05.tex,,Hierarchical Sub-sentential Alignment with IBM Models for Statistical Phrase-based Machine Translation,,"In this paper, we describe a novel method for joint word alignment and symmetrization. Based on initial parameters from simple IBM models, we synchronously parse the parallel sentence pair under the framework of bracket transduction grammar constraints. Our 2-phase method can achieve nearly the same run-time as {\tt{fast\_align","['Introduction', 'Related Works', 'Joint Alignment and Symmetrization Model', 'Experiments', 'Conclusion']",,,,,,,,
V24N05-01.tex,,Improvement in Domain-Specific Named Entity Recognition by Utilizing the Real-World Data,本稿では，将棋の解説文に対する固有表現を題材として，テキスト情報に加えて実世界情報を参照する固有表現認識を提案する．この題材での実世界情報は，固有表現認識の対象となる解説文が言及している将棋の局面である．局面は，盤面上の駒の配置と持ち駒であり，すべての可能な盤面状態がこれによって記述できる．提案手法では，まず各局面の情報をディープニューラルネットワークの学習方法の1つであるstacked auto-encoderを用いて事前学習を行う．次に，事前学習の結果をテキスト情報と組み合わせて固有表現認識モデルを学習する．提案手法を評価するために，条件付き確率場による方法等との比較実験を行った．実験の結果，提案手法は他の手法よりも高い精度を示し，実世界情報を用いることにより固有表現認識の精度向上が可能であることが示された．,"In this paper, we propose a method that utilizes real-world data to improve named entity recognition (NER) for a particular domain. Our proposed method integrates a stacked auto-encoder (SAE) and a text-based deep neural network for achieving NER. Initially, we train the SAE using real-world data, then the entire deep neural network from sentences annotated with named entities (NEs) and accompanied by real world information.  In our experiments, we chose Japanese chess as our subject. The dataset consists of pairs of a game state and commentary sentences about it annotated with game-specific NE tags. We conducted NER experiments and verified that referring to real-world data improves the NER accuracy.","['はじめに', '関連研究', '将棋解説コーパス', '提案手法', '評価実験', 'おわりに']",,,,,,,,
V24N05-02.tex,,Towards a Consistent Segmentation Level across Multiple Chinese Word Segmentation Corpora,,"One of the crucial problems facing current Chinese natural language processing (NLP) is the ambiguity of word boundaries, which raises many further issues, such as different word segmentation standards and the prevalence of out-of-vocabulary (OOV) words. We assume that such issues can be better handled if a consistent segmentation level is created among multiple corpora. In this paper, we propose a simple strategy to transform two different Chinese word segmentation (CWS) corpora into a new consistent segmentation level, which enables easy extension of the training data size. The extended data is verified to be highly consistent by 10-fold cross-validation. In addition, we use a synthetic word parser to analyze the internal structure information of the words in the extended training data to convert the data into a more fine-grained standard. Then we use two-stage Conditional Random Fields (CRFs) to perform fine-grained segmentation and chunk the segments back to the original Peking University (PKU) or Microsoft Research (MSR) standard. Due to the extension of the training data and reduction of the OOV rate in the new fine-grained level, the proposed system achieves state-of-the-art segmentation recall and F-score on the PKU and MSR corpora.","['INTRODUCTION', 'RELATED WORK', 'COMPONENTS', 'METHODS', 'EXPERIMENTS', 'CONCLUSIONS']",,,,,,,,
V24N05-03.tex,,Corpus-Based Analysis of the Canonical Word Order of Japanese Double Object Constructions,日本語二重目的語構文の基本語順に関しては多くの研究が行われてきた．しかし，それらの研究の多くは，人手による用例の分析や，脳活動や読み時間の計測を必要としているため，分析対象とした用例については信頼度の高い分析を行うことができるものの，多くの仮説の網羅的な検証には不向きであった．一方，各語順の出現傾向は，大量のコーパスから大規模に収集することが可能である．そこで本論文では，二重目的語構文の基本語順はコーパス中の語順の出現割合と強く関係するという仮説に基づき，大規模コーパスを用いた日本語二重目的語構文の基本語順に関する分析を行う．100億文を超える大規模コーパスから収集した用例に基づく分析の結果，動詞により基本語順は異なる，省略されにくい格は動詞の近くに出現する傾向がある，PassタイプとShowタイプといった動詞のタイプは基本語順と関係しない，ニ格名詞が着点を表す場合は有生性を持つ名詞の方が「にを」語順をとりやすい，対象の動詞と高頻度に共起するヲ格名詞およびニ格名詞は動詞の近くに出現しやすい等の結論が示唆された．,"Several studies have investigated the canonical word order of Japanese double object constructions. However, most of these studies use either manual analyses or measurements of human characteristics such as brain activities or reading times for each example. Thus, although these analyses are reliable for the examples they focus on, the findings cannot be generalized to other examples. In contrast, the trend of actual usage can be automatically collected from a large corpus.  Thus, in this study, we assume that there is a relation between the canonical word order and the proportion of each word order in a large corpus and present a corpus-based analysis of the canonical word order of Japanese double object constructions. Our analysis is based on a very large corpus comprising more than 10 billion unique sentences and suggests that the canonical word order of such constructions varies from verb to verb. Moreover, it suggests an argument whose grammatical case is infrequently omitted with a given verb tends to be placed near the verb and that there is few relation between the canonical word order and the verb type: show-type and pass-type. The dative-accusative order is more preferred when the semantic role of dative argument is animate \textsl{Possessor","['はじめに', '日本語二重目的語構文と語順', '本研究で検証する仮説', '分析に使用する用例の収集', '大規模コーパスに基づく基本語順の分析', 'おわりに']",,,,,,,,
V24N05-04.tex,,nwjc2vec: Word Embedding Data Constructed from NINJAL Web Japanese Corpus,我々は国語研日本語ウェブコーパスと word2vec を用いて単語の分散表現を構築し，その分散表現のデータを nwjc2vec と名付けて公開している．本稿では nwjc2vec を紹介し，nwjc2vec の品質を評価するために行った2種類の評価実験の結果を報告する．第一の評価実験では，単語間類似度の評価として，単語類似度データセットを利用して人間の主観評価とのスピアマン順位相関係数を算出する．第二の評価実験では，タスクに基づく評価として，nwjc2vec を用いて語義曖昧性解消及び回帰型ニューラルネットワークによる言語モデルの構築を行う．どちらの評価実験においても，新聞記事7年分の記事データから構築した分散表現を用いた場合の結果と比較することで，nwjc2vec が高品質であることを示す．,"We constructed word embedding data (named as 'nwjc2vec') using the NINJAL Web Japanese Corpus and word2vec software, and released it publicly. In this report, nwjc2vec is introduced, and the result of two types of experiments that were conducted to evaluate the quality of nwjc2vec is shown. In the first experiment, the evaluation based on word similarity is considered. Using a word similarity dataset, we calculate Spearman's rank correlation coefficient. In the second experiment, the evaluation based on task is considered. As the task, we consider word sense disambiguation (WSD) and language model construction using Recurrent Neural Network (RNN). The results obtained using the nwjc2vec were compared with the results obtained using word embedding constructed from the article data of newspaper for seven years. The nwjc2vec is shown to be high quality.","['はじめに', 'nwjc2vec の構築', '評価実験', '考察', 'おわりに']",,,,,,,,
V25N01-01.tex,,A System for Classifying Proposals and Estimating Start Pages Stated in Notice of Annual Meeting of Shareholders,本論文では，テキストマイニング技術を用いて，株主招集通知の情報をデータベースに格納する業務の効率化を実現するための応用システムの研究について述べる．効率化したい業務とは，株主招集通知に記載されている議案の開始ページを予測し，その開始ページにおける議案の議案タイトルと議案内容を分類する業務である．本研究では，これらの業務を株主招集通知のテキスト情報を用いて自動的に行うシステムを開発し，実際に運用している．本研究によって実装したシステムと従来の人手による作業の比較実験の結果，作業時間は1/10程度に短縮された．議案分類の手法としては，学習データから抽出した特徴語の重みを用いた分類，多層ニューラルネットワーク（深層学習）を用いた分類，抽出した議案タイトルを用いた分類の三手法を用いた．さらに，各手法の評価を行い，各手法の議案ごとの有効性を確認した．,"In this paper, we describe research on applied systems for realizing efficiency of work to store information of notice of annual meeting of shareholders in the database by using text mining technology. We aim to estimate start pages of proposals stated in notice of the meeting of shareholders and classify which proposal the page is. And we developed a system that automatically performs these tasks using text information of the notice of convocation of shareholders, and actually operates it. As a result of comparative experiment between our implemented system and conventional manual work, the working time was shortened to about 1/10. We propose three methods for classifying proposals. The first method classifies proposals by specialized terms extracted from training data. The second method classifies proposals by using deep learning. The final method classifies proposals by extracted proposal title. We evaluated our methods, and the effectiveness of each method was verified.","['はじめに', '議案がある開始ページの推定', '提案手法1\u3000特徴語による議案分類', '提案手法2\u3000多層ニューラルネットワークによる議案分類', '提案手法3\u3000抽出した議案タイトルを用いた議案分類', '評価', '各手法に対する考察', '応用システム', '応用システムの評価', '関連研究', 'まとめ']",,,,,,,,
V25N01-02.tex,,Development of Language Ability Measurement System (KOTOBAKARI) using Voice Recognition,言語に関する能力を，客観的かつ自動的に把握する需要が高まっている．例えば，近年，日本において認知症は身近なものとなっているが，認知症は，言語能力に何らかの特徴が表出する可能性があることはよく知られている．言語能力を測り，それらの兆候を捉えることができれば，早期発見や療養に役立つ可能性がある．また，現在，多くの留学生が日本語教育機関において日本語を学んでおり，学習者の習熟度に対し，適切な評価を与えることが各教育機関に求められている．しかし，書く能力や話す能力の評価は，主に評価者の主観によって行われており，評価者によって判断に揺れが生じうる．機械によって自動的かつ客観的に言語能力を測定することができれば，評価者による揺れの生じない評価の一つとして活用できる可能性がある．これまでにも，言語能力の測定に関する取り組みはあるものの，いずれも人手を介して測定を行うためコストが高く，気軽に測定することは難しい．そこで本研究では，手軽に言語能力を測定可能なシステム「言秤（コトバカリ）」を提案する．本提案システムでは，(1) 音声認識システムの組み込み，および (2) テキストデータから定量的に言語能力を測定する指標の採用を行うことで，従来人手で行っていたテキスト化および言語能力スコアの算出を自動化し，コストの軽減と手軽な測定を実現する．また，「被測定者自身による自己把握・状況改善（用途1）」および「被測定者以外による能力の高低の判断（用途2）」という観点から，言語能力スコア（Type・Token比）算出における音声認識システムの利用可能性について検証を行った．書き起こし結果および音声認識結果から得られる言語能力スコアは異なるため，閾値との比較のような，単純な言語能力スコアの対比による能力の高低の判断（用途2）は難しいことがわかった．また，同一時期に複数回測定し，書き起こし結果および音声認識結果から得られる言語能力スコアの相関を調べたところ，集団としては相関が見られなかった．一方，個人で分けると，相関が見られる発話者と見られない発話者がいることがわかった．相関が見られる発話者については，被測定者の言語能力スコアを継続的に測定し，その変化を観察することによる能力の判断（用途1）や言語能力の現状把握・維持・改善（用途2）ができる可能性が示唆された．,"There is a growing need for automatic measurements of language ability. With increase in the aging population, the number of older adults with dementia is expected to increase. Language disorder is considered one of the most fundamental symptom with which dementia can be detected. The identification of language defects particular to dementia symptoms may contribute to the early detection of dementia. Currently, many foreign students learn Japanese at educational institutions, which are required to provide appropriate evaluations to their students. However, differences occur because evaluators judge the language abilities of foreign students subjectively. The measured results from an automatic language ability measurement system could be used for objective evaluations. Although traditional studies manually measured language abilities, such methods are often costly. In this study, we propose the automatic language ability measurement system, ``KOTOBAKARI,'' as an alternative to traditional manual measurement methods. This system is expected to reduce the cost and time of measuring the language ability using the following steps: (1) voice recognition to obtain text data and (2) quantitative indicators for automated language ability measurement. We verify the capacity of voice recognition to measure language ability by comparing language ability to a threshold value and by assessing language ability score changes. The experimental results show that it is difficult to judge language ability by threshold value comparison, using our system. On the contrary, the language ability score of some individuals, measured with our system, was correlated with the correct score. This result indicates that these individuals can use our system to assess language ability changes by continuous measurements.","['はじめに', '関連研究', '言語能力指標', '提案システム：言秤（コトバカリ）', '検証実験', '実験結果と考察', 'おわりに']",,,,,,,,
V25N01-03.tex,,What Do People Write in Reviews for Sellers?\\---Investigation and Development of an Automatic Classification System---,"本稿では，オンラインショッピングサイト出店者に対して書かれたレビュー（以下，店舗レビュー）内の各文を，言及されているアスペクト（例えば，商品の配送や梱包）とその評価極性（肯定，否定）に応じて分類するシステムについて述べる．店舗レビュー中にどのようなアスペクトが記載されているのかは明らかでないため，まず店舗レビュー100件（487文）を対象に，各文がどのようなアスペクトについて書かれているのか調査した．その結果，14種類のアスペクトについて書かれていることがわかった．そして，この調査結果をもとに1,510件の店舗レビューに含まれる5,277文に対して人手でアスペクトおよびその評価極性のアノテーションを行い，既存の機械学習ライブラリを用いてレビュー内の文を分類するシステムを開発した．本システムを用いることで，任意のアスペクトについて，その記述を含むレビューへ効率良くアクセスしたり，その評判の時系列変化を調べたりすることが可能になる．","Herein, we describe a system that classifies each sentence in reviews for the sellers of an online shopping website based on aspects, such as shipping and packaging, and sentiment polarity (positive and negative). First, we investigated 487 sentences that were extracted from randomly selected 100 seller reviews for revealing the aspects mentioned in the reviews. This was done because the aspects in the seller reviews are not obvious. Consequently, we found that 14 aspects were described in the seller reviews. We annotate 14 aspects and their sentiment polarity for 5,277 sentences in 1,510 seller reviews that are newly and randomly chosen. Then, we train the classifica\-tion models using the existing machine learning software. Through the system based on these classification models, users can understand the trend for any aspect in a time series and easily access reviews describing aspects which they are interested in.","['はじめに', '店舗レビューには何が書かれているか？', 'アスペクトおよび評価極性の自動分類', '実験', '考察', '関連研究', '店舗レビュー分析システム', 'おわりに']",,,,,,,,
V25N01-04.tex,,Rewrite Support System for Simplifying \\Japanese News Scripts,NHK はインターネットサイト NEWS WEB EASY で外国人を対象としたやさしい日本語のニュースを提供している．やさしい日本語のニュースは日本語教師と記者の 2 名が通常のニュースを共同でやさしく書き換えて制作し，本文にはふりがな，難しい語への辞書といった読解補助情報が付与されている．本稿では NEWS WEB EASY のやさしい日本語の書き換え原則，および制作の体制とプロセスの概要と課題を説明した後，課題に対処するために開発した 2 つのエディタを説明する．1 つは書き換えを支援する「書き換えエディタ」である．書き換えエディタは先行のシステムと同様に難しい語を指摘し，書き換え候補を提示する機能を持つが，2 名以上の共同作業を支援する点，難しい語の指摘機能に学習機能を持つ点，また，候補の提示に書き換え事例を蓄積して利用する点に特徴がある．他の 1 つは「読解補助情報エディタ」である．読解補助情報エディタは，ふりがなや辞書情報を自動推定する機能，さらに推定誤りの修正結果を学習する機能を持つ．以上のように 2 つのエディタは，自動学習と用例の利用により，読解補助情報の推定の誤り，やさしい日本語の書き方の方針変更などに日々の運用の中で自律的に対応できるようになっている．本稿では 2 つのエディタの詳細説明の後，日本語教師および記者を対象に実施したアンケート調査，およびログ解析によりエディタの有効性を示す．,"NHK launched a simplified Japanese news service on the Internet (NEWS WEB EASY) in April 2012 to improve news accessibility for foreign residents in Japan. Simplified news scripts are compiled daily by a Japanese language instructor and a news reporter working together using a custom news rewrite support system comprising a rewrite support editor and a reading support information editor. The rewrite support editor identifies difficult expressions and proposes simpler alternatives with similar expression search function, and the reading support information editor helps the Japanese language instructor add auxiliary information such as {\it furigana","['はじめに', '関連研究', 'NEWS WEB EASYの概要', 'ニュースのためのやさしい日本語', '体制と制作プロセス', '制作支援システム', '書き換え支援システムの効果', '議論', 'おわりに']",,,,,,,,
V25N01-05.tex,,Development of the Clinical Corpus with Disease Name Annotation,"高度な人工知能研究のためには，その材料となるデータが必須となる．医療，特に臨床に関わる分野において，人工知能研究の材料となるデータは主に自然言語文を含む電子カルテである．このようなデータを最大限に利用するには，自然言語処理による情報抽出が必須であり，同時に，情報抽出技術を開発するためのコーパスが必要となる．本コーパスの特徴は，45,000テキストという我々の知る限りもっとも大規模なデータを構築した点と，単に用語のアノテーションや用語の標準化を行っただけでなく，当該の疾患が実際に患者に生じたかどうかという事実性をアノテーションした点の2点である．本稿では病名や症状のアノテーションを対象に，この医療コーパス開発についてその詳細を述べる．人工知能研究のための医療コーパス開発について病名や症状のアノテーションを中心にその詳細を述べる．本稿の構成は以下の通りである．まず，アノテーションの基準について，例を交えながら，概念の定義について述べる．次に，実際にアノテーターが作業した際の一致率などの指標を算出し，アノテーションのフィージビリティについて述べる．最後に，構築したコーパスを用いた病名抽出システムについて報告する．本稿のアノテーション仕様は，様々な医療テキストや医療表現をアノテーションする際の参考となるであろう．","Sufficient data is required for research on advanced AI. In the field of medicine, especially clinical medicine, information retrieval is necessary to utilize the data fully since the data---mainly clinical records---uses natural language. The corpus we developed in this study has the following strong points: (i) The corpus consists of 45,000 case reports, which is the largest to our knowledge, and (ii) not only did we standardized the terminology and the method for annotation, we also annotated ``factness,'' which notes whether or not a disease name is actually the state of the patient in a case report. This paper describes the methods to develop the medical corpus for AI research, focusing on the annotation of the disease or symptom name. First, we define the concepts contained in the annotation criteria using examples. Next, we discuss the feasibility of the annotation through giving some indexes such as agreement rate. Finally, we report the development of the disease-name extraction system based on the corpus. We believe this corpus is a good reference for future clinical annotation.","['はじめに', '関連研究', 'コーパス構築の全体像', '病名タグ付け', '病名コーディング', '応用システム：病名抽出器の構築', 'おわりに']",,,,,,,,
V25N02-01.tex,,Neural Machine Translation Models using Binarized Prediction and Error Correction,本論文では，ニューラル翻訳モデルで問題となる出力層の時間・空間計算量を，二値符号を用いた予測法により大幅に削減する手法を提案する．提案手法では従来のソフトマックスのように各単語のスコアを直接求めるのではなく，各単語に対応付けられたビット列を予測することにより，間接的に出力単語の確率を求める．これにより，最も効率的な場合で従来法の対数程度まで出力層の計算量を削減可能である．このようなモデルはソフトマックスよりも推定が難しく，単体で適用した場合には翻訳精度の低下を招く．このため，本研究では提案手法の性能を補償するために，従来法との混合モデル，および二値符号に対する誤り訂正手法の適用という2点の改良も提案する．日英・英日翻訳タスクを用いた評価実験により，提案法が従来法と比較して同等程度のBLEUを達成可能であるとともに，出力層に要するメモリを数十分の1に削減し，CPUでの実行速度を5倍から10倍程度に向上可能であることを示す．,"In this paper, we propose a new method for calculating the output layer in neural machine translation systems with largely reduced computation cost based on binary code. The method is performed by predicting a bit array instead of actual output symbols to obtain word probabilities, and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, since learning proposed model is more difficult than softmax models, we also introduce two approaches to improve translation quality of the proposed model: combining softmax and our models and using error-correcting codes. Experiments on English-Japanese bidirectional translation tasks show proposed models achieve that their BLEU approach the softmax, while reducing memory usage on the order of one tenths, and also improving decoding speed on CPUs by x5 to x10.","['はじめに', '詳細な問題定義と従来研究', '二値符号予測に基づく単語推定モデル', '二値符号予測モデルの改良', '実験', 'おわりに']",,,,,,,,
V25N02-02.tex,,Improving Chinese Semantic Role Labeling using High-quality Surface and Deep Case Frames,,"This paper presents a method for improving semantic role labeling (SRL) using a large amount of automatically acquired knowledge. We acquire two varieties of knowledge, which we call surface case frames and deep case frames. Although the surface case frames are compiled from syntactic parses and can be used as rich syntactic knowledge, they have limited capability for resolving semantic ambiguity. To compensate for the deficiency of the surface case frames, we compile deep case frames from automatic semantic roles. We also consider quality management for both types of knowledge in order to get rid of the noise brought from the automatic analyses. The experimental results show that Chinese SRL can be improved using automatically acquired knowledge and the quality management shows a positive effect on this task.","['INTRODUCTION', 'RELATED WORK', 'BASELINE SRL MODELS', 'APPLYING HIGH-QUALITY SURFACE CASE FRAMES TO SRL', 'MAIN PROBLEM OF SURFACE CASE FRAMES', 'APPLYING HIGH-QUALITY DEEP CASE FRAMES TO SRL', 'EXPERIMENTS', 'CONCLUSION \\& FUTURE WORK']",,,,,,,,
V25N02-03.tex,,Text Simplification without Simplified Corpora,難解なテキストと平易なテキストからなる大規模な単言語パラレルコーパスを用いて，テキスト平易化が活発に研究されている．しかし，英語以外の多くの言語では平易に書かれた大規模なコーパスを利用できないため，テキスト平易化のためのパラレルコーパスを構築するコストが高い．そこで本研究では，テキスト平易化のための大規模な疑似パラレルコーパスを自動構築する教師なし手法を提案する．我々の提案するフレームワークでは，リーダビリティ推定と文アライメントを組み合わせることによって，生コーパスのみからテキスト平易化のための単言語パラレルコーパスを自動構築する．統計的機械翻訳を用いた実験の結果，生コーパスのみを用いて学習した我々のテキスト平易化モデルは，平易に書かれた大規模なコーパスを用いて学習した従来のテキスト平易化モデルと同等の性能で平易な同義文を生成できた．,"Several studies on automated text simplification are based on a large-scale monolingual parallel corpus constructed from a comparable corpus comprising complex text and simple text. However, constructing a parallel corpus for text simplification is expensive as large-scale simplified corpora are not available in many languages other than English. Therefore, we propose an unsupervised method that automatically builds a pseudo-parallel corpus to train a text simplification model. Our framework combines readability assessment and sentence alignment and automatically constructs a text simplification corpus from only a raw corpus. Experimental results show that a statistical machine translation model trained using our corpus can generate simpler synonymous sentences performing comparably to models trained using a large-scale simplified corpus.","['はじめに', '関連研究', '生コーパスから疑似パラレルコーパスを構築するためのフレームワーク', '単語分散表現のアライメントに基づく文間類似度を用いた文アライメント', '実験1: 難解な文と平易な文のアライメント', '実験2: 英語のテキスト平易化', '実験3: 日本語のテキスト平易化', 'おわりに']",,,,,,,,
V25N03-01.tex,,Analysis of Japanese Word Sense Disambiguation \\ through Kana-Kanji Conversion,本論文では，日本語語義曖昧性解消に存在する問題点を文中のひらがなを漢字に直すかな漢字換言タスクを通して明らかにする．素性について分散表現と自己相互情報量を組み合わせる手法を考案し実験を行った結果，かな漢字換言においてベースラインに比べ約 2 ポイント高い精度を得ることができた．日本語の語義曖昧性解消タスクを用いた検証においても，PMI を用い文全体から適切な単語を素性として加えることが有効であることを示した．かな漢字換言の利点を活かし，大量の訓練データを用いたときのかな漢字換言の精度の比較を行った結果，非常に大きい訓練データを用いた場合分散表現を用いたどの手法でもほぼ同じ精度を得られることがわかった．その一方で同じ精度を得るために必要な訓練データは指数関数的に増えていくため，少ない訓練データで精度を上げる手法が語義曖昧性解消において重要であることを確認した．また，BCCWJ と Wikipedia から作成した訓練データとテストデータを相互に使い実験し，各ドメインにあった訓練データを使うことが精度向上において重要であることを確認した．,"In this paper, we investigate a problem existing in Japanese word sense disambiguation (WSD) through a HiraganaKanji conversion task. In choosing words to consider as features, we propose a method that employs word embeddings and pointwise mutual information and evaluate the proposed method. The experimental results suggest that our method is more effective than other methods using word embeddings. We conduct an experiment using SemEval 2010 Japanese WSD Task and our proposed method achieve better accuracy. We also compare the accuracy when changing the amount of training data. We find that the difference in accuracy between the methods becomes small when a very large amount of training data is used. We have confirmed that the method of improving accuracy while using fewer training data is important in WSD because the number of sentences required to obtain high accuracy increases exponentially. We also experiment on the domain of data and confirmed that using datasets for ambiguity matching in each domain is important in improving accuracy.","['はじめに', '関連研究', 'かな漢字換言', '提案手法', '実験', '結論', '換言対象となるひらがなと漢字候補']",,,,,,,,
V25N03-02.tex,,Learning Semantic Textual Relatedness using Natural Deduction Proofs,文と文がどのような意味的関係にあるかという文間の関連性の計算は，情報検索や文書分類，質問応答などの自然言語処理の基盤を築く重要な技術である．文の意味をベクトルや数値で表現する手法は未だ発展途上であり，自然言語処理分野においては，様々な機械学習による手法が活発に研究されている．これらの手法では，文字や単語を単位としたベクトルを入力として，それらの表層的な出現パターンとその振る舞いを学習することで，文ベクトルを獲得している．しかし，否定表現を含む文など，文の構造的意味を正確に表現できるかは自明ではない．一方で，形式意味論においては，表現力の高い高階論理に基づいて意味の分析を行う研究が発展しているが，文間の関連性のような，連続的な意味的関係を表現することが困難である．そこで本研究では，機械学習と論理推論という二つの手法を組み合わせて文間の関連性を計算する手法を提案する．具体的には，文間の含意関係を高階論理の推論によって判定するシステムの実行過程から，文間の関連性に寄与する特徴を抽出し，文間の関連性を学習する．文間類似度学習と含意関係認識という2つの自然言語処理タスクに関して提案手法の評価を行った結果，推論の過程に関する情報を特徴量に用いることによって，いずれのタスクにおいても精度が向上した．また，含意関係認識用データセットの一つであるSICKデータセットの評価では，最高精度を達成した．,"Learning semantic textual relatedness is a core research subject in natural language processing. Vector-based models are often used to compute sentence representations from words or predicate-argument structures, but these models cannot capture semantics accurately with consistency. Conversely, logical semantic representations can capture sentence semantics in depth and with much greater accuracy, but their symbolic nature does not offer graded notions of textual similarity. We propose a method for learning semantic textual relatedness by combining shallow features with features extracted from natural deduction proofs using bidirectional entailment relations between sentence pairs. For the natural deduction proofs, we use ccg2lambda, a higher-order automatic inference system that converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs. We evaluate our system using two major NLP tasks: learning textual similarity and recognizing textual entailment. Our experiments demonstrate that our approach can outperform other logic-based systems and we obtain high performance levels for the RTE task using the SICK dataset. Our evaluations also demonstrate that features derived from the proofs are effective for learning semantic textual relatedness and we quantify our contribution to the research area.","['はじめに', '関連研究', '提案手法', '実験', 'まとめ']",,,,,,,,
V25N04-01.tex,,Bunsetsu-based Dependency Relation and Coordinate Structure Annotation on `Balanced Corpus of Contemporary Written Japanese',本稿では『現代日本語書き言葉均衡コーパス』のコアデータに対する文節係り受け・並列構造情報\modified{の,This article presents syntactic annotation for `Balanced Corpus of Contemporary Written Japanese'. We propose a syntactic annotation schema wherein the {\it bunsetsu,"['はじめに', '『現代日本語書き言葉均衡コーパス』', 'アノテーション作業で扱った問題', '既存のアノテーション基準との違いの概要', '各論', 'BCCWJ-DepParaの基礎統計', 'おわりに']",,,,,,,,
V25N04-02.tex,,Comparison of Template- and Neural-based Methods for Sports Summary Generation,本研究では，日本で人気のある野球に着目し，Play-by-playデータからイニングの要約文の生成に取り組む．Web上では多くの野球に関する速報が配信されている．戦評は試合終了後にのみ更新され，“待望の先制点を挙げる”のような試合の状況をユーザに伝えるフレーズ（本論文ではGame-changing Phrase; GPと呼ぶ）が含まれているのが特徴であり，読み手は試合の状況を簡単に知ることができる．このような特徴を踏まえ，任意の打席に対して，GPを含む要約文を生成することは，試合終了後だけでなく，リアルタイムで試合の状況を知りたい場合などに非常に有益であるといえる．そこで，本研究ではPlay-by-playデータからGPを含む要約文の生成に取り組む．また，要約生成手法としてテンプレート型文生成手法とEncoder-Decoderモデルを利用した手法の2つを提案する．,"In this study, we propose inning summarization methods to generate a simple and sophisticated summary of baseball games using play-by-play data. We focus on the two information sources of inning reports and game summaries. First, we generate a basic sentence using an inning report; further, the basic sentence is integrated with an explanatory phrase to generate inning summaries that contain expressions such as ``the long-awaited first score'' in game summaries. We refer to these phrases as the game-changing phrases (GPs). Readers can easily understand the situation of a game using GPs. In this study, we investigate the template- and neural-based methods of summary generation. Additionally, we evaluate the two methods and discuss their advantages and disadvantage.","['はじめに', '関連研究', 'テンプレート型生成手法', 'ニューラル型生成手法', '実験', 'おわりに', 'ROUGEによる評価']",,,,,,,,
V25N04-03.tex,,Hierarchical Coordinate Structure Analysis for Japanese Statutory Sentences Using Neural Language Models,,"We propose a method for analyzing the hierarchical coordinate structure of Japanese statutory sentences using neural language models (NLMs). Our method deterministically identifies hierarchical coordinate structures according to their rigorously defined descriptive rules. In addition, our method identifies all conjuncts in each coordinate structure using NLM-based scoring. Furthermore, it does not rely on any training data labeled with coordinate structures. An experiment demonstrates that our method drastically outperforms an existing method for Japanese statutory sentences.","['Introduction', 'Related Work', 'Coordinate Structures Unique to Japanese Statutory Sentences', 'Conventional Method for Japanese Statutory Sentences', 'Neural Language Model', 'Proposed Method', 'Experiment', 'Discussion', 'Conclusion']",,,,,,,,
V25N04-04.tex,,Grammatical Error Detection Using Error- and Grammaticality-Specific Word Embeddings,本稿では，文法誤り検出のための正誤情報と文法誤りパターンを考慮した単語分散表現の学習手法を提案する．これまでの文法誤り検出で用いられている単語分散表現の学習では文脈だけをモデル化しており，言語学習者に特有の文法誤りを考慮していない．そこで我々は，正誤情報と文法誤りパターンを考慮することで文法誤り検出に特化した単語分散表現を学習する手法を提案する．正誤情報とは，n-gram単語列内のターゲット単語が誤っているのか正しいのかというラベルである．これは単語単位の誤りラベルを元に決定される．誤りパターンとは，学習者が誤りやすい単語の組み合わせである．誤りパターンは大規模な学習者コーパスから単語分散表現の学習のために抽出することが可能である．この手法で学習した単語分散表現で初期化したBidirectional Long Short-Term Memoryを分類器として使うことで，First Certificate in Englishコーパスに対する文法誤り検出において世界最高精度を達成した．,"In this study, we improve grammatical error detection by learning word embeddings that consider grammaticality and error patterns. Most existing algorithms for learning word embeddings usually model only the syntactic context of words and do not consider grammatical errors specific to language learners. Therefore, we propose methods to learn word embeddings specialized for grammatical errors by considering grammaticality and grammatical error patterns. We determine grammaticality of n-gram sequence from the annotated error tags and extract grammatical error patterns for word embeddings from large-scale learner corpora. Experimental results show that a bidirectional long-short term memory model initialized by our word embeddings achieved the state-of-the-art accuracy by a large margin in an English grammatical error detection task on the First Certificate in English dataset.","['はじめに', '先行研究', '単語分散表現学習の従来手法：C\\&W Embedding', '正誤情報と誤りパターンを考慮した単語分散表現', '文法誤り検出の実験設定', '文法誤り検出の実験結果', '考察', 'まとめ']",,,,,,,,
V25N04-05.tex,,Similarity and Replaceability Feature Representations of Word Sequences for Identifying Coordination Boundaries,並列構造解析の主たるタスクは並列する句の範囲を同定することである．並列構造は文の構文・意味の解析において有用な特徴となるが，これまで決定的な解析手法が確立されておらず，現在の最高精度の構文解析器においても誤りを生じさせる主たる要因となっている．既存の並列句範囲の曖昧性解消手法は並列構造の類似性のみの特性や構文解析器の結果に強く依存しているという問題があった．本研究では，近年自然言語解析に広く使用されているリカレントニューラルネットワークを用いて，構文解析の結果を用いずに単語の表層形と品詞情報のみから並列句の類似性と可換性の特徴ベクトルを計算し，並列構造の範囲を予測する手法を提案する．Penn TreebankとGENIAコーパスを用いた実験の結果，提案手法によって先行研究を上回る解析精度を得た．,The task of coordinate structure analysis is to identify coordinating phrases called {\it conjuncts,"['はじめに', '並列構造解析', '関連研究', '提案手法', '実験', 'おわりに']",,,,,,,,
V25N04-06.tex,,Domain Adaptation using Word Embeddings for Word Sense Disambiguation,,"In this paper, we propose domain adaptation using word embeddings for word sense disambiguation (WSD). The validity for WSD of word embeddings derived from a huge corpus such as Wikipedia had already been shown, but their validity in a domain adaptation framework has not been previously discussed. If word embeddings are valid in this new context, the impact of the document type of the corpora on WSD is still unknown. Therefore, we investigate the performances of domain adaptation in WSD using word embeddings from the source, target and general corpora and examine (1) whether the word embeddings are valid for domain adaptation of WSD and (2) if they are, the effects of the document type of the corpora from which the word embeddings are derived. We used three corpora of distinct document types and performed domain adaptation experiments using the document types as the domains. The experiments, conducted using Japanese corpora, revealed that the accuracy of WSD was highest when we used the word embeddings obtained from the target corpora together with a general corpora.","['Introduction', 'Related Work', 'Domain Adaptation in WSD using Word Embeddings', 'Experiment', 'Discussion', 'Conclusions']",,,,,,,,
V25N05-01.tex,,Improved BTG-based Preordering for SMT via Parallel Parameter Averaging: An Empirical Study,,"Preordering has proven useful in improving the translation quality of statistical machine translation (SMT), especially for language pairs with different syntax. The top-down bracketing transduction grammar (BTG)-based preordering method \cite{nakagawa2015efficient","['Introduction', 'Preordering', 'Learning Preordering Models', 'Solutions to Reduce Noise', 'Experimental Setup', 'Experimental Results', 'Conclusion']",,,,,,,,
V25N05-02.tex,,Replacement of Unknown Words Using an Attention Model in Japanese to English Neural Machine Translation,"ニューラル機械翻訳では，従来の統計的機械翻訳に比べ文法的に流暢な文が生成されるが，出力結果に未知語が含まれることがしばしば指摘される．この問題に対処する方法としては，学習コーパス中の低頻度語を分割したり，未知語に位置情報を付け加えるなどの方法があるが，どれも日英翻訳では効果が低い．そこで本論文では，アテンションから構成した単語アライメント表を用いて出力文中の未知語と対応する入力文中の単語を見つけ，その単語を翻訳した単語で未知語を置き換えることで未知語をなくす手法を提案する．本論文の有効性を示すために ASPEC, NTCIR-10 の 2 種類のコーパスを用いて実験を行った結果，本論文で提案する単語アライメント表の構成法を用いると，未知語を全く発生させず，かつ，BLEU 値を向上させることができた．","It is well known that machine translation using recurrent neural networks often composes fluent sentences but may include many unknown words. Although there have been many works to address the unknown word problem, they are ineffective in Japanese to English translation. In this study, we propose a hybrid method that makes an alignment table using an attention weight matrix, detects input words that are aligned with each unknown words, and finally replaces those unknown words with the translated words using a statistical machine translation method. We evaluate our approach by using two corpora: ASPEC and NTCIR-10. The results showed that the proposed method generated no unknown words and improved the BLEU (BiLingual Evaluation Understudy) score.","['はじめに', '背景', '手法', '実験', 'おわりに']",,,,,,,,
V25N05-03.tex,,Between Reading Time \\ and the Information Status of Noun Phrases,日本語は冠詞のない言語である．日本語名詞句の情報の状態は，テキストに陽に表出せず，限られた文脈情報や世界知識のみに基づく手法では推定することは難しい．情報の状態は情報の新旧や定・不定などの観点で分析される．しかしながら，日本語の言語処理においては，この概念が適切に扱われていない．そこで，\modified{本稿では，まず，日本語名詞句の情報状態について解説する．,"Japanese noun phrases are not marked by articles. The information status of Japanese noun phrases is not overt. Due to limited contextual information and world knowledge, it is difficult to estimate the information status, which is analyzed through the given/new status or indefinite/definite status. However, in Japanese language processing, the notion of the information status is yet to be understood. In this paper, we explain the information status of Japanese noun phrases. Then, we explore how the information status of Japanese noun phrases is estimated through the reading time. As a first step, we investigate the correlation between reading time and the information status of Japanese noun phrases. The statistical evaluation shows that readers' information status affects reading time in Japanese.","['はじめに', '関連研究', '名詞句の情報の状態について', '読み時間の収集方法', 'データと分析手法', '結果と考察', 'おわりに', '固定要因の詳細']",,,,,,,,
V25N05-04.tex,,"A Reference-less Evaluation Metric Based on Grammaticality, Fluency, and Meaning Preservation in Grammatical Error Correction",文法誤り訂正の研究開発では，訂正システムの性能を自動評価することは重要であると考えられている．従来の自動評価手法では参照文が必要であるが，参照文は人手で作成しなければならないため，コストが高く網羅性に限界がある．この問題に対処するために，参照文を用いず，文法性の観点によって訂正を評価する参照無し手法が提案されたが，従来の参照有り手法の性能を上回ることはできなかった．そこで本研究では，先行研究で提案された手法を拡張し，参照無し手法の可能性について調査する．具体的には，文法性に加えて流暢性と意味保存性を組み合わせた参照無し手法が，従来の参照有り手法よりも人手評価スコアを正確に予測できることを実験的に示す．また，参照無し手法は文単位でも適切な評価が可能であることと，文法誤り訂正システムに応用可能であることを示す．,"In grammatical error correction (GEC), the automatic evaluation of system performance is thought to be an essential driving force. Previous methods for automated system assessment require gold-standard references, which have to be created manually and thus tend to be both expensive and limited in coverage. To address this problem, a reference-less approach has recently emerged; however, previous reference-less metrics, which only consider the grammaticality of system outputs, have not performed as well as reference-based metrics. In this study, we explore the potential of extending a prior grammaticality-based method to establish a reference-less evaluation method for GEC systems. We empirically show that a reference-less metric that combines both fluency and meaning preservation with grammaticality provides a better estimate of manual scores than that of commonly used reference-based metrics. Additionally, we show that the reference-less metric can provide appropriate evaluation at the sentence-level and that it can be applied to GEC systems.","['序論', '自動評価尺度の評価方法', '既存の評価尺度', '提案手法', '実験', '結論']",,,,,,,,
V25N05-05.tex,,Detecting Untranslated Content for Neural Machine Translation,ニューラル機械翻訳 (NMT) は入力文の内容の一部が翻訳されない場合があるという問題があるため，NMTの実用には訳出されていない内容を検出できることが重要である．著者らはアテンションの累積確率と出力した目的言語文から入力文を生成する逆翻訳の確率という2種類の確率による，入力文の内容の欠落に対する検出効果を調査した．日英の特許翻訳での訳抜けした内容の検出実験を実施し，アテンションの累積確率と逆翻訳の確率はいずれも効果があり，逆翻訳はアテンションより効果が高く，これらを組み合わせるとさらに検出性能が向上することを確認した．また，訳抜けの検出を機械翻訳結果の人手修正のための文選択に応用した場合に効果があることが分かった．,"Despite its promise, neural machine translation (NMT) presents a serious problem in that source content may be mistakenly left untranslated. The ability to detect untranslated content is important for the practical use of NMT. We evaluated two types of probability with which to identify untranslated content: the cumulative attention probability and the back translation probability from a target sentence to the source sentence. Experiments were conducted to discover missing content in Japanese to English patent translations. The results of the investigation revealed that both the types of probability were each effective, back translation was more effective than attention, and the combination of the two resulted in further improvements. Furthermore, we confirmed that the detection of untranslated content was effectual in terms of sentence selection for the human post-editing processing of machine translation results.","['はじめに', 'ニューラル機械翻訳', '訳抜けした内容の検出', '翻訳スコアへの適用', 'ポストエディットのための文選択', '実験', '関連研究', 'おわりに']",,,,,,,,
V25N05-06.tex,,Syntactic Matching Methods in Pivot Translation,,"The pivot translation is useful method for translating between languages that contain little or no parallel data by utilizing equivalents in an intermediate language such as English. Commonly, phrase-based or tree-based pivot translation methods merge source--pivot and pivot--target translation models into a source--target model. This tactic is known as triangulation. However, the combination is based on the surface forms of constituent words, and it often produces incorrect source--target phrase pairs because of interlingual differences and semantic ambiguities in the pivot language. The translation accuracy is thus degraded. This paper proposes a triangulation approach that utilizes syntactic subtrees in the pivot language to avoid incorrect phrase combina\-tions by distinguishing pivot language words by their syntactic roles. The results of the experiments conducted on the United Nations Parallel Corpus demonstrate that the proposed method is superior to other pivot translation approaches in all tested combinations of languages.","['Introduction', 'Machine Translation Framework', 'Pivot Translation Methods', 'Triangulation with Syntactic Matching', 'Experiments', 'Conclusion']",,,,,,,,
V26N01-01.tex,,Japanese Universal Dependencies Corpora,Universal Dependencies (UD) は，共通のアノテーション方式で多言語の構文構造コーパスを言語横断的に開発するプロジェクトである．  2018年6月現在，約60の言語で100以上のコーパスが開発・公開されており，多言語構文解析器の開発，言語横断的な構文モデルの学習，言語間の類型論的比較などさまざまな研究で利用されている． 本稿では UD の日本語適応について述べる．日本語コーパスを開発する際の問題点として品詞情報・格のラベル・句と節の区別について議論する．また，依存構造木では表現が難しい，並列構造の問題についても議論する．最後に現在までに開発したUD 準拠の日本語コーパスの現状を報告する．,"Universal Dependencies (UD) is an international project to develop multilingual dependency treebanks in a uniform annotation scheme, aiming at cross lingual learning from multilingual corpora and quantitative comparison of languages. As of mid 2018, more than 100 corpora for about 60 languages have been released. This paper describes the definition of annotations for Japanese. We discuss the localization issues of PoS tags, case marking dependency labels and the difference between phrase and clause in Japanese.  We present the issues of coordination structures, which cannot be represented solely by the dependency tree structures. We also report the current status of UD Japanese corpora we have constructed.","['はじめに', 'Universal Dependencies', '日本語UDの定義', '日本語特有の論点', '並列構造', '日本語の言語資源と変換', '関連研究', 'おわりに']",,,,,,,,
V26N01-02.tex,,Extractive and Abstractive Summarization for Multiple-sentence Questions,インターネット上のコミュニティQAサイトや学会での質疑応答の場面などにおいて，人々は多くの質問を投げかける．このような場面で用いられる質問には，核となる質問に加え補足的な情報をも付与され，要旨の把握が難しくなることもある．補足的な情報は正確な回答を得るには必要であるが，質問の要旨を素早く把握したいといった状況においては必ずしも必要でない．そこで本稿では，新たなテキスト要約課題として，複数文から構成される質問{テキスト,"Questions are asked in many situations such as sessions at conferences and inquiries through emails. In such situations, questions can be often lengthy and hard to understand, because they often contain peripheral information in addition to the main focus of the question. Thus, we propose the task of question summarization. In this research, we firstly analyzed question-summary pairs extracted from a Community Question Answering (CQA) site, and found that there exists the questions that cannot be summarized by extractive approaches, but abstractive approaches are required. We created a dataset by regarding the question-title pairs posted on a CQA site as question-summary pairs. By using the data, we trained extractive and abstractive summarization models, and compared them based on the ROUGE score and manual evaluation.  Our experimental results show an abstractive method, the encoder-decoder with the copying mechanism, achieves better scores both on ROUGE-2 F-measure and the evaluation by human judges.","['はじめに', '{質問要約課題', '関連研究', '質問応答サイトからの質問{テキスト', 'データと比較手法', '実験', 'おわりに']",,,,,,,,
V26N01-03.tex,,Discourse Act Classification Using Discussion Patterns with Neural Networks,本稿ではオンライン議論における談話行為を分類するモデルを提案する．提案モデルでは談話行為を分類するために，ニューラルネットワークを用いて議論のパターンを学習する．談話行為の分類において議論のパターンを取り入れる重要性は既存の研究においても確認されているが，対象としている議論に併せたパターン素性を設計する必要があった．提案モデルではパターン素性を用いずに，木構造およびグラフ構造を学習する層を用いて議論のパターンを学習する．提案モデルを Reddit の談話行為を分類するタスクで評価したところ，従来手法と比較して Accuracy で 1.5\%，$\mathrm{F,"We proposed a model that classifies discussion discourse acts using discussion patterns with neural networks. Several attempts have been made in earlier works to analyze texts that are used in various discussions. The importance of discussion patterns has been explored in those works but their methods required a sophisticated design to combine pattern features with a classifier. Our model introduces tree learning approaches and a graph learning approach to capture discussion patterns without pattern features. In an evaluation to classify discussion discourse acts in Reddit, the model achieved improvements of 1.5\% in accuracy and 2.2 in $\mathrm{F","['はじめに', '関連研究', '提案モデル', '実験', '考察', 'おわりに']",,,,,,,,
V26N01-04.tex,,A* CCG Parsing with a Supertag and Dependency Factored Model,,"Combinatory Categorial Grammar (CCG) is a strongly lexicalized grammatical formalism, in which the vast majority of parsing decisions involve assigning a supertag to indicate the correct syntactic role. We propose an A* parsing model for CCG that exploits this characteristics, by modeling the probability of a tree through the supertags and resolving the remaining ambiguities by its syntactic dependencies. The key of our method is that it predicts the probabilities of supertags and dependency heads independently using a strong unigram model defined over bi-directional LSTMs. The factorization allows precomputation of probabilities for all possible trees for a sentence, which, combined with an A* algorithm, enables very efficient decoding. The proposed model achieves the state-of-the-art results on English and Japanese CCG parsing. In addition, we conduct Recognizing Textual Entailment (RTE) experiments by integrating the proposed parser within logic-based RTE systems. We demonstrate that such integration leads to improved performance in English RTE experiments.","['Introduction', 'Background', 'Proposed Method', 'CCG to Dependency Conversion', 'Tri-training', 'Parsing Experiments', 'RTE Experiments', 'Related Work', 'Conclusion', 'Details of English CCG Grammar', 'Details of Japanese CCG Grammar']",,,,,,,,
V26N01-05.tex,,Interpretation of Implicit Conditions in Database \\ Search Dialogues,本論文は，データベース検索対話においてデータベースフィールドに直接言及しないが，データベースへのクエリを構成する上で有益な情報をユーザ発話から取り出す課題を提案する．このような情報を本論文では非明示的条件と呼ぶ．非明示的条件を解釈し，利用することによって，対話システムはより自然で効率的な対話を行うことができる．本論文では，非明示的条件の解釈を，ユーザ発話をデータベースフィールドに関連付け，同時にその根拠となる発話の断片を抽出する課題として定式化する．この課題を解くために，本論文では，サポートベクタマシン (SVM)，回帰型畳込みニューラルネットワーク (RCNN)，注意機構を用いた系列変換による3つの手法を実装した．不動産業者と顧客との対話を収集したコーパスを用いた評価の結果，注意機構を用いた系列変換による手法の性能が優れていた．,This study focuses on database (DB) search dialogue and proposes to employ user utterance information that does not directly mention the DB field of the back-end system but is useful for constructing DB queries. We name this type of information \textit{implicit conditions,"['はじめに', '関連研究', 'データと問題設定', 'データベースフィールドへの分類および根拠抽出手法', '評価実験', '結論']",,,,,,,,
V26N01-06.tex,,Recursive Neural Network-Based Preordering for Statistical Machine Translation and Its Analysis,統計的機械翻訳において，原言語と目的言語における語順の違いは翻訳精度に大きく影響することが知られている．この問題に対して，翻訳器に入力する前に原言語の語順を並び替える事前並び替え手法が提案されている．先行研究において最高性能を達成しているNakagawaの手法では事前並び替えの学習のために素性テンプレートの設計が必要である．本稿では，データから直接素性ベクトルを学習するRecursive Neural Networkを用いた事前並び替え手法を提案する．英日・英仏・英中の言語対を用いた評価実験の結果，英日翻訳では素性テンプレートの設計を必要とせず，Nakagawaの手法と遜色ない精度を達成した．また実験結果の詳細な分析を行い，事前並び替えに影響を与える要因を分析した．そして近年の機械翻訳において主流となっているニューラル機械翻訳における事前並び替えの効果についても検証した．,"Word-order differences between source and target languages significantly affect statistical machine translation. This problem can be effectively addressed by preordering. A state-of-the-art preordering method would involve manually designed feature templates. In this paper, we propose a method that uses a recursive neural network that can learn end-to-end preordering. English-Japanese, English-French, and English-Chinese datasets are extensively evaluated. The results confirm that this method achieves an English-to-Japanese translation quality that is comparable with that of the state-of-the-art method, without manually designed feature templates. In addition, a detailed analysis examines the factors affecting preordering and translation quality as well as the effects of preordering in neural machine translation.","['はじめに', '関連研究', 'Recursive Neural Networkによる事前並び替え', '翻訳性能評価', 'まとめ']",,,,,,,,
V26N01-07.tex,,Comprehensive Annotation and Analysis of \\ Temporal Information of Events,テキスト中には過去・現在・未来における様々な事象が記述されており，その内容を理解するためにはテキスト中の時間情報を正確に解釈する必要がある．これまで，事象情報と時間情報を関連付けたコーパスが構築されてきたが，これらは開始と終了が比較的明確な事象に着目したものであった．本研究では，網羅的かつ表現力豊かな時間情報アノテーション基準を導入し，京都大学テキストコーパス中の113文書に対するアノテーションとその分析を行った．同コーパスには既に述語項関係や共参照関係のアノテーションガなされており，本アノテーションと合わせてテキスト中の事象・エンティティ・時間を対象とした統合的な時間情報解析に活用することが可能となった．,"Various past, present, and future events are described in text. To understand such text, correct interpretation of temporal information is essential. For this purpose, many corpora associating events with temporal information have been constructed. Although these corpora focus on expressions with strong temporality, many expressions have weak temporality, but are still clues to understanding temporal information. In this article, we propose an annotation scheme that comprehensively anchors textual expressions to the time axis. Using this scheme, we annotated 113 documents in the Kyoto University Text Corpus. Because the corpus has already been annotated with predicate-argument structures and coreference relations, it can now be utilized for integrated information analysis of events, entities, and time.","['はじめに', '関連研究', 'アノテーション基準', 'アノテーション結果', 'アノテータ間でのタグの揺れ分析', '時間情報推定', '本アノテーションの応用', 'おわりに']",,,,,,,,
V26N01-08.tex,,Neural Machine Translation with CKY-based Convolutional Attention,本論文では，ニューラル機械翻訳 (NMT) の性能を改善するため，CKY アルゴリズムから着想を得た，畳み込みニューラルネットワーク (CNN) に基づく新しいアテンション構造を提案する．提案のアテンション構造は，CKY テーブルを模倣した CNN を使って，原言語文中の隣接する単語／句の全ての可能な組み合わせを表現する．提案のアテンション構造を組み込んだ NMT は，CKY テーブルの各セルに対応する CNN の隠れ状態に対するアテンションスコア（言い換えると，原言語文中の単語の組み合わせに対するアテンションスコア）に基づき目的言語の文を生成する．従来の文構造に基づく NMT は予め構文解析器で解析した文構造を活用するが，提案のアテンション構造を用いる NMT は，原言語文の構文解析を予め行うことなく，原言語の文に潜む構造に対するアライメントを考慮した翻訳を行うことができる．Asian Scientific Paper Excerpt Corpus (ASPEC) 英日翻訳タスクの評価実験により，提案のアテンション構造を用いることで，従来のアテンション構造付きのエンコーダデコーダモデルと比較して，1.43 ポイント BLEU スコアが上昇することを示す．さらに，FBIS コーパスにおける中英翻訳タスクにおいて，提案手法は，従来のアテンション構造付きのエンコーダデコーダモデルと同等かそれ以上の精度を達成できることを示す．,"This paper proposes a new attention mechanism for neural machine translation (NMT) based on convolutional neural networks (CNNs), which is inspired by the CKY algorithm. The proposed attention represents every possible combination of source words (e.g., phrases and structures) through CNNs, which imitates the CKY table in the algorithm. NMT, incorporating the proposed attention, decodes a target sentence on the basis of the attention scores of the hidden states of CNNs. The proposed attention enables NMT to capture alignments from underlying structures of a source sentence without sentence parsing. The evaluations on the Asian Scientific Paper Excerpt Corpus (ASPEC) English-Japanese translation task show that the proposed attention gains 1.43 points in BLEU as compared to a conventional attention-based encoder decoder model.  Furthermore, the proposed attention is at least comparable to, or better than, a conventional attention-based encoder decoder model on the FBIS Chinese-English translation task.","['はじめに', '関連研究', 'アテンションに基づくニューラル機械翻訳(ANMT)', '提案モデル', '実験', '考察', 'まとめ']",,,,,,,,
V26N01-09.tex,,Neural Network-based Chinese Joint Syntactic Analysis,ニューラルネットワークに基づく係り受け解析モデルは，近年の深層学習を利用した言語処理研究の中でも大きな潮流となっている．しかしながら，こうした係り受け解析モデルを中国語などの言語に適用した際には，パイプラインモデルとして同時に用いられる単語分割や品詞タグ付けモデルの無視できない誤りによって性能が伸び悩む問題が存在する．これに対しては，単語分割・品詞タグ付けと係り受け解析の統合モデルを利用し，単語分割と構文木作成とを同時に行うことでその双方の改善が期待される．加えて，中国語においては個々の文字が固有の意味を持ち，構文解析では，文字やその組み合わせである文字列もしくは部分単語の情報が単語単位の情報と並んで本質的な役割を果たすことが期待される．本研究では，ニューラルネットワークに基づいて，単語分割と品詞タグ付け，もしくは単語分割と品詞タグ付け，係り受け解析の統合構文解析を行うモデルを提案する．また，同時に，文字列や部分単語の情報を捉えるために，文字や単語の分散表現に加えて，文字列の分散表現を利用する．,"Recently, dependency parsers with neural networks have outperformed existing parsers. When these parsing models are applied to Chinese sentences, they are used in a pipeline model with word segmentation and POS tagging models. In such cases, parsing models do not work well because of word segmentation and POS tagging errors. This can be solved by joint models of word segmentation, POS tagging and dependency parsing. In addition to this, Chinese characters have their own meanings, so the meanings of characters, character strings and sub-words are as important as the meanings of words in dependency parsing. In this study, we propose a neural network-based joint word-segmentation, POS tagging and dependency parsing model in addition to a joint word-segmentation and POS tagging model. We exploit not only word and character embeddings but also character string embeddings in all our models.","['はじめに', '関連研究', 'モデル', '実験', '将来研究', '結論']",,,,,,,,
V26N02-01.tex,,Automatically Computable Metrics to Generate Metaphorical Verb Expressions,,"The automatic generation of metaphorical expressions helps us write imaginative texts such as poems or novels. This paper proposes a new metaphor generation task, evaluation metrics, and a method to solve the task. Our task is formalized as a problem of finding metaphorical paraphrases for a literal Japanese phrase consisting of a subject, an object, and a verb. We use four evaluation metrics: \emph{synonymousness","['Introduction', 'Related Works', 'Task', 'Method', 'Dataset', 'Experiment', 'Conclusion and Future Work']",,,,,,,,
V26N02-02.tex,,Between Reading Time and Clause Boundaries in Japanese ---Wrap-up Effect in a Head-Final Language---,本論文では，リーダビリティ評価を目的として，日本語テキストの読み時間と節境界分類の対照分析を行う．日本語母語話者の読み時間データ BCCWJ-EyeTrack と節境界情報アノテーションを『現代日本語書き言葉均衡コーパス』上で重ね合わせ，ベイジアン線形混合モデルを用いて節末で，どのように読み時間が変わるかについて検討した．結果，英語などの先行研究で言われている節末で読み時間が長くなるという wrap-up effect とは反対の結果が得られた．他の結果として，節間の述語項関係が読み時間の短縮に寄与することがわかった．,"This paper presents a contrastive analysis between reading time and clause boundary categories in the Japanese language in order to estimate text readability. We overlaid reading time data of BCCWJ EyeTrack, and clause boundary categories annotation on the Balanced Corpus of Contemporary Written Japanese. Statistical analysis based on the Bayesian linear mixed model shows that the reading time behaviours differ among the clause boundary categories. The result does not support the wrap-up effects of clause-final words. Another result we arrived at is that the predicate-argument relations facilitate the reading speed of native Japanese speakers.","['はじめに', '関連研究', 'データ', '統計モデル', '結果', 'おわりに', '詳細な結果']",,,,,,,,
V26N02-03.tex,,Annotating a Driving Experience Corpus with Behavior and Subjectivity,,"To communicate with humans in a human-like manner, systems need to understand behavior and psychological states in situations of human-machine interactions, such as in the cases of autonomous driving and nursing robots. We focus on driving situations as they are part of our daily lives and concern safety. To develop such systems, a corpus annotated with behavior and subjectivity in driving situations is necessary. In this study, subjectivity includes emotions, polarity, sentiments, human judgments, perceptions, and cognitions. We construct a driving experience corpus (DEC) (261 blog articles, 8,080 sentences) with four manually annotated tags. First, we annotate spans with driving experience tags (DE). Then, three tags, other's behavior (OB), self-behavior (SB), and subjectivity (SJ), are annotated within DE spans. In addition to describing the guidelines, we present corpus specifications, agreement between annotators, and three major difficulties during the development: the extended self, important information, and voice in mind. Automatic annotation experiments were conducted on the DEC using Conditional Random Fields-based methods. On the test set, the F-scores were about .55 for both OB and SB and approximately. 75 for SJ, respectively. We provide error analysis that reveals difficulties in interpreting nominatives and differentiating behavior from subjectivity.","['Introduction', 'Related Work', 'Guidelines', 'Annotation', 'Automatic Annotation Experiments', 'Conclusions and Future Work']",,,,,,,,
V26N02-04.tex,,Unsupervised All-words WSD Using Synonyms and Embeddings,all-words 語義曖昧性解消（以下all-words WSD (word sense disambiguation)）とは文書中のすべての単語の語義ラベルを付与するタスクである．単語の語義は文脈，すなわち周辺の単語によって推定でき，周辺の単語同士が類似している場合中心の単語同士の語義も類似していると考える．そこで本研究では，対象単語とその類義語群から周辺単語の分散表現を作成し，ユークリッド距離を計算することで対象単語の語義を予測した．また，語義の予測結果をもとにコーパスを語義ラベル列に変換し，語義の分散表現を作成した．語義の分散表現を用いて周辺単語ベクトルを作成し直し，再び語義の予測を行った．コーパスには分類語彙表番号がアノテーションされた『現代日本語書き言葉均衡コーパス』(BCCWJ) を利用した．本研究では分類語彙表における分類番号を語義とし，類義語も分類語彙表から取得した．本研究では，提案手法とランダムベースライン，Pseudo Most Frequent Sense (PMFS)，Yarowskyの手法，LDAWNを比較し，提案手法が勝ることを示した．,"All-words word-sense disambiguation (all-words WSD) involves identifying the senses of all words in a document. Since a word's sense depends on the context, such as surrounding words, similar words are believed to have similar sets of surrounding words. Therefore, we predict target word senses by calculating Euclidean distances between the target words' surrounding word vectors and their synonyms using word embeddings. In addition, we replace word tokens in the corpus with their concept tags, that is, article numbers of the Word List by Semantic Principles using prediction results. After that, we create concept embeddings with the concept tag sequence and predict the senses of the target words using the distances between surrounding word vectors, which consist the word and concept embeddings. This paper shows that concept embedding improved the performance of Japanese All-words WSD.","['はじめに', '関連研究', '比較対象となるベースライン手法', '概念辞書の類義語と分散表現を利用したall-words WSD', '評価実験', '結果', '考察', 'おわりに']",,,,,,,,
V26N02-05.tex,,Detecting Nonstandard Word Usages on Social Media,ソーシャルメディアにおいては，辞書に掲載されているような用法とは全く異なる使われ方がされている単語が存在する．本論文では，ソーシャルメディアにおける単語の一般的ではない用法を検出する手法を提案する．提案手法では，ある単語が一般的ではない使われ方がされていた場合，その周辺単語は一般的な用法として使われた場合の周辺単語と異なるという仮説に基づいて，着目単語とその周辺単語の単語ベクトルを利用し，注目している単語の周辺単語が均衡コーパスにおける一般的な用法の場合の周辺単語とどの程度異なっているかを評価することにより，一般的ではない用法の検出を行う．ソーシャルメディアにおいて一般的ではない用法を持つ40単語を対象に行った実験の結果，均衡コーパスと周辺単語ベクトルを用いる提案手法の有効性を確認できた．また，一般的でない用法の検出においては，単語ベクトルの学習手法，学習された単語ベクトルの扱い方，学習コーパスを適切に選択することが重要であることがわかった．,"We focus on nonstandard usages of common words on social media, where words, sometimes, are used in a totally different manner from that of their original or standard usage. In this work, we attempt to distinguish nonstandard usages on social media from standard ones in an unsupervised manner. We also constructed new Twitter dataset consisting of 40 words with nonstandard usages and then used the dataset for evaluation in an experiment. For this task, our basic idea is that nonstandard usage can be measured by the inconsistency between the target word's expected meaning and the given context. For this purpose, we use context embeddings derived from word embeddings. Our experimental results show that the model leveraging the context embedding outperforms other methods and also provide us with findings, for example, on how to construct context embeddings, and which corpus to use.","['はじめに', '評価データの作成', '単語の一般的ではない用法の検出', '実験', '関連研究', 'まとめ']",,,,,,,,
V26N02-06.tex,,Classification of Phonological Changes Reflected in Text: Toward a Characterization of Written Utterances,「こりゃひでえ」（元の形：「これはひどい」）のような音変化表現は，対話エージェントの発話や小説のセリフの自動生成において，話者であるキャラクタを特徴付けるための強力な手段となると考えられる．音変化表現を発話のキャラクタ付けに利用するために，本研究では，(i)キャラクタの発話に現れる音変化表現を収集し，(ii)それらを基に，音変化表現を人為的に発生させるための知識を整理した．具体的には，収集した音変化表現を現象と生起環境の観点で分類し，137種類のパターンとして整理した．そして，これらのパターンが小説やコミックで用いられる音変化表現の80\%以上をカバーすることを確認した．さらに，(iii)音変化表現がキャラクタらしさを特徴付ける手段になるという仮説を検証するために，小説やコミックにおける発話文の話者（キャラクタ）を推定する実験を行い，音変化表現のパターンの情報を利用することで，推定性能が向上するキャラクタが存在することを確認した．,"Phonological changes reflected in text can be powerful in characterizing utterances of dialogue agents or characters' lines in narratives. To use phonological changes to automatically characterize utterances, (i) we collected phonologically changed expressions from characters' written utterances and (ii) formalized the knowledge required to generate phonologically changed expressions. In particular, we categorized the expressions into 137 patterns by analyzing them from the points of the phenomena concerned and the environments of the occurrences. We experimentally confirmed that the patterns cover more than 80\% of the phonologically changed expressions used in novels and comics. Furthermore, (iii) to investigate whether phonological change patterns can be effective in characterization, we conducted an experiment that estimated speakers (characters) of the utterances and confirmed that the information on phonological changes improved the performance of speaker estimation for several characters.","['はじめに', '関連研究', '音変化表現の種類に関する分析', 'パターンの網羅性の検証', '発話のキャラクタ付けにおける音変化表現の有用性検証', 'まとめと今後の課題', '検証用データの書誌情報', '音変化表現のパターン']",,,,,,,,
V26N02-07.tex,,Word-based Japanese Typed Dependency Parsing with Grammatical Function Analysis,本論文では，従来の文節依存構造（文節係り受け）による構文解析と異なり，解析結果の部分構造が構文の構成素 (constituent) と一致し，解析結果から文法機能情報を直接取得できる日本語の構文解析を提案する．提案する構文解析は，単語間の依存構造に基づき，依存構造に付加されたラベル（文法機能タイプ）により格関係や連体修飾節の種別等の統語情報（文法機能情報）を表示する．この特徴により，文節依存構造解析では通常別工程として処理していた述語項構造解析を，単語依存構造解析では構文構造と自然に統合して扱うことが可能になる．京都大学テキストコーパス，現代日本語書き言葉均衡コーパスの一部に対して構築したコーパスを用いた評価実験により，単語依存構造解析は，従来の文節依存構造解析とほぼ同等の精度を保ちつつ，述語項構造情報等の詳細な統語情報を獲得可能であることを報告する．,"We present a novel scheme for word-based Japanese typed dependency parsing which integrates syntactic structure analysis and grammatical function analysis such as predicate-argument structure analysis. Compared to bunsetsu-based dependency parsing, which is predominantly used in Japanese NLP, it provides a natural way of extracting syntactic constituents. This makes it possible to jointly decide dependency and predicate-argument structure, which is usually implemented as two separate steps. By using grammatical functions as dependency types, we can obtain the detailed syntactic information from parsing results, while keeping the converted bunsetsu-based dependency accuracy as high as CaboCha, one of the state-of-the-art dependency parsers.","['はじめに', '関連研究', '日本語文法機能タイプ付き依存構造の設計', '既存コーパスの変換による単語依存構造コーパスの構築', '構文解析器への適用評価', '単語間依存構造の課題', 'まとめ']",,,,,,,,
V26N02-08.tex,,Domain Adaptation in Japanese Predicate-Argument Structure Analysis considering First and Second Person Exophora,本稿では，日本語述語項構造解析に分野適応の技術を導入することを提案する．述語項構造がアノテーション付けされた現代日本語書き言葉均衡コーパス (BCCWJ-PAS) において，メディアの違いにより項の種類の出現頻度が異なることがわかった．特に外界照応においてその傾向は顕著である．過去の日本語述語項構造解析の多くは，新聞記事コーパスを対象にしているために，この傾向には注目していなかった．この問題に取り組むため，まず，RNNベースの述語項構造解析器をベースラインとして使用し，3種類の異なる分野適応技術とその組み合わせを導入した．現代日本語書き言葉均衡コーパス (BCCWJ-PAS) を用いた評価実験では，述語項構造解析には分野依存性があることがわかった．特にガ格とヲ格の解析では，分野適応が日本語述語項構造解析の性能向上に有効であることが確認され，QAテキストの解析においてベースラインと比較しF1値が最大で，{$0.030$,This paper proposes introducing domain adaptation into Japanese predicate-argument structure \modify{(PAS),"['本研究の位置付け', '述語項構造解析', '深層リカレントモデル', '分野適応', '実験', '結論']",,,,,,,,
V26N02-09.tex,,Neural Japanese Zero Anaphora Resolution with \\Candidate Reduction Using Large-scale Case Frames,本論文では日本語文内・文間ゼロ照応解析モデルを提案する．文間ゼロ照応解析において複数格の同時推定を行う際，複数の文をまたぐ大量の格要素の組合せ候補を取り扱う必要があり，これはゼロ照応解析モデルの訓練，解析に際して重大な障害となる．この問題に対して，我々は格フレームの情報を用いた効果的な解候補削減手法を提案する．提案解候補削減を用いて複数格を同時推定したモデルと解候補削減を用いずにそれぞれの格を独立に推定したモデルを日本語均衡コーパス上で比較し，0.056の精度向上を確認した．また，ローカルアテンション付きRNNを導入することで，文間ゼロ照応解析の精度が上昇することも確認した．,"This paper presents a model for Japanese zero anaphora resolution that deals with both intra- and inter-sentential zero anaphora. Our model resolves anaphora for multiple cases simultaneously by utilising and comparing information from other cases. This simultaneous resolution requires the consideration of many combinations of antecedent candidates, which could be a crucial obstacle in both the training and resolving phases. To cope with this problem, we have proposed an effective candidate pruning method using case frame information. We compared the model, which estimates multiple cases simultaneously, by using our proposed candidate pruning method and model, which estimates each case independently without a candidate reduction method in a Japanese balanced corpus. The results confirmed a 0.056-point increase in accuracy. Furthermore, we also confirmed that the introduction of local attention Recurrent Neural Network increases the accuracy of inter-sentential anaphora resolution.","['はじめに', '関連研究', 'モデルと素性', '格フレーム中の分散表現を利用した解候補削減', '評価実験', '結果と議論', '結論', '素性詳細']",,,,,,,,
V26N03-01.tex,,Service-Oriented Utterance Sentence Analysis in Small Computers,"本稿では，様々なサービスを有する小型計算機に言葉による命令を受理させる手法について述べる．小型計算機における発話文解析では，サービスのための規定の発話文を必ず受理すること，および，ユーザからの発話文を追加的に学習することを低い計算コストで行うことが要求される．そこで，サービスごとに語義やチャンクを正確に区別するため，サービスごとにパージング結果を格納するアレイを設け，強化学習を用いて発話文解析を進める手法を提案する．実験において自動車旅行を支援する車載器の発話文解析に応用したところ，軽量に動作し，クローズドテストにおいて 0.99, オープンテストにおいて 0.81 という精度で発話文解析が可能であることを確認した．","This paper addresses a method to analyze command utterance sentences in small computers providing various services. All compulsory command utterances for the services must be accepted, and the utterances by the user must be able to be learned at a low computational cost. So as to determine the chunks and their sense based on their service, our proposal method contains {\it parsing arrays","['はじめに', '発話文と諸定義', '提案手法', '応用事例および性能評価', '考察', 'おわりに']",,,,,,,,
V26N03-02.tex,,Generating Candidate Responses for Supporting Human-to-human Communication of Quality of Life,我々は，高齢者のQuality of Life (QOL) を家族に伝えることで，高齢者と離れて住む家族とのコミュニケーションを活性化することを目指している．高齢者のQOL表出発話（高齢者のQOLを推定するのに有用な手がかりを含んだ発話）の生成を補助するシステム構築に向けて，本論文では (1) QOLラベルつき対話コーパスを構築するための方法論を提案し，(2) QOLラベルを用いた特定のQOL情報を伝達する応答の生成について議論する．具体的には，模擬的なワーカを用いることの妥当性を予備実験により示した上で，クラウドソーシングを効果的に利用して高齢者が主体となる高齢者のQOL表出発話を大規模に収集し，QOLラベルつき対話コーパスを構築した．構築したコーパスを用いた応答生成実験により，「着くとすぐに本を読んでいるよ」のような家族の発話に対する高齢者の応答候補として，高齢者のQOLが《健康満足感 (positive)》の場合は「今度本を読んであげよう」，《健康満足感 (negative) 》の場合は「私は新聞を読むのも億劫だよ」など，QOL情報の伝達に役立つ応答が生成されることを確認した．,"We aim to build a system to improve the quality of communication between elderly individuals and their families by sharing information about the elderly members’ quality of life (QOL). As the first step in the creation of a system to generate utterances containing information on QOL, we propose a methodology for building a QOL-labeled dialog corpus and discuss the generation of responses that transmit specific QOL information using QOL labels. To build the corpus, we conducted preliminary experiments to demonstrate that certain crowd workers provide responses that are indistinguishable from those given by elderly individuals. Based on this, we constructed a large-scale QOL-labeled dialog corpus using crowdsourcing effectively. Our response generation experiments using the constructed corpus demonstrated that the generated utterances can be useful in transmitting QOL information. For example, the prompt ``She reads a book as soon as she gets home'' generated the response ``I'll read her a book next time'' in case of the QOL label is 《health satisfaction (positive)》 or ``Even reading the newspaper is difficult for me'' in case of 《health satisfaction (negative)》.","['序論', 'QOLラベルつき対話コーパスの構築', 'QOLラベルつき対話コーパスを用いたQOL表出発話の生成', '結論', '用例データベースの規模', '生成応答に表出するQOL状態に関する人手評価結果の詳細', 'QOL状態のポジティブ・ネガティブに関する人手評価結果の詳細', '生成応答の適切さに関する人手評価結果の詳細']",,,,,,,,
V26N03-03.tex,,Metric for Automatic Machine Translation Evaluation based on Pre-trained Sentence Embeddings,本稿では，参照文を用いた文単位での機械翻訳自動評価手法について述べる．現在のデファクトスタンダードであるBLEUをはじめとして，多くの従来手法は文字や単語の$N$-gramに基づく素性に頼っており，文単位での評価にとっては限定的な情報しか扱えていない．そこで本研究では，文全体の大域的な情報を考慮するために，事前学習された文の分散表現を用いる機械翻訳自動評価手法を提案する．提案手法では，大規模コーパスによって事前学習された文の符号化器を用いて，翻訳文と参照文の分散表現を得る．そして，翻訳文と参照文の分散表現を入力とする回帰モデルによって，人手でラベル付けされた翻訳品質を推定する．WMT-2017 Metrics Shared Taskにおける翻訳品質のラベル付きデータセットを用いた実験の結果，我々の提案手法は文単位の全てのto-English言語対において最高性能を達成した．,"This study describes a segment-level metric for automatic machine translation evaluation (MTE). Although various MTE metrics have been proposed, most MTE metrics, including the current de facto standard BLEU, can handle only limited information for segment-level MTE. Therefore, we propose an MTE metric using pre-trained sentence embeddings in order to evaluate MT translation considering global information. In our proposed method, we obtain sentence embeddings of MT translation and reference translation using a sentence encoder pre-trained on a large corpus. Then, we estimate the translation quality by a regression model based on sentence embeddings of MT translation and reference translation as input. Our metric achieved state-of-the-art performance in segment-level metrics tasks for all to-English language pairs on the WMT dataset with human evaluation score.","['はじめに', '関連研究', '事前学習された文の分散表現を用いた機械翻訳の自動評価', '評価実験', '分析', 'おわりに']",,,,,,,,
V26N03-04.tex,,Surprisal through Word Embeddings,ヒトの文処理のモデル化として Hale によりサプライザルが提案されている．サプライザルは文処理の負荷に対する情報量基準に基づいた指標で，当該単語の文脈中の負の対数確率が文処理の困難さをモデル化するとしている．日本語において眼球運動測定を用いて文処理の負荷をモデル化する際に，統語における基本単位である文節単位の読み時間を集計する．一方，単語の文脈中の生起確率は形態素や単語といった単位で評価し，この齟齬が直接的なサプライザルのモデル化を難しくしていた．本論文では，この問題を解決するために単語埋め込みを用いる．skip-gram の単語埋め込みの加法構成性に基づき，文節構成語のベクトルから文節のベクトルを構成し，隣接文節間のベクトルのコサイン類似度を用いて，文脈中の隣接尤度をモデル化できることを確認した．さらに，skip-gram の単語埋め込みに基づいて構成した文節のベクトルのノルムが，日本語の読み時間のモデル化に寄与することを発見した．,"The concept of surprisal was proposed by Hale as a psycholinguistic model of sentence processing costs based on the information theory. Surprisal measures a word’s negative log probability in context and can be used to model the difficulty in processing a sentence. If this difficulty is estimated using the eye-tracking method, the reading time can be estimated using base phrase units in Japanese. In addition, word probability is estimated from the frequency of morphemes or word units in Japanese. We introduced word embeddings to address the discrepancy in units, which makes it difficult to model surprisal in Japanese. The additive property of skip-gram word embeddings enabled us to compose a base phrase vector from word vectors in the base phrase. We confirmed that the cosine similarity between two adjacent base phrase vectors can be used to model the contextual probability of the bi-gram of the base phrase and found that the norm of the base phrase correlates with reading time in Japanese.","['はじめに', '前提', '分析手法', '結果と考察', 'おわりに', '分析結果（詳細）', 'Second Pass Time について']",,,,,,,,
V26N04-01.tex,,Construction and Analysis of Multiword Expression-aware Dependency Corpus,複単語表現 (MWE) は統語的または意味的な非構成性を有する複数の単語からなるまとまりである．統語的な依存構造の情報を利用し，かつ意味理解が必要なタスクでは，単語ベースの依存構造よりもMWEを考慮した依存構造，即ち MWE を統語的な単位とする依存構造の方が好ましい．広範囲の連続MWEを依存構造で考慮するために，本稿では Ontonotes コーパスに対して新たに形容詞MWEを注釈し，複合機能語と形容詞 MWE の双方を考慮した依存構造コーパスを構築した．また，意味理解が必要なタスクでは句動詞などの非連続な出現を持ちうる MWE（動詞 MWE）の認識も重要である．依存構造の情報は動詞 MWE 認識で有効な特徴量として働くと期待されるため，本稿では連続MWEを考慮した依存構造と動詞 MWE の双方を予測する問題に取り組む．前者については以下の 3 つのモデルを検討する: (a) 連続MWE認識と，MWE を考慮した依存構造解析のパイプライン，(b) 連続MWEの範囲を依存関係ラベルとして符号化した head-initial な依存構造を解析するモデル，および (c) 連続MWE認識と上記 (b) の階層的マルチタスク学習モデルである．実験の結果，連続MWE認識ではパイプラインモデルとマルチタスクモデルがほぼ同等のF値を示し，head-initial な依存構造の解析器を 1.7 ポイント上回った．動詞MWE認識では系列ラベリングベースの認識器を上述のマルチタスクモデルに組み込むことで F 値が 1.3 ポイント向上した．,"Multiword expressions (MWEs) consist of multiple words with syntactic or semantic non-compositionality. Natural Language Processing (NLP) tasks exploiting syntactic dependency information and requiring the understanding of the meaning of texts prefer the use of MWE-aware dependency trees (MWE-DTs)---where each MWE is a syntactic unit---to word-based dependency trees. To treat various continuous MWEs as syntactic units in dependency trees, this study conducts adjective MWE annotations on the OntoNotes corpus and constructs a dependency corpus that is aware of both the functional and adjective MWEs. In NLP tasks requiring a semantic understanding, it is also important to recognize verbal MWEs (VMWEs) such as phrasal verbs, which are likely to have discontinuous occurrences. Since dependency information can be used as an effective feature in VMWE recognition, this study examines the tasks to predict both MWE-DTs and VMWEs. For MWE-DTs, it explores the following three models: (a) a pipeline model of continuous MWE recognition (CMWER) and MWE-aware dependency parsing, (b) a model to predict a word-based dependency tree that encodes MWE spans as dependency labels (the head-initial dependency parser), and (c) the hierarchical multitask learning (HMTL) model of CMWER and the model in (b). The experimental results show that the pipeline and HMTL-based models show similar F1-scores in CMWER, which are 1.7 points better than the F1-score of the head-initial dependency parser. With respect to VMWE recognition, the results show an F1 improvement of 1.3 points by integrating the sequential labeler into the above mentioned HMTL-based model.","['はじめに', 'MWEを考慮した依存構造コーパスの構築', '連続MWEを考慮した依存構造解析，およびVMWE認識を行うためのモデル', '連続MWEを考慮した依存構造解析，およびVMWE認識を行うモデルの評価実験', '関連研究', '結論']",,,,,,,,
V26N04-02.tex,,Contextualized Multi-Sense Word Embedding,近年，多くの自然言語処理タスクにおいて単語分散表現が利用されている．しかし，各単語に 1 つの分散表現を生成するアプローチでは，多義語における各語義が一つの分散表現に集約されてしまい，それらを区別することができない．そのため，先行研究では品詞やトピックごとに異なる分散表現を生成するが，語義を区別するには粒度が粗いという課題がある．そこで，本研究では各単語に対してより粒度の細かい複数の分散表現を生成するための 2 つの手法を提案する．1 つ目は，依存関係にある単語を手がかりとして，予め複数の分散表現を生成しておく手法である．2 つ目は，文脈中の全ての単語を考慮して語義に対応する分散表現を動的に生成する双方向言語モデルを利用する手法である．単語間の意味的類似度推定タスクおよび語彙的換言タスクにおける評価実験の結果，より細かい粒度で分散表現を生成する提案手法が先行研究よりも高い性能を発揮することを示した．,"\footnote[0]{本論文の内容の一部は，The 32nd Pacific Asia Conference on Language, Information and Computation \cite{ashihara-18b","['はじめに', '関連研究', '提案手法', '実験設定', '文脈中での単語間の意味的類似度推定タスク', '語彙的換言タスク', 'DMSEの分析', 'まとめ']",,,,,,,,
V26N04-03.tex,,Word Rewarding Model Using a Bilingual Dictionary for Neural Machine Translations,ニューラル機械翻訳は従来手法の句に基づく統計的機械翻訳に比べて，文法的に流暢な翻訳を出力できる．しかし訳抜けや過剰翻訳などの問題が指摘されており，翻訳精度に改善の余地がある. このような問題に対して従来の句に基づく統計的機械翻訳では，対訳辞書を用いてデコーダ制約を実装することにより翻訳精度を改善していたが，ニューラル機械翻訳では対訳辞書を有効活用するアプローチが明らかではない．本稿では対訳辞書を単語報酬モデルによりニューラル機械翻訳に適用する手法を提案する．提案手法は，テスト時のデコーディングの際に対訳辞書に存在する単語に報酬を与えることでそれらの出現確率を高め，翻訳精度の向上を図る．提案手法は辞書をニューラル機械翻訳モデルとは独立して適用するため，辞書の更新や変更を簡単に行える利点がある．また指定された語彙を出力するよう制約を加える方法より少ない計算量で翻訳精度を改善できる．さらにアテンションを利用して対訳辞書を活用する手法と組み合わせると性能を改善できることを実験的に示した．,"Even though outputs of neural machine translation are more fluent compared to those of conventional phrase-based statistical machine translation, under and over generation are still major problems. While the translation quality of phrase-based statistical machine translation has improved due to the use of a bilingual dictionary by the decoder constraint, the same approach cannot be directly applied to neural machine translation. This paper proposes a rewarding model to apply the bilingual dictionary to neural machine translation. The proposed model first predicts the target words for the translation using the bilingual dictionary and then increases their decoder output probabilities at an inference. As the model uses the bilingual dictionary as an independent resource for the neural model, it can easily update or change the dictionary if required. The proposed model was found to improve translation quality even though it has less computational complexity than lexically constrained decoding that forces output of specified words. The results also confirmed that when combined with a method that biased the decoder to output dictionary entries using attention weights, the proposed method further improved the translation quality.","['はじめに', '注意機構付きニューラル機械翻訳', '単語報酬モデル', '実験設定', 'シミュレーション辞書を用いた実験', '実際の辞書を用いた評価', '既存手法との比較', '関連研究', 'おわりに', '\\ref{sec:test_result']",,,,,,,,
V27N01-01.tex,,Analyzing the Effect of Domain Adaptation for Dependency Parsers by Clustering Parsing Errors,"%
$BK\O@J8$G$O!$F|K\8l78$j<u$12r@O4o$KBP$9$kDI2C71N","%
This paper reports on an analysis of the effect of additional training of Japanese dependency parsers across multiple domains from a bird's-eye view. Parsing errors were collected before and after additional training using target domain data. We conducted cluster analysis of the parsing errors represented as dense real vectors, which were obtained from the internal states of the parser. Through quantitative and qualitative analysis of the clusters, the types and numbers of the parsing errors across multiple target domains were investigated. Several hypotheses concerning the effect of additional training were developed on the basis of the cluster analysis and verified through statistical analysis of the corpus. The results suggest that the main effect of additional training was learning the difference in the distributions of the correct syntactic structures for similar word sequences in different domains.","['\x1b$B$O$8$a$K\x1b(B', '\x1b$B4XO""8&5f\x1b(B\\label{sec:related_works', '\x1b$BJ,@O<jK!\x1b(B\\label{sec:teian', '\x1b$B<B83\x1b(B', '\x1b$B$*$o$j$K\x1b(B']",,,,,,,,
V27N01-02.tex,,A New Method to Find Zero Pronouns Referring to Entry Words and Estimate their Surface Cases in the Context of a World History Glossary and its Descriptions,"%
$BK\9F$G$O!$@$3&;K$K4X$9$kBg3XF~;nO@=RLdBj$KBP$7$F<+F0MWLs<jK!$K4p$E$-2rEz$r<+F0@8@.$9$k:]$NCN<18;$N0l$D$H$7$F@$3&;KMQ8l=8$KCmL\$7!$8+=P$78l$H8l<aIt$KJ,$+$l$F$$$kJ8=q%G!<%?$+$i2rEz$H$J$kJ8>O$r:n@.$9$k$?$a$K!$8l<aJ8$K$*$1$k8+=P$78l$K>H1~$9$k%<%mBeL>;l$H$=$NI=AX3J$r?dDj$9$k<jK!$rDs0F$9$k!%K\9F$N07$&%?%9%/$O!$@h9T;l8uJd$,8+=P$78l$K8B$i$l$F$$$k0lJ","In this paper, we focus on the usage of a world history glossary as one of the knowledge sources for automated answer generation of essay-type questions. The questions we use were derived the University of Tokyo's entrance examinations on world history, and the answer generation uses a multi-document summarization methodology. In the automated answer generation, the glossary's descriptions we used as part of the answers. However, entry words were often omitted from the descriptions. To make complete sentences from entry words and their description, we propose a method to find zero pronouns referring to entry words inside the descriptions and estimate their surface cases. This paper's task differs from conventional zero anaphora resolution in the following two ways. First, with this method the entry word is the only candidate for the antecedent, as opposed to having to select one zero pronoun among several zero pronouns. Second, context information of the antecedent, which may be a useful clue in anaphora resolution, does not exist for the entry words, because the entry word appears alone and is not embedded in a sentence. Evaluation results based on a world history glossary revealed that the proposed method would be more effective than the existing method using zero anaphora analysis with Kurohashi-Nagao Parser. Furthermore, we attempted a method to generate pseudo training data from ordinary sentences in a textbook because we observed low accuracy when the entry word was embedded with low-frequency surface cases. Additionally, the results demonstrated that the introduction of the data improves the estimation of ``o''-case and ``ni''-case  in terms of F-measure.","['\x1b$B$O$8$a$K\x1b(B', '\x1b$B@$3&;KO@=RLdBj2rEz%7%9%F%`\x1b(B', '\x1b$B@$3&;KMQ8l=8$N8+=P$78l$H8l<aJ8$N4X78$K4X$9$kM=HwD4::\x1b(B', '\x1b$B4XO""8&5f\x1b(B', '\x1b$B5!3#3X=,$rMQ$$$?8+=P$78l$K>H1~$9$k%<%mBeL>;l$H$=$NI=AX3J$N?dDj<jK!\x1b(B', '\x1b$BI>2A<B83\x1b(B', '\x1b$B9M;!\x1b(B', '\x1b$B$^$H$a\x1b(B']",,,,,,,,
V27N01-05.tex,,Word Familiarity Rate and Register Type Estimation Using a Bayesian Linear Mixed Model,"%
$BK\O@J8$G$O!XJ,N`8lWCI=A","%
This paper presents research on word familiarity rate estimation using the `Word List by Semantic Principles'. We collected rating information on 96,557 words in the `Word List by Semantic Principles' via Yahoo! crowdsourcing. We asked 3,392 subject participants to use their introspection to rate the familiarity and register information of words based on the five perspectives of `KNOW', `WRITE',  `READ', `SPEAK', and `LISTEN', and each word was rated by at least 16 subject participants. We used Bayesian linear mixed models to estimate the word familiarity rates.  We also explored the ratings with the semantic labels used in the `Word List by Semantic Principles'.","['\x1b$B$O$8$a$K\x1b(B', '\x1b$B4XO""8&5f\x1b(B', '\x1b$B<jK!\x1b(B', '\x1b$B?dDj$7$?C18l?FL)EY!&0LAj>pJs$NJ,@O\x1b(B', '\x1b$B$*$o$j$K\x1b(B']",,,,,,,,
