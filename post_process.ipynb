{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "京都  大学  で  自然  言語  処理  を  専攻  する . 123 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from pyknp import Juman\n",
    "\n",
    "jumanpp = Juman()\n",
    "\n",
    "text = '京都 大学 で 自然 言語 処理 を 専攻 する.123'\n",
    "mlist = jumanpp.analysis(text)\n",
    "output = ' '.join([mrph.midasi.strip() for mrph in mlist.mrph_list()])\n",
    "print(output, type(output))\n",
    "# ['吾輩', 'は', '猫', 'である', '。']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('./NLP_JP_CORPUS_INCREMENTAL.json', orient='records', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把所有的逗号和句号进行统一\n",
    "def replace_comma(text):\n",
    "    text = text.replace(' ', '　') # JUMAN++无法处理半角空格\n",
    "    text = text.replace('@', '') # JUMAN++似乎存在bug，会错误读取@\n",
    "    text = text.replace('#', '') # JUMAN++似乎存在bug，会错误读取#\n",
    "    text = text.replace(',', '，')\n",
    "    text = text.replace('.', '．')\n",
    "    return text\n",
    "\n",
    "df[['abs_intro', 'abs_method', 'abs_result', 'abs_conclusion','sec_intro', 'sec_method', 'sec_result', 'sec_conclusion']] = \\\n",
    "    df[['abs_intro', 'abs_method', 'abs_result', 'abs_conclusion', 'sec_intro', 'sec_method', 'sec_result', 'sec_conclusion']].map(replace_comma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本 論文 で は 「 目 を 盗む 」 や 「 かたず を 飲む 」 など の 述語 型 定型 表現 を コーパス から 自動 抽出 する こと を 目的 に ， 従来 の 相互 情報 量 の 条件 を 緩める 方向 で ， 名詞 動詞 間 の 共起 性 を 測る 新たな 基準 を 提案 する ． <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from pyknp import Juman\n",
    "\n",
    "jumanpp = Juman()\n",
    "\n",
    "mlist = jumanpp.analysis(df['abs_intro'][9])\n",
    "output = ' '.join([mrph.midasi for mrph in mlist.mrph_list()])\n",
    "print(output, type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknp import Juman\n",
    "jumanpp = Juman()\n",
    "\n",
    "# preprocess by JUMAN++\n",
    "def preprocess(text):\n",
    "    split = text.split('．') # 按照．进行句子分割\n",
    "    output = []\n",
    "    for num, seq in enumerate(split):\n",
    "        mlist = jumanpp.analysis(seq)\n",
    "        output.append(' '.join([mrph.midasi.strip() for mrph in mlist.mrph_list()]))\n",
    "    return '．'.join(output).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "InvalidParameter: byte size of input string (4322) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (4240) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (6317) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (4308) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (6570) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (5061) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (8022) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (7469) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (15046) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (6291) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (9145) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (9887) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (6897) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (6970) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (4314) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (6422) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (9480) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (14385) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (5575) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (11920) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (7043) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (4395) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (5202) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (8550) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (5536) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (4247) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (4186) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38InvalidParameter: byte size of input string (5280) is greater than maximum allowed (4096)\n",
      "backtrace:\n",
      "    resetForInput at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer_impl.cc:21\n",
      "    analyze at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/analysis/analyzer.cc:46\n",
      "    analyzeWith at /home/is/gamarivan-a/jumanpp-2.0.0-rc3/src/core/input/stream_reader.h:38"
     ]
    }
   ],
   "source": [
    "df[['abs_intro', 'abs_method', 'abs_result', 'abs_conclusion','sec_intro', 'sec_method', 'sec_result', 'sec_conclusion']] = \\\n",
    "    df[['abs_intro', 'abs_method', 'abs_result', 'abs_conclusion', 'sec_intro', 'sec_method', 'sec_result', 'sec_conclusion']].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'はじめ に 自然 文 検索 や 翻訳 ， レコメンデーション など に 使用 可能な 解析 システム を 実現 した ．2000 年 に ( 南  1974 ; 白井 1995 ) を 参考 に して 文節 に 強 さ を 決めて ， 同じ 強 さ の 文節 で は ， 連 用 修飾 格 は 直後 の 用言 に ， 連 体 修飾 格 は 直後 の 体言 に 係る と いう 規則 を 用いて 構文 解析 プログラム を 開発 した ．しかし 実際 の 構文 構造 は ， 文節 を 飛び越して 係る 場合 が 見受け られた ．文法 的な 情報 だけ で は 不十分だ と 考え ， 意味 的な 情報 の 導入 を 検討 した 結果 ， シソーラス を 組み 込んで 用語 同士 の 意味 的な 距離 を 測って ， その 距離 に よって 係り 先 を 決定 する 手法 を 開発 した ．この 解析 システム を 自然 文 検索 に 用いる 場合 ， 同じ 内容 の こと を 言って いる の に いく つ も の 書き 方 が 許さ れて いる こと から しばしば 検索 漏れ が 発生 する ．この 異形 式 同 内容 に 対応 する ため ， 用語 の 標準 化 ， 係り 受け の 正規 化 を 実現 した ．さらに ， 翻訳 など で 使用 する こと を 考えて ， 文節 意図 （ 4 ．1 で 述べる ） を 把握 し やすく する ため に 係り 受け と それ に 続く 付属 語 の 並び を まとめた 形 で 管理 した ．手 作業 で 収集 した 辞書 に 手 作業 で いろいろな 情報 を 付加 して 機能 を 実現 する と いう 方式 で 開発 した ．統計 的な 手法 は 用いて い ない ．文末 に 試用 サイト の URL を 示した ので 試用 して いただき たい ．'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sec_intro'][291] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sections'] = df[['sec_intro', 'sec_method', 'sec_result', 'sec_conclusion']].values.tolist()\n",
    "df['abs_incremental'] = df[['abs_intro', 'abs_method', 'abs_result', 'abs_conclusion']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sections'] = df['sections'].apply(lambda x: x[:2] + [''.join(x[2:])])\n",
    "df['abs_incremental'] = df['abs_incremental'].apply(lambda x: x[:2] + [''.join(x[2:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['はじめ に 自然 文 検索 や 翻訳 ， レコメンデーション など に 使用 可能な 解析 システム を 実現 した ．2000 年 に ( 南  1974 ; 白井 1995 ) を 参考 に して 文節 に 強 さ を 決めて ， 同じ 強 さ の 文節 で は ， 連 用 修飾 格 は 直後 の 用言 に ， 連 体 修飾 格 は 直後 の 体言 に 係る と いう 規則 を 用いて 構文 解析 プログラム を 開発 した ．しかし 実際 の 構文 構造 は ， 文節 を 飛び越して 係る 場合 が 見受け られた ．文法 的な 情報 だけ で は 不十分だ と 考え ， 意味 的な 情報 の 導入 を 検討 した 結果 ， シソーラス を 組み 込んで 用語 同士 の 意味 的な 距離 を 測って ， その 距離 に よって 係り 先 を 決定 する 手法 を 開発 した ．この 解析 システム を 自然 文 検索 に 用いる 場合 ， 同じ 内容 の こと を 言って いる の に いく つ も の 書き 方 が 許さ れて いる こと から しばしば 検索 漏れ が 発生 する ．この 異形 式 同 内容 に 対応 する ため ， 用語 の 標準 化 ， 係り 受け の 正規 化 を 実現 した ．さらに ， 翻訳 など で 使用 する こと を 考えて ， 文節 意図 （ 4 ．1 で 述べる ） を 把握 し やすく する ため に 係り 受け と それ に 続く 付属 語 の 並び を まとめた 形 で 管理 した ．手 作業 で 収集 した 辞書 に 手 作業 で いろいろな 情報 を 付加 して 機能 を 実現 する と いう 方式 で 開発 した ．統計 的な 手法 は 用いて い ない ．文末 に 試用 サイト の URL を 示した ので 試用 して いただき たい ．',\n",
       " '構文 構造 の 決定 ( 白井 1995 ) で は 等しい 階層 的 認識 構造 の あいだ で は ， 構文 構造 を 文節 の 連 体 修飾 格 は 体言 に ， 連 用 修飾 格 は 用言 に それぞれ 最初の 文節 に 係る と いう アルゴリズム で 決定 して いた ．しかし 複数 の 受け の 候補 が ある とき に ， 文法 情報 だけ で は 正しく 決定 さ れ ない と いう 問題 が ある ．( 藤尾 2000 ) で は 統計 的な 手法 で 決定 して いる が ， 様々な 分野 に 対して 大量の 解析 済み の データ を 準備 する の は 容易で は ない ．そこ で 筆者 ら は シソーラス （ 5 ．1 で 述べる ） を 用いて 用語 同士 の 意味 的な 距離 を 計算 して ， その 距離 の 近い ところ に 係る と いう 方式 を 採用 した ．意味 的な 距離 に よって 構文 構造 を 決定 する 複数 の 受け の 候補 が ある とき に ， 図 1 の ように 意味 的な 近 さ （ 2 ．2 で 述べる ） で 直後 の 文節 を 飛び越して 係る こと が ある ．「 ネット で 」 と いう 連 用 修飾 格 は ， 「 行く 」 か 「 調べる 」 と いう 用言 に 係る 可能 性 が ある ．これ まで は ， 直後 に ある と いう こと で 「 行く 」 と いう 文節 に 係る ように なって いた ．意味 的な 距離 を 測って 比較 する と 次 の ように なる ．「 ネット で 」 - - - 「 調べる 」 の 方 が 意味 的な 距離 が 近い ので ， 「 ネット で 」 と いう 文節 は 「 調べる 」 と いう 文節 に 係る ように した ．* * 連体 修飾 格 で の 例 * * 同様に 連 体 修飾 格 でも 意味 的な 距離 で 係り 先 を 決める ．図 2 で は ， 「 おいしい 」 と いう 形容詞 が 文法 情報 だけ で 評価 する と 「 長野 の 」 か ， 「 リンゴ を 」 か どちら か の 名詞 の 文節 に 係る 可能 性 が ある ．同様に 用語 同士 の 意味 的な 距離 を 測って 「 リンゴ を 」 と いう 文節 に 係 ける ．* * 並列 構造 で の 例 * * ( 黒崎 1992 ) で は ， 並列 構造 を 付属 語 の 類似 性 で 決めて いる ．筆者 ら の システム で は シソーラス を 用いて 名詞 の 意味 的な 距離 で 決めて いる ．「 ビール 」 と 「 お 酒 」 と は 意味 的な 距離 が 近い ので 並列 構造 に なる が ， 「 先生 」 と 「 お 酒 」 は 並列 構造 に は なら ない ．用語 同士 の 意味 的な 距離 の 定義 係り 受け 解析 で 係り 先 を 決める ため に 2 つ の 用語 間 の 意味 的な 距離 を 定義 した ．本来 ， 用語 同士 の 意味 的な 距離 は アナログ 的な もの である ．極端な 場合 は 人 に よって も 異なる が ， シソーラス 上 の 用語 同士 の 関係 から 表 1 の よう に 定義 した ．係り 受け 語 は 慣用 的に よく 係り 受け を 構成 する 用語 の 組み合わせ を ネット など から 手 作業 で 収集 した ．間 に 挟まる 助詞 と 良し あし の 情報 も 持って いる ．また 係り 受け 語 に は ， 係 の 用語 に 意味 が 指定 できる （ 5 ．2 に 示す ） ．* * 例 * *  （ 人 ） が  飲む    （ 人 ） は 人 の 意味 で 人 の 意味 の 用語 すべて を 指定 できる ．筆者 ら の シソーラス で は 直近 の 関係 語 と の 関係 しか 持って い ない ．関係 語 と さらに その 関係 語 と の 意味 的な 距離 は それぞれ の 意味 的な 距離 を 加算 する こと に した ．狭義 語 の さらに 狭義 語 と の 意味 的な 距離 は xmath 0 で xmath 1 である と 定義 した ．こう する こと で ， シソーラス に お互い の 関係 が 登録 さ れて い ない 用語 間 の 意味 的な 距離 を 定義 した ．経験 的に あまり 遠い 関係 の 用語 同士 の 距離 は 評価 して も 意味 が ない ので 一定 の 距離 で 足 切り を して いる ．足 切り の 値 は 係り 先 を 決める とき と ， 並列 構造 を 決める とき と で は 異なる ．並列 構造 を 決める とき の ほう が ， 広く 関係 を 評価 して いる ．並列 構造 を 決める とき に は ， 係り 受け 語 は 考慮 し ない ．* * 意味 的な 距離 を 測る とき に 多義語 を 区別 して いる * * 意図 した の と 異なる 意味 の 用語 と の 距離 を 測って しまう こと が 問題 に なる こと が ある ．例えば 「 お 稲荷 さん 」 に は 2 つ の 意味 が ある ．多義語 を それぞれ の 意味 で 区別 し ないで 計算 する と ， 「 稲荷 神社 」 と 「 いなり ずし 」 と が 0 （ 同義語 ） に なって しまう ．この こと を 防ぐ ため に ， 我々 の システム で は 「 お 稲荷 さん 」 の 2 つ の 意味 を 区別 して 別の 用語 と して 管理 して いる ．その 結果 「 稲荷 神社 」 と 「 いなり ずし 」 と の 意味 的な 距離 は 未定 義 （ 無限大 ） に なる ので 「 いなり ずし に 参拝 する 」 など と いう 無意味な 係り 受け は 排除 さ れる ．係り 受け データ の 整理 日本 語 で は 同じ こと を 言う のに いく つ か の 書き 方 が 許さ れて いる ．自然 文 検索 など で 漏れ を 少なく する ため に 形 を 整理 する ．用語 の 標準 化 日本 語 は 表記 の 揺れ を 含めて 同義語 が 多い ．( 国分 ，  岡野 2010 ) 著者 と 検索 者 と で 異なる 表記 が 使わ れる こと が 検索 漏れ の 一因 に なって いる ．検索 対象 データベース ， 検索 文 ともに 係り 受け に した あと シソーラス を 用いて なか の 用語 を 言語 工学 研究 所 が 推奨 する 用語 に 標準 化 する ．誤った 表記 ， 差別 語 も 標準 の 表記 に 置き換える ．* * 例 1 * * * * 例 2 * *  係り 受け を 正規 化 用言 の 使い 方 に は 限定 用法 と 叙述 用法 が ある ．自然 文 検索 で 「 青い リンゴ 」 （ 限定 用法 ） と 書いて ある 記事 を 「 リンゴ が 青い 」 （ 叙述 用法 ） と いう 係り 受け で 検索 して も 検索 でき ない ．検索 できる ように する ため に ， 原 記事 ， 検索 文 ともに 係り 受け は 限定 用法 の もの を すべて 叙述 用 法 に 統一 して ， 正規 化 する ．用言 が 動詞 の 場合 は ， 動詞 の 性質 に よって 名詞 と の 間 に 挟む 助詞 が 異なる ．当面 ， 間 に 挟む 助詞 も ， 表 2 の 4 種類 に 限定 して いる ．情報 の 付与 今後 ， 本 解析 システム を 様々な 目的 へ 適用 を 進める 予定 である ．そこ で ， 辞書 上 の 情報 を もと に 用途 に 応じて 解析 結果 に 必要な 情報 を 付与 する ．文節 意図 を 付与 する 解析 の 精度 を 上げる ため と ， 翻訳 など で 必要な 情報 を 取り出し やすく する ため に ， 「 係り 受け の 語幹 まで 」 と ， それ に 続く 「 付属 語 の 並び 」 を まとめて 管理 して いる ．さらに ， 付属 語 の 並び の 持つ 表 3 の ような 性質 を 「 文節 意図 」 と 呼ぶ こと と する ．これ は 一般に 命題 に 含ま れる 否定 ・ 肯定 ， ボイス ， テンス など を ， モダリティー と 一緒に した もの である （ 益岡 2000 ） ．乾 健太郎 「 KURA 」 佐藤 理史 「 醍醐 プロジェクト 」 は ， ここ で いう 係り 受け の 部分 を より 分かり やすく する ため の 置き換え を 説明 した もの である ので ， 本 システム と は 目的 が 異なる ．* * 文 の 文節 意図 の 把握 * * 翻訳 以外 でも ， 例えば 内容 が 「 依頼 」 の 記事 を 集めよう と した とき ， 記事 を 解析 して 係り 受け に まで して も ， 結局 人手 で 読み 直して 分類 する 必要 が あった ．文末 の 文節 意図 が 「 依頼 」 の 文 を 含む 記事 を 集める と ， その 目的 が 達成 さ れる ．ほとんど の 場合 ， 文末 の 文節 意図 が 文 全体 の 文節 意図 を 表して いる ．* * 文節 意図 と 主語 の 推定 * * 省略 さ れた 主語 は 文脈 を 調べ ない と 分から ない 場合 も ある が ， 文節 意図 を 調べる と 推測 できる 場合 が ある ．翻訳 など の ため に 省略 さ れた 主語 の 人称 を 推定 する （ 4 ．3 で 述べる ） ．同じ 文節 意図 でも 丁寧 さ の 違い など で いろいろな 書き 方 が ある ．表 4 に 「 依頼 」 の 文節 意図 の 例 を 上げた が ， ここ に 上げた の は その 一部 で この ほか に も いく つ も の 書き 方 が ある ．ひと つ の 文節 が 複数 の 文節 意図 を 持つ こと が ある ．たとえば 下記 の 文節 は ， 「 禁止 」 ， 「 否定 」 ， 「 疑問 」 ， 「 推量 」 ， 「 丁寧 」 の 5 つ の 文節 意図 を 持って いる ．付属 語 の 組み合わせ に よって ， 文節 意図 は 変化 する このような 複雑な 文節 意図 を 持つ 文節 を 扱う ため と ， 解析 の 速度 を 速く する ため に ， 辞書 上 で は 数 の 付属 語 を 「 付属 語 の 並び 」 と して まとめた 形 で 管理 して いる ．筆者 ら の システム で は 解析 辞書 （ 5 ．2 で 述べる ） に 1，300，000 行 の 「 付属 語 の 並び 」 を 持って いる ．良し あし ， 注目 度 を 付与 する レコメンデーション で は ， 良し あし の 情報 を 手がかり の ひと つ に する ．単独 で 良し あし の 決め られる 辞書 上 の 用語 に は ， 良し あし の フラグ を 付けて ある ．しかし ， 用語 単独 で は 良し あし が 決め られ ず ， 係り 受け 関係 を 調べ ない と 決め られ ない こと が ある ．「 寿命 」 ， 「 延びる 」 ， 「 短い 」 など 用語 は 単独 で は 良し あし の 性質 は 持って い ない が ， 組み合わさ れた とき に 良し あし の 性質 が 出て くる ．筆者 ら の シソーラス で は ， 「 係り 用語 」 と ， 「 受け の 用語 」 と ， 「 間 に はさま れた 格 助詞 」 と ， 「 良し あし 」 の 組 で 管理 して いる ．シソーラス の 係り 受け 語 を 調べて 係り 受け の 良し あし を 決める ．係り ， 受け の それぞれ の 用語 の 同義語 ， 狭義 語 を シソーラス で 拡張 して ， 係り 受け 語 と して 登録 さ れて い ない 係り 受け に も 対応 できる ように した ．* * 否定 の 文節 * * 良し あし は 否定 が ある と 逆転 する ．日本 語 で は 「 ない 」 と 書いて あって も 否定 だ と は 決め られ ない （ 表 5 参照 ） ．否定 に なる か どう かも 付属 語 の 並び に 記述 して ある （ 5 ．2 で 述べる ） ．* * 注目 度 を 付与 して 良し あし を 辞書 に 登録 する こと が できる * * 「 良し あし 」 の 判断 基準 は 普遍 的な もの で は ない ．ユーザー に よって 異なる こと が ある ．また 自社 や 競合 他 社 の 商品 名 の ように その 評判 を いつも 注目 して おき たい 用語 も ある ．筆者 ら の システム は ユーザー に より 適切な レコメンデーション を する ため に ， このような 用語 または 係り 受け に 注目 度 ， 良し あし を つけて 登録 する 仕組み が 用意 して ある （ 表 6 参照 ） ．主語 の 人称 の 推定 日本 語 は しばしば 主語 が 省略 さ れる が ， そのまま 翻訳 する と 訳文 が あいまいに なる こと が 少なく ない ．また ビジネス 文書 でも 主語 を 解析 して 認識 する こと は 重要である ．実際 の 文章 で は ， 待遇 表現 や 文節 意図 で 暗に 主語 の 人称 を 示して いる ．本 論文 で は この 点 に 注目 して ， 文節 意図 の 情報 を 利用 する こと に より 主語 を 推定 する 方法 を 説明 する ．* * 待遇 表現 に よって 主語 の 人称 を 推定 する * * ビジネス 文書 で は ， 待遇 表現 は 適切に 使用 さ れて いる ので 有効な 推定 法 である ．解析 辞書 に は 文節 が どの 待遇 表現 に なる か が 記述 して ある ．謙譲語 が 使わ れて いる 動詞 の 主語 は 1 人称 である ．* * 例 * *  申し上げた の は  →  （ 1 人称 が ） 申し上げた の は 尊敬 語 が 使わ れて いる 動詞 の 主語 は 2 人称 ないしは 3 人称 である ．* * 例 * *  おっしゃった の は  →  （ 2 人称 が ） おっしゃった の は * * 文節 意図 に よって 主語 の 人称 を 推定 する * * 待遇 表現 でも 主語 が 推定 でき なかった とき に 文節 意図 に よって 主語 の 人称 を 推定 する （ 表 3 参照 ） ．例えば ， 文節 意図 が 「 意志 」 の とき は ， 主語 は 1 人称 である ．* * 例 * *  飲み たい 「 意志 」   →  （ 1 人称 が ） 飲み たい 文節 意図 が 「 依頼 」 「 指示 」 の とき は ， 受け の 主語 は 2 人称 である ．* * 例 * *  送って くれ 「 依頼 」  →  （ 2 人称 が ） 送って くれ  辞書 ブログ や メール に 代表 さ れる ような 文書 を 扱う ため に ， 時事 的な 用語 や 省略 語 も 積極 的に 登録 して いる ．送り仮名 や 訳語 など の 差異 に よる 異 表記 語 も 網羅 的に 収集 した ．よく 使わ れる 用語 であれば 誤った 用語 （ 例  「 キューピット 」 cupid ） も 積極 的に 採択 して ある ．反面 ， 古語 や 文学 作品 に しか 出て こ ない 用語 は 採択 して い ない ．すべて の 辞書 は 共通の 品詞 に 分類 して ある ．用言 は 語幹 と 活用 形 で 管理 して いる ．シソーラス 自然 言語 処理 を 目的 と した 一般 語 を 主 と する シソーラス である ．いわゆる 名詞 だけ で なく ， 動詞 ， 形容詞 ， 形容動詞 ， 副詞 ， 代名詞 ， 擬態 語 さらに 慣用句 まで を 登録 して いる ．「 広義 語 - - - 狭義 語 」 の 関係 は ， 自然 言語 処理 で 広義 語 に 適用 した 規則 が 狭義 語 に も 適用 できる ように 同じ 属性 の もの だけ と した ．「 自動車 」 - - - 「 タイヤ 」 の ような 全体 - - - 部分 関係 は 関連 語 と した ．品詞 の 異なる 用語 ， 「 自動詞 」 - - - 「 他 動詞 」 の 対応 など も 関連 語 と した ．用語 間 の 意味 関係 と して ， 表 7 の もの を 用意 した ．詳細 は ( 国分 ， 岡野 2010 ) を 参照 さ れ たい ．* * シソーラス の その他 の 項目 * * エラー フラグ    誤った 表記 ， 差別 語 品詞        動詞 の 活用 形 を 含めて 24 種類 注目 度       レコメンデーション の ため に ユーザー が マーク した 用語 ．異なり 語 の 数    440，000 語 ( 竹内 2008 ) は 動詞 の 性質 を 分類 する ため に 動詞 の それぞれ の 性質 で 係り 受け する 名詞 の 一例 を 示した もの である ．一方 筆者 ら の この シソーラス で は ， 受け に なる 個々 の 用言 を 中心 に ， その 用言 の 係り と なり 得る 名詞 を なるべく 網羅 的に 収集 した ．解析 辞書 ここ で 実現 して いる アルゴリズム は ， 辞書 の 情報 で 制御 する 方式 を とって いる ．その ため に 必要に なる 情報 が 各 用語 に 付与 して ある ．* * 自立 語 * * 自立 語 の 構成 要素 である 接頭辞 ， 接尾辞 ， 助数詞 など も 含む ．語 数      240，000 語 品詞      動詞 の 活用 形 も 含めて 24 種類 良し あし    自立 語 の 良し あし が 記入 して ある 否定 フラグ * * 名詞 の 意味 * *     係り 受け 関係 を 調べる ため に 次の 10 種類 ある ．1 つ の 用語 が 複数 の 意味 を 重複 して 持てる ．* * 意味          例 * *  人           先生 ， 山田 ， 雅子  機関          学校 ， 研究 所  物           机 ， 物理 現象 も 含む  時           昨年  場所          東京 ， 駅前  数 量詞         5 本 ， 少し  抽象 名詞        芸術 ， 甘 さ  動作 名詞        サ 変動 詞  数詞          9 ， 二百  不定          代名詞 ， 未知 語 など で 意味 が 決定 でき ない もの ．* * 用言 * *  かっこ 内 は 活用 語尾 である ．* * 品詞 ・ 活用 形      例 * *  サ 変名 詞 形       勉強 （ する ）  サ 変 非 名詞 形      察 （ する ）  ザ 変          信 （ ずる ）  一 段          生き （ る ）  カ 行 五 段        書 （ く ）  カ 行 五 段 例外      行 （ く ）  ガ 行 五 段        泳 （ ぐ ）  サ 行 五 段        押 （ す ）  タ 行 五 段        立 （ つ ）  ナ 行 五 段        死 （ ぬ ）  バ 行 五 段        遊 （ ぶ ）  マ 行 五 段        飲 （ む ）  ラ 行 五 段        走 （ る ）  ラ 行 五 段 例外      おっしゃ （ る ）  ワア 行 5 段       買 （ う ）  ワ 行 五 段 例外      問 （ う ）  形容詞         青 （ い ）  形容 動詞        閑静 （ な ）  形容 動詞 と ／ たる 形   矍鑠 （ たる ）  副 詞          さっぱり  連体詞         こんな  打ち消し の 動詞     年 端 も いか （ ない ： 助動詞 ）  打ち消し の 形容詞    必要 （ ない ： 形容詞 ） * * 動詞 の 性質 * *  自 ・ 他動詞   限定 用法 から 叙述 用法 に 変換 する とき に 挟む 助詞 を 決定 する ため ．移動 性 の 動詞  自動詞 である が 「 を 格 」 を とる ．例  道 を 行く  待遇 表現    尊敬 語 ・ 謙譲語  翻訳 など で 主語 を 決定 する ため ．例  おっしゃる   （ 尊敬 語 ）           申し上げる   （ 謙譲語 ） * * 付属 語 の 並び * * 文節 意図 を 付与 する ため に 助詞 ， 助動詞 だけ で なく ， いわゆる 機能 表現 と その 組み合わせ を まとめた 形 で 扱って いる ．* * 付属 語 * *  助詞  助動詞 と その 活用 語尾  形式 名詞  機能 表現 の ため の 動詞 および その 活用 語尾   例えば 推量 を 表す 「 〜 かも 知れ ない 」 と いう とき の 「 知れる 」 と いう 動詞 ．機能 表現 の ため の 形容詞 および その 活用 語尾  行 数      1，300，000 行  エラー フラグ  間違った 表記  文節 意図    表 3 参照  並列 フラグ   並列 を 構成 し うる か どう か  待遇 表現    尊敬 語 ・ 謙譲語  否定 フラグ など',\n",
       " '結果 CGM （ 消費 者 生成 メディア ） の 例 と して Yahoo ! 知恵 袋 データ の 2004 年 4 月 分 の 質問 記事 （ 5，957 記事 ， 15，883 文 ） を 用いて ， 評価 した ．まず ， 筆者 ら の システム と Cabocha と の 解析 精度 を 比較 した ．会話 体 の 文章 な ので 両者 と も あまり 良い 結果 は で なかった が 正解 率 は Cabocha を 13 ．8 ポイント 上回って いる ．両 システム の 処理 時間 も 比較 して みた ．筆者 ら の システム は シソーラス を 参照 して いる ので ， 処理 が 遅く なる 恐れ が あった が ， 両者 は ほとんど 差 が なかった ．これ は ， 筆者 ら の システム で は 付属 語 を まとめた 形 で 扱う こと に よって ， 辞書 へ の アクセス 回数 を 減らした ため と 考え られる ．次に ， 筆者 ら の システム で シソーラス を 組み 込んで 解析 した 結果 と ， 組み 込ま ないで 解析 した 結果 と の 差 を 調べた ．係り 受け 構造 の 違い と ， 並列 構造 の 違い と に 分けて 集計 した ．しかし ， シソーラス を 組み 込んだ 結果 ， シソーラス を 組み 込ま ない とき に は 正しく 解析 できた 結果 を ， かえって 間違った 構造 に して しまった 場合 も あった ．差分 の 生じた 172 文 の 内訳 は 改善 160 文 ， 悪化 12 文 ， 差引 148 文 全体 15，883 文 に 対して 0 ．9 % の 向上 が 観測 さ れた ．* * 成功 例 * * 「 音楽 が いつまで たって も 始まり ませ ん ．」 「 音楽 ／ が ／ 始ま ／ 」 と いう 係り 受け が 登録 さ れて いる ので ， 「 音楽 が 」 と いう 文節 が 「 たって も 」 と いう 文節 で は なく ， 「 始まり ませ ん 」 と いう 文節 に 係った ．* * 間違った 構造 に して しまった 例 * * 「 警察 の 方 に 話 が いって いる か わかり ませ ん 」 「 話 ／ が ／ 分かる ／ 」 と いう 係り 受け が 登録 さ れて いた ため ， 「 話 が 」 と いう 文節 が 「 いって いる の か 」 と いう 文節 で は なく ， 「 わかり ませ ん 」 と いう 文節 に 係って しまった ．下記 の サイト から 使って 見 られる ように して ある ので ， 試用 して 評価 して いただく こと を 希望 する ．urlおわり に CGM の ような 会話 体 の 文章 を 扱う ため に は ， より 一層 の 誤り を 含んだ データ に 対応 できる こと が 要求 さ れる ．辞書 は 手 作業 で 収集 した もの で ， 漏れ も 多い と 思う ．係り 受け が シソーラス に 登録 さ れて い ない ため に ， 今回 の 評価 の 対象 に なら なかった 記事 も ある と 思わ れる ．今後 大 規模 コーパス を 解析 して ， 係り 受け を 抽出 して シソーラス の 係り 受け 語 を 充実 さ せて いく 計画 である ．高速 化 に ついて も 現在 改造 中 で 近日 中 に 発表 する 予定 である ．全 世界 に 言語 の 数 は 多い ．現在 まで に 日本 語 から 外国 語 へ の 翻訳 プログラム が 未 着手 の 言語 を 対象 に ， 翻訳 プログラム が 作れ ない か と 思って いる ．今後 ， 外部 の 人 を 含めて 実用 化 を 進め たい と 思って いる ．自然 文 検索 や 翻訳 だけ で なく ， いろいろな 応用 が 考え られる ．係り 受け だけ で 「 良し あし 」 を 決定 して いる が ， 3 つ 以上 の 文節 が 組み合わさって 「 良し あし 」 が 決定 さ れる 場合 が ある ．これ から 充実 さ せて いく 必要 が ある ．']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sections'][291]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['日本 語 文章 要約 システム に ついて 報告 する ．一般に ，  質 の 良い 文章 要約 を 行う ため に は ，  ある 一 つ の 言語 現象 だけ を とらえた 談話 解析 だけ で は 不十分である ．なぜなら ，  談話 に 関わる 言語 現象 は 相互 に 関連 して いる から である ．',\n",
       " '本 研究 で は この 観点 から ，  日本 語 で の 様々な 表層 的 特徴 を できる だけ 多く 利用 して ，  日本 語 文章 の 要約 を 試みる ．',\n",
       " '本稿 で は 実際 に 計算 機上 で 試作 した 論 説文 要約 システム に 関して ，  これ で 用い られて いる 論 説文 要約 の 手法 の 紹介 と ，  これ に よって 出力 さ れた 文章 の 評価 を 行う ．']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['abs_incremental'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_join(lst):\n",
    "    result = []\n",
    "    incremental_str = ''\n",
    "    for item in lst:\n",
    "        incremental_str = ''.join([incremental_str, item]).strip()\n",
    "        result.append(incremental_str)\n",
    "    return result\n",
    "\n",
    "df['abs_incremental'] = df['abs_incremental'].apply(incremental_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['本稿 で は ， 比喩 の 理解 過程 に おける 再 解釈 の 段階 に ついて 考察 した ．',\n",
       " '本稿 で は ， 比喩 の 理解 過程 に おける 再 解釈 の 段階 に ついて 考察 した ．名詞 の 意味 を 確率 を 利用 して 表現 した ．比喩 表現 の 意味 は ， 喩詞 の 意味 に 影響 さ れた 被 喩詞 の 意味 である ．被 喩詞 の 意味 と 比喩 表現 の 意味 と の 違い を 示す 指標 と して ， 明瞭 性 と 新奇 性 と を 情報 量 を 用いて 定義 した ．明瞭 性 が 大きい 値 と なる とき は ， 比喩 表現 に おいて 属性 に 関する 不確定 さ が 減少 した とき である ．新奇 性 が 大きい 値 と なる とき は ， 比喩 表現 が 稀な 事象 を 表わす とき である ．SD 法 に より 比喩 表現 の 意味 を 測定 し ， 明瞭 性 と 新奇 性 と を 求めた ．',\n",
       " '本稿 で は ， 比喩 の 理解 過程 に おける 再 解釈 の 段階 に ついて 考察 した ．名詞 の 意味 を 確率 を 利用 して 表現 した ．比喩 表現 の 意味 は ， 喩詞 の 意味 に 影響 さ れた 被 喩詞 の 意味 である ．被 喩詞 の 意味 と 比喩 表現 の 意味 と の 違い を 示す 指標 と して ， 明瞭 性 と 新奇 性 と を 情報 量 を 用いて 定義 した ．明瞭 性 が 大きい 値 と なる とき は ， 比喩 表現 に おいて 属性 に 関する 不確定 さ が 減少 した とき である ．新奇 性 が 大きい 値 と なる とき は ， 比喩 表現 が 稀な 事象 を 表わす とき である ．SD 法 に より 比喩 表現 の 意味 を 測定 し ， 明瞭 性 と 新奇 性 と を 求めた ．明瞭 性 が 属性 の 顕著 性 の パターン に 対応 する 数値 である こと と ， 明瞭 性 と 新奇 性 と が 比喩 の 理解 容易 性 の 指標 と して 適当である こと と を 示した ．']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abs_incremental'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlist = jumanpp.analysis(jum['abs_incremental'][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [mrph.midasi.strip() for mrph in mlist.mrph_list()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "test = len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of 'apple' is 5\n",
      "Length of 'banana' is 6\n",
      "Length of 'orange' is 6\n",
      "Length of 'grape' is 5\n"
     ]
    }
   ],
   "source": [
    "# 字符串的列表\n",
    "string_list = ['apple', 'banana', 'orange', 'grape']\n",
    "\n",
    "# 使用循环计算字符串长度并打印输出\n",
    "for word in string_list:\n",
    "    length = len(word)  # 计算每个字符串的长度\n",
    "    print(f\"Length of '{word}' is {length}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = df.to_json(orient='records', force_ascii=False)\n",
    "\n",
    "\n",
    "# 将 JSON 数据保存到文件\n",
    "with open('./NLP_JP_CORPUS_INCREMENTAL_JUMAN.json', 'w', encoding='utf-8') as file:\n",
    "    file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "jum = pd.read_json('./NLP_JP_CORPUS_INCREMENTAL_JUMAN.json', orient='records', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n",
      "336\n",
      "303\n",
      "214\n",
      "148\n",
      "336\n",
      "411\n",
      "279\n",
      "421\n",
      "252\n",
      "190\n",
      "114\n",
      "228\n",
      "305\n",
      "376\n",
      "301\n",
      "207\n",
      "217\n",
      "306\n",
      "239\n",
      "214\n",
      "321\n",
      "265\n",
      "214\n",
      "145\n",
      "311\n",
      "358\n",
      "287\n",
      "246\n",
      "242\n",
      "196\n",
      "311\n",
      "320\n",
      "346\n",
      "245\n",
      "374\n",
      "329\n",
      "312\n",
      "390\n",
      "318\n",
      "247\n",
      "302\n",
      "195\n",
      "373\n",
      "293\n",
      "195\n",
      "326\n",
      "336\n",
      "257\n",
      "289\n",
      "376\n",
      "315\n",
      "398\n",
      "178\n",
      "350\n",
      "275\n",
      "265\n",
      "351\n",
      "340\n",
      "476\n",
      "228\n",
      "252\n",
      "335\n",
      "221\n",
      "358\n",
      "219\n",
      "328\n",
      "166\n",
      "269\n",
      "182\n",
      "252\n",
      "351\n",
      "305\n",
      "367\n",
      "322\n",
      "432\n",
      "244\n",
      "413\n",
      "264\n",
      "390\n",
      "191\n",
      "415\n",
      "308\n",
      "256\n",
      "77\n",
      "230\n",
      "387\n",
      "287\n",
      "241\n",
      "575\n",
      "286\n",
      "290\n",
      "214\n",
      "242\n",
      "273\n",
      "224\n",
      "195\n",
      "255\n",
      "235\n",
      "374\n",
      "323\n",
      "320\n",
      "380\n",
      "344\n",
      "165\n",
      "358\n",
      "180\n",
      "273\n",
      "313\n",
      "373\n",
      "278\n",
      "236\n",
      "278\n",
      "213\n",
      "373\n",
      "341\n",
      "195\n",
      "398\n",
      "270\n",
      "213\n",
      "269\n",
      "346\n",
      "233\n",
      "388\n",
      "413\n",
      "278\n",
      "288\n",
      "249\n",
      "577\n",
      "314\n",
      "317\n",
      "396\n",
      "280\n",
      "282\n",
      "326\n",
      "287\n",
      "315\n",
      "439\n",
      "330\n",
      "337\n",
      "284\n",
      "292\n",
      "312\n",
      "414\n",
      "271\n",
      "303\n",
      "312\n",
      "237\n",
      "213\n",
      "528\n",
      "324\n",
      "359\n",
      "99\n",
      "337\n",
      "168\n",
      "238\n",
      "245\n",
      "277\n",
      "148\n",
      "193\n",
      "450\n",
      "277\n",
      "312\n",
      "307\n",
      "454\n",
      "400\n",
      "286\n",
      "423\n",
      "324\n",
      "322\n",
      "385\n",
      "315\n",
      "306\n",
      "276\n",
      "268\n",
      "151\n",
      "179\n",
      "281\n",
      "233\n",
      "221\n",
      "267\n",
      "211\n",
      "254\n",
      "137\n",
      "174\n",
      "267\n",
      "239\n",
      "136\n",
      "192\n",
      "270\n",
      "240\n",
      "211\n",
      "240\n",
      "197\n",
      "223\n",
      "245\n",
      "266\n",
      "291\n",
      "393\n",
      "275\n",
      "361\n",
      "214\n",
      "308\n",
      "135\n",
      "389\n",
      "284\n",
      "305\n",
      "213\n",
      "265\n",
      "217\n",
      "286\n",
      "220\n",
      "184\n",
      "154\n",
      "179\n",
      "228\n",
      "264\n",
      "243\n",
      "394\n",
      "339\n",
      "349\n",
      "172\n",
      "122\n",
      "320\n",
      "205\n",
      "219\n",
      "216\n",
      "215\n",
      "136\n",
      "362\n",
      "219\n",
      "369\n",
      "245\n",
      "366\n",
      "468\n",
      "532\n",
      "282\n",
      "218\n",
      "310\n",
      "200\n",
      "326\n",
      "465\n",
      "168\n",
      "468\n",
      "250\n",
      "343\n",
      "252\n",
      "365\n",
      "336\n",
      "339\n",
      "206\n",
      "388\n",
      "163\n",
      "267\n",
      "374\n",
      "261\n",
      "281\n",
      "244\n",
      "151\n",
      "228\n",
      "407\n",
      "271\n",
      "155\n",
      "272\n",
      "218\n",
      "257\n",
      "205\n",
      "305\n",
      "193\n",
      "219\n",
      "222\n",
      "272\n",
      "254\n",
      "767\n",
      "202\n",
      "261\n",
      "398\n",
      "309\n",
      "316\n",
      "280\n",
      "165\n",
      "165\n",
      "179\n",
      "302\n",
      "197\n",
      "802\n",
      "224\n",
      "140\n",
      "248\n",
      "182\n",
      "223\n",
      "208\n",
      "258\n",
      "206\n",
      "283\n",
      "379\n",
      "440\n",
      "249\n",
      "402\n",
      "359\n",
      "352\n",
      "255\n",
      "268\n",
      "296\n",
      "190\n",
      "199\n",
      "423\n",
      "168\n",
      "181\n",
      "318\n",
      "168\n",
      "776\n",
      "496\n",
      "234\n",
      "296\n",
      "327\n",
      "234\n",
      "250\n",
      "282\n",
      "163\n",
      "296\n",
      "358\n",
      "267\n",
      "590\n",
      "239\n",
      "153\n",
      "237\n",
      "347\n",
      "242\n",
      "224\n",
      "204\n",
      "201\n",
      "261\n",
      "327\n",
      "231\n",
      "185\n",
      "158\n",
      "361\n",
      "278\n",
      "400\n",
      "336\n",
      "98\n",
      "300\n",
      "345\n",
      "360\n",
      "142\n",
      "161\n",
      "161\n",
      "191\n",
      "373\n",
      "351\n",
      "293\n",
      "256\n",
      "275\n",
      "321\n",
      "193\n",
      "302\n",
      "439\n",
      "354\n",
      "316\n",
      "325\n",
      "363\n",
      "216\n",
      "251\n",
      "283\n",
      "223\n",
      "283\n",
      "265\n",
      "360\n",
      "192\n",
      "298\n",
      "251\n",
      "195\n",
      "388\n",
      "268\n",
      "230\n",
      "272\n",
      "224\n",
      "228\n",
      "242\n",
      "552\n",
      "254\n",
      "375\n",
      "340\n",
      "266\n",
      "228\n",
      "297\n",
      "399\n",
      "242\n",
      "261\n",
      "219\n",
      "229\n",
      "209\n",
      "228\n",
      "202\n",
      "372\n",
      "214\n",
      "223\n",
      "244\n",
      "354\n",
      "275\n",
      "166\n",
      "312\n",
      "264\n",
      "272\n",
      "243\n",
      "263\n",
      "193\n",
      "194\n",
      "252\n",
      "280\n",
      "403\n",
      "263\n",
      "249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "117366"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "\n",
    "\n",
    "# 统计平均长度\n",
    "length = 0\n",
    "def get_avg_len(text):\n",
    "    global length\n",
    "    text = text[-1]\n",
    "    encode = tokenizer.encode(text, add_special_tokens=False)\n",
    "    print(len(encode))\n",
    "    length += len(encode)\n",
    "\n",
    "jum['abs_incremental'].apply(get_avg_len)\n",
    "\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283.4927536231884"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "117366 / 414"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqence = \"\"\"これ を 解決 する ため には 文内 の できる だけ 広い 範囲 を 同時 的 に 調べる こと が 必要 で ある ．\n",
    "\"\"\"\n",
    "en = tokenizer.encode(seqence, add_special_tokens=False)\n",
    "len(en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
